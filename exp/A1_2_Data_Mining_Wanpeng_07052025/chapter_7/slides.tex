\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Overview}
    \begin{block}{What are Neural Networks?}
        Neural Networks (NNs) are computational models inspired by the human brain, designed to recognize patterns and solve complex problems. They consist of interconnected processing nodes (neurons) organized in layers that transform input data into output predictions.
    \end{block}
    
    \begin{block}{Key Objectives of Neural Networks}
        \begin{itemize}
            \item Feature Learning: Automatically discover patterns and features from raw data.
            \item Scalability: Handle vast amounts of input data efficiently.
            \item Generalization: Ability to make accurate predictions on unseen data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Structure}
    \begin{block}{Structure of a Neural Network}
        \begin{enumerate}
            \item \textbf{Input Layer:} Receives input data; each neuron corresponds to one feature.
            \item \textbf{Hidden Layers:} Intermediate layers for transformations; complexity increases with more layers.
            \item \textbf{Output Layer:} Produces the final outputs representing categories or continuous values.
        \end{enumerate}
    \end{block}

    \begin{block}{How Neural Networks Work}
        Neural networks learn from data through a process called \textbf{training}. 
        \begin{itemize}
            \item Feedforward Propagation: Input is passed through, activating neurons.
            \item Loss Calculation: Difference between predicted and actual output is quantified using a loss function.
            
            \begin{equation}
            \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
            
            \item Backpropagation: Model adjusts weights based on error using gradient descent.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Practical Example}
    \begin{block}{Practical Example}
        Consider building a neural network to classify images of cats and dogs:
        \begin{itemize}
            \item \textbf{Input Layer:} Each pixel represents an input feature.
            \item \textbf{Hidden Layers:} Extract features like edges, textures, and shapes.
            \item \textbf{Output Layer:} Two neurons—one for "cat" and one for "dog."
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Neural networks are at the forefront of modern data mining and machine learning techniques, offering extraordinary capabilities in tasks ranging from image classification to natural language processing. Understanding their framework and functioning is crucial for leveraging their full potential in supervised learning applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Presentation Overview}
  \tableofcontents[hideallsubsections]
\end{frame}

\begin{frame}[fragile]{History of Neural Networks - Introduction}
    \begin{block}{Introduction}
        Neural networks have evolved significantly since their inception, transforming the landscape of machine learning. Understanding this history is crucial to grasp how these models operate today in various applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]{History of Neural Networks - Early Foundations}
    \begin{enumerate}
        \item \textbf{Early Foundations (1940s-1960s)}
            \begin{itemize}
                \item \textbf{1943}: Warren McCulloch and Walter Pitts proposed the first model of a neuron, introducing the concept of networks of simple processing units (neurons) that could perform logic functions.
                \item \textbf{1958}: Frank Rosenblatt developed the Perceptron, an algorithm for supervised learning of binary classifiers. The Perceptron initially sparked interest in neural networks as it could learn weights for inputs to produce binary output.
            \end{itemize}
        \item \textbf{Key Point}: The early models were simple and did not account for complexities found in real-world problems.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{History of Neural Networks - The Dark Age}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{The Dark Age (1970s-1980s)}
            \begin{itemize}
                \item Interest in neural networks waned following the publication of "Perceptrons" by Marvin Minsky and Seymour Papert in 1969, which highlighted limitations of single-layer networks.
                \item This resulted in a decline in funding for research, leading to the "AI winter."
            \end{itemize}
        \item \textbf{Key Point}: The limitations of early networks stifled development, delaying advancements in the field.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{History of Neural Networks - Resurgence}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Resurgence (1980s-1990s)}
            \begin{itemize}
                \item \textbf{1986}: The introduction of backpropagation by Geoffrey Hinton, David Rumelhart, and Ronald Williams revived interest in neural networks, allowing for efficient training of multi-layer networks.
                \item The 1990s saw the development of various architectures (e.g., Convolutional Neural Networks) and techniques to combat overfitting, like regularization and dropout.
            \end{itemize}
        \item \textbf{Key Point}: Backpropagation was a critical development enabling deeper networks to learn effectively, igniting a new wave of research.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{History of Neural Networks - Deep Learning Revolution}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{The Deep Learning Revolution (2000s-Present)}
            \begin{itemize}
                \item The era of “deep learning” began with the increase in data availability and computational power, especially through GPUs.
                \item \textbf{2012}: AlexNet won the ImageNet competition, showcasing deep convolutional networks' capabilities, leading to widespread adoption.
                \item As of the 2020s, neural networks are widely used in various industries, including healthcare, finance, and autonomous vehicles.
            \end{itemize}
        \item \textbf{Key Point}: Advances in technology and data availability have led neural networks to outperform traditional algorithms, becoming state-of-the-art in numerous applications.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{History of Neural Networks - Conclusion}
    \begin{block}{Conclusion}
        The trajectory of neural networking shows cycles influenced by theoretical advancements, funding availability, and technological improvements. Understanding this evolution allows us to appreciate contemporary neural networks and anticipate future developments.
    \end{block}
\end{frame}

\begin{frame}[fragile]{History of Neural Networks - Illustration Idea}
    \begin{block}{Illustration Idea}
        Consider a timeline chart spanning from the 1940s to the present, marking key milestones like the Perceptron, the introduction of backpropagation, and significant milestones in deep learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Callout Formula for Neurons}
    The output of a neuron can be computed as:  
    \begin{equation}
        y = f\left(\sum_{i=1}^{n} w_i \cdot x_i + b\right)
    \end{equation}
    Where:
    \begin{itemize}
        \item \(y\): Output of the neuron
        \item \(f\): Activation function (e.g., sigmoid, ReLU)
        \item \(w_i\): Weights for each input
        \item \(x_i\): Inputs
        \item \(b\): Bias term
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Neural Networks - Overview}
    \begin{block}{Fundamental Architecture Components}
        Neural networks consist of layers inspired by the human brain's structure, critical for processing data:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Neural Networks - Layers}
    \begin{enumerate}
        \item \textbf{Input Layer}
        \begin{itemize}
            \item \textbf{Definition}: The first layer receiving input data.
            \item \textbf{Function}: Each neuron corresponds to a feature of the input data.
            \item \textbf{Example}: For a 28x28 pixel image, there are 784 neurons.
        \end{itemize}

        \item \textbf{Hidden Layers}
        \begin{itemize}
            \item \textbf{Definition}: Intermediate layers where data transformations occur.
            \item \textbf{Function}: Neurons extract features and patterns.
            \item \textbf{Example}: 2 hidden layers with 128 and 64 neurons can learn complex representations.
        \end{itemize}

        \item \textbf{Output Layer}
        \begin{itemize}
            \item \textbf{Definition}: The final layer providing results.
            \item \textbf{Function}: Its structure depends on the task.
            \item \textbf{Example}: 1 neuron for binary classification with a sigmoid function.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Neural Networks - Key Points}
    \begin{itemize}
        \item The number of layers and neurons affects the ability to capture patterns.
        \item \textbf{Weights and Biases}: Adjusted during training; critical for learning.
        \item \textbf{Forward Propagation}: Data flows through layers to produce predictions, passing through activation functions.
    \end{itemize}
    
    \begin{block}{Example Architecture Illustration}
        \texttt{Input Layer (784 neurons)} \\
        \texttt{ ↓ } \\
        \texttt{Hidden Layer 1 (128 neurons)} \\
        \texttt{ ↓ } \\
        \texttt{Hidden Layer 2 (64 neurons)} \\
        \texttt{ ↓ } \\
        \texttt{Output Layer (1 neuron for binary classification)}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Neural Networks - Conclusion}
    Understanding the architecture is fundamental for building effective models. 
    It forms the basis for how systems learn from data and make predictions. 
    Next, we will explore various types of neural networks and their applications in machine learning.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Types of Neural Networks}
  \begin{block}{Overview}
    Neural networks come in various architectures, each tailored to specific types of data and tasks. 
    Understanding the main types—Feedforward Neural Networks, Convolutional Neural Networks, and Recurrent Neural Networks—equips you with the knowledge to choose the right model for your application.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feedforward Neural Networks (FNN)}
  \begin{itemize}
    \item \textbf{Concept:}
    \begin{itemize}
      \item Simplest form where information moves in one direction—from input nodes through hidden layers to output nodes.
      \item No cycles or loops exist in the network.
    \end{itemize}

    \item \textbf{Key Characteristics:}
    \begin{itemize}
      \item Composed of an input layer, one or more hidden layers, and an output layer.
      \item Mainly used for classification and regression tasks.
    \end{itemize}

    \item \textbf{Example:}
    In a FNN for image classification, the input layer receives pixel values, the hidden layers process this information, and the output layer predicts the class label.
  \end{itemize}

  \vspace{0.5cm}
  \textbf{Diagram:}
  \begin{center}
    Input Layer $\rightarrow$ Hidden Layer(s) $\rightarrow$ Output Layer
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Convolutional Neural Networks (CNN)}
  \begin{itemize}
    \item \textbf{Concept:}
    \begin{itemize}
      \item Specialized for processing structured grid data like images.
      \item Utilizes convolutional layers to apply filters and extract features.
    \end{itemize}

    \item \textbf{Key Characteristics:}
    \begin{itemize}
      \item Consists of convolutional layers, pooling layers, and fully connected layers.
      \item Great at capturing spatial hierarchies in images.
    \end{itemize}

    \item \textbf{Example:}
    In image recognition, convolutional layers detect edges, shapes, and patterns; pooling layers reduce dimensionality.
  \end{itemize}

  \vspace{0.5cm}
  \textbf{Diagram:}
  \begin{center}
    Input Image $\rightarrow$ [Conv Layer] $\rightarrow$ [Pool Layer] $\rightarrow$ [Fully Connected Layer] $\rightarrow$ Output
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Recurrent Neural Networks (RNN)}
  \begin{itemize}
    \item \textbf{Concept:}
    \begin{itemize}
      \item Designed for sequential data, where relationships between data points are crucial (e.g., time series, text).
    \end{itemize}

    \item \textbf{Key Characteristics:}
    \begin{itemize}
      \item Contains loops allowing information to flow back into earlier steps.
      \item Capable of maintaining a 'memory' of previous inputs, beneficial for tasks like language modeling.
    \end{itemize}

    \item \textbf{Example:}
    In language processing, an RNN can predict the next word by considering the context provided by previous words.
  \end{itemize}

  \vspace{0.5cm}
  \textbf{Diagram:}
  \begin{center}
    Input Sequence $\rightarrow$ [RNN Cell] $\rightarrow$ [RNN Cell] $\rightarrow$ Output Sequence
    \\[0.2cm]
    \quad $\downarrow$ \quad \text{Feedback Loop} \quad $\uparrow$
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Feedforward Neural Networks} are foundational but limited to static data.
    \item \textbf{Convolutional Neural Networks} excel in tasks involving images due to their ability to recognize spatial hierarchies.
    \item \textbf{Recurrent Neural Networks} are essential for tasks that involve sequences, utilizing their memory capability to consider context effectively.
  \end{itemize}

  By understanding these types of neural networks, you can make informed choices for your projects and harness the strengths of each architecture for optimal results in supervised learning tasks.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Activation Functions - Overview}
  \begin{block}{Explanation}
    Activation functions are essential components of neural networks, allowing the network to introduce non-linearity into its computations. Non-linearity enables learning complex patterns rather than just linear combinations, improving model fitting and generalization to new data.
  \end{block}

  \begin{block}{Role in Neural Networks}
    \begin{itemize}
      \item \textbf{Decision Boundaries:} Determine output based on inputs, creating decision boundaries for classification.
      \item \textbf{Transformation of Inputs:} Convert the weighted sum of inputs into final outputs, affecting signal propagation.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Activation Functions - Common Types}
  \begin{enumerate}
    \item \textbf{ReLU (Rectified Linear Unit)}
      \begin{itemize}
        \item \textbf{Formula:} 
        \[
        f(x) = \max(0, x)
        \]
        \item \textbf{Characteristics:}
        \begin{itemize}
          \item Introduces non-linearity simply and efficiently.
          \item Mitigates the vanishing gradient problem.
        \end{itemize}
        \item \textbf{Example:}
        \begin{itemize}
          \item If \( x = -2 \), \( f(-2) = 0 \)
          \item If \( x = 3 \), \( f(3) = 3 \)
        \end{itemize}
      \end{itemize}
      
    \item \textbf{Sigmoid}
      \begin{itemize}
        \item \textbf{Formula:}
        \[
        f(x) = \frac{1}{1 + e^{-x}}
        \]
        \item \textbf{Characteristics:}
        \begin{itemize}
          \item Outputs in range (0, 1), suitable for binary classification.
          \item Can suffer from vanishing gradients.
        \end{itemize}
        \item \textbf{Example:}
        \begin{itemize}
          \item If \( x = 0 \), \( f(0) = 0.5 \)
          \item If \( x = 10 \), \( f(10) \approx 0.99995 \)
        \end{itemize}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Activation Functions - Additional Types}
  \begin{enumerate}[resume]
    \item \textbf{Tanh (Hyperbolic Tangent)}
      \begin{itemize}
        \item \textbf{Formula:}
        \[
        f(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
        \]
        \item \textbf{Characteristics:}
        \begin{itemize}
          \item Outputs in range (-1, 1), centering around zero.
          \item Like sigmoid, suffers from vanishing gradients but often performs better.
        \end{itemize}
        \item \textbf{Example:}
        \begin{itemize}
          \item If \( x = 0 \), \( f(0) = 0 \)
          \item If \( x = 2 \), \( f(2) \approx 0.9640 \)
        \end{itemize}
      \end{itemize}
  \end{enumerate}
  
  \begin{block}{Key Points}
    \begin{itemize}
      \item Activation functions allow non-linear transformations vital for complex data relationships.
      \item Choice impacts neural network performance; select based on context.
      \item Be aware of issues like vanishing/exploding gradients during training.
    \end{itemize}
  \end{block}
  
  \begin{block}{Conclusion}
    Understanding activation functions is critical for optimizing neural networks. The right choice enhances learning, speeds convergence, and improves overall performance.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - What is Forward Propagation?}
    \begin{itemize}
        \item Forward propagation is the process of passing input data through a neural network to obtain an output.
        \item It is the initial step in training a neural network.
        \item Enables the network to make predictions based on learned patterns from training data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - How it Works}
    \begin{enumerate}
        \item \textbf{Input Layer}:
        \begin{itemize}
            \item Data is fed into the network.
            \item Each neuron represents a feature of the input data.
        \end{itemize}
        
        \item \textbf{Weights and Biases}:
        \begin{itemize}
            \item Each connection has a weight (w) and each neuron has a bias (b).
            \item These parameters determine the influence of input data on the output.
        \end{itemize}

        \item \textbf{Calculating Weighted Sum}:
        \begin{equation}
            z = \sum (w_i * x_i) + b
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Activation Function and Example}
    \begin{itemize}
        \item \textbf{Activation Function}:
        \begin{equation}
            a = \text{ActivationFunction}(z)
        \end{equation}
        Common activation functions include:
        \begin{itemize}
            \item ReLU: \( a = \max(0, z) \)
            \item Sigmoid: \( a = \frac{1}{1 + e^{-z}} \)
            \item Tanh: \( a = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \)
        \end{itemize}

        \item \textbf{Example of Forward Propagation Steps}:
        \begin{enumerate}
            \item For each hidden layer neuron:
            \begin{equation}
                z_1 = w_{11} \cdot x_1 + w_{12} \cdot x_2 + b_1
            \end{equation}
            \begin{equation}
                z_2 = w_{21} \cdot x_1 + w_{22} \cdot x_2 + b_2
            \end{equation}
            \item Apply the activation function:
            \begin{equation}
                a_1 = \text{ReLU}(z_1), \quad a_2 = \text{ReLU}(z_2)
            \end{equation}
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Final Steps and Key Points}
    \begin{itemize}
        \item \textbf{Final Output Calculation}:
        \begin{equation}
            z_{output} = w_{out1} \cdot a_1 + w_{out2} \cdot a_2 + b_{out}
        \end{equation}
        \begin{equation}
            output = \sigma(z_{output})
        \end{equation}

        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item Determines how inputs influence outputs.
            \item Each output depends on weights, biases, and activation functions.
            \item Necessary for making predictions in neural networks.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Function - Overview}
    \begin{block}{Understanding the Loss Function}
        The loss function, also known as the cost function or objective function, quantifies how well a neural network's predictions match the actual outcomes.
        \begin{itemize}
            \item Measures the difference between predicted values and true values.
            \item The goal of training is to minimize this loss.
        \end{itemize}
    \end{block}
    \begin{block}{Importance in Neural Network Training}
        The loss function guides the learning process during training, directly influencing weight adjustments and impacting model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Function - Types and Functions}
    \begin{block}{Types of Loss Functions}
        \begin{itemize}
            \item \textbf{Regression Loss Functions}
                \begin{itemize}
                    \item Mean Squared Error (MSE):
                    \begin{equation}
                         \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
                    \end{equation}
                \end{itemize}
            \item \textbf{Classification Loss Functions}
                \begin{itemize}
                    \item Binary Cross-Entropy:
                    \begin{equation}
                        L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
                    \end{equation}
                    \item Categorical Cross-Entropy:
                    \begin{equation}
                        L(y, \hat{y}) = -\sum_{i} y_i \log(\hat{y}_i)
                    \end{equation}
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Function - Example and Summary}
    \begin{block}{Illustrative Example}
        Consider a model predicting house prices:
        \begin{itemize}
            \item True Prices: [250k, 300k, 200k]
            \item Predicted Prices: [240k, 310k, 215k]
            \item Using MSE:
            \begin{equation}
                \text{MSE} = \frac{1}{3}[(250 - 240)^2 + (300 - 310)^2 + (200 - 215)^2] = \frac{425}{3} \approx 141.67
            \end{equation}
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Choice of loss function depends on the task (regression vs. classification).
            \item Minimizing the loss function improves model accuracy.
            \item Loss values provide critical feedback for optimization algorithms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Backpropagation}
  \begin{block}{Understanding Backpropagation in Neural Networks}
    Backpropagation is a crucial algorithm used for training neural networks by minimizing their error. It calculates the gradient of the loss function with respect to the weights of the network and updates those weights using gradient descent.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{The Process of Backpropagation}
  \begin{enumerate}
    \item \textbf{Forward Pass}:
    \begin{itemize}
      \item Input data is fed through the network layer by layer to produce an output.
      \item The loss function evaluates the difference between the predicted output and the actual target.
    \end{itemize}
    
    \item \textbf{Backward Pass}:
    \begin{itemize}
      \item The error from the output layer is propagated backward through the network.
      \item Gradients of the loss function with respect to each weight are calculated using the chain rule.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Gradient Descent Optimization}
  \begin{block}{Gradient Descent}
    A first-order optimization algorithm to minimize a function.
  \end{block}

  \begin{equation}
    w_{new} = w_{old} - \eta \cdot \frac{\partial L}{\partial w}
  \end{equation}

  Where:
  \begin{itemize}
    \item \( w_{new} \) = Updated weight
    \item \( w_{old} \) = Current weight
    \item \( \eta \) = Learning rate (controls step size)
    \item \( \frac{\partial L}{\partial w} \) = Gradient of the loss function
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Conclusion}
  \begin{enumerate}
    \item \textbf{Chain Rule in Action}: Backpropagation relies heavily on the chain rule to compute gradients layer-wise.
    
    \item \textbf{Learning Rate (\( \eta \))}: A critical hyperparameter. High values can cause divergence; low values can lead to slow convergence.
    
    \item \textbf{Iterative Process}: Backpropagation is performed across many iterations (epochs), where the network continually adjusts weights.
  \end{enumerate}

  \begin{block}{Conclusion}
    Backpropagation is foundational to efficiently training neural networks. By understanding the relationship between weights, outputs, and errors, we can significantly enhance model performance.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Process - Overview}
    \begin{block}{Overview of the Neural Network Training Process}
        Neural networks learn from data through a systematic training process. Key components include:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Process - Key Components}
    \begin{enumerate}
        \item \textbf{Epochs:}
            \begin{itemize}
                \item An epoch is a complete pass through the training dataset.
                \item Example: 10 epochs means the network uses the dataset 10 times.
            \end{itemize}
        
        \item \textbf{Batch Size:}
            \begin{itemize}
                \item Refers to the number of samples used in one iteration.
                \item Example: 1,000 samples with a batch size of 100 takes 10 iterations to complete one epoch.
            \end{itemize}
        
        \item \textbf{Overfitting:}
            \begin{itemize}
                \item Occurs when the model learns training data too well, leading to poor performance on unseen data.
                \item Signs include a large difference in training and validation accuracy.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Process - Cycle and Diagram}
    \begin{block}{Training Cycle Steps}
        \begin{enumerate}
            \item Shuffle the Training Data
            \item Divide into Batches
            \item Forward Propagation
            \item Calculate Loss
            \item Backpropagation
            \item Repeat for each batch until epochs are completed
        \end{enumerate}
    \end{block}
    
    \begin{block}{Visualization}
        \begin{center}
            \includegraphics[width=\linewidth]{diagram.png} % Placeholder for the diagram
        \end{center}
        % Note: A diagram illustrating the training process can be added here.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Process - Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Balance Training:} Choose appropriate epochs and batch sizes to avoid overfitting and underfitting.
        \item \textbf{Evaluation:} Regularly use validation data to monitor performance.
        \item \textbf{Regularization Techniques:} Consider dropout or weight decay to combat overfitting.
    \end{itemize}

    \begin{block}{Conclusion}
        The training process of neural networks is iterative and involves critical decisions regarding epochs, batch sizes, and strategies to prevent overfitting.
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Hyperparameter Tuning - Overview}
  \begin{block}{Introduction to Hyperparameters}
    Hyperparameters are parameters set before the learning process begins, influencing model performance and efficiency.
  \end{block}
  
  \begin{block}{Key Hyperparameters}
    \begin{itemize}
      \item Learning Rate ($\eta$)
      \item Number of Layers and Neurons
      \item Batch Size
      \item Activation Functions
      \item Regularization Techniques
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Hyperparameter Tuning - Significance}
  \begin{block}{Key Hyperparameters to Consider}
    \begin{enumerate}
      \item \textbf{Learning Rate ($\eta$)}:
        \begin{itemize}
          \item Too high: Quick convergence to suboptimal solutions
          \item Too low: Long training time and potential stagnation
        \end{itemize}
      \item \textbf{Number of Layers and Neurons}:
        \begin{itemize}
          \item More depth captures complex patterns, but risks overfitting.
        \end{itemize}
      \item \textbf{Batch Size}:
        \begin{itemize}
          \item Affects speed of training and convergence (typical sizes: 16-256).
        \end{itemize}
      \item \textbf{Activation Functions}:
        \begin{itemize}
          \item Affect learning dynamics (e.g., ReLU mitigates vanishing gradient).
        \end{itemize}
      \item \textbf{Regularization Techniques}:
        \begin{itemize}
          \item E.g., L1, L2, dropout to reduce overfitting.
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Hyperparameter Tuning - Techniques}
  \begin{block}{Techniques for Hyperparameter Tuning}
    \begin{itemize}
      \item \textbf{Grid Search}:
        \begin{itemize}
          \item Exhaustive search through hyperparameter space; computationally expensive.
        \end{itemize}
      \item \textbf{Random Search}:
        \begin{itemize}
          \item Efficiently samples hyperparameter space, reduced computational time.
        \end{itemize}
      \item \textbf{Bayesian Optimization}:
        \begin{itemize}
          \item Proactively explores hyperparameter space for efficiency.
        \end{itemize}
      \item \textbf{Automated Tuning}:
        \begin{itemize}
          \item Algorithms like Hyperband, Optuna for automated optimization.
        \end{itemize}
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    Tuning hyperparameters is essential to harness the potential of neural networks for improved performance.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Hyperparameter Tuning - Code Example}
  \begin{lstlisting}[language=Python]
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

def create_model(learn_rate=0.01):
    model = Sequential()
    model.add(Dense(12, input_dim=8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    optimizer = Adam(lr=learn_rate)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, verbose=0)
param_grid = {'learn_rate': [0.001, 0.01, 0.1], 'batch_size': [10, 20, 30]}
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X_train, Y_train)
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - Introduction}
  Neural networks have transformed numerous industries by leveraging their ability to learn complex patterns from data. 
  Applications span across:
  \begin{itemize}
      \item Image Recognition
      \item Natural Language Processing (NLP)
      \item Healthcare
  \end{itemize}
  They are at the forefront of artificial intelligence innovations today.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - Image Recognition}
  \textbf{Concept:} Neural networks, particularly Convolutional Neural Networks (CNNs), excel at processing and understanding visual data.

  \textbf{Example:} 
  \begin{itemize}
      \item \textbf{Facial Recognition:} Facebook uses neural networks to tag friends in photos by learning from vast datasets of labeled images.
  \end{itemize}

  \textbf{Key Points to Emphasize:}
  \begin{itemize}
      \item CNNs efficiently detect patterns in pixel data.
      \item Applications include security systems, autonomous vehicles, and medical imaging.
  \end{itemize}
  
  \textbf{Illustration:} 
  Imagine a neural network as a complex filter that progressively extracts features from images, from simple edges to complex objects.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - NLP and Healthcare}
  \textbf{Natural Language Processing (NLP)}
  \begin{itemize}
      \item \textbf{Concept:} Neural networks model human language by processing sequences of words, often utilizing Recurrent Neural Networks (RNNs) and Transformers.
      
      \item \textbf{Example:} Customer service bots using neural networks, such as GPT (Generative Pre-trained Transformer), to understand user queries and generate responses.
      
      \item \textbf{Key Points to Emphasize:}
      \begin{itemize}
          \item Applications include sentiment analysis, translation, and text generation.
          \item Neural networks understand context and subtleties in language.
      \end{itemize}
  \end{itemize}
  
  \textbf{Healthcare}
  \begin{itemize}
      \item \textbf{Concept:} Neural networks analyze complex medical data for diagnostics and treatment recommendations.
      
      \item \textbf{Example:} Google’s DeepMind detects eye diseases by analyzing retinal scans accurately, often outperforming human specialists.
      
      \item \textbf{Key Points to Emphasize:}
      \begin{itemize}
          \item Enhance predictive modeling leading to personalized medicine.
          \item Applications in radiology, genomics, and patient monitoring.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - Conclusion}
  The applications of neural networks are vast and impactful. Their ability to learn and generalize from large datasets allows for innovative solutions across multiple fields, continuously reshaping how we interact with technology.

  \textbf{Code Snippet:}
  \begin{lstlisting}[language=Python]
# Example of using a neural network for a classification task
from keras.models import Sequential
from keras.layers import Dense

# Define model
model = Sequential()
model.add(Dense(units=64, activation='relu', input_dim=8)) # First layer
model.add(Dense(units=1, activation='sigmoid')) # Output layer

# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other Supervised Learning Techniques}
    \begin{block}{Understanding Neural Networks in Context}
        Neural networks are a robust class of models used in supervised learning, especially for complex tasks such as image and speech recognition. Comparing their attributes with other methods like Decision Trees and Support Vector Machines is essential.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Comparison Criteria}
    \begin{itemize}
        \item \textbf{Complexity:}
            \begin{itemize}
                \item \textbf{Neural Networks:} Highly flexible, efficiently models non-linear relationships.
                \item \textbf{Decision Trees:} Simple, interpretable; splits data into branches based on decisions.
                \item \textbf{Support Vector Machines:} Effective in high-dimensional spaces; maximizes the margin between classes.
            \end{itemize}
        
        \item \textbf{Interpretability:}
            \begin{itemize}
                \item \textbf{Neural Networks:} Often seen as "black boxes", difficult to interpret.
                \item \textbf{Decision Trees:} Highly interpretable; decisions easily traced visually.
                \item \textbf{Support Vector Machines:} Less interpretable than Decision Trees, but insights can be drawn from decision boundaries.
            \end{itemize}
        
        \item \textbf{Training Time:}
            \begin{itemize}
                \item \textbf{Neural Networks:} Requires more data and resources with longer training times.
                \item \textbf{Decision Trees:} Fast training with lower costs; grows quickly.
                \item \textbf{Support Vector Machines:} Training varies; efficient on small datasets but struggles on large ones.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Summary}
    \begin{enumerate}
        \item \textbf{Image Classification:}
            \begin{itemize}
                \item \textbf{Neural Networks:} CNNs excel in learning spatial hierarchies.
                \item \textbf{Decision Trees:} Struggle with complexity but can handle simpler datasets.
                \item \textbf{Support Vector Machines:} Effective if data is linearly separable; kernel tricks help but complexity increases.
            \end{itemize}
        
        \item \textbf{Spam Detection:}
            \begin{itemize}
                \item \textbf{Neural Networks:} Captures text nuances but requires substantial data.
                \item \textbf{Decision Trees:} Can effectively classify based on clear rules (keywords).
                \item \textbf{Support Vector Machines:} Efficient with clear margins for text classification.
            \end{itemize}
        
        \item \textbf{Summary of Key Points:}
            \begin{itemize}
                \item Neural networks are powerful yet less interpretable and data-intensive.
                \item Decision trees are easy to interpret but may struggle with complex data.
                \item SVMs are versatile but need tuning based on data characteristics.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations of Neural Networks - Introduction}
    Neural networks are powerful tools for supervised learning, excelling in tasks such as:
    \begin{itemize}
        \item Image recognition
        \item Natural language processing
        \item Complex pattern detection
    \end{itemize}
    However, they also face significant challenges and limitations in data mining that must be considered.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations of Neural Networks - Data Requirements}
    \begin{block}{1. Data Requirements}
        \begin{itemize}
            \item \textbf{Data Volume}: Requires large amounts of data to train effectively. Small datasets may lead to overfitting.
                \begin{itemize}
                    \item Example: A model trained on only a few hundred images may struggle with unseen data.
                \end{itemize}
            \item \textbf{Data Quality}: Poor quality data can drastically affect performance.
                \begin{itemize}
                    \item Illustration: A medical diagnosis model with many healthy records might fail to identify patients with diseases.
                \end{itemize}
            \item \textbf{Preprocessing Needs}: Data must often be preprocessed (normalized, standardized) for better training outcomes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations of Neural Networks - Interpretability and Others}
    \begin{block}{2. Interpretability}
        \begin{itemize}
            \item \textbf{Black Box Nature}: Neural networks operate as 'black boxes', making decision processes opaque.
                \begin{itemize}
                    \item Key Point: Lack of transparency can hinder trust in sensitive domains such as healthcare and finance.
                \end{itemize}
            \item Example: In loan approvals, banks may struggle to explain why applications were denied despite accurate predictions.
        \end{itemize}
    \end{block}

    \begin{block}{3. Computational Requirements}
        \begin{itemize}
            \item \textbf{High Computational Power}: Training deep neural networks requires significant computational resources (GPU, TPU).
            \item \textbf{Energy Consumption}: Training can be energy-intensive, raising sustainability concerns.
        \end{itemize}
    \end{block}

    \begin{block}{4. Hyperparameter Tuning}
        \begin{itemize}
            \item \textbf{Complexity}: Many hyperparameters must be tuned, requiring extensive experimentation.
            \item Example: A small change in the learning rate can result in either convergence failure or divergence.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations - Introduction}
  Neural networks have revolutionized data mining and machine learning by enabling the analysis of vast and complex datasets. However, these advancements come with significant ethical considerations that need to be addressed to ensure fair and responsible use.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Ethical Considerations - Bias in Data}
  \begin{enumerate}
      \item \textbf{Bias in Data}
      \begin{itemize}
          \item \textbf{Definition:} Systematic errors that can lead to unfair treatment based on attributes such as race, gender, or socioeconomic status.
          \item \textbf{Sources of Bias:}
          \begin{itemize}
              \item Historical data reflecting societal prejudices (e.g., hiring data favoring certain demographics).
              \item Sampling errors where certain groups are underrepresented.
          \end{itemize}
          \item \textbf{Example:} A facial recognition system trained on predominantly white faces may misidentify individuals from other racial backgrounds, leading to misjudgments in security settings.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Ethical Considerations - Model Transparency}
  \begin{enumerate}
      \setcounter{enumi}{1} % Start enumeration from 2
      \item \textbf{Model Transparency}
      \begin{itemize}
          \item \textbf{Definition:} Clarity and understandability of how a model makes decisions; often viewed as “black boxes.”
          \item \textbf{Importance:}
          \begin{itemize}
              \item Enhances trust and accountability among users and stakeholders.
              \item Enables organizations to justify decisions made by models (e.g., loan approvals).
          \end{itemize}
          \item \textbf{Methods to Improve Transparency:}
          \begin{itemize} 
              \item Feature Importance Analysis: Identifying impactful inputs on predictions.
              \item LIME (Local Interpretable Model-agnostic Explanations): Explaining predictions in an understandable way.
          \end{itemize}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Ethical Considerations - Informed Consent and Privacy}
  \begin{enumerate}
      \setcounter{enumi}{2} % Continue numbering
      \item \textbf{Informed Consent and Privacy}
      \begin{itemize}
          \item \textbf{Informed Consent:} Users should be aware of data collection and its usage, especially in sensitive fields like healthcare.
          \item \textbf{Privacy Concerns:} Improper handling of personal data can lead to breaches and misuse.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Emphasizing Responsibility and Conclusion}
  \begin{block}{Emphasizing Responsibility}
      \begin{itemize}
          \item Organizations and data scientists must identify and mitigate biases within datasets.
          \item Transparency practices are vital for building trust and complying with legal standards.
          \item Ethical AI is a societal responsibility, requiring collaboration among technologists, ethicists, and policymakers.
      \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
      Acknowledging and addressing ethical considerations in neural networks is crucial for a fair and equitable environment. Responsible use of technology can inspire innovation while upholding values of integrity and respect.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{References for Further Reading}
  \begin{itemize}
      \item "Weapons of Math Destruction" by Cathy O'Neil
      \item "Fair Algorithms" research papers
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks}
    \begin{block}{Emerging Trends in Neural Networks}
        \begin{itemize}
            \item \textbf{Transfer Learning:} Accelerate learning by leveraging models trained on related tasks, such as fine-tuning image recognition models for medical imaging.
            \item \textbf{Explainable AI:} Developing interpretable models to provide insights into decision-making processes, utilizing techniques like SHAP.
            \item \textbf{Neural Architecture Search (NAS):} Automated methods for discovering optimal neural network architectures, reducing human effort in design.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{AI Ethics and Hardware Advancements}
    \begin{block}{AI Ethics and Responsible AI}
        \begin{itemize}
            \item Focus on reducing biases in datasets and ensuring fairness in AI applications.
            \item \textbf{Adversarial Robustness:} Research aimed at making networks resilient to adversarial attacks.
        \end{itemize}
    \end{block}
    \begin{block}{Advancements in Hardware}
        \begin{itemize}
            \item Rise of specialized hardware (GPUs, TPUs) accelerates model efficiency.
            \item Potential impact of quantum computing on solving complex neural network tasks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Trends in AI}
    \begin{block}{Applications in New Domains}
        \begin{itemize}
            \item Neural networks expanding into edge computing, healthcare, and renewable energy.
            \item \textbf{Example:} Analysis of medical images and prediction of patient outcomes in healthcare.
        \end{itemize}
    \end{block}
    \begin{block}{Real-time and Edge AI}
        \begin{itemize}
            \item Deploying neural networks on edge devices enhances real-time processing and reduces latency.
            \item \textbf{Example:} Efficient facial recognition on mobile devices without relying on cloud computing.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Shift towards explainable and ethical AI is crucial.
            \item Hardware advancements will enhance scalability and performance.
            \item Rapid diversification into unconventional fields presents both opportunities and challenges.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Points Recap}
    \begin{enumerate}
        \item \textbf{Definition and Functionality}
        \begin{itemize}
            \item Neural Networks (NNs) mimic the human brain to recognize patterns and solve problems in supervised learning.
            \item They process input data through layers of interconnected nodes (neurons).
        \end{itemize}

        \item \textbf{Architectural Components}
        \begin{itemize}
            \item \textbf{Input Layer}: Receives input features.
            \item \textbf{Hidden Layers}: Perform computations and extract features.
            \item \textbf{Output Layer}: Produces final predictions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Points Recap (Cont'd)}
    \begin{enumerate}[resume]
        \item \textbf{Training Process}
        \begin{itemize}
            \item \textbf{Forward Propagation}: Input data generates predictions.
            \item \textbf{Loss Function}: Measures difference between predicted and actual outputs.
            \item \textbf{Backpropagation}: Updates weights based on error, optimizing the model.
        \end{itemize}

        \item \textbf{Activation Functions}
        \begin{itemize}
            \item Functions like Sigmoid, ReLU, and Softmax allow NNs to learn complex patterns.
        \end{itemize}

        \item \textbf{Overfitting and Regularization}
        \begin{itemize}
            \item Overfitting occurs when the model learns noise, leading to poor generalization.
            \item Techniques: dropout, early stopping, L2 regularization.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Discussion Points & Engagement}
    \begin{block}{Applications of Neural Networks}
        \begin{itemize}
            \item Image and speech recognition
            \item Natural language processing
            \item Predictive analytics in healthcare, finance, and automotive fields
        \end{itemize}
    \end{block}
    
    \begin{block}{Discussion Points}
        \begin{itemize}
            \item \textbf{Emerging Trends}: Innovative architectures like Transformers.
            \item \textbf{Ethics in AI}: Bias, interpretability, accountability.
        \end{itemize}
    \end{block}

    \begin{block}{Questions \& Engagement}
        \begin{itemize}
            \item What aspects of NNs do you find most challenging?
            \item Can you share experiences of using NNs in real-world applications?
            \item Do you have queries about specific architectures or training techniques?
        \end{itemize}
    \end{block}
\end{frame}


\end{document}