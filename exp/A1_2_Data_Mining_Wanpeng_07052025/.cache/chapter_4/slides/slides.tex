\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Supervised Learning Techniques]{Chapter 4: Supervised Learning Techniques - Logistic Regression}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Logistic Regression}
    \begin{block}{Overview}
        Logistic regression is a supervised learning algorithm used for binary classification. It is significant for its interpretability and effectiveness in data mining.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Logistic Regression?}
    \begin{itemize}
        \item A statistical method for binary classification.
        \item Learns from labeled datasets with known output labels.
        \item Predicts probabilities that map into discrete classes (e.g., 0 or 1).
    \end{itemize}
    \begin{block}{Example}
        Predict whether an email is 'spam' (1) or 'not spam' (0).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Sigmoid Function:} 
        \begin{equation}
            \sigma(z) = \frac{1}{1 + e^{-z}}
        \end{equation}
        This function outputs values between 0 and 1.
        
        \item \textbf{Odds and Log-Odds:} Odds are the ratio of probabilities, and log-odds are expressed as:
        \begin{equation}
            \text{log-odds} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Application}
    \begin{itemize}
        \item Dataset: Loan applicants with features such as income, credit score, and loan amount.
        \item \textbf{Training Phase:} Estimates coefficients (\(\beta\)) to predict loan default.
        \item \textbf{Prediction Phase:} Classifies applicants based on computed probabilities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Data Mining}
    \begin{itemize}
        \item \textbf{Interpretability:} Coefficients indicate feature-relationship strengths.
        \item \textbf{Wide Usage:} Applied in healthcare, marketing, and finance.
        \item \textbf{Foundation for Complex Models:} Basis for logistic extensions and neural networks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Ideal for binary classification tasks.
            \item Relies on the sigmoid function for probability output.
            \item Widely applicable and interpretable across industries.
        \end{itemize}
    \end{block}
    
    Logistic regression is a vital technique for predictive analytics in numerous domains.
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 1}
    \frametitle{Learning Objectives for Logistic Regression}
    By the end of this section, students should be able to:
    \begin{enumerate}
        \item \textbf{Understand the Purpose of Logistic Regression}:
        \begin{itemize}
            \item Grasp how logistic regression serves as a statistical method primarily used for binary classification problems (e.g., email spam vs. not spam).
            \item Recognize its importance in predicting probabilities and making informed decisions.
        \end{itemize}
        
        \item \textbf{Identify the Key Components of Logistic Regression}:
        \begin{itemize}
            \item Define the dependent variable as categorical (binary) and the independent variables as predictors.
            \item Understand concepts like odds and odds ratio in relation to probability.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 2}
    \frametitle{Learning Objectives for Logistic Regression (cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Learn the Mathematical Formulation}:
        \begin{itemize}
            \item Understand the logistic function:
            \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
            \end{equation}
            \item Recognize how the coefficients ($\beta$ terms) represent the influence of each predictor.
        \end{itemize}
        
        \item \textbf{Implement Logistic Regression using a Software Tool}:
        \begin{itemize}
            \item Gain practical experience using Python with `scikit-learn`:
            \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
            \end{lstlisting}
            \item Assess model performance using metrics like accuracy, precision, and recall.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 3}
    \frametitle{Learning Objectives for Logistic Regression (cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Interpret the Results}:
        \begin{itemize}
            \item Learn to interpret logistic regression output, focusing on predictor significance and confounding variables.
            \item Discuss the impact of predictors on the odds of the target event occurring.
        \end{itemize}

        \item \textbf{Apply Logistic Regression to Real-world Problems}:
        \begin{itemize}
            \item Explore case studies where logistic regression has been applied (e.g., credit scoring, medical diagnosis).
        \end{itemize}
    \end{enumerate}
    
    \textbf{Key Takeaways:}
    \begin{itemize}
        \item Logistic regression is essential in supervised learning for binary classification.
        \item Mastering its implementation and interpretation is crucial for data analytics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Logistic Regression?}
    \begin{block}{Definition}
        \textbf{Logistic Regression} is a statistical method used for binary classification problems. It estimates the probability that a given input belongs to a particular category (e.g., 0 or 1, Yes or No).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Key Concepts}
    \begin{itemize}
        \item \textbf{Probability Estimation:} Predicts likelihood of a class using the logistic function.
        \item \textbf{Logistic Function:} Maps real values to probabilities between 0 and 1.

        \begin{equation}
        P(Y=1 | X) = \frac{1}{1 + e^{-z}} \quad \text{where } z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Examples}
    \begin{block}{Applications}
        Logistic regression is widely used in various fields including:
        \begin{itemize}
            \item \textbf{Healthcare:} Predicting disease presence (e.g., diabetes yes/no).
            \item \textbf{Finance:} Classifying loan default risk (default/no default).
            \item \textbf{Marketing:} Determining campaign response (respond/no respond).
        \end{itemize}
    \end{block}

    \begin{block}{Example: Predicting Exam Results}
        Consider predicting if a student will pass (1) or fail (0) based on hours studied:
        \begin{equation}
        P(pass | hours) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot \text{hours})}}
        \end{equation}
        With $\beta_0 = -4$ and $\beta_1 = 0.9$, for 5 hours of study:
        \begin{equation}
        P(pass | 5 \text{ hours}) \approx 0.622
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation - Overview}
    \begin{block}{Overview of Logistic Regression}
        Logistic regression is utilized for binary classification tasks. It models the relationship between a dependent binary variable and one or more independent variables.
    \end{block}
    \begin{itemize}
        \item **Key Concepts:**
        \begin{itemize}
            \item Logistic Function
            \item Odds Ratios
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation - The Logistic Function}
    \begin{block}{The Logistic Function}
        The logistic function is defined as:
        \begin{equation}
            f(z) = \frac{1}{1 + e^{-z}}
        \end{equation}
        where \( z \) is given by:
        \begin{equation}
            z = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n
        \end{equation}
        
        **Key Characteristics:**
        \begin{itemize}
            \item Ranges between 0 and 1
            \item S-shaped and asymptotic
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        For predicting student pass/fail based on study hours \( X \):
        \begin{equation}
            z = -4 + 0.5 \times (8) = 0
        \end{equation}
        Calculating the logistic function, we have:
        \begin{equation}
            f(0) = \frac{1}{1 + e^{0}} = 0.5
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation - Odds and Odds Ratios}
    \begin{block}{Odds and Odds Ratios}
        **Odds** are defined as:
        \begin{equation}
            \text{Odds} = \frac{P(Y=1)}{P(Y=0)}
        \end{equation}
        For our example, if \( P(Y=1) = 0.5 \), then:
        \begin{equation}
            \text{Odds} = \frac{0.5}{0.5} = 1
        \end{equation}

        **Odds Ratios**:
        \begin{equation}
            \text{Odds Ratio (OR)} = \frac{\text{Odds}_{\text{group 1}}}{\text{Odds}_{\text{group 2}}}
        \end{equation}

        Example: If odds for students studying > 5 hours is 3:1, OR = 3.
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Model Implementation Steps for Logistic Regression}
  \begin{block}{Overview}
    Implementing logistic regression involves several systematic steps to ensure that model predictions are meaningful and accurate. The following sections outline the key phases: data preparation, model fitting, and prediction.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{1. Data Preparation}
  \begin{itemize}
    \item \textbf{Data Cleaning:} Handle missing values and outliers.
    \begin{itemize}
      \item Example: Impute missing values using mean or mode, or drop observations if considerable.
    \end{itemize}
    
    \item \textbf{Feature Selection:} Choose relevant features that influence the target variable.
    \begin{itemize}
      \item Example: For predicting exam success, relevant features could include hours studied, attendance, and previous grades.
    \end{itemize}
    
    \item \textbf{Feature Encoding:} Convert categorical variables into numerical format.
    \begin{itemize}
      \item Example: One-hot encoding for "Gender" (Male, Female) to two binary features (Gender\_Male, Gender\_Female).
    \end{itemize}
    
    \item \textbf{Feature Scaling:} Normalize or standardize numerical features to improve model performance.
    \begin{itemize}
      \item Example: Income ranging from 0 to 100,000 and age from 0 to 100 may require scaling.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{2. Model Fitting}
  Logistic regression predicts the log-odds of the probability \( p \) that the outcome is 1 (i.e., success):
  
  \begin{equation}
  \text{log} \left( \frac{p}{1 - p} \right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n
  \end{equation}

  Where \( \beta_0 \) is the intercept, and \( \beta_1, \beta_2, ..., \beta_n \) are the coefficients for each feature \( X \).

  \begin{block}{Implementation in Python}
  \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assume X is your features and y is your target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

# Making predictions
y_pred = model.predict(X_test)
  \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3. Prediction}
  After the model is trained, it can make predictions on new data.
  
  \begin{itemize}
    \item \textbf{Interpreting Predictions:} The output is a probability score between 0 and 1.
    \begin{itemize}
      \item \textbf{Thresholding:} If the probability \( p \) is greater than 0.5, the predicted class is 1 (success); otherwise, it is 0 (failure).
    \end{itemize}
    
    \begin{block}{Example Code for Predictions}
    \begin{lstlisting}[language=Python]
predictions_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities
predictions_class = (predictions_prob > 0.5).astype(int)  # Convert to binary
    \end{lstlisting}
    \end{block}
  \end{itemize}

  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Data preparation is crucial as it significantly influences model performance.
      \item Logistic regression can be extended to multiclass classification using One-vs-Rest (OvR).
      \item Regularization techniques (L1 and L2) can be applied to prevent overfitting and improve generalizability.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Importance}
    \begin{block}{Importance of Data Preparation for Logistic Regression}
        Data preparation is a crucial step in the machine learning pipeline. 
        Effective data preparation ensures that the data fed into the model is:
        \begin{itemize}
            \item Accurate
            \item Informative
            \item Suitable for analysis
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Handling Missing Values}
    \begin{block}{Handling Missing Values}
        \begin{itemize}
            \item **Why Important**: Logistic regression cannot handle missing values natively, leading to:
            \begin{itemize}
                \item Biased estimates
                \item Model failure
            \end{itemize}

            \item **Methods**:
            \begin{enumerate}
                \item Deletion: Remove records with missing values.
                \item Imputation:
                \begin{itemize}
                    \item Mean/Median Imputation
                    \item Mode Imputation
                    \item Advanced Methods (e.g., K-Nearest Neighbors)
                \end{itemize}
            \end{enumerate}
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        Consider a dataset with a feature `Age` containing missing entries:
        \begin{lstlisting}[language=Python]
        import pandas as pd
        df['Age'].fillna(df['Age'].median(), inplace=True)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Feature Scaling}
    \begin{block}{Feature Scaling}
        \begin{itemize}
            \item **Why Important**: Ensures features have equal weight in model learning.
            \item **Methods**:
            \begin{itemize}
                \item Normalization: Scales features to a range of [0, 1].
                \begin{equation}
                X' = \frac{X - X_{min}}{X_{max} - X_{min}}
                \end{equation}
                \item Standardization: Centers the feature to mean = 0, std = 1.
                \begin{equation}
                X' = \frac{X - \mu}{\sigma}
                \end{equation}
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        Consider feature `Income` ranging from 20,000 to 100,000 and `Age` from 18 to 90:
        \begin{lstlisting}[language=Python]
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        df[['Income', 'Age']] = scaler.fit_transform(df[['Income', 'Age']])
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Prepare data thoroughly for improved model performance.
            \item Choose methods wisely based on dataset specifics.
            \item Data preparation is an iterative process and should be revisited.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Investing time in data preparation is vital for building effective logistic regression models. Ensure data is clean, complete, and correctly scaled to enhance model accuracy and reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feature Selection - Introduction}
  \begin{block}{Introduction to Feature Selection}
    Feature selection is a crucial process in building effective logistic regression models. It involves selecting the most relevant features from the dataset to:
    \begin{itemize}
        \item Improve model performance
        \item Enhance interpretability
        \item Reduce overfitting
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feature Selection - Importance}
  \begin{block}{Why is Feature Selection Important?}
    \begin{enumerate}
      \item \textbf{Improves Model Accuracy:} 
      \begin{itemize}
          \item Eliminates irrelevant or redundant features.
      \end{itemize}
      \item \textbf{Reduces Overfitting:} 
      \begin{itemize}
          \item A simpler model is less likely to capture noise.
      \end{itemize}
      \item \textbf{Enhances Interpretability:} 
      \begin{itemize}
          \item Fewer features make it easier to understand the model.
      \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feature Selection - Techniques}
  \begin{block}{Feature Selection Techniques}
    \begin{enumerate}
      \item \textbf{Filter Methods:} Based on statistical tests.
      \begin{itemize}
        \item \textit{Examples:}
        \begin{itemize}
          \item \textbf{Chi-Squared Test:} Measures the dependence between two categorical variables.
          \item \textbf{Correlation Coefficient:} 
          \begin{equation}
          r = \frac{cov(X, Y)}{\sigma_X \sigma_Y}
          \end{equation}
        \end{itemize}
      \end{itemize}

      \item \textbf{Wrapper Methods:} Based on model performance.
      \begin{itemize}
        \item \textit{Example:} Recursive Feature Elimination (RFE).
      \end{itemize}

      \item \textbf{Embedded Methods:} Feature selection as part of the algorithm.
      \begin{itemize}
        \item \textit{Example:} 
        \begin{equation}
        \min \left( \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} | \beta_j | \right)
        \end{equation}
        \textbf{(LASSO)}
      \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training and Testing the Model - Overview}
    \begin{itemize}
        \item Importance of splitting the dataset into Training and Testing sets.
        \item Procedures for training the logistic regression model.
        \item Key points and considerations during model development.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Splitting the Dataset}
    \begin{block}{Training Set}
        This portion of the data is used to train the model, comprising about \textbf{70\%--80\%} of the total data.
    \end{block}
    
    \begin{block}{Testing Set}
        A separate subset, usually \textbf{20\%--30\%} of the dataset, used to evaluate the performance of the trained model.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Example of Data Splitting:}
        \begin{itemize}
            \item If you have a dataset of \textbf{1,000 instances}:
            \begin{itemize}
                \item Training Set: 700 to 800 instances
                \item Testing Set: 200 to 300 instances
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Procedures for Training the Logistic Regression Model}
    \begin{enumerate}
        \item \textbf{Data Preprocessing}
            \begin{itemize}
                \item Handling Missing Values: Fill or remove missing data.
                \item One-Hot Encoding: Convert categorical variables into a suitable format.
                \item Feature Scaling: Normalize or standardize numerical features.
            \end{itemize}
        
        \item \textbf{Model Training}
            \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Split data into features (X) and target (y)
X = dataset.iloc[:, :-1]  # Features
y = dataset.iloc[:, -1]   # Target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
model = LogisticRegression()
model.fit(X_train, y_train)
            \end{lstlisting}

        \item \textbf{Model Validation}
            \begin{itemize}
                \item Use techniques like \textbf{k-fold cross-validation} to assure robust model performance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Separation of Datasets:} Avoids overfitting.
        \item \textbf{Training Iterations:} Be ready to adjust model parameters and retrain as needed.
        \item \textbf{Random State Parameter:} Ensures reproducibility of results, vital for model validation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Visual Representation Suggestion}
    \begin{itemize}
        \item \textbf{Dataset Split Diagram:} Illustrate a rectangle divided into two sections labeled "Training Set" (70-80\%) and "Testing Set" (20-30\%).
        \item Include arrows leading to 'Model Training' and 'Model Evaluation' stages to show the flow of the procedure.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Model Evaluation Metrics}
  \begin{block}{Overview}
    When evaluating the performance of a logistic regression model, it’s essential to use appropriate metrics to understand its effectiveness in predicting binary outcomes. The primary evaluation metrics are:
    \begin{itemize}
      \item Accuracy
      \item Precision
      \item Recall
      \item F1 Score
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Metrics - Accuracy and Precision}
  \begin{enumerate}
    \item \textbf{Accuracy}
      \begin{itemize}
        \item \textbf{Definition:} The ratio of correctly predicted instances (both true positives and true negatives) to the total instances.
        \item \textbf{Formula:} 
          \begin{equation}
          \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
          \end{equation}
        \item \textbf{Example:} If a model correctly predicts 80 out of 100 instances, its accuracy is 80\%.
      \end{itemize}

    \item \textbf{Precision}
      \begin{itemize}
        \item \textbf{Definition:} The ratio of true positives to the total predicted positives.
        \item \textbf{Formula:}
          \begin{equation}
          \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
          \end{equation}
        \item \textbf{Example:} If a model predicts 30 instances as positive but only 20 are true positives, precision is approximately 67\%.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Metrics - Recall and F1 Score}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Recall (Sensitivity)}
      \begin{itemize}
        \item \textbf{Definition:} The ratio of true positives to the actual positives.
        \item \textbf{Formula:}
          \begin{equation}
          \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
          \end{equation}
        \item \textbf{Example:} If there are 50 positive instances and the model correctly identifies 30, recall is 60\%.
      \end{itemize}

    \item \textbf{F1 Score}
      \begin{itemize}
        \item \textbf{Definition:} The harmonic mean of precision and recall, balancing the trade-off between them.
        \item \textbf{Formula:}
          \begin{equation}
          \text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
          \end{equation}
        \item \textbf{Example:} If precision is 0.67 and recall is 0.60, the F1 score is approximately 0.63.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Importance of Balance:} Precision and recall can be in tension; balancing them using the F1 score is often crucial.
    \item \textbf{Context Matters:} Different applications may prioritize these metrics differently. For instance, in spam detection, recall may be more critical.
    \item \textbf{Multiple Metrics:} Relying on a single metric can provide a skewed view; it's vital to evaluate all metrics together.
  \end{itemize}
  
  \begin{block}{Additional Notes}
    - Use confusion matrices to visualize metrics.
    - Support metrics with domain knowledge for informed decision-making.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Results: Overview}
    \begin{block}{Overview}
        Interpreting results from a logistic regression model is crucial for understanding how predictors influence the likelihood of a binary outcome. Key aspects covered include:
    \end{block}
    \begin{itemize}
        \item Coefficients
        \item Odds Ratios
        \item Implications for decision-making
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Coefficients in Logistic Regression}
    \begin{block}{Definition}
        Coefficients ($\beta$) represent the change in the log-odds of the dependent variable for a one-unit increase in the predictor variable, holding all other variables constant.
    \end{block}
    \begin{itemize}
        \item Positive $\beta$: Increased likelihood of the event.
        \item Negative $\beta$: Decreased likelihood of the event.
    \end{itemize}
    
    \begin{block}{Example}
        For a coefficient $\beta_1 = 0.5$ for "age":
        \begin{itemize}
            \item Each additional year increases the log-odds of the event by 0.5.
        \end{itemize}
    \end{block}

    \begin{equation}
        \text{log-odds} = \ln\left(\frac{P}{1-P}\right)
    \end{equation}

    \begin{equation}
        P = \frac{1}{1 + e^{-\text{log-odds}}}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Odds Ratios (OR)}
    \begin{block}{Definition}
        The odds ratio is the exponential of the coefficient:
        \[
        OR = e^{\beta}
        \]
    \end{block}
    \begin{itemize}
        \item $OR > 1$: Increased odds of the event.
        \item $OR < 1$: Decreased odds of the event.
        \item $OR = 1$: No effect.
    \end{itemize}
    
    \begin{block}{Example}
        For $\beta_1 = 0.5$:
        \begin{itemize}
            \item Odds Ratio $OR = e^{0.5} \approx 1.65$.
            \item Interpretation: A one-unit increase in "age" increases the odds of purchasing by about 65\%.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Multicollinearity}
    \begin{block}{Understanding Multicollinearity}
        Multicollinearity refers to a situation in regression analyses where two or more predictor variables are highly correlated, complicating the isolation of their individual effects on the outcome variable.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Multicollinearity Problematic?}
    \begin{itemize}
        \item \textbf{Inflated Standard Errors:} Increases the chance that the model assigns larger standard errors to coefficient estimates, complicating hypothesis testing.
        \item \textbf{Unstable Coefficients:} Small data changes can lead to significant coefficient variations, reducing reliability.
        \item \textbf{Reduced Model Predictive Power:} May cause overfitting, leading to poor performance on unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Detecting Multicollinearity}
    \begin{enumerate}
        \item \textbf{Correlation Matrix:} Examine correlation coefficients between pairs of predictors. High values indicate multicollinearity.
        \item \textbf{Variance Inflation Factor (VIF):} A VIF greater than 10 suggests multicollinearity.
        \begin{equation}
            VIF_i = \frac{1}{1 - R^2_i}
        \end{equation}
        where \(R^2_i\) is the R-squared from regressing the \(i\)th predictor against all others.
        \item \textbf{Condition Index:} A value greater than 30 signals potential multicollinearity issues (derived from eigenvalues of the correlation matrix).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mitigating Multicollinearity}
    \begin{itemize}
        \item \textbf{Remove Highly Correlated Predictors:} Drop one variable from correlated pairs (e.g., keep `Income` over `Credit Score`).
        \item \textbf{Combine Variables:} Create composite variables (e.g., Body Mass Index from `Height` and `Weight`).
        \item \textbf{Regularization Techniques:}
        \begin{itemize}
            \item \textit{Lasso Regression:} Reduces coefficients to zero, selecting variables.
            \item \textit{Ridge Regression:} Adds a penalty to the coefficients, reducing complexity.
        \end{itemize}
        \item \textbf{Principal Component Analysis (PCA):} Transforms correlated variables into uncorrelated principal components.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Multicollinearity complicates the interpretation of logistic regression models.
            \item Detection and mitigation are essential for model performance and interpretability.
            \item Techniques such as VIF and regularization assist in addressing multicollinearity.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding and managing multicollinearity is crucial for building robust logistic regression models, enhancing predictive power and reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Case Study: Logistic Regression in Action}
    \begin{block}{Case Study Overview}
        In this case study, we examine how logistic regression is utilized by a healthcare organization to predict patient outcomes, 
        specifically focusing on the likelihood of patients being readmitted to the hospital within 30 days of discharge.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Background and Dataset}
    \begin{block}{Background}
        Understanding readmission rates is crucial for improving patient care and optimizing hospital resources. 
        High readmission rates can indicate issues with care quality or patient compliance.
    \end{block}
    
    \begin{block}{Dataset}
        \begin{itemize}
            \item \textbf{Target Variable}: Readmission within 30 days (0 = No, 1 = Yes)
            \item \textbf{Predictor Variables}:
                \begin{itemize}
                    \item Age (continuous)
                    \item Number of previous admissions (count)
                    \item Discharge diagnosis (categorical - one-hot encoding)
                    \item Length of stay prior to discharge (continuous)
                    \item Medication adherence (binary - 0 = Non-compliant, 1 = Compliant)
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Model}
    \begin{block}{Model Overview}
        Logistic regression is well-suited for binary outcome variables. 
        We model the probability of readmission using the logistic function:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{- (β_0 + β_1X_1 + β_2X_2 + ... + β_nX_n)}}
        \end{equation}
        Where:
        \begin{itemize}
            \item $P(Y=1 | X)$ = probability of readmission
            \item $β_0$ = intercept
            \item $β_n$ = coefficients for predictor variables
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation Steps}
    \begin{enumerate}
        \item \textbf{Data Preprocessing}:
            \begin{itemize}
                \item Handle missing values.
                \item Normalize continuous variables.
                \item One-hot encode categorical variables.
            \end{itemize}
        \item \textbf{Model Training}:
            \begin{itemize}
                \item Split dataset into training (70\%) and testing (30\%).
                \item Fit the logistic regression model using training data.
            \end{itemize}
        \item \textbf{Model Evaluation}:
            \begin{itemize}
                \item Use metrics like Accuracy, Precision, Recall, and AUC-ROC.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Findings and Conclusion}
    \begin{block}{Key Findings}
        \begin{itemize}
            \item Significant predictors of readmission: Medication adherence and number of previous admissions.
            \item Achieved AUC-ROC score: 0.85, indicating good predictive accuracy.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        This case study illustrates the application of logistic regression in healthcare settings to predict patient readmissions, 
        enabling proactive management and improved patient outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Logistic regression is effective for binary classification tasks.
        \item Proper data preprocessing is vital for model accuracy.
        \item Evaluation metrics inform model performance and areas for improvement.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Implementation}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_auc_score

# Load dataset
data = pd.read_csv('patient_data.csv')

# Preprocess data (handle missing values, encode categorical variables)
# ...

# Split data
X = data.drop('readmission', axis=1)
y = data['readmission']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
auc_score = roc_auc_score(y_test, y_pred)
print(f"AUC-ROC Score: {auc_score:.2f}")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Pitfalls in Logistic Regression}
    \begin{block}{Introduction}
        When implementing logistic regression, practitioners often encounter challenges that can lead to inaccurate models or misinterpretation of results. Recognizing these pitfalls is crucial for successful application. Below are common pitfalls along with strategies to avoid them.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Pitfalls - Multicollinearity and Overfitting}
    \begin{enumerate}
        \item \textbf{Ignoring Multicollinearity:}
        \begin{itemize}
            \item \textbf{Explanation:} Occurs when independent variables are highly correlated.
            \item \textbf{Impact:} Inflated standard errors; difficulty in determining significance.
            \item \textbf{Solution:}
            \begin{itemize}
                \item Detect: Use Variance Inflation Factor (VIF).
                \item Address: Remove/combine correlated variables or use PCA.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Overfitting the Model:}
        \begin{itemize}
            \item \textbf{Explanation:} Model learns noise instead of patterns.
            \item \textbf{Impact:} Good performance on training data but poor on unseen data.
            \item \textbf{Solution:}
            \begin{itemize}
                \item Use Cross-Validation: Employ k-fold to evaluate model.
                \item Limit Features: Simplify model by reducing predictors.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Pitfalls - Linearity in Logits and Assumptions}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Assuming Linearity in Logits:}
        \begin{itemize}
            \item \textbf{Explanation:} Assumes a linear relationship between log odds and predictors.
            \item \textbf{Impact:} Non-linear relationships lead to inaccurate predictions.
            \item \textbf{Solution:}
            \begin{itemize}
                \item Transform Variables: Use polynomial or interaction terms.
                \item Check Residuals: Plot predicted vs. residuals to identify patterns.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Neglecting to Check Assumptions:}
        \begin{itemize}
            \item \textbf{Explanation:} Relies on specific assumptions about data.
            \item \textbf{Impact:} Violating assumptions can lead to biased estimates.
            \item \textbf{Solution:}
            \begin{itemize}
                \item Diagnostic Checks: Use the Hosmer-Lemeshow test, residual analysis.
                \item Evaluate Influential Points: Use leverage statistics and Cook's distance.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Pitfalls - Coefficient Interpretation and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Misinterpreting Coefficients:}
        \begin{itemize}
            \item \textbf{Explanation:} Coefficients represent the change in log odds for a one-unit change in predictor.
            \item \textbf{Impact:} Misinterpretation misleads stakeholders about influence.
            \item \textbf{Solution:}
            \begin{itemize}
                \item Use Odds Ratios: Convert coefficients for easier interpretation.
                \item Contextualize Results: Present findings with real-world implications.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Address multicollinearity for reliable estimates.
            \item Utilize cross-validation to avoid overfitting.
            \item Transform features for non-linear relationships.
            \item Conduct thorough assumption checks.
            \item Clearly communicate logistic regression coefficients.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Recognizing and addressing these common pitfalls can greatly enhance the reliability and interpretability of logistic regression models, leading to better decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advanced Topics in Logistic Regression}
    \begin{block}{Overview}
        This section provides a brief overview of advanced topics related to logistic regression, including:
        \begin{itemize}
            \item Multilevel Modeling
            \item Regularization Techniques
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multilevel Modeling}
    \begin{itemize}
        \item \textbf{Definition}: Extends the logistic regression framework to situations where data are nested (e.g., students within schools).
        \item \textbf{Example}: Analyzing the effect of teaching methods on student performance across multiple classrooms.
        \item \textbf{Key Points}:
        \begin{itemize}
            \item \textbf{Random Effects}: Introduces random intercepts/slopes to account for variability in different groups.
            \item \textbf{Model Form}:
            \begin{equation}
                \text{Logit}(p_{ij}) = \beta_0 + \beta_1 \text{(X)} + u_{0j}
            \end{equation}
            Here, \(u_{0j}\) represents the random effect associated with group \(j\).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques}
    \begin{itemize}
        \item \textbf{Definition}: Methods employed to prevent overfitting by adding a penalty term to the loss function.
        \item \textbf{Types of Regularization}:
        \begin{enumerate}
            \item \textbf{L1 Regularization (Lasso)}:
            \begin{itemize}
                \item Adds the absolute value of coefficients as a penalty.
                \item \textbf{Cost function}:
                \begin{equation}
                    \text{Cost function} = -\sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)] + \lambda \sum_{j=1}^{m} | \beta_j |
                \end{equation}
                \item Encourages sparsity by driving some coefficients to zero.
            \end{itemize}
            \item \textbf{L2 Regularization (Ridge)}:
            \begin{itemize}
                \item Adds the square of coefficients as a penalty.
                \item \textbf{Cost function}:
                \begin{equation}
                    \text{Cost function} = -\sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)] + \lambda \sum_{j=1}^{m} \beta_j^2
                \end{equation}
                \item Shrinks coefficients towards zero, reducing model complexity.
            \end{itemize}
        \end{enumerate}
        \item \textbf{When to Use}:
        \begin{itemize}
            \item Use Lasso for feature selection (some features should not impact the outcome).
            \item Use Ridge when all features contribute but need control over their influence.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways}
    \begin{enumerate}
        \item \textbf{Understanding Logistic Regression:} 
        A statistical method for binary classification estimating the probability of an outcome.
        
        \item \textbf{Mathematical Foundation:} 
        Uses the logistic function to model prediction:
        \[
        P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \cdots + \beta_nX_n)}}
        \]
        
        \item \textbf{Binary Outcomes and Odds:} 
        Focuses on the odds of an event, e.g., estimated odds of 2.33 from a 70\% chance.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Further Insights}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Gradient Descent for Optimization:}
        Finds optimal coefficients via:
        \[
        \beta_{new} = \beta_{old} - \eta \nabla J(\beta)
        \]

        \item \textbf{Application in Data Mining:} 
        Used in healthcare, finance, and marketing to inform decision-making.
        
        \item \textbf{Interpretability:} 
        Coefficients provide insights into feature impacts, enhancing communication with stakeholders.
        
        \item \textbf{Limitations and Alternatives:} 
        Struggles with non-linear relationships; alternatives include SVMs, Decision Trees, and Neural Networks.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary and Next Steps}
    \begin{block}{Review of Advanced Topics}
        Introduced multilevel modeling and regularization techniques (Lasso, Ridge) to enhance logistic regression performance.
    \end{block}
    
    \textbf{Conclusion:} 
    Logistic regression is foundational in supervised learning, facilitating model of binary outcomes and significant data-driven decisions.

    \textbf{Next Steps:}
    We will invite questions and discussions on practical applications of logistic regression in your respective fields.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Questions and Discussion - Overview}
  \begin{block}{Introduction}
    In this segment, we will explore logistic regression through questions and discussion on its applications and significance.
  \end{block}
  \begin{itemize}
    \item Logistic regression is a powerful statistical method for binary classification.
    \item Understanding is crucial across various fields such as healthcare and finance.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Questions and Discussion - Key Points}
  \begin{enumerate}
    \item \textbf{Clarification of Key Concepts}
      \begin{itemize}
        \item What challenges do you face in understanding logistic regression?
        \item How does the logistic function map predicted values to probabilities?
      \end{itemize}
    \item \textbf{Applications of Logistic Regression}
      \begin{itemize}
        \item Real-world scenarios include:
          \begin{itemize}
            \item Disease prediction (Yes/No)
            \item Email classification (spam/not spam)
            \item Customer churn prediction
          \end{itemize}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Questions and Discussion - Further Topics}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Comparison with Other Techniques}
      \begin{itemize}
        \item How does logistic regression compare with decision trees or support vector machines?
        \item What are the strengths and limitations of logistic regression?
      \end{itemize}
    \item \textbf{Model Evaluation}
      \begin{itemize}
        \item Performance metrics include:
          \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
            \item F1 Score
            \item ROC Curve and AUC
          \end{itemize}
      \end{itemize}
    \item \textbf{Hands-On Example}
      \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Load dataset
data = pd.read_csv('data.csv')  # Example dataset
X = data[['feature1', 'feature2']]  # Features
y = data['target']  # Target variable

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate the model
print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))
      \end{lstlisting}
    \end{enumerate}
\end{frame}


\end{document}