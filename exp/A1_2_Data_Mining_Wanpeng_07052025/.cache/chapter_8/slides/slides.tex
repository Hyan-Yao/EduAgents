\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}

% Title Page Information
\title[Ensemble Learning]{Chapter 8: Supervised Learning Techniques - Ensemble Learning Methods}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Ensemble Learning}
    \begin{block}{What is Ensemble Learning?}
        Ensemble Learning is a machine learning paradigm that combines multiple individual models (learners) to create a stronger overall model. The core idea is that by aggregating multiple predictive models, we can achieve greater accuracy and robustness than any single model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Ensemble Learning}
    \begin{itemize}
        \item \textbf{Improved Accuracy}: Reduces overall error rate by learning a more generalized model from diverse models.
        \item \textbf{Robustness to Overfitting}: Averages predictions to mitigate overfitting, capturing different patterns.
        \item \textbf{Handling Noise}: Navigates noisy datasets better by leveraging strengths of various models.
        \item \textbf{Flexibility}: Applicable to different types of base learners across diverse problem domains.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Ensemble Learning Methods}
    \begin{enumerate}
        \item \textbf{Bagging (Bootstrap Aggregating)}:
            \begin{itemize}
                \item Train multiple models on different subsets created by bootstrapping.
                \item Example: Random Forest, averaging predictions from multiple decision trees.
                \item \begin{equation}
                \hat{y} = \frac{1}{N} \sum_{i=1}^{N} \hat{y}_i
                \end{equation}
            \end{itemize}
        
        \item \textbf{Boosting}:
            \begin{itemize}
                \item Sequentially train models focusing on correcting previous errors.
                \item Example: AdaBoost and Gradient Boosting.
                \item \begin{equation}
                \text{Weight Update}: w_i \gets w_i \cdot e^{-\alpha y_i \hat{y}_i}
                \end{equation}
            \end{itemize}
        
        \item \textbf{Stacking}:
            \begin{itemize}
                \item Combine predictions from various models using a meta-learner.
                \item Example: Logistic regression as a meta-learner combining predictions from decision trees, SVMs, etc.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Ensemble learning enhances predictive performance by leveraging model diversity.
        \item It is crucial for dealing with complex datasets and improving prediction reliability.
        \item Knowing different methods (Bagging, Boosting, and Stacking) helps practitioners choose the best approach for their specific problem.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 1}
    \frametitle{Learning Objectives for Chapter 8}
    By the end of this chapter, you will be able to:
    \begin{enumerate}
        \item \textbf{Define Ensemble Learning:}
        \begin{itemize}
            \item Understand the concept and its distinction from single model approaches.
            \item Recognize the significance of combining multiple models to improve predictions.
        \end{itemize}
        
        \item \textbf{Identify Different Ensemble Methods:}
        \begin{itemize}
            \item Learn about popular techniques such as Bagging, Boosting, and Stacking.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 2}
    \frametitle{Learning Objectives for Chapter 8 (cont'd)}
    By the end of this chapter, you will be able to (continued):
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Understand the Benefits of Ensemble Methods:}
        \begin{itemize}
            \item Improved accuracy by reducing variance and bias.
            \item Greater model stability and performance across various scenarios.
        \end{itemize}
        
        \item \textbf{Apply Ensemble Techniques to Real-World Problems:}
        \begin{itemize}
            \item Implementation in model selection, hyperparameter tuning, and various domains.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 3}
    \frametitle{Learning Objectives for Chapter 8 (cont'd)}
    By the end of this chapter, you will be able to (continued):
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Evaluate Ensemble Model Performance:}
        \begin{itemize}
            \item Assess effectiveness using metrics like Accuracy, F1 Score, Precision, and Recall.
        \end{itemize}
        
        \item \textbf{Implement Ensemble Learning in Python:}
        \begin{block}{Sample Code}
        \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier

# Example - Random Forest Classifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
        \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Ensemble Learning?}
    \begin{block}{Definition}
        Ensemble learning is a machine learning paradigm that combines multiple models to produce a more accurate and robust predictive performance than any individual model.
    \end{block}
    \begin{itemize}
        \item Leverages the strength of several models (base learners) for better generalization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rationale Behind Combining Models}
    \begin{itemize}
        \item \textbf{Mitigating Overfitting:} Reduces the likelihood of overfitting by aggregating predictions from multiple models.
        \item \textbf{Increased Robustness:} Provides safeguards against individual model biases, ensuring better overall performance.
        \item \textbf{Improved Performance:} Diverse models can yield stronger results, often outperforming single models.
    \end{itemize}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Diversity is crucial in ensemble models.
            \item Majority voting in classification; averaging in regression.
            \item Popular techniques: Bagging, Boosting, Stacking.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Ensemble Learning}
    \begin{itemize}
        \item Consider a scenario of classifying animal images:
        \begin{itemize}
            \item \textbf{Model A:} Decision tree may overfit some images.
            \item \textbf{Model B:} SVM may struggle with noisy images.
        \end{itemize}
        \item By creating an ensemble:
        \begin{itemize}
            \item Decision tree classifies an image as “Dog”.
            \item SVM classifies the same as “Cat”.
            \item A third model identifies correctly as “Dog”.
        \end{itemize}
        \item \textbf{Final Classification:} Majority vote leads to an accurate classification as “Dog”.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Types of Ensemble Methods - Overview}
  \begin{block}{Ensemble Methods}
    Ensemble methods are powerful techniques in machine learning that combine multiple models to improve predictive performance. The major types of ensemble methods include:
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Types of Ensemble Methods - Bagging}
  \begin{enumerate}
    \item \textbf{Bagging (Bootstrap Aggregating)}
      \begin{itemize}
        \item \textit{Concept:} Creates multiple subsets of the training dataset via bootstrapping and trains a model on each subset.
        \item \textit{Goal:} Reduces variance to prevent overfitting.
        \item \textit{Example:} Random Forest aggregates predictions of decision trees.
        \item \textit{Key Point:} Works best with high-variance models, decreasing variance without increasing bias.
      \end{itemize}
      
      \begin{block}{Illustration}
        Imagine a diverse group of friends giving varied advice. Averaging their suggestions leads to a balanced decision.
      \end{block}

      \begin{equation}
      \text{Final Prediction} = \frac{1}{n} \sum_{i=1}^{n} \hat{y}_i
      \end{equation}
      where \( \hat{y}_i \) is the prediction from each model.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Types of Ensemble Methods - Boosting}
  \begin{enumerate}
    \setcounter{enumi}{1}
    \item \textbf{Boosting}
      \begin{itemize}
        \item \textit{Concept:} A sequential method where each new model corrects errors of prior models, emphasizing mistakes.
        \item \textit{Goal:} Reduces both bias and variance for greater robustness.
        \item \textit{Example:} AdaBoost focuses on hard-to-predict instances.
        \item \textit{Key Point:} Effective for improving weak learners.
      \end{itemize}

      \begin{block}{Illustration}
        Picture a student who learns from each exam, improving by focusing on weaker subjects.
      \end{block}

      \begin{equation}
      F(x) = \sum_{m=1}^{M} \alpha_m h_m(x)
      \end{equation}
      where \( \alpha_m \) is the weight of each weak learner \( h_m \).
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Types of Ensemble Methods - Stacking}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Stacking}
      \begin{itemize}
        \item \textit{Concept:} Trains multiple base learners and combines their predictions using a meta-learner.
        \item \textit{Goal:} Exploit strengths of different models to improve accuracy.
        \item \textit{Example:} Combining decision tree, logistic regression, and SVM via a meta-learner.
        \item \textit{Key Point:} Often leads to better generalization and captures various patterns.
      \end{itemize}

      \begin{block}{Illustration}
        Consider a panel of judges evaluating a talent show, where a head judge calculates the final score based on individual scores.
      \end{block}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Ensemble Methods}
  \begin{itemize}
    \item \textbf{Bagging:} Reduces variance with multiple independent models.
    \item \textbf{Boosting:} Sequentially improves predictions by focusing on errors.
    \item \textbf{Stacking:} Combines various models with a meta-learner for improved predictions.
  \end{itemize}
  Ensemble methods leverage the collective power of individual models, making them robust and versatile for predictive problems in machine learning.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Bagging Explained - Introduction}
  \begin{block}{What is Bagging?}
    Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that enhances the stability and accuracy of machine learning algorithms. 
  \end{block}
  \begin{itemize}
    \item Aims to reduce variance.
    \item Helps mitigate overfitting in complex models.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Bagging Explained - How it Works}
  \begin{enumerate}
    \item \textbf{Bootstrap Sampling:}
    \begin{itemize}
      \item Create multiple subsets of the dataset by sampling with replacement.
      \item Example: From a dataset of 100 samples, create 10 subsets of 100 samples each.
    \end{itemize}
    
    \item \textbf{Training Models:}
    \begin{itemize}
      \item Each subset (bag) is used to train a separate model.
      \item Models can be of the same type, trained on different data subsets.
    \end{itemize}
    
    \item \textbf{Aggregation of Predictions:}
    \begin{itemize}
      \item Classification: Majority voting is used for final predictions.
      \item Regression: Predictions are averaged across all models.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Bagging Explained - Reducing Variance}
  \begin{block}{Variance Reduction}
    Bagging significantly reduces model variance without increasing bias. This leads to more robust predictions.
  \end{block}
  \begin{equation}
    \text{Var}(Y) = \text{Var}\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) = \frac{\sigma^2}{n}
  \end{equation}
  where \( \sigma^2 \) is the variance of individual model predictions and \( n \) is the number of models.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Bagging Explained - Random Forests}
  \begin{block}{Random Forests}
    Random Forest is a popular machine learning algorithm that utilizes bagging by building multiple decision trees.
  \end{block}
  \begin{itemize}
    \item Trained on different random samples of the data.
    \item Considers a random subset of features when making splits.
  \end{itemize}

  \begin{block}{Benefits of Random Forests}
    \begin{itemize}
      \item High Accuracy: Combining trees yields better predictions.
      \item Robustness: Less prone to overfitting due to bagging and feature randomness.
      \item Feature Importance: Provides insights on feature relevance for further modeling.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Boosting Techniques - Overview}
  \begin{block}{Introduction to Boosting}
    Boosting is an ensemble learning technique aimed at improving predictive performance by combining multiple weak learners into a strong learner. It reduces bias rather than variance, making it effective for complex datasets.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Boosting Techniques - Key Features}
  \begin{itemize}
    \item \textbf{Sequential Learning}: Models are trained sequentially, where each new model corrects the errors of the previous ones.
    \item \textbf{Focus on Difficult Cases}: Boosting emphasizes instances that are misclassified by earlier models.
    \item \textbf{Adaptive Algorithms}: Each weak learner adapts based on the performance of its predecessors.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Common Boosting Techniques - AdaBoost}
  \begin{block}{AdaBoost (Adaptive Boosting)}
    \begin{itemize}
      \item \textbf{Concept}: Combines multiple weak classifiers into a strong classifier.
      \item \textbf{Mechanism}: Sequential training with increased weights on misclassified instances.
      \item \textbf{Formula}:
      \begin{equation}
        F(x) = \sum_{m=1}^{M} \alpha_m h_m(x)
      \end{equation}
      where \( h_m(x) \) is the m-th weak learner and \( \alpha_m \) is its weight.
      \item \textbf{Example}: In a scenario where three models classify correctly 80\%, 85\%, and 90\% of the time, AdaBoost would give more weight to the third model in the final prediction.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Common Boosting Techniques - Gradient Boosting}
  \begin{block}{Gradient Boosting}
    \begin{itemize}
      \item \textbf{Concept}: Builds models in a stage-wise fashion, optimizing a loss function via gradient descent.
      \item \textbf{Mechanism}: Fits new weak learners to the residuals of previous models.
      \item \textbf{Formula}:
      \begin{equation}
        F_{m}(x) = F_{m-1}(x) + \gamma h_m(x)
      \end{equation}
      where \( \gamma \) is the learning rate, and \( h_m(x) \) is the new weak learner.
      \item \textbf{Example}: For predicting housing prices, each model predicts the difference between actual and predicted prices, refining the estimations progressively.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Boosting Techniques - Key Points}
  \begin{itemize}
    \item \textbf{Reduction of Bias}: Effective at reducing bias, improving predictive accuracy.
    \item \textbf{High Performance}: Techniques like AdaBoost and Gradient Boosting excel in various competitions and real-world applications.
    \item \textbf{Overfitting}: Care is needed to avoid overfitting, especially with complex weak learners or excessive iterations.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Boosting Techniques - Conclusion}
  Boosting techniques, such as AdaBoost and Gradient Boosting, are powerful supervised learning methods that enhance model accuracy through sequential learning by reducing bias. Mastering these methods is key to improving predictive modeling outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stacking Explained - Overview}
    \begin{block}{What is Stacking?}
        Stacking, or stacked generalization, is an ensemble learning technique that combines multiple models to enhance predictive performance beyond any single model. 
    \end{block}
    \begin{itemize}
        \item Multiple predictive models (base learners) are combined.
        \item A meta-learner is used to blend the predictions for the final output.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Stacking Works}
    \begin{enumerate}
        \item \textbf{Model Training:}
        \begin{itemize}
            \item Train diverse models (e.g., decision trees, neural networks).
            \item Each model learns unique patterns from the data.
        \end{itemize}

        \item \textbf{Generating Predictions:}
        \begin{itemize}
            \item Base models predict outcomes on a validation dataset.
        \end{itemize}

        \item \textbf{Meta-Model Training:}
        \begin{itemize}
            \item Predictions of the base models create a new dataset for the meta-learner.
            \item A simpler model is trained to combine these predictions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Stacking}
    \begin{block}{Models Used}
        \begin{itemize}
            \item Model A: Decision Tree
            \item Model B: Support Vector Machine (SVM)
            \item Model C: Neural Network
        \end{itemize}
    \end{block}

    \begin{block}{Predictions from Models}
        \begin{itemize}
            \item Model A predicts: [0.7, 0.2, 0.9]
            \item Model B predicts: [0.6, 0.3, 0.8]
            \item Model C predicts: [0.8, 0.1, 0.85]
        \end{itemize}
    \end{block}

    \begin{block}{Meta-learner Features}
        Features used for meta-learner:
        \[
        \text{Features} = 
        \begin{bmatrix}
            0.7 & 0.6 & 0.8 \\
            0.2 & 0.3 & 0.1 \\
            0.9 & 0.8 & 0.85
        \end{bmatrix}
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points of Stacking}
    \begin{itemize}
        \item \textbf{Diversity of Models:} Improves understanding of varied patterns.
        \item \textbf{Layered Learning:} Adds a meta-learner to optimize aggregate predictions.
        \item \textbf{Flexibility:} Supports diverse model combinations based on requirements.
        \item \textbf{Performance Improvement:} Often leads to better accuracy and robustness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula for Meta-Learner Prediction}
    If we denote \(y_i\) as the original target label and \(f_k(x)\) as the prediction from the k-th base model, the meta-learner predicts as follows:
    \begin{equation}
        \hat{y} = g(f_1(x), f_2(x), \ldots, f_k(x))
    \end{equation}
    where \(g\) is the meta-learner function.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Stacking}
    \begin{lstlisting}[language=Python]
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Define base models
base_models = [
    ('dt', DecisionTreeClassifier()),
    ('svm', SVC(probability=True))
]

# Meta-learner
meta_model = LogisticRegression()

# Create a stacking model
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# Fit the model to your data
stacking_model.fit(X_train, y_train)

# Make predictions
predictions = stacking_model.predict(X_test)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Stacking is a powerful ensemble method that employs unique strengths of various models through meta-learning. This approach enhances prediction capability and generalizes better on unseen data, making it highly effective in machine learning applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Ensemble Methods - Overview}
    \begin{block}{Advantages of Using Ensemble Methods}
        Ensemble methods combine multiple predictive models to enhance overall performance.
    \end{block}
    
    \begin{itemize}
        \item Improved Accuracy
        \item Robustness
        \item Generalization
        \item Flexibility
        \item Handling Imbalanced Datasets
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Ensemble Methods - Improved Accuracy}
    \begin{block}{Improved Accuracy}
        - **Explanation:** By aggregating predictions from various models, ensemble methods often yield higher accuracy than individual models.
    \end{block}

    \begin{exampleblock}{Example}
        Suppose we have three classifiers (A, B, and C). If A predicts "Yes" 70\%, B predicts "Yes" 60\%, and C predicts "Yes" 80\%, a simple majority voting ensemble would predict "Yes" with higher confidence.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Ensemble Methods - Robustness and Generalization}
    \begin{block}{Robustness}
        - **Explanation:** Ensemble methods mitigate the risk of overfitting and underfitting by balancing each other's weaknesses.
        - **Illustration:** If one model is overly sensitive to noise, the ensemble averages out errors, leading to more reliable results.
    \end{block}
    
    \begin{block}{Generalization}
        - **Explanation:** Combining models results in better generalization to unseen data.
        - **Key Point:** Generalization performance can be quantified by the \textbf{bias-variance trade-off}. Ensemble methods lower variance and improve performance across different datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Ensemble Methods - Flexibility and Handling Imbalanced Datasets}
    \begin{block}{Flexibility}
        - **Explanation:** Ensemble methods can effectively combine models from different families, utilizing the strengths of various algorithms.
        - **Example:** In Stacking, different models are trained and their predictions are used as input for a final model, harnessing different methodologies.
    \end{block}
    
    \begin{block}{Handling Imbalanced Datasets}
        - **Explanation:** Ensembles can improve performance on minority classes by assigning different weights to classes or modifying prediction thresholds.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Summary Key Points}
        \begin{itemize}
            \item Ensemble methods improve performance by leveraging diverse model predictions.
            \item They offer robustness against noise and variation.
            \item Enhanced generalization to new, unseen data leads to increased usability in practical applications.
        \end{itemize}
    \end{block}
    
    \textbf{Conclusion:} Ensemble methods are powerful tools in supervised learning that provide enhanced accuracy, robustness, and generalization capabilities.
    
    For further exploration, refer to common ensemble algorithms such as Random Forests, XGBoost, and LightGBM which exemplify these advantages in practice.
\end{frame}

\begin{frame}
    \frametitle{Common Algorithms in Ensemble Learning}
    \begin{block}{Overview}
        Ensemble learning combines predictions from multiple base models to create a stronger overall model, leveraging the "wisdom of the crowd."
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Ensemble Learning Algorithms}
    \begin{enumerate}
        \item \textbf{Random Forests}
        \item \textbf{XGBoost (Extreme Gradient Boosting)}
        \item \textbf{LightGBM}
        \item \textbf{AdaBoost (Adaptive Boosting)}
        \item \textbf{Bagging (Bootstrap Aggregating)}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Random Forests}
    \begin{itemize}
        \item \textbf{Description}: An ensemble of decision trees using the bagging method, reducing overfitting.
        \item \textbf{Use Case}: Ideal for classification tasks like predicting customer churn.
        \item \textbf{Key Feature}: Handles both classification and regression, resistant to noise.
    \end{itemize}
    \begin{block}{Code Example}
        \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{XGBoost and LightGBM}
    \begin{itemize}
        \item \textbf{XGBoost}:
        \begin{itemize}
            \item \textbf{Description}: Boosting algorithm improving performance through sequentially focusing on prior errors.
            \item \textbf{Use Case}: Effective in competitions and structured data scenarios.
            \item \textbf{Key Feature}: Regularization to reduce overfitting.
        \end{itemize}
        \begin{block}{Code Example}
            \begin{lstlisting}[language=Python]
import xgboost as xgb
model = xgb.XGBClassifier()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
            \end{lstlisting}
        \end{block}
        
        \item \textbf{LightGBM}:
        \begin{itemize}
            \item \textbf{Description}: Efficient histogram-based boosting algorithm, designed for speed and performance.
            \item \textbf{Use Case}: Used in big data scenarios for real-time predictions.
            \item \textbf{Key Feature}: Supports parallel learning, handles categorical features natively.
        \end{itemize}
        \begin{block}{Code Example}
            \begin{lstlisting}[language=Python]
import lightgbm as lgb
dtrain = lgb.Dataset(X_train, label=y_train)
params = {'objective': 'binary'}
model = lgb.train(params, dtrain)
predictions = model.predict(X_test)
            \end{lstlisting}
        \end{block}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{AdaBoost and Bagging}
    \begin{itemize}
        \item \textbf{AdaBoost}:
        \begin{itemize}
            \item \textbf{Description}: Reweights instances based on previous classifiers' errors.
            \item \textbf{Use Case}: Works well for binary classification problems.
            \item \textbf{Key Feature}: Converts weak learners into strong ones by focusing on difficult instances.
        \end{itemize}
        \begin{block}{Code Example}
            \begin{lstlisting}[language=Python]
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier(n_estimators=50)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
            \end{lstlisting}
        \end{block}
        
        \item \textbf{Bagging}:
        \begin{itemize}
            \item \textbf{Description}: Generates multiple datasets through bootstrap sampling.
            \item \textbf{Use Case}: Can be applied to any algorithm, with Random Forest being a specific example.
            \item \textbf{Key Feature}: Reduces variance and improves stability.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Diversity Matters}: Performance relies on the diversity of individual models.
        \item \textbf{Handling Bias and Variance}: Balances bias and variance by combining multiple learners.
        \item \textbf{Hyperparameter Tuning}: Essential for optimizing ensemble algorithm performance.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Ensemble learning algorithms like Random Forests, XGBoost, LightGBM, and AdaBoost draw on multiple models' strengths to enhance predictive performance, proving invaluable across various domains, from research to industry applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics for Ensemble Models}
    \begin{block}{Understanding Performance Metrics}
        To effectively evaluate the performance of ensemble models such as Random Forests, XGBoost, and LightGBM, we use several key performance metrics. 
        These metrics help us understand both the overall effectiveness of our model and its strengths and weaknesses in specific scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Accuracy}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correct predictions to the total predictions made.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Predictions}}
            \end{equation}
            \item \textbf{Use Case}: Best for balanced datasets. In cases of class imbalance, accuracy alone can be misleading.
        \end{itemize}
        \item \textbf{Example}: If an ensemble model correctly classifies 80 out of 100 instances, then its accuracy is 80\%.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Precision, Recall, and F1 Score}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Precision}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted positive observations to the total predicted positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Use Case}: Crucial when the cost of false positives is high, such as in spam detection.
        \end{itemize}
        \item \textbf{Example}: If out of 50 predicted spam emails, 30 were actual spam, then the precision is 60\%.
        
        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted positive observations to all actual positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Use Case}: Important in cases like disease screening, where missing a positive instance can have significant consequences.
        \end{itemize}
        \item \textbf{Example}: If there are 100 actual spam emails and the model detects 70, the recall is 70\%.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - F1 Score}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of precision and recall, offering a balance between both.
            \item \textbf{Formula}:
            \begin{equation}
                \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Use Case}: Ideal for datasets with significant class imbalance where both false positives and false negatives are critical.
        \end{itemize}
        \item \textbf{Example}: If precision is 0.6 and recall is 0.7, the F1 score would be approximately 0.65.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Ensemble methods can improve model performance; selecting the right performance metrics based on the application and specific dataset characteristics is essential.
        \item Using multiple metrics provides a more comprehensive view of an ensemble model's performance, facilitating better-informed decision-making.
    \end{itemize}
    \begin{block}{Conclusion}
        By properly understanding and implementing these performance metrics, data scientists can accurately assess and enhance the efficacy of their ensemble learning models in various applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Use Cases of Ensemble Learning - Overview}
  \begin{block}{Introduction to Ensemble Learning}
    Ensemble learning combines multiple models to improve performance, reduce overfitting, and enhance the robustness of predictions.
    By leveraging the strengths of various algorithms, ensemble methods can yield better results than individual models.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Use Cases of Ensemble Learning - Applications}
  \begin{enumerate}
    \item \textbf{Finance and Banking}
      \begin{itemize}
        \item \textbf{Credit Scoring:} Predicting credit risk by combining classification models enhances accuracy.
        \item \textbf{Fraud Detection:} Gradient Boosting analyzes transaction patterns to identify fraud.
      \end{itemize}
    
    \item \textbf{Healthcare}
      \begin{itemize}
        \item \textbf{Disease Diagnosis:} Aggregating predictions from classifiers improves diagnostic accuracy.
        \item \textbf{Patient Readmission Risk:} Predicting readmission risk using models like Random Forests enables timely interventions.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Use Cases of Ensemble Learning - Continued Applications}
  \begin{enumerate}
    \setcounter{enumi}{2}  % Start from the 3rd item
    \item \textbf{Marketing}
      \begin{itemize}
        \item \textbf{Customer Segmentation:} Clustering customers to tailor marketing strategies.
        \item \textbf{Churn Prediction:} Forecasting customer retention challenges using models like AdaBoost.
      \end{itemize}
    
    \item \textbf{E-commerce}
      \begin{itemize}
        \item \textbf{Recommendation Systems:} Combining filtering methods for personalized experiences.
        \item \textbf{Sales Forecasting:} Analyzing sales factors using Decision Tree ensembles for accurate predictions.
      \end{itemize}

    \item \textbf{Telecommunications}
      \begin{itemize}
        \item \textbf{Network Intrusion Detection:} Enhancing security by improving detection rates through diverse models.
        \item \textbf{Quality of Service Prediction:} Forecasting service issues with multiple predictive models.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Conclusion}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item \textbf{Versatility:} Applicable across various domains, showcasing adaptability.
      \item \textbf{Enhanced Performance:} Aggregation improves accuracy and robustness compared to single models.
      \item \textbf{Algorithm Diversity:} Effectiveness depends on diversity among base learners.
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    Ensemble learning presents significant potential across industries, helping solve complex problems and optimizing decision-making processes.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Ensemble Learning - Overview}
    \begin{block}{Overview}
        Ensemble learning methods, while powerful and popular, come with their own set of challenges. Understanding these challenges is crucial for successful implementation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Ensemble Learning - Key Challenges (Part 1)}
    \begin{enumerate}
        \item \textbf{Overfitting}
            \begin{itemize}
                \item \textbf{Concept}: Overfitting occurs when an ensemble model learns the noise in the training data instead of the underlying pattern.
                \item \textbf{Example}: An ensemble of deep decision trees captures intricate patterns but performs poorly on unseen data.
                \item \textbf{Mitigation Strategies}: Use pruning, limit tree depth, or apply regularization.
            \end{itemize}
        
        \item \textbf{Increased Computation Time}
            \begin{itemize}
                \item \textbf{Concept}: Combining multiple models increases training and prediction time.
                \item \textbf{Example}: A Random Forest with hundreds of decision trees demands significant computational resources.
                \item \textbf{Consideration}: Assess latency for real-time applications.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Ensemble Learning - Key Challenges (Part 2)}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Complexity of Implementation}
            \begin{itemize}
                \item \textbf{Concept}: Implementing and tuning ensemble methods can be complex.
                \item \textbf{Example}: In stacked generalization, choose base and meta-learners and tune their hyperparameters carefully.
                \item \textbf{Tip}: Utilize libraries like Scikit-learn for built-in functions for ensemble techniques.
            \end{itemize}
        
        \item \textbf{Diminished Returns}
            \begin{itemize}
                \item \textbf{Concept}: Adding models does not always lead to proportional performance improvement.
                \item \textbf{Example}: An ensemble of three models may yield more significant accuracy than a single model, but adding more may yield minimal gains.
            \end{itemize}
        
        \item \textbf{Model Diversity}
            \begin{itemize}
                \item \textbf{Concept}: An ensemble's effectiveness depends on the diversity of individual models.
                \item \textbf{Example}: Combining multiple models of the same type may not capture varied perspectives.
                \item \textbf{Recommendation}: Use different algorithms or data subsets to enhance model diversity.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Implementing Ensemble Methods}
    \begin{block}{Introduction to Ensemble Methods}
        Ensemble methods combine multiple learning algorithms to achieve better predictive performance than individual models. They leverage diversity in models to enhance overall accuracy and robustness.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Implementing Ensemble Methods - Part 1}
    \begin{enumerate}
        \item \textbf{Choose the Right Base Learners:}
            \begin{itemize}
                \item \textit{Diversity is Key}: Select models imposing different types of errors. 
                \item \textit{Example}: Combining decision trees with linear models can yield complementary strengths.
            \end{itemize}
        
        \item \textbf{Utilize Bagging \& Boosting Approaches:}
            \begin{itemize}
                \item \textbf{Bagging (Bootstrap Aggregating)}: Reduces variance by training models on different data subsets. \\
                    \textit{Example}: Random Forest trains numerous decision trees on random samples and averages predictions.
                
                \item \textbf{Boosting}: Focuses on misclassified instances iteratively. \\
                    \textit{Example}: Gradient Boosting Machines refine weak learners for better performance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Implementing Ensemble Methods - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Monitor for Overfitting:}
            \begin{itemize}
                \item Use k-fold cross-validation for validation.
                \item Monitor performance metrics to detect overfitting.
            \end{itemize}
        
        \item \textbf{Combine Models Wisely:}
            \begin{itemize}
                \item Techniques like Voting, Averaging, or Stacking can be employed.
                \item \textit{Voting}: Majority voting for classification.
                \item \textit{Averaging}: Average predictions for regression.
                \item \textit{Stacking}: Build a meta-model on base model outputs.
            \end{itemize}
        
        \item \textbf{Feature Management:}
            \begin{itemize}
                \item Carefully engineer features to minimize sensitivity to irrelevant features.
                \item Use PCA for dimensionality reduction.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Implementing Ensemble Methods - Summary}
    \begin{enumerate}
        \setcounter{enumi}{6}
        \item \textbf{Evaluate Performance Effectively:}
            \begin{itemize}
                \item Use metrics like precision, recall, F1-score, and ROC-AUC for classification; RMSE for regression.
                \item Maintain a consistent evaluation framework.
            \end{itemize}
        
        \item \textbf{Conclusion:}
            Implementing ensemble methods effectively boosts model performance by leveraging diversity, fine-tuning hyperparameters, and using robust evaluation strategies.
        
        \item \textbf{Key Takeaways:}
            \begin{itemize}
                \item Choose diverse base learners.
                \item Utilize bagging and boosting.
                \item Monitor for overfitting.
                \item Combine model predictions wisely.
                \item Optimize features and hyperparameters.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example: Random Forest in Python}
    \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load dataset
X, y = load_data()  # Assume load_data() is a predefined function
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)
print(f'Model Accuracy: {accuracy:.2f}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Takeaways - Overview}
    \begin{block}{Overview of Ensemble Learning Methods}
        Ensemble learning methods combine multiple models to improve overall performance in supervised learning tasks. 
        The core idea is to leverage the strengths of diverse algorithms to enhance robustness and accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Takeaways - Definition and Types}
    \begin{enumerate}
        \item \textbf{Definition of Ensemble Learning}
        \begin{itemize}
            \item \textbf{Ensemble Learning}: Technique where multiple models (base learners) are trained to solve the same problem and combine their predictions for better outcomes.
            \item Individual models might perform poorly alone, but together they tend to achieve greater accuracy.
        \end{itemize}

        \item \textbf{Types of Ensemble Methods}
        \begin{itemize}
            \item \textbf{Bagging (Bootstrap Aggregating)}: Reduces variance by training multiple models on different subsets of the dataset. 
            \begin{itemize}
                \item \textit{Example:} Random Forests.
            \end{itemize}
    
            \item \textbf{Boosting}: Sequentially builds models focusing on errors of previous models. 
            \begin{itemize}
                \item \textit{Example:} AdaBoost and Gradient Boosting.
            \end{itemize}
    
            \item \textbf{Stacking}: Combines predictions using a new model (meta-learner) trained on these outputs. 
            \begin{itemize}
                \item \textit{Example:} Logistic regression combining outputs of decision trees and support vector machines.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Takeaways - Benefits and Implementation}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue count from previous frame
        \item \textbf{Benefits of Ensemble Techniques}
        \begin{itemize}
            \item \textbf{Improved Accuracy}: More robust predictions due to collective decision-making.
            \item \textbf{Reduced Overfitting}: Less sensitivity to noise by averaging multiple models.
            \item \textbf{Versatility}: Applicable to various learning problems (classification, regression, etc.).
        \end{itemize}

        \item \textbf{Key Formulas}
        \begin{equation}
            \hat{y}_{ensemble} = \sum_{i=1}^{N} w_i \cdot \hat{y}_i
        \end{equation}
        \begin{itemize}
            \item Where $\hat{y}_{ensemble}$ is the final prediction, $w_i$ is the weight of each model, and $\hat{y}_i$ is each individual model's prediction.
        \end{itemize}

        \item \textbf{Practical Implementation Tips}
        \begin{itemize}
            \item Assess the diversity of base models.
            \item Tune hyperparameters for each model.
            \item Utilize cross-validation for performance evaluation.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A}
    \begin{block}{Overview of Ensemble Learning Techniques}
        Ensemble learning methods combine multiple models to achieve stronger predictive performance than individual models can provide. Key advantages include:
        \begin{itemize}
            \item \textbf{Reduce Overfitting:} Averaging predictions from multiple models reduces the risk of overfitting.
            \item \textbf{Improve Accuracy:} Different models capture various data patterns, resulting in more robust predictions.
            \item \textbf{Enhance Stability:} Models tend to be more stable in their predictions despite variations in training data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ensemble Methods}
    \begin{itemize}
        \item \textbf{Bagging (Bootstrap Aggregating):}
            \begin{itemize}
                \item Example: Random Forest
                \item Constructs multiple decision trees using random subsets of data, averaging their predictions.
            \end{itemize}
        
        \item \textbf{Boosting:}
            \begin{itemize}
                \item Example: AdaBoost
                \item Sequentially focuses on previous model errors, adjusting weights iteratively.
            \end{itemize}
        
        \item \textbf{Stacking:}
        \begin{itemize}
            \item Combines predictions from multiple models using a meta-learner for final predictions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Points and Engagement}
    \begin{block}{Discussion Points}
        \begin{enumerate}
            \item \textbf{Applications in Real-World Scenarios:} 
            Industries include finance (credit scoring), healthcare (diagnostic predictions), and retail (sales forecasting).
            \item \textbf{Pros and Cons of Different Methods:}
            Bagging is useful for variance reduction; boosting excels in bias reduction.
            \item \textbf{Implementation Considerations:}
            Focus on performance metrics, computational efficiency, and model interpretation.
        \end{enumerate}
    \end{block}

    \begin{block}{Engage the Audience}
        \begin{itemize}
            \item What challenges have you faced when implementing ensemble methods?
            \item Are there situations where ensemble techniques may not be ideal?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrap-Up}
    \begin{block}{Closing Statement}
        Make use of ensemble learning techniques to enhance your models! Feel free to share your questions or insights on the applications and implications of these powerful tools in supervised learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Steps in Learning - Overview}
  As we transition to the next chapter, we will delve deeper into the practical applications and advanced concepts related to ensemble learning methods. Expect to explore the following key areas:
  
  \begin{enumerate}
    \item \textbf{Implementing Ensemble Methods}:
      \begin{itemize}
        \item Practical coding examples using libraries like scikit-learn in Python.
        \item How to build and evaluate different ensemble models such as Random Forests, Gradient Boosting, and AdaBoost.
      \end{itemize}
      
    \item \textbf{Hyperparameter Tuning}:
      \begin{itemize}
        \item Understanding the concept of hyperparameters.
        \item Techniques for optimizing model performance, including grid search and random search approaches.
      \end{itemize}
      
    \item \textbf{Comparative Analysis}:
      \begin{itemize}
        \item Evaluating ensemble methods against single learning algorithms.
        \item Key performance metrics to consider, such as accuracy, precision, recall, and F1 score.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Steps in Learning - Continued}
  \begin{enumerate}[resume]
    \item \textbf{Real-World Applications}:
      \begin{itemize}
        \item Case studies demonstrating ensemble learning in various fields (e.g., healthcare for disease prediction, finance for credit scoring).
      \end{itemize}
      
    \item \textbf{Challenges and Solutions}:
      \begin{itemize}
        \item Discuss potential pitfalls in ensemble learning, such as overfitting and computational efficiency.
        \item Strategies to mitigate these issues.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Encouragement for Exploration}
  We strongly encourage you to engage with hands-on exercises and projects related to ensemble learning. Experimenting with:
  
  \begin{itemize}
    \item \textbf{Building your own ensemble models}: Start with datasets from sources like Kaggle or the UCI Machine Learning Repository.
    \item \textbf{Utilizing notebooks}: Run examples in Jupyter Notebooks for an interactive learning experience.
  \end{itemize}

  Here’s a simple outline of a code snippet to create a Random Forest model using scikit-learn:

  \begin{block}{Example Code}
  \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load your dataset
# X, y = load_data_function() 

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred))
  \end{lstlisting}
  \end{block}
\end{frame}


\end{document}