\frametitle{Conclusion - Summary and Important Formula}
  \begin{block}{Summary}
    Decision trees are a foundational technique in supervised learning, known for their interpretability and versatility. While they handle different data types well, it is essential to be aware of their limitations for effective use.
  \end{block}

  \begin{block}{Important Formula: Gini Impurity}
    The Gini Impurity for decision node split is given as:
    \begin{equation}
      Gini(D) = 1 - \sum_{i=1}^{C} (p_i)^2
    \end{equation}
    Where $p_i$ represents the proportion of classes in dataset $D$.
  \end{block}

  \begin{block}{Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize Decision Tree Classifier and fit to the data
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train, y_train)

# Make predictions
predictions = clf.predict(X_test)
    \end{lstlisting}
  \end{block}
