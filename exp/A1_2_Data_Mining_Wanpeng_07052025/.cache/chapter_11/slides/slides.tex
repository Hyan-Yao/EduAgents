\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning}
    \begin{block}{Overview of Unsupervised Learning}
        Unsupervised learning is a category of machine learning techniques that deals with data without labeled responses. 
        The objective is to find hidden structures or patterns in unlabeled data, facilitating insights that inform decision-making processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Data Mining}
    \begin{enumerate}
        \item \textbf{Pattern Recognition:}
        \begin{itemize}
            \item Identifies structures in data that may not be immediately apparent.
            \item Example: Customer segmentation based on purchasing behavior.
        \end{itemize}

        \item \textbf{Dimensionality Reduction:}
        \begin{itemize}
            \item Reduces data volume while preserving essential information.
            \item Techniques: PCA, t-SNE.
            \item Example: Visualizing images in 2D space.
        \end{itemize}

        \item \textbf{Anomaly Detection:}
        \begin{itemize}
            \item Identifies outliers within datasets.
            \item Example: Fraud detection in financial transactions.
        \end{itemize}

        \item \textbf{Feature Learning:}
        \begin{itemize}
            \item Discovers features from raw data for predictive modeling.
            \item Example: Autoencoders in deep learning.
        \end{itemize}

        \item \textbf{Data Preprocessing:}
        \begin{itemize}
            \item Assists with cleaning and organizing data.
            \item Example: Grouping items to handle missing values.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Examples}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{No Labeled Data Required:} Suitable for exploratory analysis.
            \item \textbf{Versatility:} Applicable in finance, healthcare, marketing.
            \item \textbf{Foundation for Advanced Techniques:} Supports generative models and clustering algorithms.
        \end{itemize}
    \end{block}

    \begin{block}{Example of Clustering}
        Imagine a dataset representing fruits based on features like weight, color, and sweetness. 
        Unsupervised learning can cluster these fruits into groups like citrus and berries without labeled categories.
    \end{block}

    \begin{equation}
        J = \sum_{i=1}^{k} \sum_{j=1}^{n} \left \| x_j^{(i)} - \mu_i \right \|^2
    \end{equation}
    where:
    \begin{itemize}
        \item \( J \) = total cost function,
        \item \( n \) = number of data points,
        \item \( x_j^{(i)} \) = data point,
        \item \( \mu_i \) = centroid of cluster \( i \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Unsupervised learning is essential for understanding complex datasets, extracting meaningful information without labels. 
        Techniques like clustering, dimensionality reduction, and anomaly detection are vital in a data scientist's toolkit, enabling insights that drive impactful decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Models Overview - Definition}
    
    \begin{block}{Definition of Generative Models}
        Generative models are a category of unsupervised learning algorithms that learn the underlying distribution of data. Unlike discriminative models focused on predicting labels given features, generative models aim to generate new data points that resemble a training dataset.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Learning the Data Distribution:} Generative models estimate the joint probability \( P(X, Y) \).
        \item \textbf{Example Classes:}
        \begin{itemize}
            \item Gaussian Mixture Models (GMMs)
            \item Hidden Markov Models (HMMs)
            \item Generative Adversarial Networks (GANs)
            \item Variational Autoencoders (VAEs)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Models Overview - Rationale}
    
    \begin{block}{Rationale Behind Generative Models}
        The rationale for using generative models in unsupervised learning includes:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Data Augmentation:} They can synthesize new, realistic instances of data, useful for training other models.
        \item \textbf{Understanding Data Structure:} They help discover underlying data structures and relationships.
        \item \textbf{Flexible Applications:} Applicable across various domains, including image synthesis and anomaly detection.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Models Overview - Formulation and Example}
    
    \begin{block}{Formulation}
        Generative models often leverage principles from probability theory:
        \begin{equation}
            P(X) = \sum_{y} P(X|Y=y) P(Y=y)
        \end{equation}
        
        Where:
        \begin{itemize}
            \item \( P(X|Y=y) \) is the likelihood of observing data \( X \) given the class \( Y \).
            \item \( P(Y=y) \) is the prior probability of class \( Y \).
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
from sklearn.mixture import GaussianMixture
import numpy as np

# Sample data
data = np.random.rand(100, 2)

# Fit a Gaussian Mixture Model
gmm = GaussianMixture(n_components=2)
gmm.fit(data)

# Generating new samples
new_samples = gmm.sample(10)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Differences Between Generative and Discriminative Models - Overview}
  
  \begin{block}{Overview}
    Generative and discriminative models are two fundamental approaches in machine learning, particularly in unsupervised learning tasks. Understanding their differences helps in selecting appropriate models for specific tasks.
  \end{block}

  \begin{enumerate}
    \item \textbf{Generative Models} - Learn the joint probability distribution \( P(X, Y) \).
    \item \textbf{Discriminative Models} - Estimate the conditional probability \( P(Y | X) \).
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Differences Between Generative and Discriminative Models - Key Differences}
  
  \begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Feature} & \textbf{Generative Models} & \textbf{Discriminative Models} \\
      \hline
      Objective & Model the data distribution & Model the decision boundary between classes \\
      \hline
      Output & Generate new instances of data & Classify existing instances \\
      \hline
      Training & Requires modeling of entire input space & Requires labeling of input data \\
      \hline
      Flexibility & More flexible, handles missing data & Generally more efficient for classification \\
      \hline
      Computational Cost & Typically more complex & Often less computationally intensive \\
      \hline
      Examples & Naive Bayes, GANs & Logistic Regression, SVMs \\
      \hline
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Differences Between Generative and Discriminative Models - Key Points}
  
  \begin{itemize}
    \item \textbf{Data Generation vs. Classification}: 
    \begin{itemize}
      \item Generative models synthesize new data (e.g., image generation).
      \item Discriminative models focus on classification (e.g., predicting classes).
    \end{itemize}
    
    \item \textbf{Modeling Complexity}:
    \begin{itemize}
      \item Generative models often involve complicated structures.
      \item Discriminative models focus directly on class boundaries.
    \end{itemize}

    \item \textbf{Use Cases}:
    \begin{itemize}
      \item Generative models for creative generation.
      \item Discriminative models for accurate classification.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conceptual Example in Python - Discriminative Model}
  
  \begin{block}{Example Code Snippet}
    \begin{lstlisting}[language=Python]
# Example of a simple conditional model fitting using a Discriminative approach
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# Load dataset
X, y = load_iris(return_X_y=True)

# Train a Discriminative Model
model = LogisticRegression()
model.fit(X, y)

# Making predictions
predictions = model.predict(X)
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Generative Models}
    \begin{block}{Overview}
        Generative models are powerful unsupervised learning techniques that model and generate new data points. They learn data distributions enabling significant applications across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Generative Models}
    \begin{enumerate}
        \item \textbf{Image Synthesis}
        \begin{itemize}
            \item \textbf{Concept:} Create new images resembling real ones.
            \item \textbf{Example:} GANs generate photorealistic images (e.g., "This Person Does Not Exist").
            \item \textbf{Illustration:} Original vs. GAN-generated images.
        \end{itemize}

        \item \textbf{Text Generation}
        \begin{itemize}
            \item \textbf{Concept:} Producing human-like text (Natural Language Generation).
            \item \textbf{Example:} OpenAI's GPT-3 generating essays or stories from prompts.
            \item \textbf{Illustration:} Prompt with sample text generated by GPT-3.
        \end{itemize}

        \item \textbf{Anomaly Detection}
        \begin{itemize}
            \item \textbf{Concept:} Learning normal data patterns to identify outliers.
            \item \textbf{Example:} VAEs identify unusual network behavior for security.
            \item \textbf{Illustration:} Diagram of normal vs. anomalous data points.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Versatility:} Adaptable to various data forms (images, text, etc.).
        \item \textbf{Realism:} High fidelity outputs useful for real-world applications.
        \item \textbf{Learning Distributions:} Focus on understanding data distributions to generate novel instances.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas \& Concepts}
    \begin{block}{Basic Equation}
        For a generative model \( p(x) \), we estimate the joint probability \( p(x, y) \) to understand the feature distribution.
    \end{block}

    \begin{block}{In GANs}
        The generator \( G \) creates fake samples \( z' \) from random noise \( z \), while the discriminator \( D \) differentiates between real \( x \) and fake samples:
        \begin{equation}
            \text{Loss}_D = -\mathbb{E}[\log D(x)] - \mathbb{E}[\log(1 - D(G(z)))]
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Generative models exhibit a wide range of applications transforming industries through innovative data generation, text synthesis, and anomaly detection. Understanding these applications highlights the practical implications of generative modeling techniques in real-world scenarios.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Generative Models - Introduction}
    \begin{block}{Introduction to Generative Models}
        Generative models are statistical models that generate new data points based on learned distributions from training datasets. They capture the fundamental characteristics of data distributions, allowing for the generation of samples that resemble the training data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Generative Models - GMM and HMM}
    \begin{enumerate}
        \item \textbf{Gaussian Mixture Models (GMM)}  
        \begin{itemize}
            \item \textbf{Definition}: Assumes data is generated from a mixture of Gaussian distributions.
            \item \textbf{Components}:
            \begin{itemize}
                \item $K$: Number of Gaussian components
                \item Weights: Each component's relative importance
                \item Means and Covariances: Mean vector and covariance matrix for each Gaussian
            \end{itemize}
            \item \textbf{Use Case}: Cluster points into K groups based on similarity.
            \item \textbf{Example}: Modeling phonemes in speech recognition.
            \item \textbf{Formula}: 
            \begin{equation}
                P(x) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(x \mid \mu_k, \Sigma_k)
            \end{equation}
        \end{itemize}
        
        \item \textbf{Hidden Markov Models (HMM)}  
        \begin{itemize}
            \item \textbf{Definition}: Models assume a hidden process generates observable events.
            \item \textbf{Components}:
            \begin{itemize}
                \item States: Hidden states producing observable data
                \item Observations: Actual data observed
                \item Transition Probabilities: Moving from one hidden state to another
                \item Emission Probabilities: Observable events given a hidden state
            \end{itemize}
            \item \textbf{Use Case}: Part-of-speech tagging in natural language processing.
            \item \textbf{Example}: Weather states and their observed effects.
            \item \textbf{Key Concept}: Future state depends only on the current state (Markov assumption).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Generative Models - VAE}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        \item \textbf{Variational Autoencoders (VAE)}  
        \begin{itemize}
            \item \textbf{Definition}: Generative models using deep learning with an encoder-decoder architecture.
            \item \textbf{Components}:
            \begin{itemize}
                \item Encoder: Maps input data to latent space (mean and variance)
                \item Latent Space: Compressed representation of input data
                \item Decoder: Maps latent points back to data space
            \end{itemize}
            \item \textbf{Use Case}: Image and text generation, anomaly detection.
            \item \textbf{Example}: Generating new images similar to a training set.
            \item \textbf{Key Formula}:
            \begin{equation}
                L(x) = -E_{q(z|x)}[\log(p(x|z))] + D_{KL}(q(z|x) \| p(z))
            \end{equation}
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Generative models are essential for data understanding and synthesis.
            \item Each model serves unique purposes:
            \begin{itemize}
                \item GMM for clustering
                \item HMM for sequential data
                \item VAE for complex data generation
            \end{itemize}
            \item Mathematical foundations lead to practical applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Gaussian Mixture Models (GMM) - Overview}
    \begin{block}{Overview of GMM}
        Gaussian Mixture Models (GMMs) are probabilistic models that represent a dataset assuming multiple overlapping Gaussian distributions, useful for modeling complex data. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Gaussian Mixture Models (GMM) - Key Components}
    \begin{enumerate}
        \item \textbf{Gaussian Components}: 
        \begin{equation}
            p(x | \mu_k, \Sigma_k) = \frac{1}{\sqrt{(2\pi)^d |\Sigma_k|}} e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}
        \end{equation}
        \item \textbf{Mixing Coefficients}: 
        \begin{equation}
            \sum_{k=1}^K \pi_k = 1
        \end{equation}
        \item \textbf{Overall GMM Distribution}:
        \begin{equation}
            p(x) = \sum_{k=1}^K \pi_k p(x | \mu_k, \Sigma_k)
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Gaussian Mixture Models (GMM) - Data Generation}
    \begin{block}{Generating New Data Points}
        \begin{itemize}
            \item \textbf{Sampling Process}:
            \begin{itemize}
                \item Select a Gaussian component \( k \) based on mixing coefficients \( \pi_k \).
                \item Generate a sample from the selected Gaussian distribution defined by \( \mu_k \) and \( \Sigma_k \).
            \end{itemize}
            \item \textbf{Example}:
            \begin{itemize}
                \item Infants: \( \mu_1 = 0.6 \) m, \( \Sigma_1 = 0.1^2 \)
                \item Teenagers: \( \mu_2 = 1.6 \) m, \( \Sigma_2 = 0.2^2 \)
                \item Adults: \( \mu_3 = 1.75 \) m, \( \Sigma_3 = 0.1^2 \)
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hidden Markov Models (HMM) - Overview}
    \begin{itemize}
        \item Hidden Markov Models (HMMs) are statistical models used for analyzing and predicting sequential data.
        \item Effective in various domains such as:
        \begin{itemize}
            \item Speech recognition
            \item Natural language processing
            \item Bioinformatics
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hidden Markov Models (HMM) - Key Concepts}
    \begin{enumerate}
        \item \textbf{Markov Process}: 
        \begin{itemize}
            \item Assumes a system with unobserved (hidden) states.
            \item Future state depends only on the present state.
        \end{itemize}

        \item \textbf{Components of HMM}:
        \begin{itemize}
            \item \textbf{States (S)}: N hidden states.
            \item \textbf{Observations (O)}: M distinct observable outcomes.
            \item \textbf{Transition Probabilities (A)}: Matrix \(A[i][j]\) is the transition probability from state \(i\) to state \(j\).
            \item \textbf{Observation Probabilities (B)}: Matrix \(B[j][k]\) is the probability of observing \(k\) given state \(j\).
            \item \textbf{Initial State Probabilities (\(\pi\))}: Vector \(\pi[i]\) represents starting state probabilities.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hidden Markov Models (HMM) - Mathematical Model}
    \begin{block}{Transition Probability Matrix}
        \[
        A = \begin{bmatrix}
        a_{11} & a_{12} & \ldots & a_{1N} \\
        a_{21} & a_{22} & \ldots & a_{2N} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{N1} & a_{N2} & \ldots & a_{NN}
        \end{bmatrix}
        \]
    \end{block}

    \begin{block}{Emission Probability Matrix}
        \[
        B = \begin{bmatrix}
        b_{1}(O_1) & b_{1}(O_2) & \ldots & b_{1}(O_M) \\
        b_{2}(O_1) & b_{2}(O_2) & \ldots & b_{2}(O_M) \\
        \vdots & \vdots & \ddots & \vdots \\
        b_{N}(O_1) & b_{N}(O_2) & \ldots & b_{N}(O_M)
        \end{bmatrix}
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hidden Markov Models (HMM) - Applications and Example}
    \begin{itemize}
        \item \textbf{Applications}:
        \begin{itemize}
            \item \textbf{Speech Recognition}: Models phonemes and letters with states.
            \item \textbf{Natural Language Processing}: Tasks like part-of-speech tagging.
            \item \textbf{Bioinformatics}: Gene prediction and modeling DNA sequences.
        \end{itemize}

        \item \textbf{Example}: Weather Model
        \begin{itemize}
            \item States: Sunny, Rainy, Cloudy
            \item Observations: Activities (e.g., 'go for a walk', 'watch a movie')
            \item HMM predicts future weather or activity recommendations.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Variational Autoencoders (VAE) - Introduction}
    \begin{block}{Introduction to VAEs}
        Variational Autoencoders (VAEs) are powerful generative models that combine neural networks with variational inference to produce complex data distributions.
        Designed to learn a probabilistic representation of input data, VAEs excel in generating new data points similar to the training set.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Variational Autoencoders (VAE) - Key Concepts}
    \begin{itemize}
        \item \textbf{Latent Variables:} VAEs introduce a latent variable $z$ that captures the underlying structure of the data.
        \item \textbf{Encoder-Decoder Architecture:}
        \begin{itemize}
            \item \textbf{Encoder (Recognition Model):} Maps input data $x$ to a distribution $q(z|x)$, compressing data into latent space.
            \item \textbf{Decoder (Generative Model):} Maps the latent variable $z$ back to the data space to reconstruct $x$ from $z$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Variational Autoencoders (VAE) - Mathematical Formalism}
    The VAE optimizes the Evidence Lower Bound (ELBO) on the log likelihood of the data, given by:
    \begin{equation}
    \mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z))
    \end{equation}
    \begin{itemize}
        \item $p_\theta(x|z)$: Likelihood of data given the latent variable (decoder).
        \item $q_\phi(z|x)$: Approximate posterior (encoder).
        \item $p(z)$: Prior distribution over latent variables (often a standard normal distribution).
        \item $D_{KL}$: Kullback-Leibler divergence, measuring how one probability distribution diverges from another.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Variational Autoencoders (VAE) - Applications}
    \begin{itemize}
        \item \textbf{Data Generation:} New data can be generated by sampling from the prior $p(z)$ and decoding it through the decoder $p_\theta(x|z)$.
        \item \textbf{Data Reconstruction:} VAEs effectively reconstruct input data, learning key features and distributions from the input space.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Variational Autoencoders (VAE) - Example}
    Consider a VAE trained on handwritten digits (MNIST dataset):
    \begin{itemize}
        \item \textbf{Latent Space Exploration:} By varying $z$ in the latent space, new digit images that resemble those in the dataset can be generated.
        \item \textbf{Interpolation:} Interpolating between points in the latent space creates smooth transitions between different digit images.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Variational Autoencoders (VAE) - Conclusion}
    \begin{itemize}
        \item VAEs provide a probabilistic framework useful for data generation.
        \item They leverage deep learning with an unsupervised learning approach.
        \item The balance between reconstruction and regularization (via KL divergence) is crucial for successful training.
    \end{itemize}
    \begin{block}{Final Thoughts}
        VAEs bridge the gap between deep learning and probabilistic modeling, offering a contemporary approach to understand and generate complex datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Adversarial Networks (GANs) - Introduction}
    \begin{block}{What are GANs?}
        Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed for generative modeling. Introduced by Ian Goodfellow et al. in 2014, GANs enable the generation of new data instances that resemble the training data.
    \end{block}
    \begin{itemize}
        \item Two neural networks: Generator (G) and Discriminator (D)
        \item Engage in a game-like setting
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Adversarial Networks (GANs) - Adversarial Training Process}
    \begin{block}{The Adversarial Training Process}
        \begin{itemize}
            \item \textbf{Generator (G):} Creates fake data instances, aiming for realistic output.
            \item \textbf{Discriminator (D):} Evaluates the data, distinguishing between real and fake.
        \end{itemize}
    \end{block}
    \begin{enumerate}
        \item Generator creates synthetic data samples.
        \item Discriminator evaluates a mixture of real and fake samples.
        \item Feedback from Discriminator helps Generator enhance output.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Adversarial Networks (GANs) - Loss Functions}
    \begin{block}{Training Dynamics}
        \begin{equation}
            D_{loss} = -E_{x \sim p_{data}(x)}[\log D(x)] - E_{z \sim p_z(z)}[\log(1 - D(G(z)))]
        \end{equation}
        \begin{equation}
            G_{loss} = -E_{z \sim p_z(z)}[\log(D(G(z)))]
        \end{equation}
        \begin{itemize}
            \item $E$: expected value
            \item $p_{data}(x)$: probability distribution of real data
            \item $p_z(z)$: random noise input to the Generator
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Adversarial Networks (GANs) - Impact and Applications}
    \begin{block}{Impact on Generative Modeling}
        \begin{itemize}
            \item \textbf{High-Quality Outputs:} Generates high-resolution images and realistic samples.
            \item \textbf{Diversity:} Can create diverse outputs by sampling different points in the latent space.
            \item \textbf{Applications:} Used in image generation, video synthesis, text-to-image translation, etc.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Adversarial Networks (GANs) - Key Points and Example}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Operate in a zero-sum game scenario (improvement of one affects the other).
            \item Balance between Generator and Discriminator is crucial.
            \item Innovations like Conditional GANs (cGANs) provide controlled generation based on additional information.
        \end{itemize}
    \end{block}
    \begin{example}
        Imagine a dataset of cat images. The Generator learns to create cat-like images from random noise, while the Discriminator differentiates real from generated images.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generative Adversarial Networks (GANs) - Conclusion}
    \begin{block}{Conclusion}
        GANs represent a significant breakthrough in generative modeling, offering a powerful paradigm for creating new, synthetic data across various industries. Their unique adversarial nature enables continuous learning, benefiting neural networks' development.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Generative Models}
    \begin{block}{Overview}
        Generative models learn the underlying distribution of a dataset to generate new data points resembling the original data. Training involves complex techniques and unique challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques in Training Generative Models}
    \begin{enumerate}
        \item \textbf{Adversarial Training (GANs)}:
            \begin{itemize}
                \item GANs involve a generator (G) and a discriminator (D).
                \item G generates fake data; D distinguishes real from fake.
                \item \textbf{Process:}
                    \begin{itemize}
                        \item G minimizes D's accuracy, improving data realism.
                        \item D maximizes its ability to differentiate.
                    \end{itemize}
                \item \textbf{Example:} In image generation, G improves until D cannot easily identify fakes.
            \end{itemize}
        
        \item \textbf{Variational Inference (VAEs)}:
            \begin{itemize}
                \item VAEs encode data into latent space and decode it.
                \item \textbf{Loss Function:}
                \begin{equation}
                    \text{Loss} = \text{Reconstruction Loss} + \beta \cdot \text{KL Divergence}
                \end{equation}
                \item KL Divergence ensures learned distributions remain close to a prior.
            \end{itemize}
        
        \item \textbf{Normalizing Flows}:
            \begin{itemize}
                \item Transform simple distributions (e.g., Gaussian) into complex ones.
                \item Provides exact likelihood evaluation of generated data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Training Generative Models}
    \begin{enumerate}
        \item \textbf{Mode Collapse}:
            \begin{itemize}
                \item A situation where G produces limited diversity (e.g., only generating cats).
            \end{itemize}
            
        \item \textbf{Training Stability}:
            \begin{itemize}
                \item GANs may have unstable training dynamics.
                \item Solutions:
                    \begin{itemize}
                        \item Adjust learning rates and architectures.
                        \item Employ regularization.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Evaluating Model Performance}:
            \begin{itemize}
                \item No single metric can capture quality.
                \item Combine visual assessments with metrics like:
                    \begin{itemize}
                        \item Inception Score (IS)
                        \item Fréchet Inception Distance (FID)
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Training generative models, particularly GANs and VAEs, involves sophisticated techniques aimed at creating high-fidelity outputs. Challenges like mode collapse and stability must be navigated. Proper execution and evaluation are crucial for the success of these models in practical applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Generative Models - Introduction}
    Evaluating generative models is crucial to understanding their performance and quality. A robust evaluation helps ensure that the model accurately captures the underlying distribution of the data it was trained on, leading to useful and reliable outputs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Generative Models - Key Metrics}
    \begin{enumerate}
        \item \textbf{Likelihood-based Metrics}
            \begin{itemize}
                \item \textbf{Log-Likelihood}: Measures how well the model predicts the data. 
                \item \textit{Formula}:
                \begin{equation}
                    \text{Log-Likelihood} = \sum_{x \in \mathcal{X}} \log P(x)
                \end{equation}
                \item \textit{Example}: If a model assigns a likelihood of 0.8 to the actual data points, the log-likelihood is $\log(0.8)$.
            \end{itemize}
        
        \item \textbf{Diversity Metrics}
            \begin{itemize}
                \item \textbf{Frechet Inception Distance (FID)}: Compares the distance between feature vectors of generated samples and real images.
            \end{itemize}
            
        \item \textbf{Inception Score (IS)}
            \begin{itemize}
                \item \textit{Formula}:
                \begin{equation}
                    \text{IS} = \exp(\mathbb{E}_{x \sim P_g} D_{KL}(P(y|x) || P(y)))
                \end{equation}
            \end{itemize}
        
        \item \textbf{KL Divergence}
            \begin{itemize}
                \item \textit{Formula}:
                \begin{equation}
                    D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Generative Models - Methodologies}
    \begin{enumerate}
        \item \textbf{Visual Inspection}
            \begin{itemize}
                \item Generate samples and inspect them for quality and diversity.
            \end{itemize}
        
        \item \textbf{Human Evaluation}
            \begin{itemize}
                \item Use surveys or studies to gather feedback on the quality of generated outputs.
            \end{itemize}
        
        \item \textbf{Comparative Evaluation}
            \begin{itemize}
                \item Compare your generative model against benchmarks or state-of-the-art models.
            \end{itemize}
    \end{enumerate}
    
    \textbf{Key Points:}
    \begin{itemize}
        \item \textbf{Multiple Metrics}: Use a combination of metrics for a holistic view of model performance.
        \item \textbf{Context Importance}: The evaluation method should fit the type of generative model.
        \item \textbf{Continuous Improvement}: Regularly evaluate and refine models to address shortcomings.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Generative Modeling}
  \begin{block}{Overview}
    Generative models in machine learning aim to learn the underlying distribution of data and generate new samples that resemble those in the training set. 
    Despite their promise and versatile applications, these models face several significant challenges. 
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Generative Modeling - Overfitting}
  
  \begin{itemize}
    \item \textbf{Definition}: Overfitting occurs when a model learns the noise in the training data instead of the underlying distribution, resulting in poor generalization to new data.
    \item \textbf{Example}: A generative model trained on a small image dataset may memorize the exact pixels of the training images, failing to produce diverse new ones.
    \item \textbf{Illustration}: 
      \begin{itemize}
        \item Imagine a curve fitting problem where the model perfectly fits every data point, including outliers, rather than capturing the true trend.
      \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Generative Modeling - Mode Collapse and Computational Constraints}
  
  \begin{itemize}
    \item \textbf{Mode Collapse}:
      \begin{itemize}
        \item \textbf{Definition}: Mode collapse refers to producing limited diversity in outputs, often generating a few "modes" (specific types of data).
        \item \textbf{Example}: A GAN trained on a dataset of faces might generate only one or two unique facial types, neglecting the dataset's variance.
        \item \textbf{Key Point}: This leads to a lack of variability in outputs, impacting practical applicability.
      \end{itemize}
    
    \item \textbf{Computational Constraints}:
      \begin{itemize}
        \item \textbf{Definition}: Complex generative models like GANs and VAEs require significant computational resources for training.
        \item \textbf{Challenges}: 
          \begin{itemize}
            \item \textbf{Time-Intensive}: Training can take hours to days depending on dataset size and model complexity.
            \item \textbf{Hardware Requirements}: Often necessitate high-end GPUs, which may not be accessible to all practitioners.
          \end{itemize}
        \item \textbf{Example}: Training a GAN on high-resolution images can require substantial memory and processing power.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Challenges}
  
  \begin{itemize}
    \item \textbf{Overfitting}: Risk of memorization over learning.
    \item \textbf{Mode Collapse}: Generates limited outputs despite diverse training data.
    \item \textbf{Computational Constraints}: Requires significant resources and time.
  \end{itemize}
  
  \begin{block}{Visual Aid}
    \textbf{Diagram}: A flowchart showcasing the balance between model complexity, training data size, and generalization performance.
  \end{block}
  
  \begin{block}{Conclusion}
    Addressing these challenges is crucial for developing robust, high-performing generative models. Techniques like dropout for regularization or alternative training methods can help mitigate these issues. In the next slide, we will explore real-world applications of generative models.
  \end{block}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study: Generative Models in Action}
  \begin{block}{Introduction to Generative Models}
    Generative models are algorithms in unsupervised learning that learn the underlying distribution of data and generate new samples. They are used for:
    \begin{itemize}
      \item Image synthesis
      \item Text generation
      \item Anomaly detection
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study: Deep Art - Transforming Art Creation}
  \begin{block}{Overview}
    DeepArt is a platform that transforms photographs into artwork inspired by famous artists like Van Gogh and Picasso using Generative Adversarial Networks (GANs).
  \end{block}
  
  \begin{block}{Generative Process}
    \begin{enumerate}
      \item \textbf{Data Collection}: Trained on a dataset of artworks from various famous artists.
      \item \textbf{Training with GAN}:
        \begin{itemize}
          \item \textbf{Generator}: Creates new images.
          \item \textbf{Discriminator}: Assesses the realism of generated images.
        \end{itemize}
      \item \textbf{Image Generation}: Generates new images based on random noise inputs.
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Mathematics of GAN Training}
  \begin{block}{Training Iterations}
    The training aims to minimize the following loss functions:
    \begin{equation}
      \text{Loss}_{D} = -\mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] - \mathbb{E}_{z \sim p_{z}}[\log(1 - D(G(z)))]
    \end{equation}

    \begin{equation}
      \text{Loss}_{G} = -\mathbb{E}_{z \sim p_{z}}[\log D(G(z))]
    \end{equation}
  \end{block}
  Here, \( p_{\text{data}} \) represents the real data distribution while \( p_{z} \) is the noise input distribution.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Outcomes and Conclusion}
  \begin{block}{Key Outcomes}
    \begin{itemize}
      \item \textbf{Artistic Transformation}: Users receive transformed artwork in the style of their chosen artist.
      \item \textbf{Creativity Boost}: Provides new inspiration for artists.
      \item \textbf{Scalability}: Can generate a vast array of styles and compositions.
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    This case study illustrates the application of generative models in creating art, bridging technology and creativity. The evolution of these models will continue to expand their applications.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions of Generative Models}
    \begin{block}{Overview}
        Generative models are transforming various fields by generating new data points based on learned patterns. As technology advances, several emerging trends and applications will influence the future of these models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Emerging Trends}
    \begin{enumerate}
        \item \textbf{Hybrid Models}
            \begin{itemize}
                \item Combining generative with discriminative models for better prediction.
                \item \textit{Example:} Using GANs with classifiers to generate nuanced content.
            \end{itemize}
        \item \textbf{Improved Scalability}
            \begin{itemize}
                \item Focus on generating quality outputs from larger datasets efficiently.
                \item \textit{Example:} Techniques like model pruning and quantization.
            \end{itemize}
        \item \textbf{Multimodal Generation}
            \begin{itemize}
                \item Handling multi-modal data to generate outputs in various domains.
                \item \textit{Example:} Text-to-image synthesis with architectures like DALL-E.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Emerging Trends (cont.)}
    \begin{enumerate}[resume]
        \item \textbf{Real-time Generation}
            \begin{itemize}
                \item Potential for real-time applications due to hardware and algorithm advancements.
                \item \textit{Example:} Dynamic content generation in video games.
            \end{itemize}
        \item \textbf{Personalization}
            \begin{itemize}
                \item Tailoring outputs based on user preferences.
                \item \textit{Example:} Custom music or art based on user interaction data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications Across Fields}
    \begin{itemize}
        \item \textbf{Healthcare:} Generating synthetic medical images for training to protect patient privacy.
        \item \textbf{Entertainment:} Creating virtual content for movies and games.
        \item \textbf{Art and Design:} Assisting artists in producing unique designs through generative algorithms.
        \item \textbf{Finance:} Simulating market scenarios for risk assessment using synthetic data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Interdisciplinary Integration:} Merging generative models with neuroscience and cognitive science for breakthroughs.
        \item \textbf{Ethical Considerations:} Awareness of potential misuse and societal impacts is crucial.
        \item \textbf{Research and Development:} Ongoing research is essential for refining models and ensuring reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The evolution of generative models holds enormous potential for transformative applications across technology, creativity, and science. Vigilance in understanding and leveraging these directions responsibly will be vital.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Diagram: Architecture of GAN}
    \begin{block}{GAN Architecture}
        \begin{lstlisting}
Generator
   └── Random Noise → Fake Image
Discriminator
   └── Real Image or Fake Image → Classify as Real or Fake
        \end{lstlisting}
    \end{block}
    This diagram depicts the competition between the generator and discriminator in GANs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Generative Models - Introduction}
    \begin{block}{1. Introduction to Ethics in AI}
        Generative models, while powerful tools for creating data and content, raise significant ethical questions. The capabilities of these models can lead to both positive innovations and potential adverse consequences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Generative Models - Misuse}
    \begin{block}{2. Misuse of Generative Models}
        \begin{itemize}
            \item \textbf{Deepfakes:} Synthetic media where a person's likeness is altered.
            \begin{itemize}
                \item \textit{Example:} A deepfake video mimicking a public figure can mislead viewers about their statements or actions.
            \end{itemize}
            \item \textbf{Fraud:} Models exploited to create realistic images, voices, or documents for scams.
            \begin{itemize}
                \item \textit{Example:} Generating false invoices that appear legitimate to defraud companies.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Generative Models - Social Impact}
    \begin{block}{3. Social Impact}
        \begin{itemize}
            \item \textbf{Misinformation and Trust:} Creation of convincing false information erodes public trust in authentic content.
            \begin{itemize}
                \item \textit{Example:} Increased difficulty in distinguishing real news from fake posts on social media.
            \end{itemize}
            \item \textbf{Cultural Representation:} Models may reinforce stereotypes if trained on biased datasets.
            \begin{itemize}
                \item \textit{Example:} Generating images that depict certain demographics in a negative or misleading light.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Generative Models - Key Considerations}
    \begin{block}{4. Key Ethical Considerations}
        \begin{itemize}
            \item \textbf{Accountability:} Responsibility for unethical use—developer, user, or both?
            \item \textbf{Transparency:} Models should provide information about capabilities and limitations.
            \item \textbf{Bias Mitigation:} Ensure diverse representation in training datasets to avoid perpetuating stereotypes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Generative Models - Best Practices}
    \begin{block}{5. Best Practices}
        \begin{itemize}
            \item \textbf{Model Audits:} Regular assessment of models to discover and mitigate biases.
            \item \textbf{User Education:} Training on ethical implications as well as usage of generative models.
            \item \textbf{Policy Development:} Establish regulations for the use and distribution of generative technology.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Generative Models - Conclusion}
    \begin{block}{6. Conclusion}
        Ethical considerations are paramount in the development and deployment of generative models. Addressing these challenges is imperative to harness benefits responsibly while minimizing risks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion - Recap of Key Points on Generative Models}
  \begin{block}{1. What Are Generative Models?}
    Generative models are a class of statistical models designed to generate new data points based on the training data. Unlike discriminative models, they learn the underlying distribution of the data.
  \end{block}
  
  \begin{block}{2. Types of Generative Models}
    \begin{itemize}
      \item \textbf{Gaussian Mixture Models (GMM)}: Useful for clustering and density estimation, assuming data is a mixture of several Gaussian distributions.
      \item \textbf{Hidden Markov Models (HMM)}: Suitable for time series data, modeling systems that transition between different states over time.
      \item \textbf{Generative Adversarial Networks (GANs)}: Comprising a generator and discriminator, known for creating realistic images and data types.
      \item \textbf{Variational Autoencoders (VAEs)}: Learn a compressed representation of the data and can generate new samples resembling the original dataset.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion - Applications in Real-World Scenarios}
  \begin{itemize}
    \item \textbf{Image Generation}: GANs enable the creation of high-resolution images, useful for artistic applications or generating images from text descriptions.
    \item \textbf{Natural Language Processing (NLP)}: VAEs and GANs generate synthetic text that mimics human writing styles.
    \item \textbf{Healthcare}: Generative models synthesize medical data to aid in research while preserving patient privacy.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion - Key Takeaways and Final Thoughts}
  \begin{block}{Important Ethical Considerations}
    \begin{itemize}
      \item \textbf{Misuse}: Potential exploitation for creating deepfakes or generating misleading information.
      \item \textbf{Bias}: Training on biased data can perpetuate stereotypes and introduce ethical concerns in decision-making.
    \end{itemize}
  \end{block}

  \begin{block}{Key Takeaways}
    - Generative models significantly contribute to data mining and machine learning, enabling data generation and representation learning.
    - Their flexibility and applicability span various sectors, highlighting the need for responsible use and ethical oversight.
  \end{block}

  \begin{block}{Final Thoughts}
    Understanding generative models enhances our capacity to explore data beyond its surface, enabling responsible leverage of these techniques in advancing technology and society.
  \end{block}
\end{frame}


\end{document}