\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 1: Introduction to Reinforcement Learning}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Overview}
    \begin{block}{What is Reinforcement Learning (RL)?}
        Reinforcement Learning is a type of machine learning where an agent learns to make decisions by:
        \begin{itemize}
            \item Interacting with an environment.
            \item Taking actions based on its current state.
            \item Receiving feedback in the form of rewards or penalties.
            \item Learning an optimal strategy for maximizing cumulative rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Key Components}
    \begin{itemize}
        \item \textbf{Agent:} The learner or decision-maker.
        \item \textbf{Environment:} The external system with which the agent interacts.
        \item \textbf{State ($s$):} Current situation of the agent in the environment.
        \item \textbf{Action ($a$):} Choices available to the agent.
        \item \textbf{Reward ($r$):} Feedback indicating success of an action.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is RL Significant in AI?}
    \begin{itemize}
        \item \textbf{Autonomous Learning:} Systems learn from trial and error without explicit programming.
        \item \textbf{Applications in Real World:}
            \begin{itemize}
                \item \textbf{Robotics:} Teaching robots tasks like walking or object manipulation.
                \item \textbf{Game Playing:} e.g., AlphaGo mastering Go.
                \item \textbf{Healthcare:} Personalized treatment plans based on responses.
                \item \textbf{Finance:} Adaptive algorithmic trading strategies.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Process Overview}
    \begin{enumerate}
        \item \textbf{Initialization:} Start with a random policy.
        \item \textbf{Exploration:} Discover rewards and states through actions.
        \item \textbf{Exploitation:} Use known policies to maximize rewards.
        \item \textbf{Learning:} Update policies based on experienced rewards (using techniques like Q-learning).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario: The Cart-Pole Problem}
    In the Cart-Pole problem, the goal is to balance a pole on a cart that can move left or right:
    \begin{itemize}
        \item \textbf{Actions:} Move left, move right.
        \item \textbf{States:} The position and velocity of the cart and pole.
        \item \textbf{Reward:} +1 for each time step the pole is balanced.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item RL is inspired by behavioral psychology (trial-and-error learning).
        \item Balancing exploration and exploitation is crucial for effective learning.
        \item RL's real-world applications demonstrate significant potential for innovation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula: The Bellman Equation}
    A common framework in RL is the \textbf{Bellman Equation}, which expresses the relationship between the value of a state and the values of its successor states:
    \begin{equation}
        V(s) = R(s) + \gamma \sum P(s'|s,a)V(s')
    \end{equation}
    Where:
    \begin{itemize}
        \item \( V(s) \) = Value of state \( s \).
        \item \( R(s) \) = Immediate reward.
        \item \( \gamma \) = Discount factor (0 \leq \gamma < 1).
        \item \( P(s'|s,a) \) = Transition probability to the next state \( s' \) given state \( s \) and action \( a \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    In this weekâ€™s introduction to Reinforcement Learning (RL), we aim to establish a strong foundation by covering the following key learning objectives:
    
    \begin{enumerate}
        \item Understanding the Basics of Reinforcement Learning
        \item Components of Reinforcement Learning
        \item Exploration vs. Exploitation
        \item Basic Concepts of Markov Decision Processes (MDPs)
        \item Applications of Reinforcement Learning
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Understanding RL}
    \begin{block}{Understanding the Basics of Reinforcement Learning}
        \begin{itemize}
            \item Define RL and its role in Artificial Intelligence.
            \item Distinguish RL from supervised and unsupervised learning.
            \item \textbf{Key Point:} RL focuses on learning through interaction with an environment to achieve a goal.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Components of RL}
    \begin{block}{Components of Reinforcement Learning}
        \begin{itemize}
            \item \textbf{Agent:} The learner or decision-maker.
            \item \textbf{Environment:} The context or scenario in which the agent operates.
            \item \textbf{Action:} The choices that the agent can make.
            \item \textbf{Reward:} Feedback based on the actions taken by the agent.
        \end{itemize}
        \begin{block}{Example}
            In a maze-solving scenario:
            \begin{itemize}
                \item Agent: The robot
                \item Environment: The maze
                \item Rewards: Represent food at certain locations
            \end{itemize}
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Exploration vs. Exploitation}
    \begin{block}{Exploration vs. Exploitation}
        \begin{itemize}
            \item Understand the trade-off:
            \begin{itemize}
                \item \textbf{Exploration:} Trying new actions
                \item \textbf{Exploitation:} Using known rewarding actions
            \end{itemize}
            \item \textbf{Illustration:} A child trying different ice cream flavors (exploration) while also going back for their favorite (exploitation).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Markov Decision Processes}
    \begin{block}{Basic Concepts of Markov Decision Processes (MDPs)}
        \begin{itemize}
            \item Introduce the MDP framework:
            \begin{itemize}
                \item \textbf{States (S):} The situations the agent can encounter.
                \item \textbf{Actions (A):} The choices available to the agent.
                \item \textbf{Transition Function:} Probability of moving from one state to another given an action.
                \item \textbf{Reward Function:} Immediate reward received after transitioning states.
            \end{itemize}
            \item \textbf{Formula:}
            \begin{equation}
                P(s', r | s, a)
            \end{equation}
            Where:
            \begin{itemize}
                \item \( s \) is the current state
                \item \( a \) is the action taken
                \item \( s' \) is the resulting state
                \item \( r \) is the reward received
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Applications of RL}
    \begin{block}{Applications of Reinforcement Learning}
        \begin{itemize}
            \item Explore various real-world applications:
            \begin{itemize}
                \item Game playing (e.g., AlphaGo, OpenAI's Dota 2 agents)
                \item Robotics (e.g., autonomous robots learning to navigate)
                \item Recommendation systems (e.g., personalized content delivery)
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item RL is characterized by trial and error learning.
            \item The interaction between agent and environment is fundamental.
            \item Understanding exploration-exploitation balance is crucial for effective learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Agents - Overview}
    \begin{block}{Understanding Agents in Reinforcement Learning}
        In reinforcement learning (RL), an **agent** is an entity that interacts with an environment to achieve specific goals. Its primary role is to learn how to make decisions that maximize its cumulative reward.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Agents - Roles}
    \begin{block}{Roles of Agents}
        \begin{enumerate}
            \item **Decision Maker**: Chooses actions based on the current state and its learned policy.
            \item **Learner**: Gathers information from the environment and updates its understanding based on rewards and feedback.
            \item **Actor**: Executes actions that lead to transitions between states in the environment.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Agents - Interaction Framework}
    \begin{block}{Interaction with the Environment}
        An agent interacts with its environment in discrete time steps:
        \begin{itemize}
            \item **Observation**: The agent observes the current state of the environment ($s$).
            \item **Action Selection**: Selects an action ($a$) based on the observed state.
            \item **Reward Feedback**: Receives a reward ($r$) after executing the action.
            \item **State Transition**: The environment transitions to a new state ($s'$).
        \end{itemize}
        \begin{equation}
            \text{Agent} \xrightarrow{a} \text{Environment} \xrightarrow{s'} + \text{Reward}(r) \xrightarrow{\text{(back to Agent)}}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Agents - Example}
    \begin{block}{Example of an Agent in Action}
        Consider a robot navigating a maze:
        \begin{itemize}
            \item **States**: Various points in the maze (e.g., current location of the robot).
            \item **Actions**: Move up, down, left, or right.
            \item **Rewards**: Positive for reaching the goal, negative for hitting walls or obstacles.
        \end{itemize}
        The agent updates its strategy based on the observed state and received feedback.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Agents - Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Agents maximize long-term rewards through trial-and-error learning.
            \item Learning methods include policy-based (e.g., deep Q-learning) and value-based approaches.
            \item Understanding agent-environment interaction is essential to grasp RL.
        \end{itemize}
    \end{block}
    \begin{block}{Further Considerations}
        \begin{itemize}
            \item Agents can operate in single or multi-agent systems (collaborative or competitive).
            \item The complexity of decision-making varies based on the environment's dynamics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Environments - Definition}
    \begin{block}{Definition of Environments in Reinforcement Learning}
        In reinforcement learning (RL), an \textbf{environment} refers to everything that an agent interacts with while learning to achieve its goals. 
        The environment encompasses the state of the world and its dynamics, providing feedback to an agent about its actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Environments - Types}
    \begin{block}{Types of Environments}
        \begin{enumerate}
            \item \textbf{Stochastic vs. Deterministic}:
            \begin{itemize}
                \item \textbf{Stochastic Environments}: Outcomes of actions are probabilistic.
                \item \textbf{Deterministic Environments}: Each action leads to a predictable outcome.
            \end{itemize}
            \item \textbf{Fully Observable vs. Partially Observable}:
            \begin{itemize}
                \item \textbf{Fully Observable Environments}: Agent has complete information about the current state.
                \item \textbf{Partially Observable Environments}: Agent lacks complete information, leading to uncertainty.
            \end{itemize}
            \item \textbf{Static vs. Dynamic}:
            \begin{itemize}
                \item \textbf{Static Environments}: Environment does not change while the agent decides.
                \item \textbf{Dynamic Environments}: Environment can change independently of the agent's actions.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Environments - Dynamics}
    \begin{block}{Dynamics of an Environment}
        The dynamics of an environment in RL is crucial as it describes how the environment responds to the agent's actions. 
        Formally, environments can be represented using the framework of \textbf{Markov Decision Processes (MDPs)}.
    \end{block}

    \begin{block}{Key Elements of MDP}
        \begin{itemize}
            \item \textbf{States (S)}: Represents all possible situations the agent can be in.
            \item \textbf{Actions (A)}: Represents the set of all possible actions the agent can take.
            \item \textbf{Transition Model (P)}: Defines the probabilities of transitioning from one state to another given an action. 
                Notation: $ P(s'|s, a) $ indicates the probability of reaching state $ s' $ from state $ s $ by taking action $ a $.
            \item \textbf{Reward Function (R)}: Awards the agent based on the transition to a new state. 
                Notation: $ R(s, a) $ gives the expected reward received after taking action $ a $ in state $ s $.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: States - Part 1}
    \begin{block}{Understanding States in Reinforcement Learning}
        \begin{enumerate}
            \item \textbf{Definition of States:}
            \begin{itemize}
                \item A state is a specific configuration of the environment at a given time.
                \item It contains all the information needed for decision-making, possibly represented as vectors, categorical variables, or images.
            \end{itemize}
            
            \item \textbf{Role of States:}
            \begin{itemize}
                \item States determine available actions and expected outcomes.
                \item They help evaluate current situations concerning past experiences.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: States - Part 2}
    \begin{block}{Types of States}
        \begin{itemize}
            \item \textbf{Discrete States:} A finite set of distinct states. 
            \item \textbf{Continuous States:} An infinite number of possible states, such as varying positions and velocities.
        \end{itemize}
    \end{block}

    \begin{block}{Example Scenario: Grid World}
        Consider a simple Grid World where an agent can move in four directions:
        \begin{itemize}
            \item Possible states might include (0,0) - top-left and (2,2) - bottom-right corners.
            \item The agent's current position defines the state guiding its actions towards a goal.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: States - Part 3}
    \begin{block}{Significance of State Representation}
        \begin{itemize}
            \item \textbf{Decision Making:} States influence the best action based on expected future rewards.
            \item \textbf{Policy Development:} The policy outlines actions to take from each state; this is heavily influenced by state representation.
            \item \textbf{Learning Process:} Learning occurs through transitions between states and rewards.
        \end{itemize}
    \end{block}

    \begin{block}{Transition Dynamics}
        \begin{itemize}
            \item State transition occurs upon taking an action, defined by:
            \begin{equation}
                P(s' | s, a) 
            \end{equation}
            \item This transition model is essential for predicting outcomes and planning actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Actions - Overview}
    \begin{block}{Overview of Actions in Reinforcement Learning}
        In reinforcement learning (RL), \textbf{actions} are the specific choices made by an agent that influence its environment. They are pivotal in shaping the trajectory of an agent's experience by determining the next state and the potential rewards the agent can receive.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Actions - Key Components}
    \begin{itemize}
        \item \textbf{Definition of Actions}: An action is a decision made by an agent from a set of possible moves it can execute in a given state.
        
        \item \textbf{State Transition}: Actions cause transitions between states. The environment responds to an action by changing the current state to a new state, thus influencing future decisions.
        
        \item \textbf{Action Space}: The collection of all possible actions is known as the action space. It can be:
        \begin{itemize}
            \item \textbf{Discrete}: e.g., moving left, right, or staying still
            \item \textbf{Continuous}: e.g., adjusting a lever to any position
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Actions - Example and Summary}
    \begin{block}{Example: Navigating a Gridworld}
        \begin{itemize}
            \item \textbf{Scenario}: An agent navigates a 5x5 grid with possible actions: Up, Down, Left, Right.
            \item \textbf{States}: Each grid position represents a state.
            \item \textbf{Actions}: Possible moves are Up, Down, Left, Right.
            \item \textbf{Transition}: 
            \begin{itemize}
                \item If in state (2,2) and takes action "Up", moves to state (1,2).
                \item If in state (2,2) and takes action "Down", moves to (3,2).
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Summary}
        Actions in reinforcement learning are critical as they dictate the course of an agent's interaction with its environment. 
        Understanding the relationship between actions, states, and rewards forms the backbone of developing efficient learning algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Rewards}
    \begin{block}{Understanding Rewards}
        \begin{itemize}
            \item Rewards are feedback signals from the environment.
            \item They indicate the benefit or harm of an action towards achieving a goal.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Rewards}
    \begin{itemize}
        \item \textbf{Guiding Learning:} 
        \begin{itemize}
            \item Rewards guide the learning process by reinforcing desirable actions.
        \end{itemize}
        \item \textbf{Motivational Role:}
        \begin{itemize}
            \item Rewards motivate agents to explore and exploit actions. 
            \item Aim to maximize cumulative rewards over time.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Structures}
    \begin{enumerate}
        \item \textbf{Immediate Rewards:}
            \begin{itemize}
                \item Feedback received after each action.
                \item Example: Scoring a point in a game.
            \end{itemize}
        \item \textbf{Delayed Rewards:}
            \begin{itemize}
                \item Rewards received after multiple actions.
                \item Example: Winning a chess game.
            \end{itemize}
        \item \textbf{Positive and Negative Rewards:}
            \begin{itemize}
                \item Positive: Reinforces good behavior.
                \item Negative: Discourages undesirable behavior.
            \end{itemize}
        \item \textbf{Sparse vs. Dense Rewards:}
            \begin{itemize}
                \item Sparse: Feedback only at specific instances.
                \item Dense: More frequent feedback aids learning.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Rewards in RL}
    \begin{itemize}
        \item \textbf{Example 1 (Maze Navigation):}
        \begin{itemize}
            \item +10 points for reaching the exit.
            \item -1 point for hitting a wall.
        \end{itemize}
        \item \textbf{Example 2 (Robotics):}
        \begin{itemize}
            \item Positive rewards for steps forward.
            \item Negative rewards for falling or wrong steps.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Rewards are crucial for shaping agent behavior.
        \item Reward structure impacts learning efficiency and strategy.
        \item Balancing reward types is essential for effective learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula for Expected Cumulative Reward}
    \begin{equation}
        R = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
    \end{equation}
    \begin{itemize}
        \item Where:
        \begin{itemize}
            \item \( R \): Expected cumulative reward.
            \item \( r_t \): Reward at time \( t \).
            \item \( \gamma \): Discount factor (0 $\leq$ $\gamma$ < 1).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Value Functions}
    \begin{block}{Introduction to Value Functions in RL}
        In reinforcement learning, \textbf{value functions} are pivotal in guiding an agentâ€™s decision-making process. They quantify the expected future rewards an agent can obtain from a given state or state-action pair. Understanding value functions helps in identifying which actions will yield the highest rewards in the long run.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Value Functions}
    \begin{enumerate}
        \item \textbf{State Value Function (V(s))}
        \begin{itemize}
            \item \textbf{Definition}: Represents the expected return starting from state \(s\) and following a certain policy \(\pi\).
            \item \textbf{Formula}:
            \begin{equation}
                V(s) = \mathbb{E}_\pi [G_t | S_t = s]
            \end{equation}
            \item \textbf{Example}: In a grid world, if an agent is in state \(s\), \(V(s)\) predicts the total rewards it can expect by following a specific strategy from that point onward.
        \end{itemize}
        
        \item \textbf{Action Value Function (Q(s, a))}
        \begin{itemize}
            \item \textbf{Definition}: Indicates the expected return from being in state \(s\), taking action \(a\), and then following policy \(\pi\).
            \item \textbf{Formula}:
            \begin{equation}
                Q(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]
            \end{equation}
            \item \textbf{Example}: In a grid world, \(Q(s, a)\) evaluates the benefit of moving in a certain direction while in state \(s\).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Value Functions}
    \begin{itemize}
        \item \textbf{Informed Choices}: Value functions help an agent make informed choices by estimating the long-term benefits of actions rather than just short-term rewards.
        \item \textbf{Policy Improvement}: By analyzing value functions, agents can refine their policies to maximize returns iteratively.
        \item \textbf{Convergence}: Value functions play a crucial role in RL algorithms, including Value Iteration and Policy Iteration, enabling convergence to optimal policies.
    \end{itemize}
    
    \begin{block}{Illustrative Example}
        Imagine a robot navigating a maze:
        \begin{itemize}
            \item \textbf{States}: Each position in the maze.
            \item \textbf{Actions}: Moving left, right, up, or down.
            \item \textbf{Rewards}: Positive for reaching the exit, negative for hitting walls.
        \end{itemize}
        The robot utilizes value functions to evaluate the outcomes of each action.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Reinforcement Learning}
    \begin{block}{Introduction to Reinforcement Learning (RL)}
        \begin{itemize}
            \item RL is a type of machine learning focused on decision-making through trial and error.
            \item An agent receives feedback via rewards or penalties.
            \item Especially powerful for solving complex problems across diverse domains.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Robotics}
        \begin{itemize}
            \item RL teaches robots to navigate and interact with environments.
            \item \textit{Example:} Robotic arm learning to pick up and place objects.
            \item \textbf{Key Point:} Adaptation to new situations without explicit programming.
        \end{itemize}
        
        \item \textbf{Finance}
        \begin{itemize}
            \item RL algorithms optimize trading strategies based on market data.
            \item \textit{Example:} Learning optimal buy/sell strategies to maximize returns.
            \item \textbf{Key Point:} Continuously adjusting asset weights for portfolio management.
        \end{itemize}
        
        \item \textbf{Healthcare}
        \begin{itemize}
            \item Personalization of treatment plans using RL algorithms.
            \item \textit{Example:} Treatment recommendation systems adapting over time.
            \item \textbf{Key Point:} Optimizing patient outcomes through personalized strategies.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Reinforcement Learning (Cont.)}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Gaming}
        \begin{itemize}
            \item RL advancements contribute to intelligent game character development.
            \item \textit{Example:} AlphaGo program defeating a human champion in Go.
            \item \textbf{Key Point:} Engaging gameplay experiences through adaptive AI.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Ethical Considerations}
    \begin{block}{Concluding Remarks}
        \begin{itemize}
            \item Understanding RL applications highlights their versatility.
            \item Importance of considering ethical implications in deployment.
            \item Paving the way for responsible AI development by benefiting all.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamental Formula of Reinforcement Learning}
    \begin{equation}
        V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') \right)
    \end{equation}
    \begin{itemize}
        \item $V(s)$: Value function for state $s$.
        \item $R(s, a)$: Reward for action $a$ in state $s$.
        \item $\gamma$: Discount factor.
        \item $P(s'|s, a)$: Probability of transitioning to state $s'$ after action $a$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in RL}
    
    \begin{block}{Overview}
        Reinforcement Learning (RL) raises significant ethical concerns, particularly regarding:
        \begin{itemize}
            \item Fairness
            \item Bias
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Fairness and Bias}
    
    \begin{block}{Fairness}
        \begin{itemize}
            \item Equitable treatment of all individuals affected by the RL system.
            \item Questions of whether outcomes favor specific groups.
            \item Example: RL models in healthcare may favor majority demographics due to biased training data.
        \end{itemize}
    \end{block}
    
    \begin{block}{Bias}
        \begin{itemize}
            \item Inherent in many RL systems from biased data or design choices.
            \item Can manifest as algorithmic or societal bias.
            \item Example: Historical hiring data may cause discrimination against certain demographic groups.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example: Hiring Algorithms}
    
    \begin{block}{Scenario}
        An RL-based hiring system selects candidates based on past successful hires.
    \end{block}
    
    \begin{block}{Potential Bias}
        \begin{itemize}
            \item If training data primarily includes successful hires from one gender or race, bias is introduced.
            \item Systematic exclusion of qualified candidates from underrepresented groups can occur, perpetuating inequality.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Recognizing and addressing ethical implications ensures RL systems benefit all sectors of society.
    \end{block}
\end{frame}


\end{document}