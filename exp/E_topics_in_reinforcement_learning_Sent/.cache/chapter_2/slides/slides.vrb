\frametitle{Value Function and Summary}
    \textbf{Key Formula: Value Function}
    \begin{equation}
        V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(S_t, A_t) | S_0 = s \right]
    \end{equation}
    where $\gamma$ (0 â‰¤ $\gamma$ < 1) is the discount factor.

    \textbf{Summary of Key Points:}
    \begin{itemize}
        \item MDPs encapsulate a decision-making scenario with state transitions, rewards, and policies.
        \item The Markov property ensures that decisions are made based only on the current state.
        \item Transition probabilities dictate state dynamics, while rewards inform value assessments.
        \item Understanding the structure of policies is crucial for evaluating and deciding on actions.
    \end{itemize}
