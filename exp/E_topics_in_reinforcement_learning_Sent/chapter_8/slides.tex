\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 8: Performance Metrics and Evaluation}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Performance Metrics}
    \begin{block}{Overview}
        Performance metrics are quantitative measures used to evaluate and compare the effectiveness of RL models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Performance Metrics}
    \begin{enumerate}
        \item \textbf{Evaluation of Learning Efficiency:} 
        Determines how quickly and effectively an agent learns from its environment.
        
        \item \textbf{Benchmarking:} 
        Provides standard benchmarks for comparing different RL algorithms.
        
        \item \textbf{Feedback Mechanism:} 
        Offers feedback on the training process, facilitating adjustments to algorithms or hyperparameters.
        
        \item \textbf{Real-World Applicability:} 
        Quantifies model performance under real-world conditions in applications like robotics or gaming.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics in RL}
    \begin{itemize}
        \item \textbf{Cumulative Reward:} 
        Total reward received by the agent over time.
        
        \item \textbf{Average Reward:} 
        Mean of cumulative rewards over a specified time period.
        
        \item \textbf{Success Rate:} 
        Fraction of trials where the agent achieves its goal.
        
        \item \textbf{Learning Curves:} 
        Graphical representations illustrating performance metric changes over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Cumulative Reward Calculation}
    Consider an agent that receives rewards as follows over five episodes: 
    \begin{itemize}
        \item Episode 1: +5
        \item Episode 2: +10
        \item Episode 3: -3
        \item Episode 4: +7
        \item Episode 5: +0
    \end{itemize}
    The cumulative reward after 5 episodes is:
    \begin{equation} 
    \text{Cumulative Reward} = 5 + 10 - 3 + 7 + 0 = 19 
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Performance metrics are essential for assessing and improving RL models.
        \item Metrics like cumulative reward, average reward, and success rate quantify performance.
        \item Visual tools, such as learning curves, enhance understanding of performance trends.
    \end{itemize}
    This foundational understanding will set the stage for exploring specific performance evaluation techniques in the next slide.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    \begin{block}{Goal of This Week's Lessons}
        As we dive into performance evaluation in RL, we aim to achieve the following learning objectives:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Performance Metrics}
    \begin{enumerate}
        \item \textbf{Understand the Role of Performance Metrics}
        \begin{itemize}
            \item Performance metrics are essential for assessing the effectiveness of RL models. They provide a quantitative basis for comparison among different agents or algorithms.
            \item \textbf{Example:} Comparing two agents in a chess game based on their win rates or average game durations.
        \end{itemize}
        
        \item \textbf{Identify and Define Key Performance Metrics}
        \begin{itemize}
            \item Students will learn critical performance metrics used in RL.
            \item \textbf{Metrics to Focus On:}
            \begin{itemize}
                \item Cumulative Reward: Total reward received over time.
                \item Average Reward: Reward normalized by the number of actions.
                \item Success Rate: Percentage of successful outcomes in the given environment.
            \end{itemize}
            \item \textbf{Illustration:} A comparison table showing these metrics across multiple RL models.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Evaluation Methods and Code Implementation}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        \item \textbf{Explore Methods for Evaluating Policy Performance}
        \begin{itemize}
            \item Evaluation methods provide insights into how well an RL agent is performing its task.
            \item \textbf{Example:} Discussing off-policy vs. on-policy evaluation.
        \end{itemize}

        \item \textbf{Learn How to Implement Performance Evaluation in Code}
        \begin{itemize}
            \item Students will gain hands-on experience by coding performance evaluation metrics.
            \item \textbf{Code Snippet Example:}
            \begin{lstlisting}[language=Python]
def calculate_cumulative_reward(rewards):
    return sum(rewards)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Analyze Trade-offs in Performance Metrics}
        \begin{itemize}
            \item Recognizing that optimizing one metric can sometimes detrimentally affect others.
            \item \textbf{Illustration:} A graph showing trade-offs between exploration and cumulative reward.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Real-World Applications}
    \begin{enumerate}
        \setcounter{enumi}{5} % Continue numbering
        \item \textbf{Discuss Real-World Applications of Performance Metrics}
        \begin{itemize}
            \item Understanding the impact of performance evaluation in practical scenarios.
            \item \textbf{Example:} Evaluating autonomous vehicles based on safety incidents vs. travel time.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Summary}
        By the end of this week, students will have a solid understanding of how to evaluate reinforcement learning models effectively, using various performance metrics. This knowledge is critical for building robust and efficient RL systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cumulative Reward - Definition}
    \begin{block}{Definition of Cumulative Reward}
        Cumulative reward (denoted as \( R_t \)) is the total reward an agent collects over time while interacting with the environment.
    \end{block}
    
    The cumulative reward at time \( t \) is defined as:
    
    \begin{equation}
        R_t = r_t + r_{t+1} + r_{t+2} + \ldots + r_T 
    \end{equation}
    
    where:
    \begin{itemize}
        \item \( R_t \): Cumulative reward starting from time step \( t \)
        \item \( r_t, r_{t+1}, \ldots, r_T \): Rewards received from time \( t \) to terminal state \( T \)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cumulative Reward - Importance}
    \begin{block}{Importance in Assessing RL Performance}
        \begin{enumerate}
            \item \textbf{Performance Metric}: Cumulative reward is the primary metric for evaluating RL model performance; higher values indicate better performance.
            \item \textbf{Goal Alignment}: The primary aim of RL agents is to maximize cumulative reward, highlighting its role in achieving objectives.
            \item \textbf{Long-Term vs Short-Term}: Cumulative reward assists in navigating the exploration-exploitation dilemma, emphasizing both immediate and long-term outcomes.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cumulative Reward - Examples}
    \begin{block}{Examples to Illustrate Cumulative Reward}
        \begin{itemize}
            \item \textbf{Example 1}: In a grid-world:
                \[
                R = 10 - 4 = 6
                \]
            \item \textbf{Example 2}: In a video game:
                \[
                R = (5 \times 2) - 5 = 10 - 5 = 5
                \]
        \end{itemize}
        
        \begin{block}{Key Points to Emphasize}
            \begin{itemize}
                \item Cumulative reward as a goal in RL.
                \item The temporal nature of decisions and outcomes.
                \item Dynamic evaluation based on both strategy and environmental factors.
            \end{itemize}
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cumulative Reward - Code Snippet}
    \begin{block}{Code Snippet for Cumulative Reward Calculation}
        \begin{lstlisting}[language=Python]
def calculate_cumulative_reward(rewards):
    cumulative_reward = sum(rewards)
    return cumulative_reward

# Example usage:
rewards = [2, 2, -1, 3, -2]  # Sample rewards list
total_reward = calculate_cumulative_reward(rewards)
print("Cumulative Reward:", total_reward)  # Output: Cumulative Reward: 4
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Convergence Rates}
    \begin{block}{Definition}
        Convergence rates in Reinforcement Learning (RL) refer to the speed at which an RL algorithm approaches the optimal policy or value function over time.
    \end{block}
    \begin{block}{Importance}
        A higher convergence rate indicates faster achievement of satisfactory performance, crucial for training efficiency and resource management.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Significance of Convergence Rates}
    \begin{itemize}
        \item \textbf{Efficiency Measurement:} Assess how quickly an RL algorithm learns, aiding algorithm selection for specific problems.
        \item \textbf{Performance Benchmarking:} Compare convergence rates of various algorithms on the same task to identify effective methods.
        \item \textbf{Resource Optimization:} Analyze convergence rates to determine necessary computational resources during model training.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points}
    \begin{enumerate}
        \item \textbf{Convergence vs. Optimality:} Convergence signifies approaching an optimal solution but not necessarily reaching it, indicating effective learning.
        \item \textbf{Early Stopping Criteria:} Convergence rates can inform when to stop training; stable performance metrics may indicate convergence.
        \item \textbf{Stochastic Nature of Learning:} RL learning may lead to convergence towards a range rather than a single point, necessitating statistical measures.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation}
    \begin{equation}
        \text{Convergence Rate} = \lim_{n \to \infty} \frac{V_{\text{n+1}} - V_n}{n}
    \end{equation}
    \begin{itemize}
        \item $V_n$ is the value of the function at the $n$th iteration.
        \item A faster convergence is indicated by a smaller limit, suggesting fewer iterations to approach the optimal value.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    Imagine an RL agent learning to play a game. If the cumulative reward over episodes is plotted:

    \begin{itemize}
        \item The curve might rise steeply initially and level off as the agent approaches its optimal strategy.
        \item The steepness of this initial rise reflects the convergence rate.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example (Pseudo-code)}
    Here's a way to monitor convergence rates during training:
    \begin{lstlisting}[language=Python]
for episode in range(total_episodes):
    reward = run_episode(agent)
    rewards_history.append(reward)
    if episode > 0 and abs(rewards_history[-1] - rewards_history[-2]) < threshold:
        print("Convergence reached at episode:", episode)
        break
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Understanding convergence rates in RL is essential for evaluating algorithm performance and efficiency. By focusing on these rates, we can make informed decisions on training strategies and model selection, leading to better-performing agents.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualization and Analysis}
    \begin{block}{Objectives}
        \begin{itemize}
            \item Understand the importance of visualizing performance metrics in analyzing model outputs.
            \item Learn how effective visualization can highlight patterns, trends, and anomalies in model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Visualization in Model Performance Analysis}
    \begin{enumerate}
        \item \textbf{Pattern Recognition:}
        \begin{itemize}
            \item Identifies trends and correlations which may not be apparent from raw data.
            \item \textit{Example:} A line chart comparing cumulative rewards can indicate model learning changes.
        \end{itemize}
        
        \item \textbf{Comparison Across Runs:}
        \begin{itemize}
            \item Allows straightforward comparison of model performance across different settings.
            \item \textit{Example:} Bar charts comparing average rewards of agents trained with different algorithms.
        \end{itemize}

        \item \textbf{Anomaly Detection:}
        \begin{itemize}
            \item Identifies unexpected behaviors in models through performance metric plots.
            \item \textit{Example:} A scatter plot can highlight low-performing episodes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Visualization Techniques}
    \begin{itemize}
        \item \textbf{Line Graphs:} Track performance metrics over time (e.g., reward per episode).
        \item \textbf{Bar Charts:} Compare categorical metrics (e.g., success rates of different policies).
        \item \textbf{Box Plots:} Display distribution of results (e.g., rewards across multiple training runs).
        \item \textbf{Heatmaps:} Visualize state-action values, showing where models excel versus struggle.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code for Visualization}
    \begin{block}{Python Code Snippet}
        \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

# Example data for rewards over episodes
episodes = range(1, 101)
rewards = [100, 120, 110, 115, 130, 125] * 16 + [140, 145]  # Sample rewards

plt.figure(figsize=(10, 5))
plt.plot(episodes, rewards, label='Cumulative Reward', color='blue')
plt.title('Model Performance Over Time')
plt.xlabel('Episodes')
plt.ylabel('Cumulative Reward')
plt.legend()
plt.grid()
plt.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps}
    \begin{itemize}
        \item Visualizing result metrics is essential for effective analysis of model performance in reinforcement learning.
        \item It uncovers trends, enhances communication, and enables informed decision-making.
        \item Next, we will compare various performance metrics to provide a comprehensive evaluation framework for model assessment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Introduction}
    \begin{block}{Introduction to Performance Metrics in Reinforcement Learning (RL)}
        In Reinforcement Learning, performance metrics are crucial for evaluating the effectiveness of algorithms and policies. Selecting appropriate metrics allows researchers and practitioners to:
        \begin{itemize}
            \item Compare different approaches
            \item Understand their strengths and weaknesses
            \item Fine-tune models effectively
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Common Performance Metrics}
    \begin{enumerate}
        \item \textbf{Cumulative Reward (Return)} \\
        \textbf{Definition}: Total reward accumulated by an agent over its lifetime or in an episode.
        \begin{itemize}
            \item \textbf{Advantages}: Simple to compute and interpret, directly reflects the objective.
            \item \textbf{Limitations}: May mislead when episode length is ignored, does not account for performance variance.
            \item \textbf{Example}: Cumulative reward \( R = 1 + 1 + 2 = 4 \).
        \end{itemize}
        
        \item \textbf{Average Reward} \\
        \textbf{Definition}: Mean reward collected per time step over episodes.
        \begin{itemize}
            \item \textbf{Advantages}: Normalizes the reward for comparison, reduces outlier impact.
            \item \textbf{Limitations}: Masks high variance, may not reflect effectiveness in non-stationary environments.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{Average Reward} = \frac{1}{N}\sum_{t=1}^N R_t
            \end{equation}
        \end{itemize}    
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Additional Metrics}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Success Rate} \\
        \textbf{Definition}: Percentage of episodes achieving a predefined goal.
        \begin{itemize}
            \item \textbf{Advantages}: Clear metric for objective tasks, easy communication.
            \item \textbf{Limitations}: Ignores solution quality, not informative for continuous tasks.
            \item \textbf{Example}: Success in 80 out of 100 episodes leads to a success rate of 80%.
        \end{itemize}
        
        \item \textbf{Mean Squared Error (MSE) in Value Prediction} \\
        \textbf{Definition}: Average of squares of the errors in evaluating a value function.
        \begin{itemize}
            \item \textbf{Advantages}: Insight into prediction quality, useful for policy comparisons.
            \item \textbf{Limitations}: Sensitive to outliers, focuses on prediction rather than policy performance.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{MSE} = \frac{1}{N}\sum_{i=1}^N (\hat{v}_i - v_i)^2
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Context Matters}: Choose metrics aligning with task characteristics and goals.
            \item \textbf{Trade-offs}: Different metrics emphasize different performance aspects.
            \item \textbf{Combine Metrics}: Multiple metrics provide a comprehensive evaluation.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        In Reinforcement Learning, understanding and utilizing performance metrics is essential for assessing agent capabilities and guiding improvements. The choice of metric should align with specific objectives and context of the learning task.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Environmental Robustness}
    % Explanation of environmental robustness in RL context
    \begin{block}{Understanding Environmental Robustness}
        Environmental robustness in Reinforcement Learning (RL) refers to how well an RL model can perform across different conditions and variations in its environment. It assesses whether a model can maintain its performance despite changes such as different initial states, unforeseen obstacles, or altered reward structures.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Environmental Robustness}
    % Importance of environmental robustness
    \begin{itemize}
        \item \textbf{Generalization:} Ensures that an RL agent can generalize its learned strategies to unseen scenarios not encountered during training.
        \item \textbf{Resilience:} A robust model will not degrade drastically in performance when facing slight adversities or changes in environmental settings.
        \item \textbf{Real-world Application:} In practical applications (e.g., autonomous driving, robotics), unpredictability requires robustness for model deployment in real-world situations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Factors Influencing Environmental Robustness}
    % Factors influencing environmental robustness
    \begin{itemize}
        \item \textbf{Variability in Environment:} Changes in dynamics, such as moving obstacles or varying terrain.
        \item \textbf{Noise:} Stochastic elements in interactions can test the model's adaptability.
        \item \textbf{Task Complexity:} Complicated tasks may require a more sophisticated level of robustness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Environmental Robustness}
    % Examples showcasing environmental robustness
    \begin{itemize}
        \item \textbf{Robust Navigation:} A robot trained in a simulated environment adapts its strategy to navigate different physical landscapes effectively.
        \item \textbf{Game Playing:} In games like Chess or Go, small changes can drastically affect strategies. A robust RL agent learns to handle such variations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Measuring Environmental Robustness}
    % Methods to measure environmental robustness
    \begin{block}{Performance Consistency}
        Measure the standard deviation of the RL model’s performance metrics (e.g., reward accumulation) across diverse environmental conditions.
        
        \begin{equation}
            \text{Robustness} = \frac{1}{\sigma_{p}}
        \end{equation}
        where $\sigma_{p}$ is the standard deviation of the performance scores.
    \end{block}
    
    \begin{block}{Transfer Learning}
        Analyze how well a model trained in one environment performs in another. High transfer performance indicates robustness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    % Key points to emphasize
    \begin{itemize}
        \item Environmental robustness is critical for deploying RL models in real-world settings.
        \item Assessing robustness involves evaluating how well models adapt to new and variable conditions.
        \item Robust RL models lead to increased reliability and safety in applied scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    % Conclusion on environmental robustness
    Understanding and evaluating environmental robustness is essential for developing RL agents that are effective, reliable, and adaptable in diverse and unpredictable environments. As we move to the next slide, we will examine a practical case study that illustrates the application of performance metrics in evaluating the robustness of RL models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Example Case Study - Part 1}
    \textbf{Case Study: Evaluating Deep Q-Network (DQN) in Atari Games}
    
    \begin{itemize}
        \item \textbf{Introduction to DQN:}
        \begin{itemize}
            \item DQN combines Q-Learning with deep neural networks.
            \item Key innovations: Experience Replay and Target Network.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Example Case Study - Part 2}
    \textbf{Performance Metrics Used}
    
    \begin{enumerate}
        \item \textbf{Cumulative Reward}
        \begin{itemize}
            \item Definition: Total reward collected by the agent.
            \item Formula: 
            \[
            \text{Cumulative Reward} = \sum_{t=1}^{T} r_t
            \]
            where \( r_t \) is the reward at time \( t \) and \( T \) is the total time steps.
        \end{itemize}
        
        \item \textbf{Win Rate}
        \begin{itemize}
            \item Definition: Percentage of episodes won.
            \item Formula: 
            \[
            \text{Win Rate} = \frac{\text{Number of Wins}}{\text{Total Episodes}} \times 100
            \]
        \end{itemize}
        
        \item \textbf{Training Stability}
        \begin{itemize}
            \item Measurement of consistency across runs using Standard Deviation of rewards.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Example Case Study - Part 3}
    \textbf{Case Study Application: DQN on Pong}
    
    \begin{itemize}
        \item \textbf{Experimental Setup:}
        \begin{itemize}
            \item Trained on the Atari game "Pong."
            \item Key hyperparameters: Learning rate = 0.00025, Batch Size = 32, Discount Factor = 0.99.
        \end{itemize}

        \item \textbf{Results:}
        \begin{itemize}
            \item Cumulative Reward: Stabilized around +20 after 500 episodes.
            \item Win Rate: Increased from 40\% to 85\%.
            \item Stability: Standard deviation reduced from ±15 to ±5.
        \end{itemize}

        \item \textbf{Conclusion:}
        \begin{itemize}
            \item Highlights the importance of metrics for assessing DQN's effectiveness.
            \item Continuous training leads to higher stability in performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Evaluation - Overview}
    \begin{itemize}
        \item When evaluating Reinforcement Learning (RL) models, consider the ethical implications.
        \item Focus areas include:
        \begin{itemize}
            \item Bias
            \item Fairness
        \end{itemize}
        \item These implications affect both model efficacy and societal impact.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Evaluation - Key Concepts}
    \begin{enumerate}
        \item \textbf{Bias in RL Models}
        \begin{itemize}
            \item \textbf{Definition}: Favoring or discriminating against groups/outcomes.
            \item \textbf{Sources of Bias}:
            \begin{itemize}
                \item Data Bias: Non-representative training data perpetuates stereotypes.
                \item Algorithmic Bias: Some algorithms inherently favor certain decisions.
            \end{itemize}
            \item \textbf{Example}: RL hiring model may favor specific demographics based on training data.
        \end{itemize}
        
        \item \textbf{Fairness in Evaluation}
        \begin{itemize}
            \item \textbf{Definition}: Equitable treatment of individuals and groups.
            \item \textbf{Types of Fairness}:
            \begin{itemize}
                \item Individual Fairness: Similar individuals receive similar outcomes.
                \item Group Fairness: Demographic groups receive similar treatment.
            \end{itemize}
            \item \textbf{Example}: RL credit scoring model should ensure equal loan approval probabilities across demographics.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Evaluation - Importance and Methods}
    \begin{block}{Importance of Ethical Evaluation}
        Ensuring RL models are free from bias and promote fairness builds trust among users and communities. Unchecked bias can lead to harmful consequences.
    \end{block}

    \begin{block}{Methods to Mitigate Bias \& Promote Fairness}
        \begin{enumerate}
            \item Diverse Training Data: Include a wide range of scenarios and demographic groups.
            \item Fairness Metrics:
            \begin{itemize}
                \item Statistical Parity: Positive outcome rates should be the same across groups.
                \item Equal Opportunity: True positive rates should be equal among groups.
            \end{itemize}
            \item Model Auditing: Regular audits for bias using simulations to evaluate impacts.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Conclusion}
        Addressing bias and fairness leads to responsible AI systems, promoting positive societal outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Points}
    \begin{enumerate}
        \item \textbf{Importance of Performance Metrics}: Essential measures to evaluate the effectiveness of trained Reinforcement Learning (RL) models.
        \item \textbf{Commonly Used Metrics}:
        \begin{itemize}
            \item \textbf{Cumulative Reward}: Total reward an agent receives over time.
            \item \textbf{Sample Efficiency}: How quickly and effectively an agent learns from interactions.
            \item \textbf{Convergence Rate}: Speed of approaching optimal performance.
        \end{itemize}
        \item \textbf{Ethical Considerations}: Assessing fairness and potential biases in RL systems is crucial for real-world applications.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Future Exploration Areas}
    \begin{enumerate}
        \item \textbf{Developing New Metrics}: Explore metrics that incorporate safety, stability, and interpretability.
        \item \textbf{Long-Term vs. Short-Term Rewards}: Investigate frameworks for effective balance in reward types.
        \item \textbf{Scalability of Metrics}: Focus on high-dimensional environments for meaningful evaluation.
        \item \textbf{Cross-Domain Benchmarking}: Standardize performance benchmarks across various RL applications.
        \item \textbf{Integration of Human Factors}: Capture human-like decision-making in RL agents.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Formulas for Key Metrics}
    \begin{block}{Cumulative Reward (R)}
        \[
        R = \sum_{t=0}^{T} r_t
        \]
        where \( r_t \) is the reward received at time \( t \) and \( T \) is the total number of time steps.
    \end{block}
    
    \begin{block}{Sample Efficiency}
        \[
        SE = \frac{R}{N}
        \]
        where \( R \) is the cumulative reward and \( N \) is the number of interactions with the environment.
    \end{block}
\end{frame}


\end{document}