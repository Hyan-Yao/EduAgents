\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Hadoop and MapReduce]{Week 4: Introduction to Hadoop and MapReduce}
\author[John Doe]{John Doe}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}
    \title{Introduction to Hadoop and MapReduce}
    \maketitle
\end{frame}

\begin{frame}
    \frametitle{Overview of Hadoop and MapReduce}
    \begin{block}{Hadoop Framework}
        The Hadoop framework is designed to handle large datasets across distributed computing environments. Its core components, HDFS and the MapReduce processing model, empower users to efficiently store, manage, and analyze data.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{What is Hadoop?}
    \begin{itemize}
        \item \textbf{Hadoop} is an open-source framework for storage and processing of large data sets in a distributed environment.
        \item Core Components:
            \begin{itemize}
                \item \textbf{HDFS (Hadoop Distributed File System)}: Scalable, fault-tolerant, and designed for high-throughput access to application data.
                \item \textbf{YARN (Yet Another Resource Negotiator)}: Resource management layer that schedules and manages resources across the cluster.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What is MapReduce?}
    MapReduce is a programming model for processing large data sets with a parallel, distributed algorithm. It consists of two tasks:
    \begin{enumerate}
        \item \textbf{Map}: Sorts input data into key-value pairs.
        \item \textbf{Reduce}: Aggregates the Map outputs to generate a consolidated result.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Map Phase Example}
    \begin{block}{Example: Counting Word Occurrences}
        The following code snippet demonstrates how to count occurrences of words in a set of documents:
        \begin{lstlisting}[language=Java]
        public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
            private final static IntWritable one = new IntWritable(1);
            private Text word = new Text();
            public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
                StringTokenizer itr = new StringTokenizer(value.toString());
                while (itr.hasMoreTokens()) {
                    word.set(itr.nextToken());
                    context.write(word, one);
                }
            }
        }
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reduce Phase Example}
    \begin{block}{Example: Summing Counts}
        The following code snippet demonstrates how to sum the counts from the Map phase:
        \begin{lstlisting}[language=Java]
        public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
            private IntWritable result = new IntWritable();
            public void reduce(Text key, Iterable<IntWritable> values, Context context) 
                throws IOException, InterruptedException {
                int sum = 0;
                for (IntWritable val : values) {
                    sum += val.get();
                }
                result.set(sum);
                context.write(key, result);
            }
        }
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Scalability}: Hadoop can scale horizontally by adding nodes, efficiently handling petabytes of data.
        \item \textbf{Fault Tolerance}: HDFS replicates data across nodes for availability during hardware failure.
        \item \textbf{Cost-Effective}: Utilizes commodity hardware for storage and processing, reducing big data management costs.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Understanding Hadoop and the MapReduce paradigm is essential for working with big data technologies. As organizations seek to gain insights from massive datasets, knowledge of these tools becomes increasingly valuable.
    
    \begin{block}{Next Steps}
        In the next slide, we will delve deeper into the specifics of what Hadoop is and its key components.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Hadoop?}
    \begin{block}{Definition}
        Hadoop is an open-source framework designed for the distributed storage and processing of large datasets using a cluster of computers.
    \end{block}
    \begin{block}{Purpose}
        Hadoop enables organizations to handle large volumes of data, ensuring:
        \begin{itemize}
            \item Scalability
            \item Cost Efficiency
            \item Fault Tolerance
            \item Flexibility
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Hadoop}
    \begin{itemize}
        \item \textbf{HDFS (Hadoop Distributed File System)}: Stores data across multiple machines, providing high availability.
        \item \textbf{YARN (Yet Another Resource Negotiator)}: Manages resources and schedules jobs effectively.
        \item \textbf{Hadoop Common}: Libraries and utilities essential for Hadoop modules to function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Hadoop in Action}
    Imagine a retail company collecting customer purchase data from thousands of stores:
    \begin{itemize}
        \item \textbf{Data Types}: Structured (sale amounts) and Unstructured (customer reviews).
        \item \textbf{Usage of Hadoop}: Stores data in a distributed system, allows complex queries and analyses using MapReduce.
        \item \textbf{Outcome}: Actionable insights into customer behavior trends and inventory management.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Hadoop is essential for organizations leveraging big data.
        \item It offers a robust framework that is scalable, cost-efficient, and fault-tolerant.
        \item Supporting various data types enhances usability across industries.
    \end{itemize}
    \textbf{Conclusion:} Understanding Hadoop's core definition and purpose is fundamental for embarking on a big data journey.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Hadoop - Overview}
    \begin{block}{Overview}
        Hadoop is a powerful framework designed to support the processing and storage of large datasets in a distributed computing environment.
    \end{block}
    \begin{itemize}
        \item Core components:
        \begin{itemize}
            \item HDFS (Hadoop Distributed File System)
            \item YARN (Yet Another Resource Negotiator)
            \item Hadoop Common
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Distributed File System (HDFS)}
    \begin{block}{Definition}
        HDFS is the primary storage system of Hadoop, enabling robust storage of large files across a distributed network of machines.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item \textbf{Scalability:} Can store petabytes of data and scales out by adding nodes.
            \item \textbf{Fault Tolerance:} Data is replicated across multiple nodes (default replication factor is 3).
            \item \textbf{High Throughput:} Optimized for large data transfers.
        \end{itemize}
        \item \textbf{Example:} 
        \begin{itemize}
            \item A 1TB file might be split into 128MB blocks and distributed across machines for parallel processing.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Yet Another Resource Negotiator (YARN) and Hadoop Common}
    \begin{block}{YARN Definition}
        YARN is the resource management layer that schedules and manages computing tasks in the Hadoop framework.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item \textbf{Resource Allocation:} Dynamically allocates resources for applications.
            \item \textbf{Multi-tenancy:} Allows various data processing engines to share resources efficiently.
        \end{itemize}
        \item \textbf{Architecture:}
        \begin{itemize}
            \item ResourceManager: Manages cluster resources and task scheduling.
            \item NodeManager: Manages individual nodes and their resources.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Common}
    \begin{block}{Definition}
        Hadoop Common contains libraries and utilities used by other Hadoop modules, providing essential functionalities.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Shared Libraries: Necessary components for HDFS, YARN, MapReduce.
            \item Utilities: Tools for managing data and monitoring clusters.
        \end{itemize}
        \item \textbf{Example:} 
        \begin{itemize}
            \item Includes data compression libraries and file management utilities.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{HDFS - Hadoop Distributed File System}
    \begin{block}{Introduction to HDFS}
        \begin{itemize}
            \item HDFS is a highly scalable, resilient, and distributed file system designed for large datasets.
            \item An integral component of the Hadoop ecosystem, enhancing data processing and management.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of HDFS}
    \begin{enumerate}
        \item \textbf{Scalability}
            \begin{itemize}
                \item Can expand by adding more nodes to store petabytes (PB) of data.
                \item Example: A 1 PB dataset can be spread across hundreds or thousands of servers.
            \end{itemize}
        \item \textbf{Fault Tolerance}
            \begin{itemize}
                \item Automatically replicates data across nodes for accessibility during node failures.
                \item Default replication factor is typically set to 3.
            \end{itemize}
        \item \textbf{High Throughput}
            \begin{itemize}
                \item Optimized for large data sets allowing high throughput for concurrent data access.
            \end{itemize}
        \item \textbf{Streamlined Data Access}
            \begin{itemize}
                \item Designed for streaming access, ideal for sequential processing of large files.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of HDFS}
    \begin{block}{Master-Slave Architecture}
        \begin{itemize}
            \item \textbf{NameNode (Master)}: Manages HDFS metadata and namespace.
            \item \textbf{DataNode (Slave)}: Stores actual data blocks; default block size is 128 MB.
        \end{itemize}
    \end{block}
    \begin{block}{Example of HDFS Storage}
        \begin{itemize}
            \item File \texttt{example.txt} (256 MB) is split into:
            \begin{itemize}
                \item Block 1 (128 MB): Stored on DataNode A1, replicated on B1 and C1.
                \item Block 2 (128 MB): Stored on DataNode A2, replicated on B2 and C2.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{HDFS Data Workflow}
    \begin{enumerate}
        \item \textbf{Client Request}: A client requests data storage or retrieval.
        \item \textbf{Request to NameNode}: The client asks the NameNode for DataNode locations.
        \item \textbf{Data Interaction}: The client interacts with DataNodes as instructed by the NameNode.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{itemize}
        \item HDFS is crucial for handling large data volumes, forming the backbone of big data processing.
        \item Its design supports fault tolerance and data availability, essential for real-world applications.
        \item Understanding HDFS is foundational for leveraging Hadoop for efficient large-scale data processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{YARN - Yet Another Resource Negotiator}
    \begin{block}{Overview of YARN}
        YARN, which stands for Yet Another Resource Negotiator, is a critical component of the Hadoop ecosystem introduced in Hadoop 2.x. It serves as the resource management layer for Hadoop, enabling more efficient and flexible use of resources across various applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of YARN}
    \begin{enumerate}
        \item \textbf{Resource Management}
        \begin{itemize}
            \item Manages CPU and memory resources across all nodes in the Hadoop cluster.
            \item Allows multiple data processing engines like MapReduce and Spark to share resources dynamically.
        \end{itemize}

        \item \textbf{Architecture}
        \begin{itemize}
            \item \textbf{ResourceManager (RM)}: The master daemon managing resources and scheduling tasks.
            \item \textbf{NodeManager (NM)}: Manages the life cycle of containers on each node and reports resource availability to RM.
            \item \textbf{ApplicationMaster (AM)}: Coordinates the execution of applications and manages resources for them.
        \end{itemize}    
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workflow and Example of YARN}
    \begin{block}{Workflow}
        1. ResourceManager receives resource requests from various ApplicationMasters. 
        2. Allocates resources and communicates with NodeManagers.
        3. NodeManagers launch containers for executing applications.
    \end{block}

    \begin{block}{Example Scenario}
        Consider a Hadoop cluster running a MapReduce job and a Spark job:
        \begin{itemize}
                \item The ResourceManager allocates resources based on application demands and priorities.
                \item Both jobs run concurrently without interference, utilizing available resources efficiently.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits and Key Points of YARN}
    \begin{block}{Benefits}
        \begin{itemize}
            \item \textbf{Scalability}: Supports various data processing workloads on the same cluster.
            \item \textbf{Resource Efficiency}: Dynamically allocates resources based on workload demand.
            \item \textbf{Multi-Tenancy}: Allows multiple users to run diverse applications on a shared infrastructure.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Separates resource management from processing for flexibility and efficiency.
            \item Supports various data processing frameworks, enhancing Hadoop's versatility.
            \item Understanding YARN is crucial for effective application management in Hadoop clusters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        YARN revolutionizes resource management in Hadoop, enabling concurrent applications for various analysis methodologies.
        Understanding YARN is essential to leverage Hadoop's full potential in data processing.
    \end{block}
    
    \begin{block}{Next Slide Reference}
        After this introduction to YARN, we will transition to MapReduce, diving deeper into its basic concepts and programming model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to MapReduce}
    \begin{block}{What is MapReduce?}
        MapReduce is a programming model and framework designed for processing large datasets in a distributed computing environment, particularly within the Hadoop ecosystem.
    \end{block}
    \begin{itemize}
        \item Enables processing of vast amounts of data across clusters using simple programming constructs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Map Function}
        \begin{itemize}
            \item Transforms input dataset into intermediate key-value pairs.
            \item Example: In a word count application, it outputs key-value pairs (word, count).
        \end{itemize}
        
        \begin{lstlisting}[language=Python]
def map_function(document):
    for word in document.split():
        emit(word, 1)  # Emit each word with a count of 1
        \end{lstlisting}
        
        \item \textbf{Shuffle and Sort}
        \begin{itemize}
            \item Groups and sorts intermediate key-value pairs by key.
            \item Ensures all values for the same key are sent together to Reduce.
        \end{itemize}
        
        \item \textbf{Reduce Function}
        \begin{itemize}
            \item Aggregates results from the Shuffle phase.
            \item Example: Combines counts for each word to find total occurrences.
        \end{itemize}

        \begin{lstlisting}[language=Python]
def reduce_function(word, counts):
    return (word, sum(counts))  # Compute total count for each word
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MapReduce Workflow}
    \begin{itemize}
        \item \textbf{Input Data:} Data is divided into chunks across the cluster.
        \item \textbf{Mapping:} Each chunk is processed by the Map function simultaneously.
        \item \textbf{Shuffling:} Key-value pairs are sorted and grouped by key.
        \item \textbf{Reducing:} Aggregated results are computed by the Reduce function.
        \item \textbf{Output:} Final results are written to distributed storage.
    \end{itemize}
    \begin{block}{Benefits of Using MapReduce}
        \begin{itemize}
            \item Scalability
            \item Fault Tolerance
            \item Data Locality Optimization
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MapReduce Workflow}
    \begin{block}{Overview}
        MapReduce is a programming model used to process large data sets across distributed clusters. 
        The workflow consists of three main phases: \textbf{Map}, \textbf{Shuffle}, and \textbf{Reduce}. 
        Each phase plays a crucial role in transforming raw data into meaningful results.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Map Phase}
    \begin{block}{Definition}
        In the Map phase, raw input data is processed, and key-value pairs are generated as output.
    \end{block}
    \begin{itemize}
        \item \textbf{Functionality}:
        \begin{itemize}
            \item Takes a set of inputs (e.g., log files, CSVs).
            \item Processes each record to produce intermediate key-value pairs.
        \end{itemize}
        \item \textbf{Example}: 
        \begin{itemize}
            \item Input Data: \texttt{"apple": 4, "banana": 2, "apple": 3}
            \item Output from Map: \texttt{("apple", 1), ("apple", 1), ("banana", 1)}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Shuffle Phase}
    \begin{block}{Definition}
        The Shuffle phase reorganizes the data based on the keys produced from the Map function.
    \end{block}
    \begin{itemize}
        \item \textbf{Functionality}:
        \begin{itemize}
            \item Sorts and groups the intermediate key-value pairs.
            \item Ensures that all instances of the same key are sent to the same reducer.
        \end{itemize}
        \item \textbf{Example}:
        \begin{itemize}
            \item Intermediate Map Output: \texttt{("apple", 1), ("apple", 1), ("banana", 1)}
            \item Shuffle Result: 
            \begin{itemize}
                \item Reducer receives: \texttt{("apple", [1, 1]), ("banana", [1])}
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reduce Phase}
    \begin{block}{Definition}
        In the Reduce phase, the grouped data is processed to produce the final output.
    \end{block}
    \begin{itemize}
        \item \textbf{Functionality}:
        \begin{itemize}
            \item Takes the grouped key-value pairs and performs reduction operations (e.g., summation, averaging).
        \end{itemize}
        \item \textbf{Example}:
        \begin{itemize}
            \item Input to Reduce: \texttt{("apple", [1, 1]), ("banana", [1])}
            \item Output from Reduce: \texttt{("apple", 2), ("banana", 1)}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Code Snippet}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Scalability}: Each phase can be distributed across many nodes.
            \item \textbf{Parallel Processing}: The Map and Reduce tasks run in parallel, improving efficiency.
            \item \textbf{Data Handling}: The shuffling ensures relevant data for each key is processed together.
        \end{itemize}
    \end{block}
    \begin{block}{Code Snippet}
    \begin{lstlisting}[language=python]
def map_function(input):
    for line in input:               
        words = line.split()   
        for word in words:    
            emit((word, 1))

def reduce_function(key, values):
    total = sum(values)
    emit((key, total))
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Map Function - Overview}
    \begin{block}{What is the Map Function?}
        The Map function is a core component of the MapReduce programming model, integral to processing large datasets in a distributed computing environment, such as Apache Hadoop. It is designed to transform input data into key-value pairs, enabling efficient data processing and analysis.
    \end{block}

    \begin{block}{Purpose of the Map Function}
        \begin{itemize}
            \item \textbf{Data Transformation}: Converts input records into (key, value) pairs.
            \item \textbf{Parallel Processing}: Runs concurrently across multiple nodes, increasing efficiency.
            \item \textbf{Data Organization}: Prepares the output for the Shuffle phase, reducing redundancy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Map Function - Operation}
    \begin{block}{How the Map Function Works}
        \begin{enumerate}
            \item \textbf{Input Splits}: Divides input data into "input splits" for independent processing.
            \item \textbf{Mapping Process}:
            \begin{itemize}
                \item Each record is processed individually.
                \item Applies user-defined transformations to produce key-value pairs.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Map Function - Example}
    \begin{block}{Example of a Map Function}
        Suppose we have the following input data (a list of words):
        
        \begin{verbatim}
        Hello, how are you
        I am fine, thank you
        \end{verbatim}

        \textbf{Map Function Code} (in Python-like pseudocode):
        \begin{lstlisting}
def map_function(line):
    for word in line.split():
        yield (word.lower(), 1)
        \end{lstlisting}

        \textbf{Output of the Map Function}:
        
        \begin{verbatim}
        [("hello", 1), ("how", 1), ("are", 1), ("you", 1),
         ("i", 1), ("am", 1), ("fine", 1), ("thank", 1), ("you", 1)]
        \end{verbatim}
        This output represents each unique word as a key with a count of 1.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Map Function - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The Map function is the first step in the MapReduce workflow, transforming data for analysis.
            \item Operates on data independently, enabling parallel processing.
            \item Outputs key-value pairs to facilitate the next step: Shuffling.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding the Map function is essential for leveraging Hadoop and MapReduce effectively. It sets the foundation for data processing and paves the way for aggregation in the Reduce phase.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reduce Function - Overview}
    \begin{block}{Overview of the Reduce Function}
        The Reduce function is a vital part of the MapReduce paradigm in Hadoop. 
        After the \textbf{Map} phase processes data and generates intermediate key-value pairs, 
        the Reduce function aggregates these pairs, summarizing the data into a concise output.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reduce Function - How It Works}
    \begin{enumerate}
        \item \textbf{Input:} 
        The input consists of intermediate key-value pairs from the Mappers. Each unique key is sent to the corresponding Reducer with all associated values.
        
        \item \textbf{Aggregation:} 
        The Reducer performs operations on the values for each key, including:
        \begin{itemize}
            \item Counting total occurrences
            \item Finding averages
            \item Combining values
        \end{itemize}

        \item \textbf{Output:} 
        A new set of key-value pairs is produced, where each key corresponds to an aggregated result.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reduce Function - Example and Conclusion}
    \begin{block}{Example of the Reduce Function}
        \textbf{Map Output:}
        \begin{itemize}
            \item (apple, 1)
            \item (banana, 1)
            \item (apple, 1)
            \item (orange, 1)
            \item (banana, 1)
        \end{itemize}
        
        \textbf{Reduction Process:}
        For the keys:
        \begin{itemize}
            \item apple: Input -> [(1), (1)] → Output -> (apple, 2)
            \item banana: Input -> [(1), (1)] → Output -> (banana, 2)
            \item orange: Input -> [(1)] → Output -> (orange, 1)
        \end{itemize}
        
        \textbf{Final Output:}
        \begin{itemize}
            \item (apple, 2)
            \item (banana, 2)
            \item (orange, 1)
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        The Reduce function transforms intermediate outputs into meaningful results, laying the foundation for effective data analysis using Hadoop MapReduce.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reduce Function - Code Snippet}
    \begin{block}{Code Snippet for a Simple Reduce Function}
        \begin{lstlisting}[language=Python]
def reducer(key, values):
    # Sum the occurrences of the key
    total_count = sum(values)
    return (key, total_count)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing a Basic MapReduce Application}
    \begin{block}{Introduction to MapReduce}
        MapReduce is a programming model developed by Google for large-scale data processing in a distributed environment. 
        \begin{itemize}
            \item **Map Function**: Produces intermediate key-value pairs from input data.
            \item **Reduce Function**: Aggregates intermediate data to produce a final output.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Basic Structure of a MapReduce Application}
    \begin{enumerate}
        \item **Define the Map Function**: Reads input data and emits intermediate key-value pairs.
        \item **Define the Reduce Function**: Takes intermediate key-value pairs and produces the output.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Word Count Application - Map Function}
    \begin{block}{Objective}
        Count the frequency of each word in a text input.
    \end{block}
    
    \begin{block}{Step 1: Map Function}
        The Map function outputs key-value pairs for each word found in the input text.
    \end{block}
    
    \begin{lstlisting}[language=Java]
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends Mapper<Object, Text, Text, IntWritable> {
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        StringTokenizer tokenizer = new StringTokenizer(value.toString());
        while (tokenizer.hasMoreTokens()) {
            word.set(tokenizer.nextToken());
            context.write(word, one);
        }
    }
}
    \end{lstlisting}
    
    \begin{block}{Explanation}
        The `map` method processes each line of input, splits it into words, and emits each `word` with a count of `1`.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Word Count Application - Reduce Function}
    \begin{block}{Step 2: Reduce Function}
        This function aggregates the counts for each word.
    \end{block}
    
    \begin{lstlisting}[language=Java]
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }
        context.write(key, new IntWritable(sum));
    }
}
    \end{lstlisting}
    
    \begin{block}{Explanation}
        The `reduce` method sums all the counts for a particular word and emits the total count.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Word Count Application - Job Configuration}
    \begin{block}{Step 3: Job Configuration}
        Set up the job configuration specifying mapper, reducer, input/output paths, etc.
    \end{block}
    
    \begin{lstlisting}[language=Java]
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;

public class WordCount {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
    \end{lstlisting}
    
    \begin{block}{Explanation}
        The `main` method configures and runs the MapReduce job with specified input and output paths.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item MapReduce divides large datasets into manageable parts for processing.
            \item The Map function generates intermediate data, while the Reduce function aggregates it.
            \item Java is a common language for MapReduce applications, but others are viable.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Implementing a basic MapReduce application involves writing the Map and Reduce functions, along with job configuration. This example illustrates how to efficiently process data using MapReduce.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up the Development Environment - Overview}
    \begin{block}{Overview}
        Setting up the development environment for Hadoop is crucial for building and executing MapReduce applications effectively. This slide provides step-by-step guidance on the tools and configurations needed to get started with Hadoop.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up the Development Environment - Requirements}
    \begin{block}{1. Requirements for Installation}
        \begin{itemize}
            \item \textbf{Java Development Kit (JDK):} Hadoop is primarily written in Java, so JDK version 8 or above is required.
            \begin{itemize}
                \item \textbf{Installation Check:} 
                \begin{lstlisting}
                java -version
                \end{lstlisting}
            \end{itemize}
            \item \textbf{Hadoop Distribution Package:} Download the latest stable version (e.g., Apache Hadoop 3.3.x) from the official website.
            \item \textbf{Linux Environment:} A Linux-based system (like Ubuntu) is preferred. Windows users can consider using WSL (Windows Subsystem for Linux) for stability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up the Development Environment - Installation Steps}
    \begin{block}{2. Installation Steps}
        \begin{enumerate}
            \item \textbf{Install Java:}
                \begin{lstlisting}
                sudo apt update
                sudo apt install openjdk-8-jdk
                \end{lstlisting}
            \item \textbf{Set Up Hadoop:}
                \begin{lstlisting}
                tar -xzvf hadoop-3.3.x.tar.gz
                sudo mv hadoop-3.3.x /usr/local/hadoop
                \end{lstlisting}
            \item \textbf{Configure Environment Variables:}
                \begin{lstlisting}
                nano ~/.bashrc
                export HADOOP_HOME=/usr/local/hadoop
                export PATH=$PATH:$HADOOP_HOME/bin
                export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
                source ~/.bashrc
                \end{lstlisting}
            \item \textbf{Verify the Installation:}
                \begin{lstlisting}
                hadoop version
                \end{lstlisting}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up the Development Environment - Configuring Hadoop}
    \begin{block}{3. Configuring Hadoop Files}
        \begin{itemize}
            \item \textbf{Edit Configuration Files:}
                \begin{itemize}
                    \item \textbf{hadoop-env.sh:} Set Java home.
                    \begin{lstlisting}
                    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
                    \end{lstlisting}
                    \item \textbf{core-site.xml:} Core properties configuration.
                    \begin{lstlisting}[language=xml]
                    <configuration>
                        <property>
                            <name>fs.defaultFS</name>
                            <value>hdfs://localhost:9000</value>
                        </property>
                    </configuration>
                    \end{lstlisting}
                    \item \textbf{hdfs-site.xml:} HDFS properties configuration.
                    \begin{lstlisting}[language=xml]
                    <configuration>
                        <property>
                            <name>dfs.replication</name>
                            <value>1</value>
                        </property>
                    </configuration>
                    \end{lstlisting}
                \end{itemize}
            \item \textbf{Format the NameNode:}
                \begin{lstlisting}
                hdfs namenode -format
                \end{lstlisting}
        \end{itemize}
    \end{block}

    \begin{block}{4. Key Points to Remember}
        \begin{itemize}
            \item Ensure all paths are correctly set in the environment variables.
            \item It’s beneficial to run a local single-node cluster for development before deploying on a multi-node cluster.
            \item Familiarize yourself with Hadoop components like HDFS, YARN, and MapReduce.
        \end{itemize}
    \end{block}

    \begin{block}{Next Steps}
        Following this setup, proceed to run simple MapReduce jobs, which will be explored in the next slide.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Running MapReduce Jobs - Overview}
    \begin{block}{Key Concepts of MapReduce Jobs}
        MapReduce is a programming model for processing large data sets across distributed clusters using a parallel, distributed algorithm. 
        The execution involves setting up the environment, writing code, and monitoring performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Running MapReduce Jobs - Steps}
    \begin{enumerate}
        \item Develop Your MapReduce Program
        \item Compile Your Code
        \item Upload Data to HDFS
        \item Run Your Job Using Hadoop Command
        \item Monitor Job Execution
        \item View Output
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Develop Your MapReduce Program}
    \begin{itemize}
        \item Write the program in Java or other supported languages.
        \item Implement the \texttt{Mapper} and \texttt{Reducer} interfaces.
        \begin{block}{Mapper}
            Processes input data and generates intermediate key-value pairs.
        \end{block}
        \begin{block}{Reducer}
            Processes intermediate data and writes final output.
        \end{block}
    \end{itemize}
    \begin{lstlisting}[language=Java]
public class WordCount {
    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) 
                throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }
}
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Steps in MapReduce Execution}
    \begin{enumerate}
        \item \textbf{Compile Your Code:} 
            Use build tools like Apache Maven or Gradle to create a JAR file.
        \item \textbf{Upload Data to HDFS:}
            \begin{block}{Command}
            \texttt{hadoop fs -put local\_input.txt /user/hadoop/input/}
            \end{block}
        \item \textbf{Run Your Job:}
            \begin{block}{Command}
            \texttt{hadoop jar your\_program.jar MainClassName /user/hadoop/input/ /user/hadoop/output/}
            \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monitoring and Output}
    \begin{enumerate}
        \item \textbf{Monitor Job Execution:}
            Use the Resource Manager UI or:
            \begin{block}{Command}
            \texttt{yarn application -list}
            \end{block}
        \item \textbf{View Output:}
            After completion, retrieve output from HDFS:
            \begin{block}{Command}
            \texttt{hadoop fs -cat /user/hadoop/output/part-00000}
            \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Integration with the Hadoop Ecosystem: 
            MapReduce is tied to various tools such as HDFS, YARN, and Hadoop Common.
        \item Scalability:
            Capable of processing petabytes of data in a distributed environment.
        \item Error Handling:
            Implement robust error handling in your code to manage processing failures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thought}
    Mastering the execution of MapReduce jobs in Hadoop is essential for efficiently tackling big data challenges. 
    Understanding these steps will equip you with the knowledge to utilize Hadoop's capabilities in processing large datasets effectively. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Use Cases of MapReduce - Overview}
    \begin{block}{Introduction to MapReduce Applications}
        MapReduce is a programming model designed for processing large data sets across distributed clusters. Its ability to parallelize tasks and manage huge amounts of data efficiently makes it a vital tool in various industries and applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Use Cases of MapReduce - Data Analysis}
    \begin{enumerate}
        \item Data Analysis in Social Media:
        \begin{itemize}
            \item \textbf{Description:} Analyzing vast amounts of user interaction data for trends, engagement, and sentiment.
            \item \textbf{Example:} Twitter processes tweets to identify trending hashtags or sentiments.
            \item \textbf{Key Point:} Enables real-time analysis, enhancing audience engagement.
        \end{itemize}
        
        \item Log Analysis:
        \begin{itemize}
            \item \textbf{Description:} Analyzing web server log files for user activity and trends.
            \item \textbf{Example:} Facebook and Google use MapReduce for detecting anomalies and improving performance.
            \item \textbf{Key Point:} Converts raw log data into actionable insights.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Use Cases of MapReduce - Recommendations and Science}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item Recommendation Systems:
        \begin{itemize}
            \item \textbf{Description:} Personalizing recommendations based on user data.
            \item \textbf{Example:} Amazon and Netflix analyze user interactions to generate tailored suggestions.
            \item \textbf{Key Point:} Facilitates enhanced user experiences through tailored recommendations.
        \end{itemize}
        
        \item Scientific Data Processing:
        \begin{itemize}
            \item \textbf{Description:} Processing large datasets in fields like genomics and astronomy.
            \item \textbf{Example:} Researchers use MapReduce for tasks such as genomic sequence alignment.
            \item \textbf{Key Point:} Enables significant breakthroughs in understanding complex datasets.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Use Cases of MapReduce - Media Processing}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item Image and Video Processing:
        \begin{itemize}
            \item \textbf{Description:} Handling large datasets of images and videos for various processing tasks.
            \item \textbf{Example:} YouTube uses MapReduce to efficiently transcode videos.
            \item \textbf{Key Point:} Reduces time required for analysis through parallel processing capabilities.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Conclusion}
        MapReduce plays a crucial role in big data processing across industries, making it indispensable for modern data-driven decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Example MapReduce Job}
\begin{lstlisting}[language=Python]
# Mapper function
def mapper():
    for line in sys.stdin:
        for word in line.strip().split():
            print(f"{word}\t1")

# Reducer function
def reducer():
    current_word = None
    current_count = 0
    for line in sys.stdin:
        word, count = line.strip().split("\t")
        count = int(count)
        if current_word == word:
            current_count += count
        else:
            if current_word:
                print(f"{current_word}\t{current_count}")
            current_word = word
            current_count = count
    # Don't forget to output the last word
    if current_word:
        print(f"{current_word}\t{current_count}")
\end{lstlisting}
\end{frame}

\begin{frame}{Challenges and Limitations of MapReduce}
    \begin{itemize}
        \item MapReduce is powerful for large-scale data processing.
        \item However, there are common challenges in its implementation and effectiveness.
        \item Understanding these can help in making informed data processing strategies.
    \end{itemize}
\end{frame}

\begin{frame}{Common Challenges with MapReduce - Part 1}
    \begin{enumerate}
        \item \textbf{Complexity of Programming:}
            \begin{itemize}
                \item Writing MapReduce programs can be complex for those unfamiliar with the model.
                \item Developers must define \texttt{map()} and \texttt{reduce()} functions.
                \item Requires good understanding of input/output formats and data flow.
            \end{itemize}
        
        \item \textbf{Performance Issues:}
            \begin{itemize}
                \item Overheads from job setup and data serialization can hinder performance.
                \item Multiple small jobs can lead to bottlenecks.
                \item Batch smaller tasks into a single job to reduce overhead.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Common Challenges with MapReduce - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Lack of Real-time Processing:}
            \begin{itemize}
                \item MapReduce is designed for batch processing.
                \item Not suitable for real-time data processing needs.
                \item Consider alternatives like Apache Spark for streaming processes.
            \end{itemize}
        
        \item \textbf{Data Skew:}
            \begin{itemize}
                \item Imbalanced data distribution can lead to performance degradation.
                \item Custom partitioning strategies may be needed.
            \end{itemize}
        
        \item \textbf{Debugging and Monitoring Challenges:}
            \begin{itemize}
                \item Asynchronous nature makes error tracking cumbersome.
                \item Effective logging and monitoring tools are essential.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Common Challenges with MapReduce - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{5}
        \item \textbf{Limited Iterative Processing:}
            \begin{itemize}
                \item MapReduce is not designed for iterative algorithms.
                \item Algorithms like PageRank are better suited for frameworks that handle iterations.
                \item Apache Spark is suggested for effective iterative processing.
            \end{itemize}
    \end{enumerate}

    \textbf{Summary:} Understanding these challenges is crucial for designing efficient data processing solutions.
\end{frame}

\begin{frame}{Key Takeaways}
    \begin{itemize}
        \item MapReduce can be complex, especially for smaller tasks.
        \item Performance issues often arise from poor configuration and data distribution.
        \item Alternatives like Apache Spark may be better for iterative and real-time applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future of Hadoop and MapReduce}
    \begin{block}{Overview}
        The evolution of data processing demands innovative solutions that address growing data volumes, the need for real-time processing, and the integration of AI and machine learning. While Hadoop and MapReduce have been pivotal in the big data landscape, their future hinges on their adaptability to emerging trends.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Trends Influencing the Future}
    \begin{enumerate}
        \item \textbf{Cloud Adoption}
        \begin{itemize}
            \item Many organizations are migrating to cloud-based platforms (e.g., AWS, Google Cloud, Azure) for Hadoop implementations.
            \item \textit{Example:} AWS offers Amazon EMR, simplifying the setup of Hadoop clusters.
        \end{itemize}

        \item \textbf{Integration with Real-time Processing Frameworks}
        \begin{itemize}
            \item Hybrid architectures utilizing tools like Apache Spark, Apache Flink, and Kafka alongside Hadoop provide real-time data processing capabilities.
            \item \textit{Illustration:} A layered architecture showing Hadoop for batch processing and Spark for real-time analytics.
        \end{itemize}

        \item \textbf{Growth of Machine Learning}
        \begin{itemize}
            \item Hadoop ecosystems are being extended with tools like Apache Mahout and H2O.ai to facilitate model training over large datasets.
            \item \textit{Example:} Using Hadoop's distributed file system (HDFS) to store training data while deploying Spark for model training.
        \end{itemize}

        \item \textbf{Serverless Technologies}
        \begin{itemize}
            \item The rise of serverless computing models is simplifying the deployment of Hadoop jobs, focusing on pay-per-use models.
            \item \textit{Example:} Implementing Spark jobs without managing infrastructure through platform services like AWS Lambda.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Focus Areas for Improvement}
    \begin{itemize}
        \item \textbf{Usability and Accessibility:} Ensuring that tools are user-friendly and accessible to data scientists with varying levels of coding skills.
        \item \textbf{Multi-Modal Data Processing:} Future Hadoop ecosystems are expected to handle structured, semi-structured, and unstructured data seamlessly.
        \item \textbf{Data Privacy and Security:} Enhanced security features are critical as big data systems expand, especially regarding regulation compliance (e.g., GDPR).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        Hadoop and MapReduce are on the cusp of transformation, driven by advancements in cloud computing, real-time processing, and machine learning. Staying abreast of these trends will be essential for professionals seeking to harness the full potential of their big data strategies.
    \end{block}
    \begin{itemize}
        \item Embrace cloud services for scalability and ease of deployment.
        \item Integrate Hadoop with modern real-time processing frameworks to enhance data processing capabilities.
        \item Keep an eye on developments in machine learning to leverage Hadoop’s capabilities fully.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Overview of Hadoop and MapReduce}
    \begin{itemize}
        \item \textbf{Hadoop} is an open-source framework for distributed storage and processing of large datasets using clusters of computers.
        \item \textbf{MapReduce} is a programming model for processing and generating large datasets in parallel across a Hadoop cluster.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Key Concepts Covered}
    \begin{enumerate}
        \item \textbf{Hadoop Architecture}
        \begin{itemize}
            \item \textbf{HDFS (Hadoop Distributed File System)}: Breaks large files into smaller chunks distributed across nodes.
            \item \textbf{YARN (Yet Another Resource Negotiator)}: Manages resources and job scheduling.
        \end{itemize}
        
        \item \textbf{MapReduce Process}
        \begin{itemize}
            \item \textbf{Mapping}: Processes data into key-value pairs.
            \item \textbf{Reducing}: Aggregates the output from mappers to produce final output.
        \end{itemize}
        
        \item \textbf{Data Processing Workflow}
        \begin{itemize}
            \item \textbf{Input}: Data read from HDFS.
            \item \textbf{Map}: Transforms data into key-value pairs.
            \item \textbf{Shuffle and Sort}: Sorts and groups intermediate data.
            \item \textbf{Reduce}: Generates final output from grouped data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Importance of Hadoop and MapReduce}
    \begin{itemize}
        \item \textbf{Scalability}: Handles vast amounts of data by adding more nodes.
        \item \textbf{Fault-tolerance}: HDFS replicates data, ensuring availability during hardware failure.
        \item \textbf{Cost-effective}: Utilizes commodity hardware, reducing investment compared to traditional data warehouses.
    \end{itemize}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Hadoop encompasses tools like HDFS, YARN, and others (e.g., Hive, Pig).
            \item MapReduce simplifies parallel processing for large datasets.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}