\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Title Page Information
\title[Week 8: Data Management]{Week 8: Data Management with Streaming Technologies}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Streaming Technologies}
    \begin{itemize}
        \item Streaming technologies enable real-time data processing.
        \item Organizations can manage and analyze data as it is generated.
        \item This shift transforms business operations, allowing for:
        \begin{itemize}
            \item Immediate insights
            \item Quicker decision-making
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts and Significance}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Streaming Data}: Continuous flow of real-time data.
            \item \textbf{Stream Processing}: Real-time ingestion and processing to extract insights.
            \item \textbf{Event-driven Architecture}: Data flow triggered by events, enhancing responsiveness.
        \end{itemize}
    \end{block}
    
    \begin{block}{Significance of Streaming Technologies}
        \begin{enumerate}
            \item \textbf{Real-Time Insights}: Immediate actionability in response to changing conditions.
            \item \textbf{Enhanced User Experiences}: Applications deliver real-time content and engagement.
            \item \textbf{Operational Efficiency}: Continuous data processing reduces latency of traditional methods.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Example}
    \begin{block}{Applications of Streaming Technologies}
        \begin{itemize}
            \item \textbf{Financial Services}: Important in fraud detection and algorithmic trading.
            \item \textbf{IoT Devices}: Monitoring usage in real-time for resource management.
            \item \textbf{Social Media Analytics}: Analyzing user content for trend identification.
        \end{itemize}
    \end{block}

    \begin{block}{Example: Ride-sharing Application}
        \begin{itemize}
            \item \textbf{Data Sources}: User location data, traffic data, payment transactions.
            \item \textbf{Real-Time Processing}: Dynamically calculates routes and estimated arrival times.
            \item \textbf{Outcome}: Improved satisfaction and optimized fleet management.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Streaming technologies bridge data generation and processing.
        \item Enhance decision-making speed, operational efficiency, and customer engagement.
        \item Critical to understand architecture for effective data management solutions.
    \end{itemize}

    \begin{block}{Final Note}
        Streaming technologies are pivotal in modern data management, driving innovation and optimizing operations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Real-Time Data Processing}
    \begin{block}{Definition}
        Real-time data processing refers to the immediate processing of data as it is created or received, allowing for instantaneous analysis and reaction.
    \end{block}
    \begin{block}{Comparison}
        This contrasts with batch processing, where data is collected over time and processed as a group.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Advantages of Real-Time Data Management}
    \begin{enumerate}
        \item \textbf{Immediate Decision-Making:}
            \begin{itemize}
                \item Enables timely responses to events (e.g., fraud detection in finance).
            \end{itemize}
        
        \item \textbf{Enhanced Customer Experience:}
            \begin{itemize}
                \item Personalizes experiences based on current user behavior (e.g., e-commerce recommendations).
            \end{itemize}

        \item \textbf{Operational Efficiency:}
            \begin{itemize}
                \item Real-time monitoring allows for process optimization (e.g., adjusting inventory levels).
            \end{itemize}

        \item \textbf{Competitive Advantage:}
            \begin{itemize}
                \item Companies can outpace rivals by leveraging real-time insights (e.g., dynamic pricing by Amazon).
            \end{itemize}
        
        \item \textbf{Informed Predictive Analytics:}
            \begin{itemize}
                \item Enhances accuracy of analytics for better forecasting.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples & Comparison Table}
    \begin{block}{Examples of Real-Time Data Processing in Industries}
        \begin{itemize}
            \item \textbf{Finance:} Stock trading platforms for high-frequency trading.
            \item \textbf{Healthcare:} Real-time vital monitoring to improve patient outcomes.
            \item \textbf{Logistics:} Adjusting delivery routes based on real-time traffic data.
        \end{itemize}
    \end{block}
    
    \begin{table}[h]
        \centering
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Feature} & \textbf{Real-Time Processing} & \textbf{Batch Processing} \\ \hline
            Timeliness & Instantly reacts to data & Processes data at intervals \\ \hline
            Use Case & Fraud detection, live monitoring & Monthly reports, complex calculations \\ \hline
            Resource Usage & Potentially resource-intensive & Generally less intensive overall \\ \hline
            Flexibility & Highly adaptable & Rigid, fixed intervals \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Kafka - Overview}
    \begin{block}{Overview of Apache Kafka}
        Apache Kafka is an open-source stream processing platform designed to handle high-throughput, real-time data feeds. 
        It serves as a backbone for applications in various industries, allowing efficient and reliable processing of data streams.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Kafka - Key Concepts}
    \begin{itemize}
        \item \textbf{Stream Processing}: Continuous data processing allows real-time responses for use cases like fraud detection and system monitoring.
        
        \item \textbf{Distributed System}: Inherently distributed design enabling horizontal scaling for fault tolerance and high availability.
        
        \item \textbf{Decoupling Producer and Consumer}: Producers and consumers operate independently, allowing both to scale as needed.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Kafka - Main Components}
    \begin{itemize}
        \item \textbf{Producers}: Applications that publish data to Kafka topics, selecting topics and sending messages.
        
        \item \textbf{Brokers}: Servers in a Kafka cluster that manage message storage and handle requests from producers and consumers.
        
        \item \textbf{Topics}: Categories or feeds to which records are published, organized into multiple partitions for load balancing.
        
        \item \textbf{Consumers}: Applications that subscribe to topics and process streams of records in real-time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Kafka - Use Case and Key Points}
    \begin{block}{Example Use Case}
        \textbf{Real-Time Analytics:} In the retail sector, Kafka tracks user interactions on e-commerce platforms, providing crucial insights into user behavior for optimizing marketing strategies.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Event-Driven Architecture}: Kafka supports building systems that react to events in real-time.
        \item \textbf{Scalability and Reliability}: It is designed to handle high data volumes across distributed environments.
        \item \textbf{Integration Compatibility}: Connects seamlessly with frameworks like Apache Spark and Apache Flink.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Kafka - Conclusion}
    \begin{block}{Conclusion}
        Apache Kafka is a powerful real-time data management tool that supports high throughput, low latency, and reliability. 
        Understanding its architecture and components is crucial for leveraging Kafka in data-driven applications.
    \end{block}
    By mastering the foundational concepts of Apache Kafka, you will be well-prepared to explore more complex stream processing topics in future discussions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{itemize}
        \item Apache Kafka is a distributed streaming platform for real-time data pipelines and streaming applications.
        \item Designed to be:
            \begin{itemize}
                \item Highly scalable
                \item Fault-tolerant
                \item Durable
            \end{itemize}
        \item Vital for managing large volumes of data across various applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Kafka Architecture}
    \begin{enumerate}
        \item \textbf{Producers}
            \begin{itemize}
                \item Applications that publish data to Kafka topics.
                \item Can send data synchronously or asynchronously.
                \item \textit{Example:} An e-commerce website's server sending user activity data.
            \end{itemize}

        \item \textbf{Brokers}
            \begin{itemize}
                \item Kafka servers that store data and serve client requests.
                \item Ensures data availability and reliability.
                \item \textit{Example:} Other brokers handling requests if one broker crashes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Topics}
            \begin{itemize}
                \item Categories or feed names for published records.
                \item Split into partitions for parallel processing.
                \item \textit{Example:} A topic called "WeatherData" for weather monitoring.
            \end{itemize}

        \item \textbf{Consumers}
            \begin{itemize}
                \item Applications that read data from Kafka topics.
                \item Can subscribe to multiple topics; support consumer groups for load balancing.
                \item \textit{Example:} A reporting tool generating daily statistics from the "WeatherData" topic.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Diagram of Kafka Components}
    % Diagram placement with a simple representation of the architecture
    \begin{center}
        \includegraphics[width=0.8\linewidth]{kafka_diagram.png} % Replace with your diagram image
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Decoupling:} Independent operation of producers, brokers, and consumers allows scalability.
        \item \textbf{Fault Tolerance:} Data is replicated across brokers ensuring reliability.
        \item \textbf{Performance:} High throughput and low-latency messaging for large-scale data processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps}
    \begin{itemize}
        \item Understanding Kafka architecture, including Producers, Brokers, Topics, and Consumers, is essential for real-time data processing solutions.
        \item \textbf{Next Steps:} We will explore topics and partitions to understand how Kafka manages streams of data efficiently!
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Topics and Partitions - Concepts}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Kafka Topics:}
            \begin{itemize}
                \item A category or feed name for records published.
                \item Acts as a logical channel for data flow.
                \item Multi-subscriber: allows multiple producers and consumers.
            \end{itemize}
            \item \textbf{Partitions:}
            \begin{itemize}
                \item Each topic is divided into partitions.
                \item Ordered, immutable sequence of records.
                \item Basic unit of parallelism in Kafka.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Topics and Partitions - Importance}
    \begin{block}{Why Topics and Partitions Matter}
        \begin{itemize}
            \item \textbf{Scalability:}
            \begin{itemize}
                \item Distributes loads across brokers to handle high volumes.
                \item Each partition can reside on different servers for parallel processing.
            \end{itemize}
            \item \textbf{Performance:}
            \begin{itemize}
                \item Parallel reads from partitions enhance consumption speed.
                \item Retains messages for replaying data from any partition.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Topics and Partitions - Example}
    \begin{block}{Example: Social Media Application}
        \begin{itemize}
            \item \textbf{Topic:} \textit{UserPosts}
            \begin{itemize}
                \item Collects all posts from different users.
            \end{itemize}
            \item \textbf{Partitions:}
            \begin{itemize}
                \item \textit{Partition 0:} Posts from User A.
                \item \textit{Partition 1:} Posts from User B.
                \item \textit{Partition 2:} Posts from User C.
                \item \textit{Partition 3:} Posts from User D.
            \end{itemize}
            \item \textbf{Producers and Consumers:}
            \begin{itemize}
                \item Producers can publish to any partition to balance load.
                \item Consumers read from different partitions simultaneously.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream Processing Concepts}
    \begin{itemize}
        \item Overview of fundamental stream processing concepts
        \item Key topics: Event time vs. processing time and windowing
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Stream Processing - Event Time vs. Processing Time}
    \begin{block}{Event Time}
        \begin{itemize}
            \item Timestamp when an event was generated (e.g., user clicked a button).
            \item \textbf{Significance}: Critical for operations based on actual occurrence.
            \item \textbf{Example}: Weather data logs record temperature at the time observed.
        \end{itemize}
    \end{block}
    
    \begin{block}{Processing Time}
        \begin{itemize}
            \item Time when the event is processed by the system.
            \item \textbf{Significance}: Important for measuring latency; varies by performance and load.
            \item \textbf{Example}: Sensor data sent at 1 PM but received at 1:02 PM.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Stream Processing - Comparing Time Types}
    \begin{itemize}
        \item \textbf{Event Time}:
            \begin{itemize}
                \item Ideal for accurate historical analysis.
            \end{itemize}
        \item \textbf{Processing Time}:
            \begin{itemize}
                \item May cause delays or late data, impacting real-time analytics.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Windowing in Stream Processing}
    \begin{block}{Definition}
        \begin{itemize}
            \item Technique to group events in time frames for batch operations on continuous streams.
        \end{itemize}
    \end{block}

    \begin{block}{Types of Windows}
        \begin{itemize}
            \item \textbf{Tumbling Window}: Fixed-length, non-overlapping intervals for events.
            \item \textbf{Sliding Window}: Overlapping windows allowing events in multiple groups.
            \item \textbf{Session Window}: Captures events by inactivity, ideal for variable gaps.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Windowing}
    \begin{block}{Illustration}
        \texttt{Event Stream: | A | B | C | D | E | F | G |}
        \vspace{5pt} \\
        \textbf{Tumbling Window (5 Min):}
        \begin{itemize}
            \item 0-5min: | A | B | C |
            \item 5-10min: | D | E     |
            \item 10-15min: | F | G     |
        \end{itemize}
        \vspace{5pt}
        \textbf{Sliding Window (5 Min, Slide Every 1 Min):}
        \begin{itemize}
            \item 0-5min: | A | B | C |
            \item 1-6min: | B | C | D |
            \item 2-7min: | C | D | E |
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Understanding \textbf{event time} vs. \textbf{processing time} is crucial for data analysis.
        \item \textbf{Windowing} facilitates effective aggregation and analysis of streaming data.
        \item Selecting the right window type depends on the specific use case and data characteristics.
    \end{itemize}
    \vspace{10pt}
    \textbf{Conclusion:} Stream processing technologies involve real-time data management that requires a thorough understanding of event timings and appropriate grouping techniques for effective analytics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Producers - Introduction}
    \begin{block}{Introduction to Kafka Producers}
        Kafka producers are components responsible for sending records (data) to Kafka topics. They play a crucial role in the Kafka ecosystem by ensuring that data is efficiently published for downstream processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Producers - Objectives}
    \begin{block}{Objectives of Kafka Producers}
        \begin{itemize}
            \item \textbf{Reliable Message Delivery}: Ensures messages are not lost.
            \item \textbf{Data Serialization}: Converts complex data structures into a format suitable for transmission.
            \item \textbf{Partitioning}: Distributes messages across different partitions to balance load.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Producers - Step-by-Step Guide}
    \begin{enumerate}
        \item \textbf{Set Up Project Environment}
        \begin{itemize}
            \item Use languages such as Java, Python, or Node.js.
            \item Include the Kafka client library in your project dependencies.
        \end{itemize}
        \begin{lstlisting}[language=xml]
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.3.1</version>
</dependency>
        \end{lstlisting}
        
        \item \textbf{Configure the Producer Properties}
        \begin{itemize}
            \item Define configurations such as bootstrap servers, key and value serializers, and acknowledgments.
        \end{itemize}
        \begin{lstlisting}[language=java]
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        \end{lstlisting}
        
        \item \textbf{Instantiate the Kafka Producer}
        \begin{itemize}
            \item Create an instance using the configured properties.
        \end{itemize}
        \begin{lstlisting}[language=java]
KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        \end{lstlisting}
        
        \item \textbf{Send Messages to Kafka Topics}
        \begin{itemize}
            \item Use the \texttt{send} method to publish messages.
        \end{itemize}
        \begin{lstlisting}[language=java]
ProducerRecord<String, String> record = new ProducerRecord<>("my-topic", "key", "value");
producer.send(record);
        \end{lstlisting}
        
        \item \textbf{Handle Callbacks and Errors}
        \begin{itemize}
            \item Implement error handling using callbacks to monitor success or failure of message delivery.
        \end{itemize}
        \begin{lstlisting}[language=java]
producer.send(record, (metadata, exception) -> {
    if (exception != null) {
        System.err.println("Error sending message: " + exception.getMessage());
    } else {
        System.out.println("Message sent to topic " + metadata.topic() + " at offset " + metadata.offset());
    }
});
        \end{lstlisting}
        
        \item \textbf{Close the Producer}
        \begin{itemize}
            \item Properly close the producer to release resources.
        \end{itemize}
        \begin{lstlisting}[language=java]
producer.close();
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Producers - Summary and Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Serialization Importance}: Always ensure that both the key and value types are correctly serialized.
            \item \textbf{Load Balancing}: Understand how partitioning works to optimize throughput in multi-partition scenarios.
            \item \textbf{Error Handling}: Use callbacks to handle exceptions and ensure robustness.
        \end{itemize}
    \end{block}
    
    \begin{block}{Summary}
        Building a Kafka producer involves configuring properties, handling serialization, sending messages to topics, and implementing error handling. By following these steps and understanding Kafka’s architecture, you’ll be well-equipped to integrate data streams into your applications effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Producers - Next Steps}
    \begin{block}{Next Steps}
        Explore how to build Kafka consumers to process the data streams sent to topics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Consumers - Introduction}
    \begin{itemize}
        \item Kafka consumers are components for reading and processing data streams from Kafka topics.
        \item They play a crucial role in the data pipeline by consuming messages from Kafka producers.
        \item Enable real-time processing and analytics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Consumers - Key Concepts}
    \begin{itemize}
        \item \textbf{Topics:} Categories where messages are produced and consumed.
        \item \textbf{Consumer Group:} A set of consumers that share the workload, ensuring load balancing.
        \item \textbf{Offsets:} Unique identifiers for messages that help consumers track their position in the stream.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Consumers - Steps to Create a Consumer}
    \begin{enumerate}
        \item \textbf{Set Up Kafka Dependencies:} Include the necessary libraries.
        \begin{lstlisting}[language=xml]
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.3.1</version>
</dependency>
        \end{lstlisting}
        
        \item \textbf{Configure Consumer Properties:} 
        \begin{lstlisting}[language=java]
Properties properties = new Properties();
properties.put("bootstrap.servers", "localhost:9092");
properties.put("group.id", "my-consumer-group");
properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        \end{lstlisting}
        
        \item \textbf{Create the Consumer:}
        \begin{lstlisting}[language=java]
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
        \end{lstlisting}
        
        \item \textbf{Subscribe to Topics:}
        \begin{lstlisting}[language=java]
consumer.subscribe(Arrays.asList("my-topic"));
        \end{lstlisting}
        
        \item \textbf{Poll for Messages:}
        \begin{lstlisting}[language=java]
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("Consumed message: %s from partition: %d and offset: %d%n", record.value(), record.partition(), record.offset());
    }
}
        \end{lstlisting}
        
        \item \textbf{Close the Consumer:}
        \begin{lstlisting}[language=java]
consumer.close();
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Consumers - Use Cases}
    \begin{itemize}
        \item \textbf{Real-Time Analytics:} Applications consuming live game data and processing updates.
        \item \textbf{Log Processing:} Reading and processing logs to identify anomalies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Kafka Consumers - Key Points & Conclusion}
    \begin{itemize}
        \item Consumer Groups facilitate scalability by distributing message consumption.
        \item Offsets enable fault tolerance; consumers resume from the last committed offset.
        \item Proper error handling is crucial to avoid message loss.
        \item Kafka consumers are key for processing real-time data streams efficiently.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Message Serialization Formats - Overview}
    % Description: Overview of serialization in Kafka and its importance.
    \begin{block}{Overview of Serialization in Kafka}
        Serialization is the process of converting data structures or object states into a format that can be stored (in files, databases) or transmitted (over network protocols). In Kafka, it ensures that messages can be written to and read from topics in a consistent format. 
    \end{block}
    
    \begin{itemize}
        \item Ensures consistent data format in Kafka.
        \item Affects performance, compatibility, and usability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Message Serialization Formats - JSON}
    % Content for JSON serialization format
    \begin{block}{1. JSON (JavaScript Object Notation)}
        \begin{itemize}
            \item \textbf{Description:} Lightweight, human-readable data interchange format.
            \item \textbf{Usage:} Common for web APIs and service data interchange.
        \end{itemize}
        
        \begin{itemize}
            \item \textbf{Key Points:}
            \begin{itemize}
                \item Human-readable: Easy to understand and debug.
                \item Schema-less: Flexible structure, fields can change over time.
            \end{itemize}
        \end{itemize}
        
        \begin{block}{Example:}
            \begin{lstlisting}[language=json]
{
  "userId": 1,
  "userName": "Alice",
  "age": 30,
  "active": true
}
            \end{lstlisting}
        \end{block}
        
        \begin{itemize}
            \item \textbf{Cons:} Larger payload sizes; lacks type safety.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Message Serialization Formats - Avro and Protobuf}
    % Content for Avro and Protobuf serialization formats
    \begin{block}{2. Avro}
        \begin{itemize}
            \item \textbf{Description:} Binary serialization format using a JSON-defined schema.
            \item \textbf{Usage:} Often utilized in big data frameworks like Hadoop and Spark.
        \end{itemize}
        
        \begin{itemize}
            \item \textbf{Key Points:}
            \begin{itemize}
                \item Compact: Smaller sizes compared to JSON.
                \item Schema Evolution: Supports backward and forward compatibility.
            \end{itemize}
        \end{itemize}
        
        \begin{block}{Schema Example (JSON):}
            \begin{lstlisting}[language=json]
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "userId", "type": "int"},
    {"name": "userName", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "active", "type": "boolean"}
  ]
}
            \end{lstlisting}
        \end{block}
        
        \begin{itemize}
            \item \textbf{Cons:} Requires schema management; complex to set up.
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Protobuf (Protocol Buffers)}
        \begin{itemize}
            \item \textbf{Description:} Developed by Google, provides structured data serialization.
            \item \textbf{Usage:} Common in microservices and APIs for performance-critical applications.
        \end{itemize}
        
        \begin{itemize}
            \item \textbf{Key Points:}
            \begin{itemize}
                \item Efficiency: Smaller binary payloads; faster processing.
                \item Strongly Typed: Schema-defined fields help validate messages.
            \end{itemize}
        \end{itemize}
        
        \begin{block}{Schema Example (proto file):}
            \begin{lstlisting}[language=protobuf]
syntax = "proto3";
message User {
    int32 userId = 1;
    string userName = 2;
    int32 age = 3;
    bool active = 4;
}
            \end{lstlisting}
        \end{block}
        
        \begin{itemize}
            \item \textbf{Cons:} Requires knowledge of Protobuf syntax; less human-readable.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Message Serialization Formats - Summary}
    % Summary of serialization formats and when to use them
    \begin{itemize}
        \item \textbf{Choose JSON:} When human readability is a priority.
        \item \textbf{Use Avro:} In big data systems needing efficient storage and schema evolution.
        \item \textbf{Opt for Protobuf:} In performance-critical applications with a need for compact binary representation.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Choosing the right serialization format is crucial for optimal data management in Kafka streams, influencing integration and overall architecture.
    \end{block}
\end{frame}

\begin{frame}{Real-Time Data Processing Frameworks}
    \frametitle{Introduction to Real-Time Data Processing}
    \begin{itemize}
        \item Real-time data processing frameworks enable continuous processing of data streams.
        \item Facilitate instant insights and actions for data-driven decision-making.
        \item Businesses increasingly rely on processing data as it arrives.
    \end{itemize}
\end{frame}

\begin{frame}{Key Frameworks: Apache Storm and Apache Flink}
    \begin{itemize}
        \item Both frameworks integrate seamlessly with Apache Kafka for high-performance stream processing.
        \item They offer scalability, fault tolerance, and efficient resource management.
    \end{itemize}
\end{frame}

\begin{frame}{Apache Storm Overview}
    \begin{itemize}
        \item Distributed real-time computation system.
        \item Designed for low latency and high scalability.
        \item Utilizes topologies: spouts (data sources) and bolts (processing components).
    \end{itemize}
\end{frame}

\begin{frame}{Apache Storm Use Case}
    \begin{block}{Example Use Case}
        Real-Time Analytics: Processing sensor data from IoT devices to trigger alerts.
    \end{block}
    
    \begin{lstlisting}[language=Java]
    // Define a simple Storm topology
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout("inputSpout", new KafkaSpout());
    builder.setBolt("processingBolt", new ProcessingBolt())
           .shuffleGrouping("inputSpout");

    // Submit the topology for execution
    Config config = new Config();
    StormSubmitter.submitTopology("KafkaStormTopology", config, builder.createTopology());
    \end{lstlisting}
\end{frame}

\begin{frame}{Apache Flink Overview}
    \begin{itemize}
        \item Combines batch and stream processing in a unified system.
        \item Ensures consistent results with robust state management.
        \item Treats streams and batch data similarly to simplify pipeline creation.
    \end{itemize}
\end{frame}

\begin{frame}{Apache Flink Use Case}
    \begin{block}{Example Use Case}
        Fraud Detection: Monitoring transactions in real-time to flag fraudulent activities.
    \end{block}
    
    \begin{lstlisting}[language=Java]
    // Flink job for processing a Kafka stream
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    
    DataStream<String> stream = env.addSource(new FlinkKafkaConsumer<>("topic", new SimpleStringSchema(), properties));
    stream.map(value -> process(value))
          .addSink(new YourSink());

    env.execute("FlinkKafkaJob");
    \end{lstlisting}
\end{frame}

\begin{frame}{Key Points to Emphasize}
    \begin{itemize}
        \item Seamless integration with Kafka as both source and sink.
        \item Scalability for distributed environments.
        \item Fault tolerance ensures reliability in processing.
        \item Use Cases:
            \begin{itemize}
                \item Storm for pure real-time applications.
                \item Flink for complex event processing with mixed workloads.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Conclusion}
    \begin{block}{Conclusion}
        Understanding Apache Storm and Apache Flink enriches knowledge of real-time data processing frameworks. These tools allow organizations to harness the power of streaming data for enhanced decision-making in various industries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream vs. Batch Processing - Introduction}
    \begin{block}{Overview}
        Data processing can be classified into two main paradigms:
        stream processing and batch processing. 
        Understanding the differences between these methodologies is essential for effective data management strategies based on specific use cases.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream vs. Batch Processing - Definitions}
    \begin{itemize}
        \item \textbf{Stream Processing}: Continuous ingestion and processing of data in real-time as it arrives. 
        \begin{itemize}
            \item Examples: Real-time analytics, sensor data monitoring, financial transaction processing.
        \end{itemize}
        \item \textbf{Batch Processing}: Processes large volumes of data accumulated over time in structured batches.
        \begin{itemize}
            \item Examples: End-of-day reports, historical data aggregation.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream vs. Batch Processing - Comparisons}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Feature} & \textbf{Stream Processing} & \textbf{Batch Processing} \\
        \hline
        Latency & Low latency (milliseconds to seconds) & High latency (seconds to hours) \\
        \hline
        Data Volume & Continuous, unbounded streams of data & Finite, bounded sets of data \\
        \hline
        Execution Model & Event-driven, complex processing required & Scheduled tasks, predefined computations \\
        \hline
        Use Cases & Real-time analytics, fraud detection & Data warehousing, reporting \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream vs. Batch Processing - Use Cases}
    \begin{itemize}
        \item \textbf{Stream Processing Use Cases}:
        \begin{itemize}
            \item Twitter Sentiment Analysis: Analyze tweets in real-time.
            \item Fraud Detection in Banking: Monitor transactions to prevent fraud immediately.
        \end{itemize}
        \item \textbf{Batch Processing Use Cases}:
        \begin{itemize}
            \item Monthly Financial Reporting: Compile financial data after month-end.
            \item Data Warehousing: Periodic updates from transactional databases for BI tools.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream vs. Batch Processing - Example Code}
    \begin{block}{Stream Processing Example (Apache Kafka Streams in Java)}
    \begin{lstlisting}[language=Java]
StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> inputStream = builder.stream("input-topic");
inputStream.filter((key, value) -> value.contains("important"))
            .to("output-topic");
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Batch Processing Example (Using PySpark)}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Batch Processing").getOrCreate()
df = spark.read.csv("data.csv")
df.groupBy("category").sum("sales").show()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream vs. Batch Processing - Key Takeaways}
    \begin{itemize}
        \item Stream processing excels in handling real-time data for immediate insights.
        \item Batch processing is optimal for processing and analyzing large volumes of data at once.
        \item Select the processing method based on latency needs, data volume, and task nature.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stream vs. Batch Processing - Conclusion}
    Understanding the differences between stream and batch processing allows organizations to design their data management systems effectively, tailoring them to specific business needs and operational requirements. Grasping these concepts equips students to make informed decisions regarding data processing frameworks in real-world contexts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Streaming Technologies}
    Streaming technologies allow for the real-time processing of data as it is generated. This capability enables organizations to make prompt decisions based on current information, contrasting with batch processing methods. 
    \begin{itemize}
        \item Real-time processing vs batch processing
        \item Applications in decision-making and operational efficiency
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Industry-Driven Use Cases - Part 1}
    \begin{enumerate}
        \item \textbf{Finance}
            \begin{itemize}
                \item \textbf{Real-Time Fraud Detection:}
                \begin{itemize}
                    \item Monitoring transactions in real-time to identify fraudulent patterns.
                    \item Example: Banks use analytics to analyze millions of transactions and flag irregularities.
                    \item \textbf{Key Point:} Prompt decision-making reduces losses and increases security.
                \end{itemize}
            \end{itemize}
        
        \item \textbf{E-Commerce}
            \begin{itemize}
                \item \textbf{Personalized Recommendations:}
                \begin{itemize}
                    \item Analyzing customer behavior for real-time shopping experiences.
                    \item Example: Recommendations based on browsing history, increasing engagement.
                    \item \textbf{Key Point:} Enhances sales conversion rates through real-time interaction.
                \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Industry-Driven Use Cases - Part 2}
    \begin{enumerate}[resume]
        \item \textbf{Internet of Things (IoT)}
            \begin{itemize}
                \item \textbf{Smart Home Automation:}
                \begin{itemize}
                    \item Processing data from devices for improved user experience.
                    \item Example: Smart thermostats adjust settings based on temperature data in real-time.
                    \item \textbf{Key Point:} Enhances functionality and user satisfaction.
                \end{itemize}
            \end{itemize}
        
        \item \textbf{Healthcare}
            \begin{itemize}
                \item \textbf{Patient Monitoring:}
                \begin{itemize}
                    \item Continuous monitoring of vital signs for timely responses.
                    \item Example: Wearable devices alert providers during emergencies.
                    \item \textbf{Key Point:} Real-time data can save lives through timely interventions.
                \end{itemize}
            \end{itemize}

        \item \textbf{Telecommunications}
            \begin{itemize}
                \item \textbf{Network Performance Monitoring:}
                \begin{itemize}
                    \item Real-time analysis to identify network issues proactively.
                    \item Example: Monitoring call quality metrics for immediate troubleshooting.
                    \item \textbf{Key Point:} Improves user experience and operational efficiency.
                \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Streaming technologies are transforming industries by:
    \begin{itemize}
        \item Enabling immediate access to data
        \item Facilitating prompt actions
        \item Enhancing user experiences
        \item Driving innovation
    \end{itemize}
    These use cases exemplify the importance of real-time data processing in today's data-driven world.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Streaming Data Management - Introduction}
    \begin{block}{Overview}
        Streaming data management involves real-time processing and analysis of data as it is generated. This introduces unique challenges compared to traditional batch processing. Understanding these challenges is essential for developing robust streaming applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Streaming Data Management - Key Challenges}
    \begin{enumerate}
        \item \textbf{Data Reliability}
        \begin{itemize}
            \item \textbf{Definition:} Ensuring data is accurate, consistent, and complete.
            \item \textbf{Issues:}
            \begin{itemize}
                \item Data Loss: Network failures can prevent data packets from being delivered.
                \item Data Duplication: Retransmission due to errors leads to processing the same data multiple times.
            \end{itemize}
            \item \textbf{Solutions:}
            \begin{itemize}
                \item Implement acknowledgment mechanisms for data receipt confirmation.
                \item Use idempotent operations to handle duplicate data correctly.
            \end{itemize} 
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Streaming Data Management - Key Challenges (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{System Scalability}
        \begin{itemize}
            \item \textbf{Definition:} Capability to manage increased data and users without performance sacrifice.
            \item \textbf{Issues:}
            \begin{itemize}
                \item Increased Load: More streams strain CPU, memory, and bandwidth.
                \item Latency: Increased load can lead to higher processing times.
            \end{itemize}
            \item \textbf{Solutions:}
            \begin{itemize}
                \item Design systems with a distributed architecture for horizontal scaling.
                \item Use load balancing techniques for even data processing distribution.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Streaming Data Management - Additional Considerations}
    \begin{itemize}
        \item \textbf{Event Ordering}
        \begin{itemize}
            \item Critical for applications like financial transactions.
        \end{itemize}
        \item \textbf{Latency Management}
        \begin{itemize}
            \item Minimize delay between data generation and processing.
        \end{itemize}
        \item \textbf{Complexity of Implementation}
        \begin{itemize}
            \item Often requires intricate configurations and optimization strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Streaming Data Management - Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Proactive management is crucial for reliability and scalability.
            \item Follow best practices for data integrity and system architecture.
            \item Implement monitoring tools for real-time issue detection and resolution.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Streaming Data Management - Example Code Snippet}
    \begin{lstlisting}[language=Python]
# Pseudocode for a reliable data processing system
def process_stream(data_stream):
    for data in data_stream:
        if not acknowledge(data):
            log("Data not acknowledged: " + data.id)
            continue  # skip unacknowledged data
        process_data(data)  # process the received data
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future of Streaming Technologies}
    \begin{block}{Overview}
        Streaming technologies are revolutionizing the way organizations handle data, 
        offering real-time processing and analytics opportunities. 
        Key trends towards the future include:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends in Streaming Technologies}
    \begin{enumerate}
        \item \textbf{Increased Adoption of Real-Time Analytics}
        \begin{itemize}
            \item Businesses are shifting to real-time analytics for immediate insights.
            \item Example: E-commerce analyzing customer behavior in real time for personalized marketing.
        \end{itemize}
        
        \item \textbf{Enhancements in Cloud-Based Streaming Solutions}
        \begin{itemize}
            \item Cloud providers offer managed services to simplify architecture.
            \item Example: AWS Kinesis enables real-time processing with minimal setup.
        \end{itemize}
        
        \item \textbf{Integration of Machine Learning with Streaming Data}
        \begin{itemize}
            \item Streaming platforms are incorporating ML for automated decision-making.
            \item Example: Fraud detection systems analyzing transactions in real time.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends Continued}
    \begin{enumerate}[resume]
        \item \textbf{Focus on Data Governance and Security}
        \begin{itemize}
            \item Importance of data security and governance surges with streaming solutions.
            \item Example: Role-based access controls and encryption in Apache Kafka.
        \end{itemize}
        
        \item \textbf{Expansion of Open-Source Streaming Frameworks}
        \begin{itemize}
            \item Open-source projects offer flexible solutions without vendor lock-in.
            \item Example: Apache Flink's stateful computations for real-time applications.
        \end{itemize}

        \item \textbf{Event-Driven Architectures Becoming Mainstream}
        \begin{itemize}
            \item Event-driven design patterns emphasize communication through events.
            \item Example: Microservices architectures leveraging event-driven patterns for responsiveness.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Real-time vs. Batch Processing enhances decision-making.
            \item Cloud solutions are flexible and reduce complexities.
            \item Security frameworks are critical as streamed data volume grows.
            \item Open-source solutions foster community-driven improvements.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        The future of streaming technologies in data management is marked by advancements 
        that facilitate real-time analytics and enhance security. Embracing these trends will 
        provide organizations with a competitive edge.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example}
    \begin{lstlisting}[language=Python]
# Example of using KafkaConsumer in Python to consume streaming data
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'my-topic',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    group_id='my-group'
)

for message in consumer:
    print(f"Received message: {message.value.decode('utf-8')}")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise: Setting Up Apache Kafka - Overview}
    \begin{block}{Overview of Apache Kafka}
        Apache Kafka is an open-source distributed event streaming platform designed for high-throughput and low-latency data processing. It is widely used for building real-time data pipelines and streaming applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise: Setting Up Apache Kafka - Objectives}
    \begin{block}{Objectives of this Exercise}
        1. **Set Up a Local Kafka Instance**: Install and configure Apache Kafka on your machine. \\
        2. **Produce Messages**: Create producers to send messages to Kafka topics. \\
        3. **Consume Messages**: Implement consumers to read messages from Kafka topics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise: Setting Up Apache Kafka - Prerequisites}
    \begin{block}{Prerequisites}
        - **Java 8 or higher**: Kafka is built on Java. Make sure you have it installed. \\
        - **Download Kafka**: Get the latest version from the \href{https://kafka.apache.org/downloads}{Apache Kafka downloads page}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise: Step-by-Step Guide - Installation}
    \begin{block}{Step 1: Install Apache Kafka}
        - **Extract Kafka**: Unzip the downloaded Kafka archive. \\
        - **Navigate to Kafka Directory**: Open your terminal and navigate to the Kafka folder.

        \begin{lstlisting}[language=bash]
cd kafka_2.13-2.8.0
        \end{lstlisting}
    \end{block}

    \begin{block}{Step 2: Start Zookeeper}
        Kafka uses Zookeeper as a service to coordinate and manage the Kafka brokers. Start Zookeeper with the following command:

        \begin{lstlisting}[language=bash]
bin/zookeeper-server-start.sh config/zookeeper.properties
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise: Step-by-Step Guide - Starting Kafka}
    \begin{block}{Step 3: Start Kafka Server}
        With Zookeeper running, start your Kafka server:

        \begin{lstlisting}[language=bash]
bin/kafka-server-start.sh config/server.properties
        \end{lstlisting}
    \end{block}

    \begin{block}{Step 4: Create a Kafka Topic}
        Create a topic called "test-topic" to produce and consume messages:

        \begin{lstlisting}[language=bash]
bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise: Step-by-Step Guide - Producing and Consuming}
    \begin{block}{Step 5: Produce Messages to the Topic}
        Open a new terminal window and start a Kafka producer:

        \begin{lstlisting}[language=bash]
bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092
        \end{lstlisting}
        Now, type any message followed by Enter to send it to the topic.
    \end{block}

    \begin{block}{Step 6: Consume Messages from the Topic}
        Open another terminal window and start a Kafka consumer:

        \begin{lstlisting}[language=bash]
bin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092
        \end{lstlisting}
        You will see the messages that you produce in real-time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example Code}
    \begin{block}{Key Points to Emphasize}
        - **High Throughput**: Kafka is designed to handle a large volume of data efficiently. \\
        - **Scalability**: You can add more producers and consumers as needed. \\
        - **Real-time Processing**: Messages are processed in real-time, enabling immediate analytics.
    \end{block}

    \begin{block}{Example Java Producer}
        Here's a simple example of a Java producer:

        \begin{lstlisting}[language=Java]
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        producer.send(new ProducerRecord<>("test-topic", "key", "Hello Kafka!"));
        producer.close();
    }
}
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        By completing this exercise, you should now have a functional local instance of Apache Kafka, along with the ability to produce and consume messages. This knowledge is foundational for understanding how Kafka fits into larger data management architectures and designs.
    \end{block}

    \begin{block}{Next Steps}
        Be prepared for a recap and Q\&A session to clarify any doubts as we transition to the wrap-up of this week’s topics!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion: Key Points Recap}
    
    \begin{enumerate}
        \item \textbf{Understanding Streaming Technologies}:
            \begin{itemize}
                \item Enable real-time data processing and analysis.
                \item Essential for applications requiring immediate insights.
            \end{itemize}
        
        \item \textbf{Introduction to Apache Kafka}:
            \begin{itemize}
                \item A distributed streaming platform with high throughput and fault-tolerance.
                \item Key components: Producers, Consumers, Topics, Partitions, Brokers, and Zookeepers.
            \end{itemize}
        
        \item \textbf{Message Production and Consumption}:
            \begin{itemize}
                \item Producers publish messages to topics; Consumers read messages from topics.
                \item Decouples data producers from consumers for flexibility and scalability.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up Kafka}
    
    \begin{block}{Hands-on Exercise}
        \begin{itemize}
            \item Set up a local Kafka instance to produce and consume messages.
            \item Key commands:
        \end{itemize}
        
        \begin{lstlisting}[basicstyle=\footnotesize]
            # Start ZooKeeper
            ./bin/zookeeper-server-start.sh config/zookeeper.properties
            
            # Start Kafka Server
            ./bin/kafka-server-start.sh config/server.properties
            
            # Create a topic
            ./bin/kafka-topics.sh --create --topic <topic_name> --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices and Q\&A}
    
    \begin{block}{Best Practices}
        \begin{itemize}
            \item Ensure message serialization (use libraries like Avro or JSON).
            \item Monitor system health with tools like Kafka Manager.
            \item Optimize partitioning for performance and load distribution.
        \end{itemize}
    \end{block}
    
    \begin{block}{Open the Floor for Questions}
        \begin{itemize}
            \item Encourage questions about Kafka's architecture, integrations, and features.
            \item Discuss real-world applications like fraud detection and IoT data processing.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Streaming technologies, like Apache Kafka, are crucial for real-time data processing.
            \item Familiarity with Kafka enhances practical skills for data management in the industry.
            \item Engage actively and ask questions for a better learning experience.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}