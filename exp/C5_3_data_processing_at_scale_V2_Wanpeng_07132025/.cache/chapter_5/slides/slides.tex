\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

\title[Apache Spark Fundamentals]{Week 5: Apache Spark Fundamentals}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark - Overview}
    \begin{block}{Overview}
        Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. Its primary purpose is to provide a fast and general-purpose cluster-computing framework that allows data scientists and analysts to run batch and streaming data processing tasks efficiently.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark - Key Features}
    \begin{itemize}
        \item \textbf{Speed:} Spark processes data in memory, reducing disk I/O overhead, making it up to 100 times faster than Hadoop MapReduce in certain cases.
        \item \textbf{Ease of Use:} Rich APIs available in Scala, Java, Python, and R enhance accessibility for diverse users.
        \item \textbf{Unified Engine:} Supports batch processing, streaming, machine learning, and graph processing within a single platform.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Components of Apache Spark}
    \begin{enumerate}
        \item \textbf{Resilient Distributed Datasets (RDDs):}
            \begin{itemize}
                \item Fundamental data structure representing an immutable distributed collection of objects processed in parallel.
                \item Example: \begin{lstlisting}
val data = sc.parallelize(Seq(1, 2, 3, 4, 5))
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{DataFrames:}
            \begin{itemize}
                \item A distributed collection of data organized into named columns, similar to a table.
                \item Allows easier data manipulation with SQL-like queries.
            \end{itemize}
        
        \item \textbf{Spark SQL:}
            \begin{itemize}
                \item Enables SQL queries on large datasets using DataFrames.
                \item Example: \begin{lstlisting}
val df = spark.sql("SELECT * FROM tableName WHERE age > 21")
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Benefits of Using Apache Spark}
    \begin{itemize}
        \item \textbf{Scalability:} Efficiently manages large datasets across thousands of nodes.
        \item \textbf{Integration:} Compatible with various data storage systems such as HDFS, Apache Cassandra, Apache HBase, and Amazon S3.
        \item \textbf{Advanced Analytics:} Includes libraries for machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Remember}
        Apache Spark revolutionizes big data processing by making it fast, versatile, and user-friendly. It serves as a powerful tool for various data analytics needs, including batch computations, real-time data processing, and advanced analytics.
    \end{block}
    % Diagram placeholder
    \begin{block}{Diagram}
        Insert a simple diagram illustrating the architecture of Apache Spark with its components interacting with data sources and storage options.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Components of Apache Spark - Introduction}
    \begin{block}{Introduction}
        Apache Spark is a powerful open-source distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Its core components serve as the foundation for performing large-scale data processing tasks efficiently.
    \end{block}
    \begin{itemize}
        \item Resilient Distributed Datasets (RDDs)
        \item DataFrames
        \item Spark SQL
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Components of Apache Spark - RDDs}
    \begin{block}{1. Resilient Distributed Datasets (RDDs)}
        \begin{itemize}
            \item \textbf{Definition}: Fundamental data structure of Spark representing a collection of objects that can be processed in parallel across a cluster.
            \item \textbf{Key Features}:
                \begin{itemize}
                    \item Immutability: RDDs cannot be altered after creation.
                    \item Fault Tolerance: Automatic recovery of lost data due to node failures.
                \end{itemize}
            \item \textbf{Creation}:
                \begin{lstlisting}[language=Python]
rdd = spark.sparkContext.parallelize([1, 2, 3, 4])
rdd = spark.sparkContext.textFile("hdfs://path/to/file.txt")
                \end{lstlisting}
            \item \textbf{Transformations and Actions}:
                \begin{itemize}
                    \item Transformations for creating new RDDs (e.g., \texttt{map}, \texttt{filter}).
                    \item Actions return values to the driver program (e.g., \texttt{count}, \texttt{collect}).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Components of Apache Spark - DataFrames and Spark SQL}
    \begin{block}{2. DataFrames}
        \begin{itemize}
            \item \textbf{Definition}: A distributed collection of data organized into named columns.
            \item \textbf{Key Features}:
                \begin{itemize}
                    \item Schema: Defined schema for column names and types.
                    \item Optimized Execution: Built on Sparkâ€™s Catalyst optimizer.
                \end{itemize}
            \item \textbf{Creation}:
                \begin{lstlisting}[language=Python]
from pyspark.sql import Row
people_rdd = spark.sparkContext.parallelize([Row(name='Alice', age=1), Row(name='Bob', age=2)])
df = spark.createDataFrame(people_rdd)
df = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)
                \end{lstlisting}
            \item \textbf{Operations}: Performed using DataFrame APIs or SQL-like syntax.
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Spark SQL}
        \begin{itemize}
            \item \textbf{Definition}: A module for structured data processing.
            \item \textbf{Key Features}:
                \begin{itemize}
                    \item Unified Data Processing: Combine SQL with DataFrame operations.
                    \item Compatibility with existing Hive data.
                \end{itemize}
            \item \textbf{Using Spark SQL}:
                \begin{lstlisting}[language=Python]
df.createOrReplaceTempView("people")
result = spark.sql("SELECT name FROM people WHERE age > 1")
                \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding RDDs (Resilient Distributed Datasets)}
    \begin{block}{What are RDDs?}
        \textbf{RDD Definition}: Resilient Distributed Datasets (RDDs) are the fundamental data structure of Apache Spark. They are fault-tolerant collections of elements that can be processed in parallel across a distributed cluster.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of RDDs}
    \begin{enumerate}
        \item \textbf{Fault Tolerance}: RDDs can recover from node failures due to lineage information that tracks how RDDs were derived from other data.
        \item \textbf{In-Memory Computation}: RDDs allow data to be stored in memory, making it significantly faster compared to traditional disk-based storage systems.
        \item \textbf{Immutable}: Once created, the data in an RDD cannot be changed, promoting reproducible computations.
        \item \textbf{Distributed}: RDDs are distributed across the cluster, allowing for parallel processing of large datasets.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs}
    \begin{block}{From Existing Data}
        \begin{lstlisting}
# Using SparkContext to create RDD from a collection
rdd = sc.parallelize([1, 2, 3, 4, 5])
        \end{lstlisting}
    \end{block}
    
    \begin{block}{From External Datasets}
        \begin{lstlisting}
# Creating RDD from a text file
rdd_from_file = sc.textFile("path/to/textfile.txt")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformations and Actions}
    \begin{block}{Transformations}
        \textbf{Definitions}: Transformations are functions that create a new RDD from an existing one. They are lazily evaluated (execution is deferred until an action is called).
        \begin{itemize}
            \item \texttt{map(function)}: Applies a function to each element.
            \begin{lstlisting}
rdd_mapped = rdd.map(lambda x: x * 2)
            \end{lstlisting}
            \item \texttt{filter(function)}: Returns an RDD with elements that satisfy a predicate.
            \begin{lstlisting}
rdd_filtered = rdd.filter(lambda x: x > 2)
            \end{lstlisting}
            \item \texttt{flatMap(function)}: Similar to map but can return multiple values for each input element.
        \end{itemize}
    \end{block}

    \begin{block}{Actions}
        \textbf{Definitions}: Actions trigger the execution of the transformations and return results to the driver program or write data to external storage.
        \begin{itemize}
            \item \texttt{collect()}: Returns all elements of the RDD to the driver.
            \begin{lstlisting}
result = rdd.collect()
            \end{lstlisting}
            \item \texttt{count()}: Returns the number of elements in the RDD.
            \begin{lstlisting}
num_elements = rdd.count()
            \end{lstlisting}
            \item \texttt{reduce(function)}: Combines the elements using a function.
            \begin{lstlisting}
total = rdd.reduce(lambda a, b: a + b)
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item RDDs are the foundational building block of Spark and enable efficient processing of big data.
        \item They support a rich set of operations that can be combined to form complex data processing pipelines.
        \item Understanding RDDs is crucial for leveraging the full power of Apache Spark before moving on to higher-level abstractions like DataFrames.
    \end{itemize}
    
    \begin{block}{Summary}
        RDDs offer a powerful abstraction for distributed data processing in Apache Spark. They provide reliability, speed, and scalability, making them suitable for big data applications. By mastering RDDs, you can effectively harness Sparkâ€™s capabilities for large-scale data analytics.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{DataFrames in Apache Spark}
    \begin{block}{Introduction to DataFrames}
        A DataFrame is a distributed collection of data organized into named columns, similar to SQL tables or pandas DataFrames. It is designed for large datasets distributed across a cluster.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Advantages of DataFrames Over RDDs}
    \begin{enumerate}
        \item \textbf{Optimized Performance:}
            \begin{itemize}
                \item Leverages Spark's Catalyst optimizer for optimized query plans.
                \item Benefits from Tungsten's off-heap memory management for faster processing.
            \end{itemize}
        
        \item \textbf{Ease of Use:}
            \begin{itemize}
                \item Provides a higher-level abstraction with declarative syntax.
                \item Simplifies operations with built-in functions (e.g., \texttt{select}, \texttt{groupBy}).
            \end{itemize}
        
        \item \textbf{Interoperability with SQL:}
            \begin{itemize}
                \item Allows SQL-like queries directly via Spark SQL.
            \end{itemize}
        
        \item \textbf{Schema Enforcement:}
            \begin{itemize}
                \item DataFrames have a defined schema which improves data validation and quality.
            \end{itemize}
        
        \item \textbf{Integration with Big Data Tools:}
            \begin{itemize}
                \item Supports various data sources like Parquet, JSON, and Hive.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How to Use DataFrames for Structured Data Processing}
    \begin{block}{Creating a DataFrame}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("DataFrameExample") \
    .getOrCreate()

# Create DataFrame from JSON
df = spark.read.json("path/to/data.json")

# Show DataFrame
df.show()
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Common Operations}
        \begin{itemize}
            \item Select Columns:
            \begin{lstlisting}[language=Python]
df.select("columnName").show()
            \end{lstlisting}
            \item Filtering Rows:
            \begin{lstlisting}[language=Python]
df.filter(df['age'] > 21).show()
            \end{lstlisting}
            \item Aggregating Data:
            \begin{lstlisting}[language=Python]
df.groupBy("department").agg({"salary": "mean"}).show()
            \end{lstlisting}
            \item Joining DataFrames:
            \begin{lstlisting}[language=Python]
df1.join(df2, on="commonColumn", how="inner").show()
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item DataFrames are \textbf{immutable}; transformations create a new DataFrame.
        \item They significantly improve performance over RDDs for structured data tasks.
        \item DataFrames effectively handle \textbf{big data} with distributed computation.
        \item Use the \textbf{DataFrame API} and SQL syntax for intuitive data manipulation.
    \end{itemize}
    
    \begin{block}{Conclusion}
        DataFrames effectively facilitate structured data processing in Apache Spark, with clear syntax and optimizing features. Transitioning from RDDs can greatly enhance performance and usability in big data scenarios.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction to Spark SQL}
    \begin{block}{Overview of Spark SQL}
        Spark SQL is a component of Apache Spark that enables users to perform relational data processing using SQL (Structured Query Language). It integrates the capabilities of Spark's computational power with the familiarity and expressiveness of SQL.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Features of Spark SQL}
    \begin{itemize}
        \item \textbf{DataFrame API:} Combines the benefits of RDDs with SQL query capabilities.
        \item \textbf{Unified Data Access:} Interfaces for querying data across various sources like Hive, Avro, and JSON.
        \item \textbf{Support for Various Formats:} Queries structured data formats without additional parsing.
        \item \textbf{Catalyst Optimizer:} Enhances query execution with optimizations.
        \item \textbf{Compatibility with Hive:} Supports existing Hive UDFs and query language.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Execution Process of Spark SQL}
    \begin{block}{Understanding the Execution Process}
        Spark SQL works by translating SQL queries into a logical plan and then converting it into physical execution plans, allowing for optimization according to resources available.
    \end{block}
    \begin{enumerate}
        \item \textbf{SQL Queries:} Users write SQL to retrieve data.
        \item \textbf{Query Optimization:} The Catalyst optimizer processes the query.
        \item \textbf{Execution:} The physical plan is executed with Spark's distributed processing.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case}
    \begin{block}{Example SQL Query}
        \begin{lstlisting}[language=SQL]
SELECT customer_id, SUM(order_amount) AS total_spent
FROM orders
GROUP BY customer_id
ORDER BY total_spent DESC;
        \end{lstlisting}
    \end{block}
    In this query, we calculate the total spending of each customer from an orders DataFrame.
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Integration with Spark:} Combines SQL queries with other Spark APIs for seamless data processing.
        \item \textbf{Performance Improvement:} Faster performance than traditional data processing engines.
        \item \textbf{Ease of Use:} Familiar SQL syntax makes it accessible for data analysts.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Spark SQL is a powerful tool within the Apache Spark ecosystem that allows for efficient data processing and querying of structured data using a SQL interface. It combines the scalability and processing power of Spark with rich SQL capabilities, enabling users to extract meaningful insights effortlessly.
\end{frame}

\begin{frame}
    \frametitle{Comparative Analysis: RDDs vs. DataFrames vs. Spark SQL}
    
    \begin{block}{Overview}
        In Apache Spark, data can be managed using three primary abstractions: 

        \begin{itemize}
            \item RDDs (Resilient Distributed Datasets)
            \item DataFrames
            \item Spark SQL
        \end{itemize}
        
        This comparison highlights their performance, ease of use, and flexibility.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{RDDs (Resilient Distributed Datasets)}
    
    \begin{itemize}
        \item \textbf{Definition:} RDD is an immutable collection of objects partitioned across a cluster, processed in parallel.
        \item \textbf{Creation:} From existing data or by transforming other RDDs.
    \end{itemize}
    
    \begin{block}{Performance}
        \begin{itemize}
            \item \textbf{Pros:} Fine-grained control over partitioning and transformations.
            \item \textbf{Cons:} Slower performance due to serialization overhead.
        \end{itemize}
    \end{block}
    
    \begin{block}{Ease of Use}
        \begin{itemize}
            \item \textbf{Complexity:} Requires understanding of functional programming.
            \item \textbf{Example:}
            \begin{lstlisting}[language=Python]
from pyspark import SparkContext
sc = SparkContext("local", "RDD Example")
data = [1, 2, 3, 4]
rdd = sc.parallelize(data)
rdd_squared = rdd.map(lambda x: x ** 2).collect()
            \end{lstlisting}
        \end{itemize}
    \end{block}
    
    \begin{block}{Flexibility}
        \begin{itemize}
            \item \textbf{Advantage:} Handles various data types, including unstructured data.
            \item \textbf{Disadvantage:} Lacks built-in optimizations.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames and Spark SQL}

    \begin{block}{DataFrames}
        \begin{itemize}
            \item \textbf{Definition:} A distributed collection of data organized into named columns.
            \item \textbf{Creation:} From RDDs, structured DataFrames, or external sources.
        \end{itemize}

        \begin{block}{Performance}
            \begin{itemize}
                \item \textbf{Pros:} Optimized query execution via the Catalyst optimizer.
                \item \textbf{Cons:} Less control for low-level operations.
            \end{itemize}
        \end{block}

        \begin{block}{Ease of Use}
            \begin{itemize}
                \item \textbf{Complexity:} More user-friendly than RDDs, resembles database tables.
                \item \textbf{Example:}
                \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()
data = [("Alice", 1), ("Bob", 2)]
df = spark.createDataFrame(data, ["Name", "Id"])
df.show()
                \end{lstlisting}
            \end{itemize}
        \end{block}

        \begin{block}{Flexibility}
            \begin{itemize}
                \item \textbf{Advantage:} Supports various data formats and SQL-like operations.
                \item \textbf{Disadvantage:} May require upfront schema definition.
            \end{itemize}
        \end{block}
    \end{block}


    \begin{block}{Spark SQL}
        \begin{itemize}
            \item \textbf{Definition:} Enables querying structured data using SQL syntax.
            \item \textbf{Integration:} Seamlessly integrates various data sources.
        \end{itemize}

        \begin{block}{Performance}
            \begin{itemize}
                \item \textbf{Pros:} Utilizes Catalyst optimizer and Tungsten engine.
                \item \textbf{Cons:} Relies on efficient query design.
            \end{itemize}
        \end{block}

        \begin{block}{Ease of Use}
            \begin{itemize}
                \item \textbf{Complexity:} Easier for users familiar with SQL.
                \item \textbf{Example:}
                \begin{lstlisting}
SELECT Name, Id FROM myTable WHERE Id > 1;
                \end{lstlisting}
            \end{itemize}
        \end{block}

        \begin{block}{Flexibility}
            \begin{itemize}
                \item \textbf{Advantage:} Combines SQL with complex data processing using DataFrames.
                \item \textbf{Disadvantage:} Less control compared to RDDs.
            \end{itemize}
        \end{block}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Summary}
    
    \begin{itemize}
        \item \textbf{Performance:} RDDs are generally slower for structured data; DataFrames and Spark SQL have optimizations.
        \item \textbf{Ease of Use:} DataFrames and Spark SQL are more intuitive for users familiar with SQL.
        \item \textbf{Flexibility:} RDDs excel in handling unstructured data.
    \end{itemize}
    
    \begin{block}{Summary}
        Use:
        \begin{itemize}
            \item \textbf{RDDs} for low-level control and transformations.
            \item \textbf{DataFrames} for structured data manipulation with optimizations.
            \item \textbf{Spark SQL} for using SQL with Spark's processing capabilities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Data Processing Workflows in Spark}
    \begin{block}{Overview of Data Processing Workflows}
        Data processing workflows in Apache Spark involve a systematic approach to transforming and analyzing data. Structuring these workflows effectively enhances performance and scalability. Below are the essential components and best practices for creating efficient data processing workflows.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of a Spark Workflow}
    \begin{enumerate}
        \item \textbf{Data Ingestion}
        \begin{itemize}
            \item Sources: HDFS, S3, databases, structured files (CSV, JSON, etc.)
            \item Example:
            \begin{lstlisting}
df = spark.read.json("s3://mybucket/mydata.json")
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Data Transformation}
        \begin{itemize}
            \item Operations: Use functions like \texttt{map()}, \texttt{filter()}, and SQL queries.
            \item Example:
            \begin{lstlisting}
transformed_df = df.filter(df.age > 21).select("name", "age")
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Data Aggregation}
        \begin{itemize}
            \item Grouping: Use \texttt{groupBy()} with \texttt{agg()}.
            \item Example:
            \begin{lstlisting}
aggregated_df = transformed_df.groupBy("age").count()
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Data Storage/Output}
        \begin{itemize}
            \item Saving results: Write to various formats.
            \item Example:
            \begin{lstlisting}
aggregated_df.write.csv("s3://mybucket/output.csv")
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Spark Workflows}
    \begin{itemize}
        \item \textbf{Use DataFrames or Spark SQL}: Prefer these over RDDs for better performance.
        
        \item \textbf{Cache Intermediate Results}: Use \texttt{.cache()} to reduce redundant computations.
        \begin{lstlisting}
df.cache()  # Caching DataFrame
        \end{lstlisting}
        
        \item \textbf{Minimize Data Shuffling}: Design transformations to reduce data movement across the cluster.
        
        \item \textbf{Partitioning}: Carefully choose partitioning strategies to optimize parallel processing.
        
        \item \textbf{Monitor and Optimize}: Utilize Spark's web UI to monitor jobs and tune configurations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Workflow in Spark}
    \begin{enumerate}
        \item \textbf{Ingest Data}: Load transaction data from a CSV file.
        \item \textbf{Transform Data}: Clean and filter records (e.g., remove nulls).
        \item \textbf{Aggregate Data}: Calculate total sales per product category.
        \item \textbf{Store Results}: Save aggregated results to a database or filesystem.
    \end{enumerate}

    \begin{block}{Code Snippet of a Complete Workflow}
    \begin{lstlisting}
# Step 1: Ingest
df = spark.read.csv("s3://mybucket/transactions.csv", header=True, inferSchema=True)

# Step 2: Transform
clean_df = df.na.drop()
filtered_df = clean_df.filter(clean_df.amount > 0)

# Step 3: Aggregate
result_df = filtered_df.groupBy("category").agg({"amount": "sum"})

# Step 4: Store
result_df.write.format("parquet").save("s3://mybucket/aggregated_results/")
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    By following structured workflows and best practices, you can ensure that Apache Spark processes data efficiently and effectively. This approach optimizes resource usage and enhances the clarity and maintainability of your code.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Hands-On: Creating RDDs and DataFrames}
  \begin{block}{Introduction to RDDs and DataFrames}
    \begin{itemize}
      \item \textbf{RDDs (Resilient Distributed Datasets)}:
      \begin{itemize}
        \item Fault-tolerant: Recovers lost data automatically.
        \item Immutable: Transformations result in new RDDs.
        \item Lazy Evaluation: Computation triggers on action calls.
      \end{itemize}
      \item \textbf{DataFrames}:
      \begin{itemize}
        \item Schema-based: Structured and semi-structured data.
        \item Optimized execution with Catalyst optimizer.
        \item Supports extensive operations, including SQL.
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Creating RDDs}
  \begin{block}{From Existing Collections}
    \begin{lstlisting}[language=Python]
from pyspark import SparkContext
sc = SparkContext("local", "RDD Example")
data = [1, 2, 3, 4]
rdd = sc.parallelize(data)
    \end{lstlisting}
    \textbf{Explanation:} \texttt{parallelize()} converts a local collection into an RDD.
  \end{block}

  \begin{block}{From External Data Sources}
    \begin{lstlisting}[language=Python]
rdd_text = sc.textFile("hdfs://path/to/data.txt")
    \end{lstlisting}
    \textbf{Explanation:} \texttt{textFile()} reads data from a file and creates an RDD.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Creating DataFrames}
  \begin{block}{From RDD}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()
rdd = sc.parallelize([(1, "Alice"), (2, "Bob")])
df = spark.createDataFrame(rdd, schema=["id", "name"])
    \end{lstlisting}
    \textbf{Explanation:} \texttt{createDataFrame()} converts an RDD to a DataFrame with specified schema.
  \end{block}

  \begin{block}{From CSV Files}
    \begin{lstlisting}[language=Python]
df_csv = spark.read.csv("hdfs://path/to/data.csv", header=True, inferSchema=True)
    \end{lstlisting}
    \textbf{Explanation:} Reads a CSV file directly into a DataFrame with header information and infers data types.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Spark SQL for Data Analysis}
    \begin{block}{Introduction to Spark SQL}
        Spark SQL is a component of Apache Spark that allows users to run SQL queries on structured data. It integrates relational data processing with Spark's functional programming model, enabling easier handling of big data workloads through familiar SQL syntax.
    \end{block}
    
    \begin{block}{Why Use Spark SQL?}
        \begin{itemize}
            \item \textbf{Unified Data Processing}: Combines SQL queries with data processing capabilities provided by RDDs and DataFrames.
            \item \textbf{Performance}: Optimizes queries via the Catalyst optimizer and Tungsten execution engine for faster performance compared to traditional SQL engines.
            \item \textbf{Scalability}: Efficiently handles large datasets across distributed systems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Concepts of Spark SQL}
    \begin{block}{DataFrame}
        A DataFrame in Spark is similar to a table in a relational database. It is a distributed collection of data organized into named columns.
    \end{block}

    \begin{block}{SparkSession}
        To work with Spark SQL, you must first create a \texttt{SparkSession}, which is the entry point for all Spark functionality.
    \end{block}

    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Spark SQL Example") \
    .getOrCreate()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Executing SQL Queries in Spark SQL}
    \begin{block}{Steps to Execute Queries}
        \begin{enumerate}
            \item \textbf{Register DataFrames as Temp Views}:
                Create a temporary view of the DataFrame to run SQL queries on it.
                \begin{lstlisting}[language=Python]
df.createOrReplaceTempView("your_table_name")
                \end{lstlisting}

            \item \textbf{Running SQL Queries}:
                Run SQL queries like you would in any SQL database.
                \begin{lstlisting}[language=Python]
result = spark.sql("SELECT * FROM your_table_name WHERE condition_column > value")
result.show()
                \end{lstlisting}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case: Analyzing Sales Data}
    \begin{block}{Scenario}
        Analyzing sales data with a DataFrame \texttt{sales\_df} that contains columns: \texttt{order\_id}, \texttt{customer\_id}, \texttt{amount}, and \texttt{order\_date}.
        \begin{lstlisting}[language=Python]
sales_df.createOrReplaceTempView("sales")
        \end{lstlisting}
    \end{block}

    \begin{block}{SQL Query: Calculate Total Sales}
        \begin{lstlisting}[language=SQL]
SELECT SUM(amount) AS total_sales 
FROM sales 
WHERE order_date >= '2023-01-01';
        \end{lstlisting}

        \textbf{Code Implementation:}
        \begin{lstlisting}[language=Python]
total_sales = spark.sql("""
    SELECT SUM(amount) AS total_sales 
    FROM sales 
    WHERE order_date >= '2023-01-01'
""")
total_sales.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{DataFrames}: Primary abstraction for structured data in Spark SQL.
        \item \textbf{SparkSession}: Always create a SparkSession to access Spark SQL functionalities.
        \item \textbf{Temporary Views}: Use temporary views to allow SQL queries on DataFrames.
        \item \textbf{Query Complexity}: Queries can be simple or complex, leveraging the full power of SQL.
    \end{itemize}

    \begin{block}{Next Steps}
        After mastering Spark SQL, we will recap key concepts learned in the course and discuss further implications and integrations of Spark with other technologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Overview}
    \begin{itemize}
        \item Apache Spark is a powerful open-source distributed computing system.
        \item It provides an interface for programming entire clusters with:
        \begin{itemize}
            \item Implicit data parallelism
            \item Fault tolerance
        \end{itemize}
        \item Key focus: Understanding core components and functionalities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Core Components}
    \begin{enumerate}
        \item \textbf{Spark Core:}
            \begin{itemize}
                \item Underlying engine for task scheduling, memory management, etc.
                \item Uses Resilient Distributed Datasets (RDDs) for fault-tolerant processing.
            \end{itemize}
        \item \textbf{Spark SQL:}
            \begin{itemize}
                \item Facilitates querying of structured data with SQL and DataFrame APIs.
                \item \textbf{Example Code:}
                \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ExampleSQL").getOrCreate()
df = spark.sql("SELECT * FROM my_table WHERE age > 30")
df.show()
                \end{lstlisting}
            \end{itemize}
        \item \textbf{Spark Streaming:}
            \begin{itemize}
                \item Processes real-time data streams using batch processing principles.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Continued}
    \begin{enumerate}[resume]
        \item \textbf{Spark MLlib:}
            \begin{itemize}
                \item Scalable machine learning library for various tasks.
                \item Provides optimized implementations for algorithms like classification and regression.
            \end{itemize}
        \item \textbf{GraphX:}
            \begin{itemize}
                \item API for graph processing to analyze social networks or hierarchies.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Unified Processing Model for batch, streaming, ML, and graph.
            \item Performance Optimization through in-memory data processing.
            \item Strong Integration with Hadoop, Cassandra, etc.
            \item Scalability from single machines to large clusters.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}