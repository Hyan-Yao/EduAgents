\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 6: Hands-On Lab: Integrating APIs into Spark]{Week 6: Hands-On Lab: Integrating APIs into Spark}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{Department of Computer Science \\ University Name \\ Email: email@university.edu \\ Website: www.university.edu}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to API Integration in Spark}
    \begin{block}{Overview of API Integration in Spark}
        An API (Application Programming Interface) allows different software applications to communicate with each other, enabling seamless access to data and functionalities from external services.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Integrate APIs into Spark Workflows?}
    \begin{itemize}
        \item Enhanced capabilities for accessing various external data sources, services, and functionalities.
        \item Supercharge data processing and analytical workflows.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of API Integration in Spark}
    \begin{enumerate}
        \item \textbf{Enhanced Data Accessibility:}
            \begin{itemize}
                \item Pull data from sources like REST APIs, databases, and cloud storage directly into Spark.
                \item \textit{Example:} Using a weather API for current weather analysis.
            \end{itemize}
        \item \textbf{Seamless Integration with Third-Party Services:}
            \begin{itemize}
                \item Utilize external tools, e.g., ML models and data visualization platforms.
                \item \textit{Example:} Integration with ML cloud services for leveraging pre-trained models.
            \end{itemize}
        \item \textbf{Improved Collaboration:}
            \begin{itemize}
                \item Building modular systems for better teamwork between engineers and scientists.
                \item \textit{Example:} Data scientists accessing ML models via APIs for Spark jobs.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Interconnectivity:} APIs allow for a connected architecture within Spark, leading to flexible data pipelines.
        \item \textbf{Standardized Communication:} Enables combining disparate data sources easily.
        \item \textbf{Time Efficiency:} Reduces development time by avoiding the need for internal data connections.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Sample Code Snippet}
    Hereâ€™s a basic example of using a Python API client to fetch data and process it in Spark:
    \begin{lstlisting}[language=Python]
import requests
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("SampleAPIIntegration").getOrCreate()

# Fetch data from an API
response = requests.get("https://api.example.com/data")
data = response.json()

# Create a Spark DataFrame from the fetched data
df = spark.createDataFrame(data)

# Perform operations on the DataFrame
df.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Incorporating API integrations into Spark empowers data professionals to create dynamic workflows. 
    This enhances analysis, decision-making, and leverages cutting-edge technologies across diverse domains.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding APIs - Overview}
    \begin{block}{What is an API?}
        An Application Programming Interface (API) is a set of protocols and tools that allows different software applications to communicate with each other. 
    \end{block}
    
    \begin{block}{Importance of APIs}
        APIs facilitate data access, enhance application functionality, and enable real-time processing in data workflows.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of APIs}
    \begin{itemize}
        \item \textbf{Web APIs:} Enabling interactions over the internet (e.g., RESTful API, SOAP)
        \item \textbf{Library/API Frameworks:} Pre-written code collections for application use (e.g., jQuery)
        \item \textbf{Operating Systems APIs:} Interfaces for OS interactions (e.g., WinAPI for Windows)
        \item \textbf{Database APIs:} Communication interfaces with database management systems (e.g., ODBC, JDBC)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Using a Weather API}
    \begin{block}{Use Case}
        To analyze sales trends impacted by weather, you can employ a weather API:
        \begin{enumerate}
            \item Fetch real-time weather data using the API.
            \item Utilize Spark to analyze this data alongside sales data for insights.
        \end{enumerate}
    \end{block}
    
    \begin{lstlisting}[language=Python]
import requests
import pandas as pd

# Fetching data from a weather API
response = requests.get('https://api.example.com/weather')
weather_data = response.json()

# Converting to DataFrame for use in Spark
weather_df = pd.DataFrame(weather_data)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of API Integration in Spark - Introduction}
    \begin{block}{Overview}
        In this section, we will explore the significant advantages of integrating APIs (Application Programming Interfaces) into Apache Spark. This integration enhances Spark's capabilities, providing users with more robust tools for processing large datasets efficiently.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of API Integration in Spark - Key Benefits}
    \begin{enumerate}
        \item \textbf{Data Accessibility}
        \begin{itemize}
            \item APIs enable Spark to connect seamlessly with various data sources.
            \item This facilitates the extraction of real-time data for analysis.
        \end{itemize}
        \item \textbf{Enhanced Functionality}
        \begin{itemize}
            \item Integrating APIs extends Spark's built-in capabilities.
            \item Incorporation of third-party libraries and services such as machine learning tools.
        \end{itemize}
        \item \textbf{Real-Time Processing}
        \begin{itemize}
            \item APIs provide mechanisms for streaming data.
            \item Essential for applications requiring instant data insights.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of API Integration in Spark - Examples and Code Snippet}
    \begin{block}{Examples}
        \begin{itemize}
            \item \underline{Data Accessibility Example}: 
                Spark can collect and analyze real-time tweets using the Twitter API.
            \item \underline{Enhanced Functionality Example}:
                Integration of machine learning APIs like TensorFlow with Spark enhances predictive analytics capabilities.
            \item \underline{Real-Time Processing Example}:
                Use of Spark Streaming with a RESTful API to process live data from IoT devices.
        \end{itemize}
    \end{block}

    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
import requests
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("API Integration").getOrCreate()

# Fetch data from a REST API
response = requests.get("https://api.example.com/data")
data = response.json()  # Convert response to JSON format

# Create DataFrame from JSON data
df = spark.read.json(spark.sparkContext.parallelize([data]))

# Show DataFrame
df.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of API Integration in Spark - Conclusion}
    \begin{block}{Summary}
        Integrating APIs into Spark empowers users to harness vast datasets with enhanced functionality and real-time capabilities. 
        This integration is essential for modern data processing tasks in our increasingly data-driven world.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up the Environment - Overview}
    \begin{block}{Overview}
        Integrating APIs into Spark requires a well-established environment that includes essential software, libraries, and tools to facilitate interactions with various APIs seamlessly.
    \end{block}
    \begin{itemize}
        \item Importance of choosing the right Integrated Development Environment (IDE)
        \item Installation of Apache Spark
        \item Language-specific setups for Python and Scala
        \item Required libraries for enhanced Spark functionalities
        \item Authentication mechanisms for API integration
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up the Environment - Software}
    \begin{enumerate}
        \item \textbf{Integrated Development Environment (IDE)}:
        \begin{itemize}
            \item \textbf{IntelliJ IDEA}: Great for Scala; requires Scala plugin.
            \item \textbf{PyCharm}: Ideal for Python-based Spark applications.
            \item \textbf{Eclipse}: Can be used with Scala IDE plugin.
        \end{itemize}
        
        \item \textbf{Apache Spark Installation}:
        \begin{itemize}
            \item Download from the official \texttt{Apache Spark} website.
            \item Set up environment variables:
            \begin{lstlisting}
export SPARK_HOME=/path/to/spark
export PATH=$SPARK_HOME/bin:$PATH
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Programming Language Setup}:
        \begin{itemize}
            \item For Python:
            \begin{lstlisting}
pip install pyspark
            \end{lstlisting}
            \item For Scala: Ensure Scala is properly installed.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up the Environment - Libraries and Code Snippet}
    \begin{enumerate}
        \item \textbf{Required Libraries}:
        \begin{itemize}
            \item \textbf{Spark SQL/JDBC}: For querying databases via JDBC connectors.
            \item \textbf{Requests}: For making HTTP requests in Python.
            \begin{lstlisting}
pip install requests
            \end{lstlisting}
            \item \textbf{Json4s}: For JSON parsing in Scala applications.
        \end{itemize}
        
        \item \textbf{Example Code Snippet}:
        Making a simple API request in PySpark:
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
import requests

# Start Spark session
spark = SparkSession.builder.appName("API Integration").getOrCreate()

# API request example
response = requests.get("https://api.example.com/data")
data = response.json()

# Convert to Spark DataFrame
df = spark.read.json(sc.parallelize(data))
df.show()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right APIs}
    Criteria for selecting appropriate APIs for Spark workflows based on project requirements and data sources.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Criteria for Selecting APIs - Part 1}
    \begin{enumerate}
        \item \textbf{Understand Your Project Requirements}
        \begin{itemize}
            \item \textbf{Data Type}: Identify the data (e.g., structured, semi-structured, unstructured).
            \begin{itemize}
                \item Example: For structured data (like CSV files), use APIs that return DataFrames directly.
            \end{itemize}
            \item \textbf{Processing Needs}: Assess if your project needs batch processing, stream processing, or interactive queries.
            \begin{itemize}
                \item Example: For real-time analytics, use APIs that provide streaming data endpoints.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Criteria for Selecting APIs - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Performance Considerations}
        \begin{itemize}
            \item \textbf{Latency and Throughput}: Evaluate how the API handles data transmission.
            \begin{itemize}
                \item Example: Use benchmarking tools to test API response times.
            \end{itemize}
            \item \textbf{Rate Limits}: Be aware of limitations on API calls.
            \begin{itemize}
                \item Key Point: APIs with high rate limits are better for intensive data processing.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Documentation and Community Support}
        \begin{itemize}
            \item \textbf{Quality of Documentation}: Look for well-structured guides and code samples.
            \item \textbf{Community and Support}: Active communities can provide valuable resources.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Criteria for Selecting APIs - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{4} % Continue numbering from previous frame
        \item \textbf{Security and Compliance}
        \begin{itemize}
            \item \textbf{Authentication Methods}: Ensure secure protocols are supported.
            \begin{itemize}
                \item Key Point: Security is a priority, especially for sensitive data.
            \end{itemize}
            \item \textbf{Compliance Standards}: Check for regulations such as GDPR.
            \begin{itemize}
                \item Example: Use APIs with clear compliance statements to mitigate legal risks.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Conclusion}
        \begin{itemize}
            \item Choosing the right APIs is essential for successful Spark workflows.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Step: Integrating APIs into Spark Workflows}
    Stay tuned for our next slide where we will provide a step-by-step methodology for incorporating the chosen APIs into your Spark applications!
\end{frame}

\begin{frame}{Integrating APIs into Spark Workflows}
    \begin{block}{Overview}
        In todayâ€™s data-driven environment, leveraging Application Programming Interfaces (APIs) is essential for enhancing Spark applications. This slide presents a step-by-step methodology that will guide you in effectively integrating APIs into your Spark workflows.
    \end{block}
\end{frame}

\begin{frame}{Step-by-Step Methodology - Part 1}
    \begin{enumerate}
        \item \textbf{Identify API Requirements}:
        \begin{itemize}
            \item Determine the data or functionality needed from external APIs (e.g., weather data, social media feeds, financial information).
            \item Refer to previous slides on choosing the right APIs based on project needs.
        \end{itemize}
        
        \item \textbf{Set Up the Spark Environment}:
        \begin{itemize}
            \item Configure your Spark environment to access external APIs. Install necessary libraries (e.g., \texttt{requests}, \texttt{http4s}, etc.).
            \item \textbf{Example Code Snippet (Python)}:
            \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("API Integration Example") \
    .getOrCreate()
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Step-by-Step Methodology - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Fetch API Data}:
        \begin{itemize}
            \item Use HTTP methods (GET, POST) to interact with the API and retrieve required data.
            \item \textbf{Example Code Snippet}:
            \begin{lstlisting}[language=Python]
import requests

api_url = "https://api.example.com/data"
response = requests.get(api_url)
data = response.json()  # Parse JSON response
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Transform API Data}:
        \begin{itemize}
            \item Transform the fetched data into a format compatible with Spark DataFrames.
            \item \textbf{Example of Conversion to DataFrame}:
            \begin{lstlisting}[language=Python]
from pyspark.sql import Row

rows = [Row(**item) for item in data]
df = spark.createDataFrame(rows)
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Step-by-Step Methodology - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Integrate with Spark Operations}:
        \begin{itemize}
            \item Use Spark SQL or DataFrame operations to manipulate the API-fetched data.
            \item \textbf{Example}:
            \begin{lstlisting}[language=Python]
df.filter(df['column'] > value).show()
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Store/Utilize Data}:
        \begin{itemize}
            \item Decide whether to store results back to an external data source (like a database) or utilize them directly in the Spark application.
            \item \textbf{Example of Writing DataFrame to a Hive Table}:
            \begin{lstlisting}[language=Python]
df.write.mode('overwrite').saveAsTable("example_table")
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Understanding API Data}: Grasp the structure and format of data returned by APIs.
            \item \textbf{Error Handling}: Implement error handling for API calls (timeouts, data consistency).
            \item \textbf{Performance Considerations}: Consider latency involved in API calls and caching strategies.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Integrating APIs into Spark workflows enhances data accessibility and broadens analytics scope. By following the outlined methodologies, you can seamlessly incorporate external data into your Spark applications, leading to richer insights.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Hands-On Exercise: API Integration}
    \begin{block}{Objective}
        To actively engage students in the practical application of integrating APIs into Apache Spark workflows.
    \end{block}
    \begin{block}{Description}
        Practical lab activity to implement API integrations; students will follow a guided exercise using sample APIs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Concepts Explained}
    \begin{block}{API Integration}
        \begin{itemize}
            \item An API (Application Programming Interface) allows different software systems to communicate.
            \item In Spark, APIs enhance data processing by:
            \begin{itemize}
                \item Fetching real-time data
                \item Integrating external data sources
                \item Adding functionalities
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Example APIs to Use}
    \begin{enumerate}
        \item OpenWeatherMap API: Fetch current weather data based on location.
        \item REST Countries API: Get information regarding countries, including demographics and geography.
        \item JSONPlaceholder: A fake online REST API for testing and prototyping.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Exercise}
    \begin{enumerate}
        \item \textbf{Set Up Your Environment:}
            \begin{itemize}
                \item Ensure you have Apache Spark installed.
                \item Set up a notebook (e.g., Jupyter, Zeppelin) or an IDE (like PyCharm) with PySpark.
            \end{itemize}
            
        \item \textbf{Choose a Sample API:}
            \begin{itemize}
                \item Integrate the OpenWeatherMap API.
                \item Sign up for a free API key at OpenWeatherMap.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Making API Calls}
    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
        import requests

        api_key = "YOUR_API_KEY"
        city_name = "CITY_NAME"
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city_name}&appid={api_key}"

        response = requests.get(url)
        weather_data = response.json()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Processing the Data with Spark}
    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import col

        spark = SparkSession.builder.appName("API Integration").getOrCreate()
        df = spark.read.json(sc.parallelize([weather_data]))
        df.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Analyzing the Weather Data}
    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
        df.select(col("main.temp"), col("main.humidity")).show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Common Errors}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Understand API responses: Familiarize with JSON structures.
            \item DataFrame manipulation: Learn to handle Spark DataFrames from API data.
            \item Real-world applications: Recognize the benefits of integrating APIs.
        \end{itemize}
    \end{block}
    
    \begin{block}{Common Errors to Watch For}
        \begin{itemize}
            \item API rate limiting: Be aware of request quotas.
            \item JSON parsing issues: Validate structure of returned data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Summary and Next Steps}
    \begin{block}{Summary}
        This hands-on exercise enhances your skills in integrating APIs with Spark, allowing you to work with real-time data and create dynamic applications.
    \end{block}
    
    \begin{block}{Next Steps}
        After this exercise, we will review common challenges and solutions in API integration in the following slide.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges and Solutions - Part 1}
    \begin{block}{Overview}
        This slide provides an overview of potential challenges in API integration with Apache Spark and their corresponding solutions.
    \end{block}
    \begin{itemize}
        \item Authentication and Authorization Issues
        \item Rate Limits and Throttling
        \item Data Format and Parsing Issues
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Authentication and Rate Limits}
    \begin{block}{1. Authentication and Authorization Issues}
        \begin{itemize}
            \item Challenge: Incorrectly configured access (e.g., API keys).
            \item Solution: Use libraries like \texttt{requests-oauthlib} and test headers with tools like Postman.
        \end{itemize}
        \begin{lstlisting}[language=Python]
import requests
from requests_oauthlib import OAuth1

# OAuth1 authentication
auth = OAuth1('your_key', 'your_secret')
response = requests.get('https://api.example.com/data', auth=auth)
        \end{lstlisting}
    \end{block}

    \begin{block}{2. Rate Limits and Throttling}
        \begin{itemize}
            \item Challenge: Exceeding request limits can block requests.
            \item Solution: Implement exponential backoff or retry logic.
        \end{itemize}
        \begin{lstlisting}[language=Python]
import time

for i in range(num_requests):
    response = requests.get('https://api.example.com/data')
    if response.status_code == 429:  # HTTP 429 Too Many Requests
        time.sleep(2 ** i)  # Exponential backoff
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Data Handling and Network Issues}
    \begin{block}{3. Data Format and Parsing Issues}
        \begin{itemize}
            \item Challenge: APIs may return inconsistent data formats.
            \item Solution: Use \texttt{pandas} to manipulate and convert data into Spark DataFrames.
        \end{itemize}
        \begin{lstlisting}[language=Python]
import pandas as pd
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("API Integration").getOrCreate()

# Fetching and converting JSON data
json_data = response.json()
df = pd.json_normalize(json_data)
spark_df = spark.createDataFrame(df)
        \end{lstlisting}
    \end{block}

    \begin{block}{4. Network and Latency Issues}
        \begin{itemize}
            \item Challenge: High latency or network instability affects performance.
            \item Solution: Utilize asynchronous API calls and optimize query parameters.
        \end{itemize}
        \begin{lstlisting}[language=Python]
import aiohttp
import asyncio

async def fetch_data(session, url):
    async with session.get(url) as response:
        return await response.json()

async def main():
    async with aiohttp.ClientSession() as session:
        url = 'https://api.example.com/data'
        data = await fetch_data(session, url)

asyncio.run(main())
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Error Handling}
    \begin{block}{5. Error Handling and Debugging}
        \begin{itemize}
            \item Challenge: Debugging errors in API responses can be complex.
            \item Solution: Implement comprehensive error handling and logging.
        \end{itemize}
        \begin{lstlisting}[language=Python]
response = requests.get('https://api.example.com/data')
if response.status_code != 200:
    print(f"Error {response.status_code}: {response.text}")
        \end{lstlisting}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Plan for Authentication
            \item Implement Rate Limiting
            \item Handle Data Flexibly
            \item Monitor and Optimize Performance
            \item Robust Error Handling
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Successful API Integration}
    \begin{block}{Introduction to API Integration in Spark}
        API integration allows external services to communicate with Spark applications, enabling enhanced data retrieval, processing, and analytical capabilities. It significantly improves Spark's functionality, providing real-time insights and expanding data sources.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Retail Analytics API Integration}
    \begin{block}{Scenario}
        A large retail company sought to optimize inventory management and enhance customer insights. By integrating a \textbf{Sales Reporting API} into their Spark workflow, they analyzed sales data in real-time to adjust inventory levels dynamically.
    \end{block}
    
    \begin{block}{Workflow Integration}
        \begin{enumerate}
            \item \textbf{Data Retrieval}:
            \begin{itemize}
                \item The Spark application calls the Sales Reporting API to fetch daily sales metrics.
                \item Python `requests` library example:
                \begin{lstlisting}[language=Python]
import requests
response = requests.get('https://api.retailsales.com/v1/sales/today')
sales_data = response.json()  # Parse the JSON response
                \end{lstlisting}
            \end{itemize}
            
            \item \textbf{Data Processing}:
            \begin{itemize}
                \item The retrieved sales data is converted into a Spark DataFrame:
                \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("RetailAnalytics").getOrCreate()
sales_df = spark.createDataFrame(sales_data)
                \end{lstlisting}
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Analysis and Outcomes}
    \begin{block}{Data Analysis}
        Key metrics such as total sales, product performance, and inventory turnover are computed:
        \begin{lstlisting}[language=Python]
total_sales = sales_df.agg({"sales_amount": "sum"}).collect()[0][0]
                \end{lstlisting}
        
        Actionable insights derived from analysis:
        \begin{lstlisting}[language=Python]
sales_df.filter(sales_df.stock_quantity < 50).show()  # Identify products needing restock
                \end{lstlisting}
    \end{block}
    
    \begin{block}{Key Outcomes}
        \begin{itemize}
            \item \textbf{Real-Time Decision Making:} Prompt response to sales trends, preventing stockouts and overstocking.
            \item \textbf{Improved Customer Satisfaction:} Real-time data adjustments led to a 15\% increase in repeat purchases.
            \item \textbf{Scalability:} Seamless integration of additional data sources as the company expanded.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        This case study highlights how effective API integration within Spark can enhance data processing, drive insights, and improve business efficiency.
    \end{block}
\end{frame}

\begin{frame}{Best Practices for API Integration}
    \begin{block}{Introduction}
        Integrating APIs (Application Programming Interfaces) into Apache Spark applications can significantly enhance data processing capabilities and enable real-time data analytics. However, employing best practices is crucial to ensure efficiency, reliability, and maintainability.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Best Practices - Use DataFrames for API Responses}
    \begin{enumerate}
        \item \textbf{Use DataFrames for API Responses}
        \begin{itemize}
            \item \textbf{Explanation}: When dealing with structured API data, convert responses into Spark DataFrames. This enables better manipulation, querying, and integration with Spark's complete ecosystem.
            \item \textbf{Example}:
            \begin{lstlisting}[language=Python]
import requests
import pandas as pd
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("API Integration").getOrCreate()

# Example API request
response = requests.get("https://api.example.com/data")
json_data = response.json()

# Convert JSON to DataFrame and then to Spark DataFrame
pdf = pd.DataFrame(json_data)
spark_df = spark.createDataFrame(pdf)
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Best Practices - Managing Rate Limits and Caching}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Manage API Rate Limits}
        \begin{itemize}
            \item \textbf{Explanation}: Most APIs have rate limits to control the number of requests within a certain timeframe. Efficiently managing these limits helps avoid service disruptions.
            \item \textbf{Key Point}: Implement exponential backoff or queueing mechanisms to handle request failures gracefully.
        \end{itemize}
        
        \item \textbf{Caching API Responses}
        \begin{itemize}
            \item \textbf{Explanation}: Caching can drastically reduce redundant API calls. Store frequently accessed data in memory or using distributed caches (like Redis).
            \item \textbf{Example}:
            \begin{lstlisting}[language=Python]
from pyspark.sql import functions as F

spark_df.cache()  # Cache DataFrame for repeated access
spark_df.show()
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Best Practices - Asynchronous Calls and Monitoring}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Use Asynchronous Calls}
        \begin{itemize}
            \item \textbf{Explanation}: Instead of blocking calls, use asynchronous programming to make multiple API requests simultaneously. This improves performance significantly, especially when dealing with numerous endpoints.
            \item \textbf{Key Point}: Libraries such as \texttt{aiohttp} can be useful for handling asynchronous requests in Python.
        \end{itemize}
        
        \item \textbf{Monitor and Log API Interactions}
        \begin{itemize}
            \item \textbf{Explanation}: Maintaining logs of API requests and responses is crucial for debugging and performance monitoring.
            \item \textbf{Example}: Use the \texttt{logging} library in Python to log the status and response times of API calls.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Best Practices - Scalable Architecture and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{5}
        \item \textbf{Design Scalable Integration Architectures}
        \begin{itemize}
            \item \textbf{Explanation}: Structure your Spark applications to scale with data increases. Consider using event-driven architectures where APIs trigger jobs within Spark.
            \item \textbf{Key Point}: Leverage tools like Apache Kafka for handling real-time data streams from APIs.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Conclusion}
        Implementing these best practices can optimize the integration of APIs into Spark applications, leading to robust and efficient data processing workflows. By focusing on DataFrames, managing rate limits, caching responses, and logging API interactions, developers can enhance their applications' performance and reliability.
    \end{block}  

    \begin{block}{Reminder}
        Every API is unique; always read and understand the documentation for specific usage patterns and limitations.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Assessment and Next Steps - Summary of Learnings}
    In this week's lab focused on integrating APIs into Apache Spark applications, we explored several key concepts:
    
    \begin{enumerate}
        \item \textbf{Understanding APIs}: Discussed what APIs (Application Programming Interfaces) are and their role in facilitating communication between software applications.
        \item \textbf{Integrating APIs with Spark}: Learned to leverage libraries (like \texttt{requests}) to extract, transform, and load data for analysis.
        \item \textbf{Best Practices}:
        \begin{itemize}
            \item \textbf{Error Handling}: Robust mechanisms to manage API rate limits and response errors.
            \item \textbf{Caching Responses}: Minimized redundant calls and optimized performance.
            \item \textbf{Rate Limiting}: Understanding and adhering to API limitations when making calls.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Assessment Criteria}
    To gauge your understanding of API integration within Spark, please reflect on the following assessment criteria:

    \begin{enumerate}
        \item \textbf{Functionality}: Does your Spark application correctly integrate the API and produce desired outputs?
        \begin{itemize}
            \item Example: Successfully fetching and processing data from the API.
        \end{itemize}
        
        \item \textbf{Code Quality}: Are best practices applied in coding? 
        \begin{itemize}
            \item Example: Code reusability through functions for API calls.
        \end{itemize}

        \item \textbf{Documentation}: Is your code well-documented for easy understanding? 
        \begin{itemize}
            \item Example: Clear comments explaining the purpose of each function.
        \end{itemize}

        \item \textbf{Performance}: How efficiently does your application handle API calls? Consider execution time and resource usage.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Preparing for Future Topics}
    As we proceed, keep the following in mind:

    \begin{enumerate}
        \item \textbf{Exploring Data Architecture}: Understanding architectural components of data processing platforms for designing scalable applications.
        \item \textbf{Advanced API Functionalities}: Learn about authentication, pagination, and data streaming in future sessions.
        \item \textbf{Hands-On Projects}: Engage in projects to apply skills in real-world scenarios, deepening understanding of API integration.
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item APIs are vital for accessing external resources and integrating diverse data sources.
            \item Following best practices ensures reliable and maintainable code.
            \item Continuous learning is essential as we build up to more complex topics in data architecture and advanced API usage.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile,plain]{Example Code Snippet}
    \begin{lstlisting}[language=Python]
import requests
from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder.appName("API Integration Example").getOrCreate()

# Function to call the API
def fetch_data(api_url):
    response = requests.get(api_url)
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception("API call failed with status code: {}".format(response.status_code))

# Example API URL
api_url = "https://api.example.com/data"
data = fetch_data(api_url)

# Create DataFrame from JSON data
df = spark.createDataFrame(data)
df.show()
    \end{lstlisting}
\end{frame}


\end{document}