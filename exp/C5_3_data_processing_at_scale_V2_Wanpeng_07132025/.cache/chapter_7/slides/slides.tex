\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Data Processing]{Week 7: Data Processing Workflows and Management Tools}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Processing Workflows}
    \begin{block}{Overview}
        A data processing workflow is a sequence of processes that data undergoes, from collection to storage, analysis, and decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Processing Workflows}
    \begin{itemize}
        \item \textbf{Efficiency}: Automation speeds up operations and reduces manual errors.
        \item \textbf{Scalability}: Designed to accommodate increasing data volumes while maintaining performance.
        \item \textbf{Data Integrity}: Helps maintain quality and accuracy of data throughout its life cycle.
        \item \textbf{Collaboration}: Standardizes processes, improving teamwork across departments.
        \item \textbf{Compliance}: Ensures adherence to regulatory standards in data handling.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Data Processing Workflow}
    \begin{enumerate}
        \item \textbf{Data Collection}: Gather data from sources (e.g., web forms, transactions).
        \item \textbf{Data Cleaning}: Remove duplicates, fix inconsistencies, standardize formats.
        \item \textbf{Data Storage}: Store cleaned data in databases or data warehouses.
        \item \textbf{Data Analysis}: Use tools to derive insights (e.g., purchase patterns).
        \item \textbf{Data Visualization}: Create dashboards for stakeholders.
        \item \textbf{Decision Making}: Use insights to inform marketing strategies and initiatives.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item Data processing workflows are crucial for efficiently managing large-scale data.
        \item Scalability and flexibility are essential for adapting to evolving data demands.
        \item Structured workflows uphold data integrity and compliance, securing trust in data usage.
    \end{itemize}
    \begin{block}{Diagram of a Typical Data Processing Workflow}
        \centering
        \texttt{[Data Sources] $\rightarrow$ [Data Collection] $\rightarrow$ [Data Cleaning] $\rightarrow$ [Data Storage] $\rightarrow$ [Data Analysis] $\rightarrow$ [Data Visualization] $\rightarrow$ [Decision Making]}
    \end{block}
\end{frame}

\begin{frame}{Understanding MapReduce Jobs}
    \begin{block}{Overview}
        MapReduce is a programming model designed for processing large data sets with a distributed algorithm on a cluster. 
        It is crucial in the paradigm of batch processing, where data is processed in bulk through a systematic approach that leverages parallel computing.
    \end{block}
\end{frame}

\begin{frame}{Core Concepts - Map Phase}
    \begin{enumerate}
        \item \textbf{Map Phase}:
        \begin{itemize}
            \item Input data is broken down into smaller pieces and transformed into key-value pairs.
            \item \textbf{Functionality}: The Mapper function takes input data, processes it, and outputs intermediate key-value pairs.
            \item \textbf{Example}: In a word count program, each word becomes a key, with the initial value set to 1.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
def mapper(line):
    for word in line.split():
        emit(word, 1)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}{Core Concepts - Shuffle and Reduce Phases}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Shuffle Phase}:
        \begin{itemize}
            \item Groups intermediate data by key, organizing the output from the mapping step.
        \end{itemize}
        
        \item \textbf{Reduce Phase}:
        \begin{itemize}
            \item The Reducer processes data associated with a key and combines it into a smaller set of outputs.
            \item \textbf{Example}: In a word count program, the reducer sums counts for each unique word.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
def reducer(word, counts):
    total_count = sum(counts)
    emit(word, total_count)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}{Key Features and Real-World Example}
    \begin{itemize}
        \item \textbf{Key Points}:
        \begin{itemize}
            \item \textbf{Scalability}: Simplifies parallel processing complexities for scalable applications.
            \item \textbf{Fault Tolerance}: Provides robust fault tolerance, re-executing tasks upon failure.
            \item \textbf{Batch Processing}: Ideal for bulk data scenarios such as log analysis and data transformations.
        \end{itemize}
    \end{itemize}

    \begin{block}{Real-World Example: Analyzing Log Files}
        In a web server with millions of log entries:
        \begin{itemize}
            \item \textbf{Map Phase}: Each log entry transforms into a key-value pair (status code, count of 1).
            \item \textbf{Shuffle Phase}: Groups all the status code entries.
            \item \textbf{Reduce Phase}: Aggregates pairs to total counts for each status code, aiding in performance analysis.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Summary}
    \begin{block}{Final Thoughts}
        MapReduce is a powerful programming model for efficiently processing vast datasets, emphasizing:
        \begin{itemize}
            \item Breaking tasks into smaller units (Map)
            \item Organizing them (Shuffle)
            \item Aggregating results (Reduce)
        \end{itemize}
        Understanding these phases is crucial for anyone involved in big data processing.
    \end{block}
\end{frame}

\begin{frame}{Components of MapReduce}
    \frametitle{Overview of MapReduce}
    MapReduce is a programming model designed for processing large datasets in a parallel and distributed manner. It divides the task into two main functions: \textbf{Map} and \textbf{Reduce}.
    \begin{itemize}
        \item Essential for harnessing the power of Big Data efficiently.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Components of MapReduce - Part 1}
    \frametitle{1. The Map Function}
    \begin{itemize}
        \item \textbf{Purpose}: Transforms input data into key-value pairs.
        \item \textbf{Operation}: Processes input records in parallel across data nodes.
    \end{itemize}
    
    \textbf{Example:} Word Count Problem
    \begin{itemize}
        \item \textbf{Input}: "Hello world hello"
        \item \textbf{Map Output}:
        \begin{itemize}
            \item ("Hello", 1)
            \item ("world", 1)
            \item ("hello", 1)
        \end{itemize}
    \end{itemize}

    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
def map_function(document):
    for word in document.split():
        yield (word.lower(), 1)  # Convert to lower case for uniformity
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Components of MapReduce - Part 2}
    \frametitle{2. The Reduce Function}
    \begin{itemize}
        \item \textbf{Purpose}: Aggregates values by keys from the output of the Map function.
        \item \textbf{Operation}: Consolidates data for final outputs (e.g., summing occurrences).
    \end{itemize}
    
    \textbf{Example:} Continuing with Word Count
    \begin{itemize}
        \item \textbf{Input}:
        \begin{itemize}
            \item ("hello", [1, 1])
            \item ("world", [1])
        \end{itemize}
        \item \textbf{Reduce Output}:
        \begin{itemize}
            \item ("Hello", 2)
            \item ("world", 1)
        \end{itemize}
    \end{itemize}

    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
def reduce_function(word, occurrences):
    return (word, sum(occurrences))
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}{Summary of MapReduce}
    \begin{itemize}
        \item Distributed Processing: Both functions run in parallel across many machines.
        \item Input/Output Format: Typically involves files in a Hadoop Distributed File System (HDFS).
        \item Scalability: Allows for horizontal scaling by adding more machines, ideal for large datasets.
    \end{itemize}
    
    \textbf{Conclusion:} Understanding Map and Reduce functions is crucial for effective data processing workflows in big data applications. Next, we will explore implementing a simple MapReduce job using Apache Hadoop.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation of a Simple MapReduce Job - Introduction}
    \begin{block}{What is MapReduce?}
        MapReduce is a programming model for processing large datasets in parallel across a distributed cluster.
        \begin{itemize}
            \item It simplifies data processing by dividing the job into two main functions:
            \item \textbf{Mapper}: Processes input data and produces key-value pairs.
            \item \textbf{Reducer}: Aggregates the key-value pairs produced by the Mapper.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation of a Simple MapReduce Job - Steps}
    \begin{enumerate}
        \item \textbf{Setting Up Your Environment:}
          \begin{itemize}
            \item Ensure Apache Hadoop is installed and configured.
          \end{itemize}
        
        \item \textbf{Write the Mapper Class:}
          \begin{itemize}
            \item Extend \texttt{Mapper<K1, V1, K2, V2>}.
            \item Here, K1, V1 are input types; K2, V2 are output types.
          \end{itemize}
        \begin{lstlisting}[language=Java]
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
    private final static LongWritable one = new LongWritable(1);
    private Text word = new Text();

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] words = value.toString().split("\\s+");
        for (String w : words) {
            word.set(w);
            context.write(word, one);
        }
    }
}
          \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation of a Simple MapReduce Job - Steps Continued}
    \begin{enumerate}[resume]
        \item \textbf{Write the Reducer Class:}
          \begin{itemize}
            \item Extend \texttt{Reducer<K2, V2, K3, V4>}.
            \item K2, V2 are input types from the Mapper; K3, V4 are output types.
          \end{itemize}
        \begin{lstlisting}[language=Java]
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class WordCountReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
    public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
        long sum = 0;
        for (LongWritable val : values) {
            sum += val.get();
        }
        context.write(key, new LongWritable(sum));
    }
}
        \end{lstlisting}

        \item \textbf{Write the Driver Class:}
          \begin{itemize}
            \item Contains the \texttt{main} method to configure and run the job.
          \end{itemize}
        \begin{lstlisting}[language=Java]
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;

public class WordCount {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(WordCountMapper.class);
        job.setCombinerClass(WordCountReducer.class);
        job.setReducerClass(WordCountReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MapReduce - Overview}
    MapReduce is a powerful framework for processing large data sets. However, it comes with its own set of challenges that may create bottlenecks and hinder performance. 
    Understanding these challenges is crucial for optimizing MapReduce jobs and ensuring effective data processing.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MapReduce - Common Issues}
    \begin{enumerate}
        \item \textbf{Data Skew}
        \begin{itemize}
            \item \textit{Explanation:} Occurs when certain keys dominate, leading to uneven task distribution.
            \item \textit{Example:} A few user IDs generate significantly more data, causing imbalanced reducer workloads.
            \item \textit{Solution:} Use logical partitioning or combiners to minimize reducers' data load.
        \end{itemize}
        
        \item \textbf{Network Bottlenecks}
        \begin{itemize}
            \item \textit{Explanation:} Heavy reliance on data shuffling can slow down job execution if network resources are limited.
            \item \textit{Example:} Simultaneous jobs may delay data transfers due to network contention.
            \item \textit{Solution:} Increase network capacity or optimize transfer protocols.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MapReduce - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Inefficient Resource Utilization}
        \begin{itemize}
            \item \textit{Explanation:} Poor configuration can lead to underutilization of resources.
            \item \textit{Example:} Too many reducers with minimal sorting result in idle resources.
            \item \textit{Solution:} Optimize the number of tasks based on data size and job complexity.
        \end{itemize}
        
        \item \textbf{Long Garbage Collection Times}
        \begin{itemize}
            \item \textit{Explanation:} Java-based jobs incur longer pauses due to garbage collection.
            \item \textit{Example:} Large jobs consume significant memory, extending GC time.
            \item \textit{Solution:} Enhance memory management through Java settings or better serialization.
        \end{itemize}
        
        \item \textbf{Complex Job Dependencies}
        \begin{itemize}
            \item \textit{Explanation:} Dependencies between jobs complicate management and execution.
            \item \textit{Example:} Failure of Job A can delay Job B, disrupting the workflow.
            \item \textit{Solution:} Employ workflow management tools to monitor and schedule jobs efficiently.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Code Snippet}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Understanding bottlenecks is essential for improving performance in MapReduce.
            \item Strategic optimization leads to enhanced efficiency in data processing.
            \item Proactive resource management ensures smoother job execution and better resource utilization.
        \end{itemize}
    \end{block}
    
    \begin{block}{Code Snippet}
    To adjust the number of reducers in a Hadoop job, configure it as shown:
    \begin{lstlisting}[language=Java]
Job job = Job.getInstance(configuration, "MyJob");
job.setNumReduceTasks(2);  // Adjust based on data size and job requirements
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Workflow Management Tools}
    Workflow Management Tools (WMTs) are software systems designed to facilitate, manage, and monitor the execution of data processing tasks and workflows.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Workflow Management Tools}
    \begin{itemize}
        \item \textbf{Automation:} Reduces time and effort by automating repetitive tasks.
        \item \textbf{Error Reduction:} Minimizes human error, ensuring reliable outcomes.
        \item \textbf{Scalability:} Efficiently handles large-scale data processing, allowing for growth.
        \item \textbf{Monitoring and Logging:} Provides insights into workflow performance and helps identify bottlenecks.
        \item \textbf{Integration:} Facilitates seamless data flow between various systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of WMTs}
    \begin{enumerate}
        \item \textbf{Workflow:} A sequence of tasks that leads to the completion of a business process.
        \item \textbf{Task:} An individual work unit executed as part of a workflow (e.g., data extraction).
        \item \textbf{Dependencies:} Relationships dictating the order of task execution.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Simple Workflow}
    Consider a data processing workflow for analyzing customer data:
    \begin{enumerate}
        \item \textbf{Data Ingestion:} Extract data from a database (Task 1).
        \item \textbf{Data Transformation:} Clean and format data (Task 2).
        \item \textbf{Data Analysis:} Perform statistical analysis (Task 3).
        \item \textbf{Result Reporting:} Generate a report based on analysis (Task 4).
    \end{enumerate}
    Note: Each task depends on the successful completion of the previous task.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Points}
    \begin{itemize}
        \item WMTs are crucial for automating and managing complex data processing tasks.
        \item They enhance workflow efficiency through automation, error reduction, and scalability.
        \item Effective utilization of these tools is vital for success in data-driven environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Thoughts}
    As you progress through this chapter, consider how various workflow management tools can streamline your data processing tasks and help manage complexity in your projects.
\end{frame}

\begin{frame}[fragile]{Popular Workflow Management Tools - Overview}
    \begin{block}{Overview of Workflow Management Tools}
        Workflow management tools are essential in data processing as they help automate complex processes, manage dependencies, and ensure the reliable execution of data workflows. In this slide, we compare three popular tools: Apache Oozie, Apache Airflow, and Luigi.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Apache Oozie}
    \begin{block}{1. Apache Oozie}
        \begin{itemize}
            \item \textbf{Description}: A workflow scheduler specifically designed for managing Hadoop jobs. 
            \item \textbf{Key Features}:
                \begin{itemize}
                    \item \textbf{Hadoop Integration}: Tightly integrated with the Hadoop ecosystem.
                    \item \textbf{Directed Acyclic Graph (DAG)}: Supports jobs that can be described as a DAG, making it easy to manage dependencies.
                    \item \textbf{Coordination and Bundling}: Can trigger workflows based on time or data availability.
                \end{itemize}
            \item \textbf{Example Use Case}: Scheduling a daily data pipeline that ingests data from a web app, processes it through a series of MapReduce jobs, and stores the output in HDFS.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Apache Airflow}
    \begin{block}{2. Apache Airflow}
        \begin{itemize}
            \item \textbf{Description}: A platform created by Airbnb for programmatically authoring, scheduling, and monitoring workflows.
            \item \textbf{Key Features}:
                \begin{itemize}
                    \item \textbf{Dynamic Pipeline Generation}: Use Python code for defining workflows, making it exceptionally flexible.
                    \item \textbf{Web UI}: An intuitive dashboard to monitor and manage workflows.
                    \item \textbf{Extensible}: Easily connects with various systems (e.g., AWS, GCP) using built-in operators.
                \end{itemize}
            \item \textbf{Example Use Case}: Automatically generating reports every week by extracting data from multiple sources, transforming it, and loading it into a reporting tool.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Luigi}
    \begin{block}{3. Luigi}
        \begin{itemize}
            \item \textbf{Description}: A Python module that helps build complex pipelines of batch jobs.
            \item \textbf{Key Features}:
                \begin{itemize}
                    \item \textbf{Task Dependency Management}: Defines dependencies between tasks and builds execution order automatically.
                    \item \textbf{Modular Architecture}: Each task can be defined separately, allowing for easier maintenance and scaling.
                    \item \textbf{Command Line Interface}: Run and monitor tasks through the CLI.
                \end{itemize}
            \item \textbf{Example Use Case}: Constructing a pipeline that downloads data, transforms it, and uploads it to a data warehouse, tracking the status of each task.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Points and Summary Table}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Apache Oozie}: Best for Hadoop-centric workflows.
            \item \textbf{Apache Airflow}: Offers unparalleled flexibility with Python.
            \item \textbf{Luigi}: Excels in task dependency management for batch jobs.
        \end{itemize}
    \end{block}

    \begin{block}{Summary Table}
        \begin{tabular}{|l|l|l|l|}
            \hline
            \textbf{Feature} & \textbf{Apache Oozie} & \textbf{Apache Airflow} & \textbf{Luigi} \\ \hline
            Integration & Hadoop & Multi-cloud & Python-based \\ \hline
            Workflow Definition & XML & Python & Python \\ \hline
            User Interface & CLI/Web & Web UI & CLI \\ \hline
            Flexibility & Low & High & Moderate \\ \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Conclusion}
    \begin{block}{Conclusion}
        Choosing the right workflow management tool depends on your specific use case, infrastructure, and team expertise. Understanding the strengths and weaknesses of each tool is essential for effective data workflow management.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building and Scheduling Workflows - Overview}
    Effective data processing relies on well-constructed workflows and efficient job scheduling. Workflows are sequences of actions or processes that manage data processing from start to finish. 
    \begin{itemize}
        \item Key guidelines for building workflows
        \item Best practices for scheduling jobs
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Workflows - Key Guidelines}
    A workflow is a structured sequence of tasks that need to be performed to process data.
    
    \begin{block}{Key Guidelines}
        \begin{itemize}
            \item \textbf{Define Task Dependencies}: Visualize dependencies using a directed acyclic graph (DAG).
            \item \textbf{Modular Design}: Break workflows into reusable components for easier updates and maintenance.
            \item \textbf{Error Handling}: Include failure pathways and retry mechanisms.
        \end{itemize}
    \end{block}

    \begin{block}{Example Workflow Steps}
        \begin{enumerate}
            \item Data Ingestion
            \item Data Cleaning
            \item Data Transformation
            \item Data Aggregation
            \item Data Storage
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Scheduling Jobs - Best Practices}
    Scheduling determines when workflows or jobs should run. Proper scheduling maximizes resource utilization.

    \begin{block}{Best Practices}
        \begin{itemize}
            \item \textbf{Time-based Scheduling}: Run jobs at specific intervals based on data freshness.
            \item \textbf{Event-based Scheduling}: Trigger workflows based on events like new data arrival.
            \item \textbf{Resource Awareness}: Schedule intensive jobs during off-peak hours.
        \end{itemize}
    \end{block}

    \begin{block}{Example Cron Syntax}
        \begin{lstlisting}
0 2 * * * /path/to/your/job
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monitoring and Managing Workflows}
    Effective monitoring and management of workflows are crucial for ensuring optimal performance, reliability, and efficiency in data processing environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Workflow Monitoring}: Continuous observation of workflow processes.
        \item \textbf{Performance Metrics}: Key performance indicators (KPIs):
        \begin{itemize}
            \item Execution Time
            \item Resource Utilization
            \item Error Rates
        \end{itemize}
        \item \textbf{Alerting and Notifications}: Set up alerts for thresholds, with notifications via email or dashboards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Monitoring}
    \begin{itemize}
        \item \textbf{Log Analysis}: Analyze logs to identify anomalies or failures.
        \item \textbf{Visualization Tools}: Use tools like Grafana or Kibana for dashboards that visualize metrics over time.
        \item \textbf{Health Checks}: Implement periodic health checks for expected workflow performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Managing Workflows}
    \begin{enumerate}
        \item \textbf{Dynamic Resource Allocation}: Adjust resources based on current workload demands (e.g., AWS Auto Scaling).
        \item \textbf{Version Control}: Maintain control over workflow definitions to manage changes.
        \item \textbf{Retry Mechanisms}: Implement logic for tasks that fail due to transient errors.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Workflow Management Tools}
    \begin{itemize}
        \item \textbf{Apache Airflow}: Author, schedule, and monitor workflows with a rich user interface.
        \item \textbf{Luigi}: Build complex data pipelines, managing dependencies and scheduling with UI support.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Incorporating robust monitoring and management strategies ensures smooth workflow operations, maximizing resource utilization and minimizing downtime. Next, we will explore a real-world case study to see these concepts in action.
\end{frame}

\begin{frame}{Case Study: Real-World Application of Data Workflows}
    \begin{block}{Overview}
        This case study examines the application of data processing workflows utilizing \textbf{MapReduce} jobs and \textbf{workflow management} tools.
        MapReduce is a programming model used for processing large datasets with a distributed algorithm on a cluster.
        Understanding its implementation helps visualize how complex data operations can be structured efficiently for scalable applications.
    \end{block}
\end{frame}

\begin{frame}{Key Concepts}
    \begin{itemize}
        \item \textbf{MapReduce Basics}
        \begin{itemize}
            \item \textbf{Map Phase}: Processes input data into key-value pairs (e.g., counting word occurrences).
            \item \textbf{Reduce Phase}: Aggregates data based on keys (e.g., summing word counts).
        \end{itemize}
        
        \item \textbf{Workflow Management}
        \begin{itemize}
            \item Tools like \textbf{Apache Oozie} and \textbf{Apache Airflow} orchestrate the execution of MapReduce jobs, managing dependencies to ensure successful job execution.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Case Study Example: Log File Analysis}
    \textbf{Context}: A social media company needed to analyze server logs to understand user engagement patterns.
    
    \begin{enumerate}
        \item \textbf{Data Ingestion}
        \begin{itemize}
            \item Logs were collected from distributed systems and stored in \textbf{Hadoop HDFS}.
        \end{itemize}

        \item \textbf{MapReduce Job Implementation}
        \begin{itemize}
            \item \textbf{Map Function}:
            \begin{lstlisting}[language=Python]
def map_function(log_entry):
    user_id, action = log_entry.split(',')
    emit(user_id, action)
            \end{lstlisting}
        
            \item \textbf{Reduce Function}:
            \begin{lstlisting}[language=Python]
def reduce_function(user_id, actions):
    return user_id, count(actions)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Workflow Management}
        \begin{itemize}
            \item \textbf{Apache Oozie} is used to schedule the MapReduce job daily, managing dependencies to ensure the job starts after logs are ingested.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Scalability}: 
        MapReduce allows processing of vast amounts of data swiftly and efficiently.
        
        \item \textbf{Automation}: 
        Workflow management tools automate job scheduling and handling dependencies, reducing manual intervention.
        
        \item \textbf{Data Insights}: 
        The output from the MapReduce jobs provided actionable insights into user behavior, enabling data-driven decision-making.
    \end{itemize}
\end{frame}

\begin{frame}{Summary and Conclusion}
    This case study illustrates the practical use of MapReduce and workflow management in real-world scenarios, showcasing effective data processing to derive insights from large datasets.
    
    \textbf{Conclusion}: The efficient implementation of data workflows and management tools enables organizations to tackle complex data processing requirements, ensuring scalability and reliability in their operations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integrating APIs in Data Workflows}
    \begin{block}{Understanding API Integration}
        \begin{itemize}
            \item \textbf{API (Application Programming Interface)}: A set of protocols enabling software applications to communicate and share data seamlessly.
            \item APIs are essential for modern data workflows as they allow various tools and services to interact.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Integrate APIs?}
    \begin{itemize}
        \item \textbf{Enhanced Functionality}: APIs add features without extensive coding or re-engineering.
        \item \textbf{Data Accessibility}: Facilitate access to external data sources for richer datasets.
        \item \textbf{Streamlined Processes}: Automates routine tasks, reducing manual effort and errors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for API Integration}
    \begin{enumerate}
        \item \textbf{Understand the API Documentation}
        \begin{itemize}
            \item Familiarize with endpoints, authentication, rate limits, and data formats.
        \end{itemize}

        \item \textbf{Use API Clients}
        \begin{itemize}
            \item Leverage client libraries (e.g., Python's \texttt{requests}).
            \item \begin{lstlisting}[language=Python]
import requests
response = requests.get('https://api.weatherapi.com/v1/current.json?key=YOUR_API_KEY&q=London')
weather_data = response.json()
print(weather_data)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Error Handling}
        \begin{itemize}
            \item Implement robust error handling for API calls (e.g., 404, 500 errors).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for API Integration (Cont.)}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering
        \item \textbf{Optimize API Calls}
        \begin{itemize}
            \item Use pagination and conditional requests to manage server load.
        \end{itemize}

        \item \textbf{Security Considerations}
        \begin{itemize}
            \item Utilize secure authentication (OAuth, API tokens) and encryption (HTTPS).
        \end{itemize}

        \item \textbf{Monitoring and Analytics}
        \begin{itemize}
            \item Track API usage with tools like API Gateway or APM for performance insights.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing API Integration}
    \begin{block}{Workflow Visualization}
        \begin{itemize}
            \item \textbf{Diagram}: Flowchart showcasing the integration of data sources, API layers, and data processing tools leading to visualization dashboards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item API integration enhances data workflows by harnessing external capabilities.
        \item Best practices ensure efficiency, security, and reliability in data processing.
        \item Continual learning on APIs maximizes integration potential in future projects.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Overview}
    \begin{block}{Overview}
        This week, we delved into the critical role of data processing workflows and management tools, emphasizing their importance in effective data handling. Understanding these concepts provides a foundation for building robust data-driven applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Key Concepts Covered}
    \begin{enumerate}
        \item \textbf{Data Processing Workflows}
            \begin{itemize}
                \item Define the sequence of steps to transform raw data into useful information.
                \item \textbf{Example:} Data collection $\rightarrow$ Data cleaning $\rightarrow$ Data transformation $\rightarrow$ Data analysis $\rightarrow$ Visualization.
            \end{itemize}
            
        \item \textbf{APIs in Data Workflows}
            \begin{itemize}
                \item Vital for integrating components of data workflows, enabling seamless data exchange.
                \item \textbf{Illustration:} APIs connect applications, databases, and services for efficient data flow.
            \end{itemize}
        
        \item \textbf{Best Practices for Workflow Design}
            \begin{itemize}
                \item Clear documentation of each workflow component.
                \item Modular design for updates and maintenance.
                \item Error handling mechanisms for enhanced reliability.
            \end{itemize}
        
        \item \textbf{Management Tools and Their Functions}
            \begin{itemize}
                \item Streamline processes, facilitate collaboration, enhance data governance.
                \item Examples: Version control, data storage, data visualization platforms.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Key Points and Final Thoughts}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Importance of Efficient Workflows:} Enhance productivity, reduce errors, and accelerate insights.
            \item \textbf{Adaptability:} Workflows must evolve with data sources and technologies to ensure relevance.
            \item \textbf{Collaboration Across Teams:} Encourage collaboration among data scientists, analysts, and IT professionals for better decision-making.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Mastering data processing workflows and management tools is essential for data professionals. By employing best practices, leveraging APIs, and using management tools, you can achieve more timely insights, enhancing the value derived from data.
    \end{block}

    \begin{alertblock}{Remember}
        A well-structured data processing workflow is not just a technical necessity; it is the foundation of a data-driven culture that can propel organizations toward success.
    \end{alertblock}
\end{frame}


\end{document}