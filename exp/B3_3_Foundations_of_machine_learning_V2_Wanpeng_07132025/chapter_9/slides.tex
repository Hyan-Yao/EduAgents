\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Evaluating Model Performance}
    \begin{block}{Overview}
        Evaluating model performance is critical in machine learning, as it enables us to understand how well our models make predictions and guides decisions in model selection and improvement.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Evaluate Model Performance?}
    \begin{enumerate}
        \item \textbf{Understanding Effectiveness}:
            Assessing a machine learning model's performance is akin to evaluating employee effectiveness; it ensures that the model meets accuracy and reliability standards.
        
        \item \textbf{Guiding Improvements}:
            Identifying areas of poor performance allows for enhancements through adjustments in training, feature selection, or algorithm use.
        
        \item \textbf{Comparative Analysis}:
            Using consistent metrics enables informed comparisons between different models, revealing which is best suited for the task.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inspiring Questions}
    \begin{itemize}
        \item How can we tell if our model is actually solving the problem we designed it for?
        \item What does it mean for a model to be "good" or "bad" based on our goals?
        \item How can we effectively communicate our model's effectiveness to non-technical stakeholders?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Model performance evaluation is essential for \textbf{trustworthiness} and \textbf{reliability} in predictive analytics.
        \item Performance metrics must align with specific task objectives; different problems require diverse evaluation approaches.
        \item Understanding trade-offs between performance metrics is crucial (e.g., improving accuracy might reduce recall).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    In the following slides, we will explore specific \textbf{model evaluation metrics} such as:
    \begin{itemize}
        \item \textbf{Accuracy}: Percentage of correct predictions.
        \item \textbf{Precision}: Ratio of correctly predicted positive observations to the total predicted positives.
        \item \textbf{Recall}: Ratio of correctly predicted positive observations to all actual positives.
        \item \textbf{F1-score}: A balance between precision and recall.
    \end{itemize}
    By the end of this chapter, you will gain insights into how these metrics can inform better model choices and enhance your machine learning projects.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The process of evaluating model performance is not just a technical step; it is a fundamental practice that drives the success of machine learning applications. Effectively applying evaluation metrics bridges the gap between model development and real-world application.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reminder}
    As we move forward, keep in mind the context and objectives specific to your projects, and how these evaluation metrics will help you achieve your goals.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation Metrics}
    \begin{block}{Introduction}
        Evaluating the performance of machine learning models is essential to ensure accurate predictions. Various metrics provide insights into different aspects of this performance. In this section, we will explore some key evaluation metrics: accuracy, precision, recall, and F1-score.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics - Part 1}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted instances to total instances.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
            \end{equation}
            \item \textbf{Example}: 80 out of 100 correct predictions yields an accuracy of 80\%.
        \end{itemize}
        
        \item \textbf{Precision}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted positive observations to total predicted positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Example}: Predicting 10 positives with 7 actual positives gives a precision of 70\%.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Recall}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted positive observations to all actual positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Example}: Identifying 10 out of 15 actual positives results in a recall of approximately 67\%.
        \end{itemize}

        \item \textbf{F1-Score}
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of precision and recall, balancing both metrics.
            \item \textbf{Formula}:
            \begin{equation}
                \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Example}: Precision at 70\% and recall at 67\% yields a single metric for holistic performance assessment.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Evaluation Metrics}
    \begin{block}{Why Are These Metrics Important?}
        \begin{itemize}
            \item \textbf{Contextual Understanding}: The choice of metric depends on the model's goals, e.g., prioritizing recall in medical diagnoses to catch all positive cases, even if precision suffers.
            \item \textbf{Different Perspectives}: These metrics illustrate various performance aspects and highlight areas for improvement.
        \end{itemize}
    \end{block}
    
    \begin{block}{Engaging Reflection Questions}
        \begin{itemize}
            \item When is high recall more beneficial than high precision?
            \item How do business goals influence the choice of evaluation metrics?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy: Definition and Importance}
    \begin{block}{Brief Summary}
        Accuracy is a key metric in machine learning that measures the performance of models. It is defined as the ratio of correct predictions to total predictions. Understanding its significance and limitations is crucial for effective model evaluation and selection.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definition of Accuracy}
    \begin{block}{Accuracy in Machine Learning}
        Accuracy is defined as:
        \begin{equation} 
        \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} 
        \end{equation}
        For example, if a model predicts whether an email is spam correctly, accuracy indicates how often this prediction aligns with reality.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Accuracy in Model Evaluation}
    \begin{itemize}
        \item \textbf{Basic Benchmark:} 
              Accuracy provides a simple method for comparing model performances.
        \item \textbf{Interpretability:} 
              Stakeholders find high accuracy percentages easy to understand, influencing their perception of model reliability.
        \item \textbf{Model Selection:} 
              Models with higher accuracy are preferred during the selection process.
        \item \textbf{Relativity:} 
              Accuracy must be evaluated in context, especially in critical applications like medical diagnosis. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples for Clarity}
    \begin{itemize}
        \item \textbf{Example 1: Weather Prediction}
              If a model predicts rain correctly for 80 out of 100 days:
              \begin{equation}
              \text{Accuracy} = \frac{80}{100} = 0.80 \text{ or } 80\%
              \end{equation}
              
        \item \textbf{Example 2: Class Imbalance Impact}
              In a rare disease screening where only 5\% of the population is affected, a model predicting "no disease" for all would achieve 95\% accuracy but be ineffective.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Accuracy is valuable for initial model performance assessments.
        \item It can be misleading in cases of class imbalance; consider additional metrics like precision and recall.
        \item Evaluation context is crucial; accuracy alone may not reflect the true effectiveness in critical applications.
    \end{itemize}
    \begin{block}{Conclusion}
        Understanding accuracy lays the groundwork for exploring more nuanced evaluation metrics, enhancing model validation and selection approaches.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Understanding Metrics}
    \begin{block}{Introduction}
        Precision and Recall are vital metrics for evaluating classification models, particularly with imbalanced class distributions.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Precision:} Measures the accuracy of positive predictions.
        \item \textbf{Recall:} Measures the ability to identify all relevant instances.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Definitions and Formulas}
    
    \textbf{Precision} answers: *Of all positive predictions, how many are correct?*
    
    \begin{equation}
        \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
    \end{equation}
    
    \textbf{Recall} answers: *Of all actual positives, how many did we correctly predict?*
    
    \begin{equation}
        \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Examples}
    
    \textbf{Example for Precision:} 
    \begin{itemize}
        \item 100 positive predictions
        \item 80 true positives, 20 false positives
    \end{itemize}
    \begin{equation}
        \text{Precision} = \frac{80}{80 + 20} = 0.8 \text{ or } 80\%
    \end{equation}

    \textbf{Example for Recall:}
    \begin{itemize}
        \item 100 actual positive cases
        \item 80 correctly detected
    \end{itemize}
    \begin{equation}
        \text{Recall} = \frac{80}{80 + 20} = 0.8 \text{ or } 80\%
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Relationship Between Precision and Recall}
    
    \begin{itemize}
        \item High Precision → Fewer false positives, possible drop in Recall.
        \item High Recall → More true positives, potential increase in false positives.
    \end{itemize}
    
    \begin{block}{Key Trade-off}
        This trade-off promotes using the F1-Score, a balance between Precision and Recall.
    \end{block}
    
    \begin{itemize}
        \item Precision matters where false positives are costly (e.g., spam detection).
        \item Recall is critical where false negatives are serious (e.g., disease detection).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    Understanding Precision and Recall is essential for model evaluation, helping to choose appropriate models based on problem requirements. Recognizing the balance in performance metrics ensures informed decisions.
    
    Next, we will explore how the F1-Score provides a unified measure to consider both aspects effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score: Balancing Precision and Recall}
    \begin{block}{What is the F1-Score?}
        The F1-score is a performance metric that combines both precision and recall into a single score. 
        It is defined as the harmonic mean of precision and recall, making it useful in contexts where class distribution is imbalanced.
    \end{block}
    
    \begin{block}{Formula}
        \begin{equation}
            \text{F1-Score} = 2 \times \left( \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \right)
        \end{equation}
        
        Where:
        \begin{itemize}
            \item Precision = \( \frac{TP}{TP + FP} \)
            \item Recall = \( \frac{TP}{TP + FN} \)
            \item **TP**: True Positives 
            \item **FP**: False Positives  
            \item **FN**: False Negatives  
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use the F1-Score?}
    \begin{itemize}
        \item \textbf{Imbalanced Classes:} In many real-world scenarios, datasets are imbalanced. For instance, in medical diagnostics, only a small percentage of cases may indicate a disease. Accuracy alone can be misleading because a model could predict the majority class effectively.
        
        \item \textbf{Balanced View:} The F1-score considers both precision and recall, providing a comprehensive view of a model's performance, especially when the cost of false positives and false negatives is substantial.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Consider a spam email classifier:
    \begin{itemize}
        \item The model flags 80 emails as spam: \( TP = 70 \), \( FP = 10 \)
        \item The model misses 30 spam emails: \( FN = 30 \)
    \end{itemize}
    
    \textbf{Calculating the metrics:}
    
    \begin{block}{Precision}
        \begin{equation}
            \text{Precision} = \frac{TP}{TP + FP} = \frac{70}{70 + 10} = 0.875
        \end{equation}
    \end{block}

    \begin{block}{Recall}
        \begin{equation}
            \text{Recall} = \frac{TP}{TP + FN} = \frac{70}{70 + 30} = 0.7
        \end{equation}
    \end{block}

    \begin{block}{F1-Score}
        \begin{equation}
            \text{F1-Score} \approx 0.7857
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item The F1-score is especially useful for applications with imbalanced classes where one class dominates.
        \item It strikes a balance between precision and recall, offering a single metric for a model's effectiveness.
        \item It complements accuracy and provides deeper insights in critical fields like healthcare, finance, and fraud detection.
    \end{itemize}
    
    In conclusion, the F1-score is a vital tool in the evaluation of model performance, enabling informed decision-making based on a comprehensive understanding of model behavior beyond simple accuracy metrics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC Curve and AUC - Introduction}
    \begin{block}{What is ROC Curve?}
        The **Receiver Operating Characteristic (ROC) curve** is a graphical representation of a binary classifier's ability to distinguish between positive and negative classes as the discrimination threshold varies. 
    \end{block}
    \begin{itemize}
        \item Depicts the trade-off between:
            \begin{itemize}
                \item **Sensitivity (True Positive Rate)**: Correctly identified positives.
                \item **Specificity (False Positive Rate)**: Incorrectly identified negatives.
            \end{itemize}
        \item Key performance evaluation tool in binary classification.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding TPR and FPR}
    \begin{itemize}
        \item **True Positive Rate (TPR)**:
        \begin{equation}
            \text{TPR} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}
        \item **False Positive Rate (FPR)**:
        \begin{equation}
            \text{FPR} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}
        \end{equation}
        \item Adjusting the threshold yields a curve plotting TPR vs. FPR.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of ROC Curve}
    \begin{itemize}
        \item A medical test predicting disease status:
        \begin{enumerate}
            \item **Point A**: Low threshold — High TPR and High FPR.
            \item **Point B**: Moderate threshold — Balanced TPR and FPR.
            \item **Point C**: High threshold — Low FPR and Low TPR.
        \end{enumerate}
        \item The curve bows upwards from (0,0) to (1,1).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Area Under the Curve (AUC)}
    \begin{block}{What is AUC?}
        The **Area Under the Curve (AUC)** quantifies the overall performance of a classifier across all thresholds.
    \end{block}
    \begin{itemize}
        \item **Interpretation of AUC Values**:
            \begin{itemize}
                \item 0.5: No better than random chance.
                \item 0.7 - 0.8: Reasonable performance.
                \item 0.8 - 0.9: Good performance.
                \item > 0.9: Excellent performance.
            \end{itemize}
        \item Highlights the trade-off between sensitivity and specificity.
        \item Independent of class distribution, useful in imbalanced datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for ROC Curve}
    \begin{block}{Python Code Example}
        Here is a simple snippet using `sklearn` for plotting the ROC curve and calculating AUC:
    \end{block}
    \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

# Generate a synthetic binary classification dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Train a Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)
y_scores = model.predict_proba(X_test)[:, 1]

# Compute ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

# Plotting
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix}
    \begin{block}{What is a Confusion Matrix?}
        A \textbf{Confusion Matrix} is a performance measurement tool used for 
        classification tasks in machine learning. It visualizes the prediction 
        results, showing the counts of actual versus predicted classifications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of a Confusion Matrix}
    In a typical binary classification context, a confusion matrix is organized into four key categories:
    \begin{enumerate}
        \item \textbf{True Positive (TP)}: Correctly predicted positive instances.
        \item \textbf{False Positive (FP)}: Incorrectly predicted positive instances (Type I error).
        \item \textbf{True Negative (TN)}: Correctly predicted negative instances.
        \item \textbf{False Negative (FN)}: Incorrectly predicted negative instances (Type II error).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    \begin{block}{Confusion Matrix Example}
        Here’s a simple representation based on a medical test for a disease:
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline
                & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
                \hline
                \textbf{Actual Positive} & True Positive (TP) & False Negative (FN) \\
                \hline
                \textbf{Actual Negative} & False Positive (FP) & True Negative (TN) \\
                \hline
            \end{tabular}
        \end{center}
        \begin{itemize}
            \item \textbf{TP:} 70 - Patients who tested positive and have the disease.
            \item \textbf{FP:} 10 - Patients who tested positive but do not have the disease.
            \item \textbf{TN:} 50 - Patients who tested negative and do not have the disease.
            \item \textbf{FN:} 5 - Patients who tested negative but actually have the disease.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Metrics}
    \begin{itemize}
        \item \textbf{Interpretation:} Quickly see how many predictions your model got right and wrong.
        \item \textbf{Use Cases:} Important where costs of FP and FN vary.
        \item \textbf{Metrics from the Confusion Matrix:}
        \begin{itemize}
            \item \textbf{Accuracy:} \(\frac{TP + TN}{TP + TN + FP + FN}\)
            \item \textbf{Precision:} \(\frac{TP}{TP + FP}\)
            \item \textbf{Recall (Sensitivity):} \(\frac{TP}{TP + FN}\)
            \item \textbf{F1 Score:} \(2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}\)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The confusion matrix is an essential tool for evaluating classification models. 
    It provides insight into model performance and helps identify areas for improvement. 
    By analyzing the four types of predictions, you can make better decisions in refining models.
    
    \begin{block}{Questions for Reflection}
        \begin{itemize}
            \item How might the confusion matrix guide your choices in refining a classification model?
            \item In what scenarios could the impact of false positives versus false negatives lead to different strategic decisions?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Introduction}
    Evaluating model performance is critical in ensuring that your machine learning models effectively solve the problem they are designed for. 
    \begin{itemize}
        \item Selecting the right evaluation metric depends on:
        \begin{itemize}
            \item Specific business case
            \item Characteristics of your dataset
        \end{itemize}
        \item This slide will guide you through understanding these metrics and choosing the most suitable one for your needs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Considerations for Choosing Metrics}
    \begin{enumerate}
        \item \textbf{Business Objectives}:
        \begin{itemize}
            \item Classification vs. Regression
            \item Implications of accuracy based on context
        \end{itemize}
        
        \item \textbf{Dataset Characteristics}:
        \begin{itemize}
            \item Imbalanced Classes: Importance of F1-Score or AUC-ROC
            \item Size of Dataset: Cross-validated accuracy for small datasets
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Metrics to Consider}
    \begin{itemize}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item Definition: Ratio of correctly predicted instances to total instances
            \item Use Case: Best for balanced datasets
        \end{itemize}
        
        \item \textbf{Precision and Recall}
        \begin{itemize}
            \item Precision: $\frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$
            \item Recall: $\frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$
            \item Use Case: Important when cost of false positives/negatives is high
        \end{itemize}
        
        \item \textbf{F1-Score}
        \begin{itemize}
            \item Definition: Harmonic mean of precision and recall
            \item Formula: $\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$
            \item Use Case: Ideal for imbalanced classes
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Evaluation Metrics in Practice}
    \begin{itemize}
        \item \textbf{Example 1: Medical Diagnosis}
        \begin{itemize}
            \item Prioritize recall to avoid false negatives which can have serious implications.
        \end{itemize}
        
        \item \textbf{Example 2: Spam Detection System}
        \begin{itemize}
            \item Precision is crucial to avoid falsely marking important emails as spam.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{itemize}
        \item Align Metrics with Goals: Each metric should relate directly to business objectives.
        \item Mitigating Bias: Consider the impact of dataset imbalance.
        \item Use Multiple Metrics: A combination of metrics often provides a fuller picture.
    \end{itemize}
    Choosing the right evaluation metric is integral to understanding your model's effectiveness in your specific application context.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Understanding the Concepts}
    \begin{block}{Overview}
        When building predictive models, it’s crucial to ensure they perform well not just on training data but also on unseen data. 
    \end{block}
    
    Two common pitfalls in this context are:
    \begin{itemize}
        \item Overfitting
        \item Underfitting
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting - Definition and Impact}
    \begin{itemize}
        \item \textbf{Definition:} Overfitting occurs when a model learns the details of the training data, including noise, negatively impacting its performance on new data.
        \item \textbf{Example:} A student memorizing answers without understanding the underlying concepts behaves like an overfitted model - performing well on training data but poorly on unseen data.
    \end{itemize}
    
    \begin{block}{Illustration}
        A complex curve fitting every data point in a training dataset indicates overfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Underfitting - Definition and Impact}
    \begin{itemize}
        \item \textbf{Definition:} Underfitting happens when a model is too simplistic to capture the underlying trend of the data.
        \item \textbf{Example:} A student who understands general ideas but cannot apply them effectively reflects this scenario; hence an underfitted model yields poor predictions on both training and unseen data.
    \end{itemize}
    
    \begin{block}{Illustration}
        A straight line in a scatter plot suggests an underfitted model failing to capture the data's complexities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact on Model Evaluation}
    \begin{itemize}
        \item \textbf{Overfitting:} Low bias but high variance; training error is low, test error is high.
        \item \textbf{Underfitting:} High bias and low variance; both training and test errors are high.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques to Mitigate Overfitting and Underfitting}
    \begin{enumerate}
        \item \textbf{Regularization:}
            \begin{itemize}
                \item Introduces a penalty for larger coefficients in regression models (L1, L2).
                \item Example: Lasso regression adds a penalty equivalent to the absolute value of the magnitude of coefficients.
            \end{itemize}
        \item \textbf{Pruning and Simplifying Models:}
            \begin{itemize}
                \item In decision trees, removing branches that add little predictive value reduces complexity.
            \end{itemize}
        \item \textbf{Cross-Validation:}
            \begin{itemize}
                \item Helps assess how results will generalize to an independent dataset.
                \item Example: K-fold cross-validation divides the dataset into K parts, using each part to validate the model trained on the others.
            \end{itemize}
        \item \textbf{Use of Ensemble Methods:}
            \begin{itemize}
                \item Techniques like bagging or boosting can improve performance by combining models' predictions to enhance generalization.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Strive for a balance in model complexity to ensure high performance on both training and testing datasets.
        \item Regular evaluation using appropriate metrics will help identify model performance and prevent overfitting and underfitting.
    \end{itemize}
    
    By understanding and addressing these issues, you will enhance your modeling strategies for more robust predictive modeling.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation Techniques}
    \begin{block}{Introduction to Cross-Validation}
        Cross-validation is a powerful statistical method used to evaluate the performance of machine learning models.
        It helps estimate how a model will generalize to unseen data, ensuring the model performs effectively beyond the training set.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Cross-Validation}
    \begin{itemize}
        \item \textbf{Robust Performance Estimation:}
        \begin{itemize}
            \item Provides reliable estimates compared to a single train-test split.
            \item Assesses outcome variability with different test datasets.
        \end{itemize}
        \item \textbf{Reduction of Overfitting:}
        \begin{itemize}
            \item Mitigates overfitting by evaluating model performance on multiple datasets.
            \item Insight into the model's ability to generalize outside the training set.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Cross-Validation Techniques}
    \begin{enumerate}
        \item \textbf{k-Fold Cross-Validation:}
        \begin{itemize}
            \item Divides dataset into \textbf{k subsets} (folds).
            \item Trains on \textbf{k-1} folds and tests on the remaining fold.
            \item Repeated \textbf{k times} for variability.
        \end{itemize}
        
        \item \textbf{Stratified k-Fold Cross-Validation:}
        \begin{itemize}
            \item Each fold maintains proportionate class representation for imbalanced datasets.
        \end{itemize}
        
        \item \textbf{Leave-One-Out Cross-Validation (LOOCV):}
        \begin{itemize}
            \item Each training set is created by leaving one sample out.
            \item Computationally intensive but offers thorough evaluation.
        \end{itemize}
        
        \item \textbf{Group k-Fold Cross-Validation:}
        \begin{itemize}
            \item Maintains group integrity in datasets (e.g., data from the same patient).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    \begin{itemize}
        \item \textbf{Without Cross-Validation:} 
        - A model might perform well on training data but poorly on new data.
        \item \textbf{With k-Fold Cross-Validation:} 
        - Evaluating on subsets ensures it can predict effectively for unseen students.
    \end{itemize}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Essential for understanding bias and variance trade-off.
            \item Critical role in model selection and overfitting prevention.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    \begin{block}{Conclusion}
        Cross-validation is indispensable in machine learning, aiding in building models that adapt and perform well in real-world scenarios. 
        Utilizing robust cross-validation techniques enhances predictive capabilities and reliability across diverse datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 1}
    \begin{block}{Recap of Key Points in Evaluating Model Performance}
        \begin{enumerate}
            \item \textbf{Understanding Model Performance}:
                \begin{itemize}
                    \item Indicates how well a model predicts outcomes based on input data.
                    \item Crucial for assessing effectiveness in real-world scenarios.
                \end{itemize}
            \item \textbf{Importance of Choosing Appropriate Metrics}:
                \begin{itemize}
                    \item Different tasks require different performance metrics.
                    \item Selecting the right metric is vital for fair assessment.
                    \begin{itemize}
                        \item Accuracy: Useful for balanced classes.
                        \item Precision and Recall: Crucial in varied consequence scenarios.
                        \item F1 Score: Balances precision and recall.
                        \item ROC-AUC: Evaluates models across different thresholds.
                    \end{itemize}
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 2}
    \begin{block}{Key Concepts Continued}
        \begin{enumerate}[resume]
            \item \textbf{Overfitting and Cross-Validation}:
                \begin{itemize}
                    \item Overfitting: Learning training data too well, leading to poor generalization.
                    \item Cross-validation techniques, such as K-Fold, provide unbiased performance estimates.
                \end{itemize}
            \item \textbf{Real-World Examples}:
                \begin{itemize}
                    \item \textit{Housing Price Prediction}: Use MAE or RMSE for average error insight.
                    \item \textit{Spam Detection}: High recall is desirable to identify spam emails.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 3}
    \begin{block}{Incorporating Insights into Decision Making}
        \begin{itemize}
            \item Evaluation results guide model improvements and deployment choices.
            \item Clear communication of results to stakeholders aids decision-making.
        \end{itemize}
    \end{block}
    
    \begin{block}{Reflection Questions}
        \begin{itemize}
            \item How might different performance metrics influence your choice of model? 
            \item Can you think of a scenario where the choice of metric changed the model's interpretation?
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Performance evaluation isn't just about numbers; it’s about context.
            \item Align chosen metrics with specific application goals.
            \item Embrace a mindset of continuous improvement based on evaluations.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}