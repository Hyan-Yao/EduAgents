# Assessment: Slides Generation - Chapter 3: Data Collection and Cleaning

## Section 1: Introduction to Data Collection and Cleaning

### Learning Objectives
- Understand the fundamental concepts and importance of data collection in machine learning.
- Recognize the significance of data cleaning and its impact on model performance.
- Apply methods for handling missing values and removing duplicates.
- Prepare a dataset for effective model training by structuring data correctly.

### Assessment Questions

**Question 1:** What is the primary goal of data collection in machine learning?

  A) To ensure data is presented in charts
  B) To gather diverse and representative samples for analysis
  C) To remove duplicate entries from a dataset
  D) To format data into CSV files

**Correct Answer:** B
**Explanation:** The primary goal of data collection is to gather diverse and representative samples which allow the model to generalize effectively to unseen data.

**Question 2:** Why is data cleaning important in the data preprocessing phase?

  A) It allows data to be quickly downloaded from the internet.
  B) It reduces the size of the dataset.
  C) It eliminates inaccuracies and inconsistencies, enhancing data reliability.
  D) It focuses solely on increasing the number of records.

**Correct Answer:** C
**Explanation:** Data cleaning is vital because it identifies and corrects inaccuracies, thereby enhancing the reliability and quality of the data used for model training.

**Question 3:** Which of the following is a method for handling missing values?

  A) Increasing the dataset size
  B) Deleting records with missing values
  C) Compressing data
  D) Ignoring missing values

**Correct Answer:** B
**Explanation:** One common method for handling missing values is to delete records with missing information, although there are other methods such as imputation.

**Question 4:** What does the term 'Garbage in, garbage out' imply in the context of machine learning?

  A) High-quality input data will always lead to poor model performance.
  B) Low-quality input data will degrade the quality of model predictions.
  C) No data quality issues will affect model accuracy.
  D) Increasing data quantity will improve all models.

**Correct Answer:** B
**Explanation:** This phrase emphasizes that poor quality input data will lead to poor model predictions and outcomes, highlighting the critical importance of high-quality data.

### Activities
- Conduct a hands-on data collection exercise where learners select a topic and collect data from online sources. They must ensure that the data collected is diverse and representative.
- Provide a dataset with intentional inaccuracies and duplicates. Ask the learners to identify and correct these issues, explaining their reasoning behind each cleaning decision.

### Discussion Questions
- Discuss a scenario where poor data collection could lead to significant issues in a machine learning project. What steps could have been taken to prevent this?
- Reflect on a real-world case where data cleaning significantly improved a modelâ€™s accuracy. What were the challenges faced during the cleaning process?

---

## Section 2: Significance of Data in Machine Learning

### Learning Objectives
- Understand the critical role of data in machine learning.
- Identify factors that affect data quality and their implications for model performance.
- Evaluate the impact of data quality on real-world machine learning applications.

### Assessment Questions

**Question 1:** What is the primary role of data in machine learning?

  A) Data is used only for visualization purposes.
  B) Data is the backbone of machine learning models.
  C) Data is not important in machine learning.
  D) Data only needs to be processed once.

**Correct Answer:** B
**Explanation:** Data forms the foundation of all machine learning models and is essential for training them to recognize patterns and make predictions.

**Question 2:** Which of the following is NOT a factor that influences the quality of data?

  A) Accuracy
  B) Completeness
  C) Size of the dataset
  D) Consistency

**Correct Answer:** C
**Explanation:** While the size of the dataset can affect performance, it is not considered a direct factor influencing data quality like accuracy, completeness, and consistency.

**Question 3:** What could be the potential outcome of training a model on poor quality data?

  A) Increased accuracy in predictions.
  B) Improved insights and discoverability.
  C) A model that performs poorly in real-world scenarios.
  D) No impact on model performance.

**Correct Answer:** C
**Explanation:** Training on poor quality data can lead to a model that fails to deliver accurate predictions and insights, adversely affecting its performance in real-world applications.

**Question 4:** How does high data quality affect machine learning outcomes?

  A) It guarantees that all models will be successful.
  B) It leads to more reliable and accurate predictions.
  C) It increases the cost of model training.
  D) It complicates the data analysis process.

**Correct Answer:** B
**Explanation:** High-quality data enhances the potential for machine learning models to make accurate predictions and generate valuable insights.

### Activities
- Analyze a dataset provided and assess its quality based on accuracy, completeness, and consistency. Discuss how the data quality would affect model training.
- Choose a machine learning application (e.g., spam detection, image classification) and create a brief plan detailing what data quality checks you would implement before training models.

### Discussion Questions
- What specific strategies would you use to ensure the accuracy of your data in your field?
- Reflect on an instance from your experience where data quality impacted a decision-making process. What lessons did you learn?
- How can businesses improve data collection methods to enhance machine learning outcomes?

---

## Section 3: Types of Data Used in Machine Learning

### Learning Objectives
- Understand the different types of data used in machine learning.
- Identify appropriate data cleaning techniques based on data type.
- Recognize the implications of data types on machine learning processes.

### Assessment Questions

**Question 1:** What type of data is organized in a fixed format like rows and columns?

  A) Unstructured Data
  B) Structured Data
  C) Semi-Structured Data
  D) Time-Series Data

**Correct Answer:** B
**Explanation:** Structured data is organized in a predefined format, making it easier to store and analyze, often found in databases.

**Question 2:** Which type of data includes text documents and images?

  A) Structured Data
  B) Semi-Structured Data
  C) Categorical Data
  D) Unstructured Data

**Correct Answer:** D
**Explanation:** Unstructured data is characterized by its lack of a predefined structure, including formats like text and images.

**Question 3:** Which technique is commonly used to handle missing values in structured data?

  A) Interpolation
  B) Tokenization
  C) Imputation
  D) One-Hot Encoding

**Correct Answer:** C
**Explanation:** Imputation is a common technique used to replace missing values in structured data with statistical measures like mean or median.

**Question 4:** What describes time-series data?

  A) Data without any structure
  B) Data collected at specific intervals
  C) Data organized into categories
  D) Data consisting solely of images

**Correct Answer:** B
**Explanation:** Time-series data consists of observations indexed in time order, useful for tracking changes over time, such as stock prices.

**Question 5:** What is a typical cleaning technique for unstructured text data?

  A) One-Hot Encoding
  B) Normalization
  C) Tokenization
  D) Smoothing

**Correct Answer:** C
**Explanation:** Tokenization breaks down text into individual terms or phrases, making it easier to process and analyze unstructured text data.

### Activities
- 1. Collect a small dataset of structured and unstructured data (like CSV files for structured data and a collection of emails for unstructured data). Perform basic cleaning operations as discussed in the slide and document the process.
- 2. Create a visualization or summary of a time-series dataset (such as temperature readings over a week) and identify any missing values or trends.

### Discussion Questions
- What are the main challenges faced when working with unstructured data compared to structured data?
- How can the choice of data type influence the selection of machine learning algorithms?

---

## Section 4: Data Collection Techniques

### Learning Objectives
- Understand different techniques of data collection and their applications.
- Identify the strengths and weaknesses of surveys, web scraping, and public datasets.
- Apply data collection methods to real-world scenarios in healthcare and social media.

### Assessment Questions

**Question 1:** What is a primary advantage of using surveys for data collection?

  A) They require advanced programming skills.
  B) They can be customized to target specific information.
  C) They automate the data collection process.
  D) They provide pre-collected datasets.

**Correct Answer:** B
**Explanation:** Surveys are easily customizable, allowing researchers to tailor questions to get specific information needed for their study.

**Question 2:** Which of the following methods involves extracting data from websites?

  A) Surveys
  B) Web Scraping
  C) Public Datasets
  D) Interviews

**Correct Answer:** B
**Explanation:** Web scraping is the process of extracting data from websites, which can be automated to save time compared to manual collection.

**Question 3:** What is an important consideration when using web scraping as a data collection method?

  A) It is free from ethical concerns.
  B) It must respect the website's terms of service.
  C) It cannot analyze data on social media.
  D) It requires no programming knowledge.

**Correct Answer:** B
**Explanation:** Web scraping should always comply with the website's terms of service to avoid legal issues.

**Question 4:** Public datasets typically come from which of the following sources?

  A) Private corporations only
  B) Government databases and research institutions
  C) Personal surveys
  D) Web scraping

**Correct Answer:** B
**Explanation:** Public datasets are often provided by government databases, research institutions, and community-driven projects, making them accessible for research.

### Activities
- Choose a healthcare topic of your interest and design a short survey (5-10 questions) to gather data on patient satisfaction. Explain your choice of questions and how you would administer the survey.
- Find a public dataset related to health or social media analysis. Perform a preliminary data analysis using basic statistical methods and present your findings.

### Discussion Questions
- What ethical considerations should be taken into account when employing web scraping for data collection?
- How might the accuracy of survey results be affected by the design of the questionnaire?

---

## Section 5: Ethical Considerations in Data Collection

### Learning Objectives
- Explain the principles of informed consent and why it is critical in data collection.
- Discuss the importance of privacy and confidentiality in research.
- Analyze the implications of using datasets without understanding their context.

### Assessment Questions

**Question 1:** What is the primary purpose of obtaining informed consent in data collection?

  A) To ensure participants are paid for their involvement
  B) To inform participants about data usage and ensure their voluntary participation
  C) To avoid legal issues only
  D) To gather data more quickly

**Correct Answer:** B
**Explanation:** Informed consent ensures that participants are fully aware of how their data will be used, promoting voluntary participation and transparency.

**Question 2:** What does the term 'confidentiality' refer to in the context of data collection?

  A) The accuracy of the collected data
  B) Safeguarding personal information and how data is handled
  C) The ability to access raw data without restrictions
  D) Only using data collected from public sources

**Correct Answer:** B
**Explanation:** Confidentiality pertains to how personal data is handled and shared, ensuring that participant information is kept private.

**Question 3:** Why is context essential when using datasets in research?

  A) It adds complexity to data analysis
  B) It prevents misinterpretation of the data and its implications
  C) Context is irrelevant; numbers speak for themselves
  D) It only benefits the researchers, not the participants

**Correct Answer:** B
**Explanation:** Understanding the context in which data was collected is crucial to prevent misleading conclusions and potential harm.

**Question 4:** What is a potential consequence of neglecting ethical standards in data collection?

  A) Increased accuracy in research findings
  B) Loss of public trust and possible legal repercussions
  C) Enhanced participation from individuals
  D) Improved data collection methods

**Correct Answer:** B
**Explanation:** Neglecting ethical standards can lead to violations of privacy, which risks the loss of public trust and can result in legal actions.

### Activities
- Conduct a mock informed consent process with your peers. Prepare a brief presentation outlining your research study, including how you would explain data usage and confidentiality.

### Discussion Questions
- What are some common challenges researchers face in obtaining informed consent?
- Can you think of a recent example where a company used data unethically? What were the implications?
- How can researchers balance the need for data collection with ethical considerations?

---

## Section 6: Understanding Data Quality

### Learning Objectives
- Understand the definition and importance of data quality in machine learning.
- Identify and describe the key dimensions of data quality.
- Evaluate real-world data scenarios for quality issues and propose solutions.

### Assessment Questions

**Question 1:** What is the primary dimension of data quality that reflects reality accurately?

  A) Completeness
  B) Accuracy
  C) Consistency
  D) Timeliness

**Correct Answer:** B
**Explanation:** Accuracy defines how closely data matches the real-world conditions it is meant to represent.

**Question 2:** Which dimension of data quality ensures there are no duplicates?

  A) Accuracy
  B) Uniqueness
  C) Validity
  D) Completeness

**Correct Answer:** B
**Explanation:** Uniqueness ensures that each record in a dataset is distinct and free from duplicates.

**Question 3:** Why is data completeness important in a customer database?

  A) It allows for faster processing of data.
  B) It prevents data loss during migration.
  C) It ensures that all necessary information is available for effective communication.
  D) It minimizes storage costs.

**Correct Answer:** C
**Explanation:** Completeness ensures that all required fields, such as contact information, are present, facilitating effective communication.

**Question 4:** What does the dimension of timeliness refer to?

  A) The degree to which data is valid
  B) The time taken to process data
  C) Data being current and available when needed
  D) The accuracy of data

**Correct Answer:** C
**Explanation:** Timeliness refers to data being up-to-date and accessible when required for analysis.

### Activities
- Analyze a dataset provided for inconsistencies in data quality related to accuracy, completeness, and consistency. Create a short report detailing your findings and suggested improvements.
- Group activity: Work in teams to identify real-world scenarios (e.g., healthcare, finance) where poor data quality could lead to significant issues. Present your findings to the class.

### Discussion Questions
- Can you think of an instance where you encountered poor data quality? What impact did it have?
- How can organizations ensure continuous monitoring of data quality? What tools or practices would you recommend?
- In your opinion, which dimension of data quality is the most critical in machine learning? Why?

---

## Section 7: Data Cleaning Processes

### Learning Objectives
- Understand the significance of data cleaning in generating reliable datasets for analysis.
- Learn different methods for handling missing data, duplicates, and outliers.
- Implement practical techniques for data cleaning using programming tools.

### Assessment Questions

**Question 1:** What is the primary purpose of data cleaning?

  A) To add new data to the dataset
  B) To improve data quality for analysis
  C) To visualize data in a graphical format
  D) To store data in a database

**Correct Answer:** B
**Explanation:** The primary purpose of data cleaning is to improve data quality for analysis, ensuring that insights derived are based on accurate and reliable data.

**Question 2:** Which method is used to fill in missing data by replacing it with the average of the existing values?

  A) Predictive Imputation
  B) Mean/Median Imputation
  C) Data Transformation
  D) Outlier Removal

**Correct Answer:** B
**Explanation:** Mean/Median Imputation is the method used to fill in missing data by replacing it with the average or median of the existing values in the dataset.

**Question 3:** When removing duplicates in a dataset, what does it ensure?

  A) Increased dataset size
  B) More diverse data points
  C) Each entry in the dataset is unique
  D) Faster data processing speed

**Correct Answer:** C
**Explanation:** Removing duplicates ensures that each entry in the dataset is unique, which prevents certain data points from being over-represented in analysis.

**Question 4:** What statistical method can be used to detect outliers based on the distribution of data?

  A) Mean Calculation
  B) Z-score
  C) Median Calculation
  D) Mode Calculation

**Correct Answer:** B
**Explanation:** The Z-score is a statistical method used to detect outliers based on how many standard deviations a data point is from the mean.

### Activities
- 1. Given a dataset with missing values, apply mean imputation to fill in the missing entries.
- 2. Write a Python script using Pandas to identify and drop duplicate entries from a provided dataset.
- 3. Using a sample dataset, calculate the Z-scores for each data point and identify potential outliers.

### Discussion Questions
- Why is it sometimes necessary to remove outliers, and how can this impact your analysis?
- How does the method of handling missing data affect the overall integrity of your dataset?
- What challenges do you anticipate when cleaning large datasets?

---

## Section 8: Tools for Data Cleaning

### Learning Objectives
- Identify and describe various tools available for data cleaning.
- Demonstrate the use of specific tools such as Google AutoML, Pandas, Excel, and OpenRefine for cleaning data.
- Understand the importance of user-friendly features in data cleaning tools.

### Assessment Questions

**Question 1:** What feature of Google AutoML enhances its user-friendliness?

  A) Command-line Interface
  B) Drag-and-Drop Interface
  C) Extensive Coding Requirements
  D) Text-Based Data Input

**Correct Answer:** B
**Explanation:** Google AutoML features a drag-and-drop interface that allows users to easily upload datasets and specify cleaning tasks, making it user-friendly for non-coders.

**Question 2:** Which Python library is specifically designed for data manipulation and cleaning?

  A) NumPy
  B) Matplotlib
  C) Pandas
  D) SciPy

**Correct Answer:** C
**Explanation:** Pandas is the Python library specifically designed for data manipulation and analysis, including capabilities for data cleaning.

**Question 3:** What function in Pandas can be used to remove duplicate entries in a DataFrame?

  A) df.dropna()
  B) df.remove_duplicates()
  C) df.drop_duplicates()
  D) df.clean_duplicates()

**Correct Answer:** C
**Explanation:** The function df.drop_duplicates() in Pandas is used to remove duplicate entries from a DataFrame.

**Question 4:** Which Excel function helps to identify errors in data and manage it accordingly?

  A) =CLEAN()
  B) =SUM()
  C) =IFERROR()
  D) =LOOKUP()

**Correct Answer:** C
**Explanation:** =IFERROR() in Excel helps manage errors by returning a specified value if an error occurs, thereby helping in data cleaning.

**Question 5:** What is a key feature of OpenRefine?

  A) Supports only CSV files
  B) Provides complex coding capabilities
  C) Clusters similar data entries
  D) Requires extensive programming knowledge

**Correct Answer:** C
**Explanation:** OpenRefine includes features for clustering similar data entries, making it easier to clean messy datasets.

### Activities
- Create a small dataset with intentional errors and duplicates. Use Google AutoML, Pandas, and Excel to clean the dataset and compare the ease and effectiveness of each tool.
- Write a short Python script using Pandas to load a CSV file, fill in missing values, and remove duplicates, then document the process.

### Discussion Questions
- What are the advantages and disadvantages of using automated tools like Google AutoML for data cleaning compared to manual coding with libraries like Pandas?
- How do you decide which data cleaning tool to use based on the characteristics of your dataset?
- Can user-friendly data cleaning tools compromise the quality or depth of the cleaning process? Discuss with examples.

---

## Section 9: Case Study: Data Cleaning in Practice

### Learning Objectives
- Understand the common challenges in data quality and cleaning.
- Learn the systematic approach to data cleaning through practical steps.
- Apply statistical methods to identify outliers in datasets.

### Assessment Questions

**Question 1:** What is one of the main challenges associated with the data collected by ShopSmart?

  A) Excessive standardization of payment methods
  B) Inconsistent formatting in 'Payment Method' field
  C) High levels of duplicate entries
  D) All of the above

**Correct Answer:** B
**Explanation:** The main challenge identified was the inconsistent formatting in the 'Payment Method' field, with variations like 'CC', 'Credit Card', and 'Debit Card'.

**Question 2:** Which method is suggested for handling outlier detection in the data cleaning process?

  A) Mean calculation
  B) Mode identification
  C) Interquartile Range (IQR) method
  D) Standard deviation analysis

**Correct Answer:** C
**Explanation:** The case study suggests using the Interquartile Range (IQR) method to define acceptable limits for quantities to identify outliers.

**Question 3:** During the data cleaning process, what action was taken for records missing essential fields?

  A) They were kept for future use
  B) They were erroneously included in analyses
  C) They were removed from the dataset
  D) They were categorized as low priority

**Correct Answer:** C
**Explanation:** Records with essential missing fields like 'Price' were removed to maintain dataset integrity.

**Question 4:** Which Python library was primarily used for data manipulation in this case study?

  A) Matplotlib
  B) NumPy
  C) Pandas
  D) SciPy

**Correct Answer:** C
**Explanation:** Pandas was the library primarily used for data manipulation to facilitate the cleaning process.

**Question 5:** What is a key takeaway from the data cleaning case study?

  A) Data cleaning is unnecessary for small datasets
  B) Automation can hinder quality control
  C) Clean data is fundamental for reliable analysis
  D) Standardization makes data less usable

**Correct Answer:** C
**Explanation:** A key takeaway is that clean data is fundamental for reliable analysis and insights for decision-making.

### Activities
- Perform a data cleaning exercise on a provided sample dataset with missing values, duplicates, and outliers.
- Create a mapping table to standardize variations in a hypothetical 'Payment Method' dataset.
- Utilize Python to analyze a dataset and identify duplicates using 'Transaction ID' and 'Customer ID'.

### Discussion Questions
- What additional challenges can arise during data cleaning that were not covered in this case study?
- How can the importance of data cleaning be communicated to non-technical stakeholders?
- In what scenarios do you think data cleaning could be overlooked, and what could be the consequences?

---

## Section 10: Best Practices for Data Collection and Cleaning

### Learning Objectives
- Understand the essential practices for effective data collection.
- Apply techniques for cleaning datasets to ensure data integrity.
- Recognize the importance of ethical considerations in data handling.

### Assessment Questions

**Question 1:** What is the first step in the data collection process?

  A) Use reliable data sources
  B) Define clear objectives
  C) Implement real-time validation
  D) Conduct regular audits

**Correct Answer:** B
**Explanation:** Defining clear objectives sets the foundation for a successful data collection process.

**Question 2:** Which practice helps to minimize inconsistencies in data entry?

  A) Use open-ended questions
  B) Implement real-time validation
  C) Allow free text for categorical variables
  D) Use dropdown menus for categorical variables

**Correct Answer:** D
**Explanation:** Using dropdown menus for categorical variables standardizes data entry and reduces errors.

**Question 3:** What should you do with missing data records during cleaning?

  A) Always remove them
  B) Fill them in with random values
  C) Use mean/mode imputation or remove based on the extent of missingness
  D) Ignore them

**Correct Answer:** C
**Explanation:** Mean/mode imputation or removing records based on the extent of missingness are common acceptable strategies.

**Question 4:** Why is it important to document your data collection and cleaning processes?

  A) To make the data look more credible
  B) For transparency and accountability
  C) To make the collection process easier
  D) To comply with software requirements

**Correct Answer:** B
**Explanation:** Documenting your processes helps maintain transparency and accountability in data handling.

**Question 5:** What is an essential ethical practice during data collection?

  A) Collect as much data as possible without consent
  B) Anonymize sensitive data
  C) Store all data unprotected
  D) Share data without permission

**Correct Answer:** B
**Explanation:** Anonymizing sensitive data ensures compliance with privacy laws and protects personal information.

### Activities
- Create a data collection plan for a hypothetical survey. Define objectives, identify reliable sources, and devise a method for standardizing data entry.
- Choose a small dataset and perform data cleaning tasks. Identify missing values, duplicates, and document the changes made to the dataset.

### Discussion Questions
- What challenges have you encountered during data collection, and how did you overcome them?
- How can the use of technology aid in standardizing data entry and ensuring quality?
- In what ways do you think ethical concerns in data collection can impact public trust in research findings?

---

## Section 11: Conclusion

### Learning Objectives
- Understand the importance of high-quality data for machine learning.
- Describe effective data collection strategies and practices.
- Explain critical steps in the data cleaning process.
- Recognize the iterative nature of data collection and cleaning in relation to machine learning.

### Assessment Questions

**Question 1:** What is the primary reason why data quality is crucial in machine learning?

  A) It reduces the size of datasets
  B) It leads to accurate models
  C) It simplifies data storage
  D) It increases the number of features available

**Correct Answer:** B
**Explanation:** High-quality datasets lead to accurate models, while poor-quality data can lead to misleading outcomes.

**Question 2:** Which of the following is NOT a part of the data cleaning process?

  A) Removing duplicates
  B) Filling in missing values
  C) Generating new data
  D) Correcting inaccuracies

**Correct Answer:** C
**Explanation:** Data cleaning focuses on correcting errors in the existing dataset, not generating new data.

**Question 3:** How does effective data collection impact machine learning?

  A) It increases model size
  B) It ensures model relies on outdated information
  C) It ensures data is representative of the problem
  D) It complicates data processing

**Correct Answer:** C
**Explanation:** Effective data collection ensures that the data is representative of the problem being solved, which is crucial for model accuracy.

**Question 4:** Why is data cleaning considered an iterative process?

  A) Because it happens only once
  B) Because data is static
  C) Because data needs to be updated regularly
  D) Because errors are easy to detect

**Correct Answer:** C
**Explanation:** Data cleaning is iterative as new data and updates may introduce new errors and necessitate reevaluation.

### Activities
- Identify a dataset relevant to a machine learning application of your choice. Perform data cleaning tasks on the dataset, including removing duplicates, handling missing values, and correcting inaccuracies. Document the steps you took and the rationale behind each decision.

### Discussion Questions
- In what ways can poor data collection practices affect business decisions?
- What role do automated tools play in the data cleaning process, and how can they enhance accuracy?

---

## Section 12: Discussion and Questions

### Learning Objectives
- Understand the role of data integrity in machine learning.
- Identify common data issues and appropriate data cleaning techniques.
- Evaluate the ethical implications of data practices.

### Assessment Questions

**Question 1:** What is the significance of data integrity in machine learning?

  A) It reduces the need for data collection
  B) It ensures models produce valid and actionable insights
  C) It eliminates the need for data cleaning
  D) It is not important in ML

**Correct Answer:** B
**Explanation:** Data integrity is crucial as it ensures that the data used for training algorithms is accurate and reliable, leading to valid predictions.

**Question 2:** Which of the following is a common method to handle missing data?

  A) Ignore the missing values completely
  B) Use imputation or drop variables
  C) Combine all data into a single group
  D) None of the above

**Correct Answer:** B
**Explanation:** Handling missing data often involves imputation or dropping variables to maintain the dataset's integrity and usefulness.

**Question 3:** What approach could be taken to manage outliers in a dataset?

  A) Remove outliers to prevent skewing results
  B) Treat outliers as important data points
  C) Ignore outliers entirely
  D) Increase the dataset size to dilute their impact

**Correct Answer:** A
**Explanation:** While removing outliers can be critical, it depends on the context and why they appear. Managing them properly ensures the model's robustness.

**Question 4:** Why is ethical consideration important in data collection?

  A) It is not important
  B) It helps balance data needs with societal values
  C) It reduces the cost of data collection
  D) It ensures data is collected quickly

**Correct Answer:** B
**Explanation:** Ethical considerations are paramount as they govern how data affects individual privacy and societal norms, ensuring that data practices align with values.

### Activities
- Research a recent case study where poor data practices led to negative outcomes in machine learning. Present your findings to the class.
- Conduct a small group exercise where each group identifies common data issues in a provided dataset and presents potential cleaning techniques.

### Discussion Questions
- What practices do you think should be standardized across the industry to enhance data integrity?
- How can emerging technologies like AI help in upholding data standards?
- Can you think of examples in recent news where data integrity issues led to negative consequences?

---

