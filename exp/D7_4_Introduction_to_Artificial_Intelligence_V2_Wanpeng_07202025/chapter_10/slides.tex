\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
  \frametitle{Introduction to Probabilistic Reasoning}
  \begin{block}{What is Probabilistic Reasoning?}
    Probabilistic reasoning refers to the use of probability to represent and reason about uncertain information. In AI, it plays a critical role in decision-making under ambiguity or incomplete information. 
  \end{block}
  
  \begin{block}{Significance in AI Decision-Making}
    \begin{itemize}
      \item Handles uncertainty in real-world situations.
      \item Improves predictions by quantifying uncertainty.
      \item Facilitates learning in machine learning algorithms.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts in Probabilistic Reasoning}
  \begin{itemize}
    \item \textbf{Probabilities and Odds:} 
      \begin{itemize}
        \item Probabilities quantify likelihood (0 to 1 scale).
        \item Odds represent the ratio of probabilities.
      \end{itemize}
    \item \textbf{Bayes’ Theorem:} 
      A foundational principle defined as:
      \begin{equation}
        P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
      \end{equation}
      Where:
      \begin{itemize}
        \item $P(A|B)$: Probability of event A given B
        \item $P(B|A)$: Probability of event B given A
        \item $P(A)$: Probability of event A
        \item $P(B)$: Probability of event B
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example Illustration of Bayes' Theorem}
  \textbf{Scenario:} Notification of possible rain tomorrow.
  
  \begin{itemize}
    \item \textbf{Prior Information:} Chance of rain on any given day: 
    \[
    P(\text{Rain}) = 0.3
    \]
    \item \textbf{Weather Report:} Reliable service reports chance of rain:
    \[
    P(\text{Report} = \text{Rain}) = 0.7
    \]
    \item Using Bayes' theorem, update your belief to make an informed decision about carrying an umbrella.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item Probabilistic reasoning enhances decision-making by structuring uncertainty.
    \item Understanding Bayes' theorem and conditional probabilities is crucial for effective AI systems.
    \item Applications range from everyday decisions (like weather checks) to complex AI systems (like autonomous vehicles).
  \end{itemize}

  \begin{block}{Next Steps}
    This introduction lays the groundwork for more advanced topics like Bayesian networks and machine learning algorithms. In the following slide, we will outline learning objectives to deepen your understanding.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    \begin{block}{Learning Objectives for Week 10}
        In this week’s exploration of probabilistic reasoning, we aim to achieve the following specific learning objectives:
    \end{block}
    \begin{itemize}
        \item Understand the Basics of Probability
        \item Grasp Bayes' Theorem
        \item Explore Bayesian Networks
        \item Construct and Analyze Bayesian Networks
        \item Evaluate Decision-Making Under Uncertainty
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Bayes' Theorem}
    \begin{enumerate}
        \item \textbf{Understand the Basics of Probability:}
        \begin{itemize}
            \item Define probability and its significance in modeling uncertainty.
            \item Distinguish between types of probabilities: prior, likelihood, and posterior.
        \end{itemize}

        \item \textbf{Grasp Bayes' Theorem:}
        \begin{itemize}
            \item Understand the formulation and interpretation of Bayes' theorem.
            \item Learn the mathematical formula:
            \begin{equation}
            P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
            \end{equation}
            Where:
            \begin{itemize}
                \item $P(A|B)$ = Posterior probability
                \item $P(B|A)$ = Likelihood
                \item $P(A)$ = Prior probability
                \item $P(B)$ = Marginal likelihood
            \end{itemize}
            \item Apply Bayes' theorem in real-world scenarios, such as medical diagnosis or spam filtering.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Bayesian Networks}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Explore Bayesian Networks:}
        \begin{itemize}
            \item Define a Bayesian network as a graphical model representing a set of variables and their conditional dependencies.
            \item Understand the structure: nodes (random variables) and directed edges (dependencies).
        \end{itemize}
        
        \item \textbf{Construct and Analyze Bayesian Networks:}
        \begin{itemize}
            \item Learn how to build a simple Bayesian network for disease prediction based on symptoms.
            \item Use the network to calculate probabilities and update beliefs based on new evidence.
        \end{itemize}
        
        \item \textbf{Evaluate Decision-Making Under Uncertainty:}
        \begin{itemize}
            \item Discuss how probabilistic reasoning aids in informed decision-making.
            \item Examine case studies where Bayesian approaches enhanced decision-making processes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Bayes' Theorem? - Overview}
    Bayes' Theorem is a fundamental principle in probability theory that describes how to update the probability of a hypothesis based on new evidence. 
    \begin{itemize}
        \item Allows calculation of the probability of an event given prior knowledge.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem - The Formula}
    The mathematical formulation of Bayes' Theorem is:
    \begin{equation}
    P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
    \end{equation}
    
    Where:
    \begin{itemize}
        \item $P(H|E)$: Posterior probability - the probability of hypothesis $H$ given evidence $E$.
        \item $P(E|H)$: Likelihood - the probability of observing evidence $E$ given that hypothesis $H$ is true.
        \item $P(H)$: Prior probability - the initial probability of hypothesis $H$ before observing evidence $E$.
        \item $P(E)$: Marginal likelihood - the total probability of observing evidence $E$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem - Example Scenario}
    Consider a medical test for a disease:
    \begin{itemize}
        \item Let **H** be the condition that a person has the disease.
        \item Let **E** represent the positive test result.
    \end{itemize}
    
    Given:
    \begin{itemize}
        \item $P(H) = 0.01$: 1\% of people have the disease.
        \item $P(E|H) = 0.9$: 90\% probability of a positive test if a person has the disease.
        \item $P(E) = 0.1$: 10\% probability of a positive test overall, which includes false positives.
    \end{itemize}
    
    Applying Bayes' Theorem:
    \begin{equation}
    P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)} = \frac{0.9 \cdot 0.01}{0.1} = 0.09
    \end{equation}

    **Conclusion**: Even with a positive test result, there is only a 9\% chance of actually having the disease.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Conditional Probability - Part 1}
    \begin{block}{Definition of Conditional Probability}
        Conditional probability quantifies the likelihood of an event occurring given that another event has occurred. It is denoted as \( P(A | B) \), read as "the probability of event A occurring given that event B has occurred."
    \end{block}

    \begin{block}{Mathematical Expression}
        \begin{equation}
        P(A | B) = \frac{P(A \cap B)}{P(B)}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( P(A | B) \) = Conditional probability of A given B
            \item \( P(A \cap B) \) = Joint probability of both A and B occurring
            \item \( P(B) \) = Probability of event B
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Conditional Probability - Part 2}
    \begin{block}{Importance in Bayes’ Theorem}
        Bayes' Theorem relates the conditional and marginal probabilities of random events:
        \begin{equation}
        P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)}
        \end{equation}
    \end{block}

    \begin{itemize}
        \item \textbf{Inference:} Updates the probability of an event with new evidence.
        \item \textbf{Decision Making:} Crucial in fields like medical diagnostics, spam detection, and finance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Conditional Probability - Part 3}
    \begin{block}{Example}
        Consider a standard deck of 52 playing cards:
        \begin{itemize}
            \item Let event A be drawing a heart.
            \item Let event B be drawing a red card.
        \end{itemize}

        To find \( P(A | B) \):
        \begin{itemize}
            \item Hearts among red cards = 13 (since there are 26 red cards: hearts and diamonds).
            \item Thus, \( P(A | B) = \frac{P(A \cap B)}{P(B)} = \frac{13/52}{26/52} = \frac{13}{26} = \frac{1}{2} \).
        \end{itemize}
        This implies there is a 50% chance the drawn red card is a heart.
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Understanding context is crucial for interpreting results.
            \item Real-world applications include healthcare, marketing, and AI.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayes' Theorem - Learning Objectives}
    \begin{itemize}
        \item Understand how Bayes' theorem is applied in real-world scenarios, particularly in AI.
        \item Identify specific examples of Bayes' theorem usage like spam filtering and diagnostic systems.
        \item Analyze the impact of probabilistic reasoning in decision-making processes within AI applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayes' Theorem - Key Concepts}
    Bayes' theorem provides a mathematical framework for updating probabilities based on new evidence. It allows AI systems to make informed decisions in scenarios with uncertainty.

    \begin{block}{Bayes' Theorem Formula}
        \begin{equation}
            P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
        \end{equation}
        where:
        \begin{itemize}
            \item \(P(A|B)\): Posterior probability (probability of event A given evidence B)
            \item \(P(B|A)\): Likelihood (probability of evidence B given event A)
            \item \(P(A)\): Prior probability (initial probability of event A)
            \item \(P(B)\): Marginal probability of evidence B
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayes' Theorem - Real-world Applications}
    \textbf{1. Spam Filtering}
    \begin{itemize}
        \item \textbf{Objective}: Identify whether an email is spam or not using previous data.
        \item \textbf{Application of Bayes' Theorem}:
        \begin{itemize}
            \item Prior probability \(P(\text{spam})\): Overall probability of receiving a spam email.
            \item Evidence \(P(\text{words | spam})\): Likelihood of certain words appearing in spam emails.
            \item The filter updates the belief \(P(\text{spam | words})\) given the words present in an email.
        \end{itemize}
        \item \textbf{Example}: If 70\% of emails are spam and a spam email contains the word "free," the filter calculates the probability of an email being spam if it includes "free."
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayes' Theorem - Real-world Applications (Cont'd)}
    \textbf{2. Medical Diagnostic Systems}
    \begin{itemize}
        \item \textbf{Objective}: Diagnose a patient based on symptoms and test results.
        \item \textbf{Application of Bayes' Theorem}:
        \begin{itemize}
            \item Prior probability \(P(\text{disease})\): Prevalence of the disease in the population.
            \item Evidence \(P(\text{test positive | disease})\): Probability of testing positive if the disease is present.
            \item The system evaluates \(P(\text{disease | test positive})\) to determine the likelihood that the patient has the disease after a positive test result.
        \end{itemize}
        \item \textbf{Example}: A disease affects 1\% of the population, with a test sensitivity of 90\% and specificity of 95\%. If a patient tests positive, Bayes’ theorem estimates the actual probability of having the disease.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayes' Theorem - Key Points}
    \begin{itemize}
        \item Bayes' theorem is a powerful tool for making probabilistic inferences in uncertain environments.
        \item Real-world applications demonstrate its utility in filtering, diagnosis, and beyond.
        \item It allows for continuous learning and adaptation as new evidence becomes available, enhancing decision-making processes in AI.
    \end{itemize}
    
    Utilizing Bayes' theorem in these applications illustrates the transformative impact of probabilistic reasoning in artificial intelligence, paving the way for smarter, more efficient systems that learn from data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayesian Networks Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand what Bayesian networks are and their components.
            \item Recognize the importance of conditional dependencies in representing variable relationships.
            \item Gain insight into practical applications of Bayesian networks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Bayesian Network?}
    A \textbf{Bayesian Network} is a graphical model that represents a set of variables and their conditional dependencies through a directed acyclic graph (DAG).
    \begin{itemize}
        \item \textbf{Nodes:} Represent random variables (e.g., symptoms of a disease).
        \item \textbf{Edges:} Directed links that indicate dependency (e.g., a disease causing a specific symptom).
        \item \textbf{Conditional Probability Tables (CPTs):} Each node has an associated CPT that quantifies the effects of its parents.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conditional Dependencies}
    \begin{itemize}
        \item Conditional dependencies reflect the notion that the probability of a variable can depend on the values of other related variables.
        \item This means that knowing the state of one variable can provide information about another variable.
    \end{itemize}
    \begin{block}{Example:}
        In a medical diagnosis scenario:
        \begin{itemize}
            \item \textbf{Nodes:} Disease (D), Symptom (S), Test Result (T).
            \item \textbf{Edges:} D $\rightarrow$ S and D $\rightarrow$ T.
        \end{itemize}
        Indicating both the symptom and the test result depend on the disease.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    \begin{block}{Bayes Network Example}
        \textbf{Nodes:}
        \begin{itemize}
            \item Rain (R)
            \item Traffic Jam (T)
            \item Arrive Late (L)
        \end{itemize}
        \textbf{Dependencies:}
        \begin{itemize}
            \item $P(T | R)$: Traffic jams depend on whether it is raining.
            \item $P(L | T)$: Arriving late depends on traffic jams.
        \end{itemize}
    \end{block}
    \begin{block}{Advantages of Bayesian Networks}
        \begin{enumerate}
            \item Modular Structure: Easy to update and manage as new data comes in.
            \item Inference Capabilities: Allows computation of marginal probabilities of any subset of variables.
            \item Handling Uncertainty: Effectively manages and models uncertain information.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recap and Next Steps}
    \begin{itemize}
        \item Bayesian networks provide a powerful method to model complex relationships between variables and make probabilistic inferences.
        \item Conditional dependencies are essential for understanding how variables influence each other.
    \end{itemize}
    \begin{block}{Next Steps}
        In the following slides, we will explore the structure of Bayesian networks, diving into nodes and directed edges and how they interplay in practical scenarios.
    \end{block}
    \begin{block}{Conclusion}
        Bayesian networks serve as a foundational concept in probabilistic reasoning, illustrating how uncertainty and dependencies between variables can be represented effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of Bayesian Networks - Overview}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Nodes}: Represent random variables in the network (discrete or continuous).
            \item \textbf{Directed Edges}: Arrows connecting nodes, illustrating conditional dependencies.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of Bayesian Networks - Explanation}
    \begin{itemize}
        \item \textbf{Graphical Representation}: 
            A Bayesian network is a directed acyclic graph (DAG) containing nodes and directed edges, with no cycles. Each edge represents a probabilistic relationship.
        \item \textbf{Conditional Dependencies}: 
            Directed edges indicate that the probability distribution of a node is influenced by its parent nodes. For example:
            \begin{equation}
                \text{Rain} \rightarrow \text{Umbrella}
            \end{equation}
            This shows that carrying an umbrella depends on whether it is raining.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of Bayesian Networks - Example}
    \begin{block}{Example of a Bayesian Network}
        \begin{itemize}
            \item \textbf{Nodes}:
                \begin{itemize}
                    \item Cloudy (True/False)
                    \item Rain (True/False)
                    \item Sprinkler (True/False)
                    \item Wet Grass (True/False)
                \end{itemize}
            \item \textbf{Edges}:
                \begin{itemize}
                    \item Cloudy $\rightarrow$ Rain
                    \item Cloudy $\rightarrow$ Sprinkler
                    \item Rain $\rightarrow$ Wet Grass
                    \item Sprinkler $\rightarrow$ Wet Grass
                \end{itemize}
        \end{itemize}
        In this network, the wetness of the grass depends on both rain and the sprinkler's status.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probabilities in Bayesian Networks - Introduction}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand how to assign probabilities within Bayesian networks.
            \item Distinguish between prior and posterior probabilities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probabilities in Bayesian Networks - Key Concepts}
    
    \begin{enumerate}
        \item \textbf{Bayesian Networks Overview}:
            A Bayesian network is a graphical model representing a set of variables and their probabilistic relationships. Each node signifies a variable, and the directed edges depict dependencies.
    
        \item \textbf{Probability Assignments}:
            Each node in a Bayesian network is associated with a probability distribution that quantifies the uncertainty of the variable represented by the node.
    
        \item \textbf{Prior Probabilities}:
            Defined as the initial probability of a node before observing any evidence, representing belief based on prior knowledge.
            \begin{itemize}
                \item Example: \( P(\text{Rain}) = 0.3 \)
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probabilities in Bayesian Networks - Continuing Concepts}

    \begin{enumerate}[resume]
        \item \textbf{Conditional Probabilities}:
            Express the likelihood of a node given its parent node(s). If node A influences node B, we denote this as \( P(B | A) \).

        \item \textbf{Posterior Probabilities}:
            After observing evidence, we update our probabilities. The posterior probability is the probability of a hypothesis after obtaining evidence.
            \begin{itemize}
                \item Example: \( P(\text{Rain} | \text{Cloudy}) \)
            \end{itemize}
            
        \item \textbf{Bayes' Theorem}:
            To calculate posterior probabilities, we use:
            \begin{equation}
            P(H | E) = \frac{P(E | H) \cdot P(H)}{P(E)}
            \end{equation}
            Where:
            \begin{itemize}
                \item \( P(H | E) \) = Posterior probability
                \item \( P(E | H) \) = Likelihood
                \item \( P(H) \) = Prior probability
                \item \( P(E) \) = Total probability of evidence
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probabilities in Bayesian Networks - Examples and Key Takeaways}
    
    \begin{block}{Example in Context}
        Consider a Bayesian network with nodes "Cloudy" and "Rain":
        \begin{itemize}
            \item \( P(\text{Cloudy}) = 0.4 \) (prior)
            \item \( P(\text{Rain} | \text{Cloudy}) = 0.8 \) (conditional)
        \end{itemize}
        
        After observing "Cloudy", we calculate:
        \begin{equation}
        P(\text{Rain} | \text{Cloudy}) = \frac{P(\text{Cloudy} | \text{Rain}) \cdot P(\text{Rain})}{P(\text{Cloudy})}
        \end{equation}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Probabilities model uncertainty and update beliefs with new evidence.
            \item Understanding prior and posterior probabilities is crucial for reasoning in uncertain environments.
            \item Bayes' Theorem is foundational for updating probabilities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Bayesian networks systematically handle uncertainty. By defining prior probabilities, utilizing conditional relationships, and applying Bayes' Theorem, we refine our understanding of complex systems based on emerging evidence.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Constructing a Bayesian Network}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the essential components of a Bayesian network.
            \item Learn step-by-step methods to construct a basic Bayesian network.
            \item Apply the constructed network to define relationships and probabilities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Bayesian Network?}
    \begin{block}{Definition}
        A Bayesian network is a graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). It consists of:
    \end{block}
    \begin{itemize}
        \item **Nodes**: Representing random variables.
        \item **Edges**: Indicating dependencies between these variables.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Guide to Constructing a Bayesian Network}
    \begin{enumerate}
        \item \textbf{Define Your Variables}:
            \begin{itemize}
                \item Identify a set of variables relevant to your problem.
                \item \textbf{Example}: For a weather prediction model, variables could include *Rain*, *Traffic*, and *Accident*.
            \end{itemize}
        
        \item \textbf{Determine the Relationships}:
            \begin{itemize}
                \item Establish how these variables are related.
                \item \textbf{Example}: *Rain* $\rightarrow$ *Traffic* and *Traffic* $\rightarrow$ *Accident*.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Constructing a Bayesian Network Continued}
    \begin{enumerate}[resume]
        \item \textbf{Create the Directed Graph}:
            \begin{itemize}
                \item Draw nodes and connect them with directed arrows.
                \item \textbf{Visualization}:
                \begin{center}
                \texttt{
                Rain\\
                $\downarrow$\\
                Traffic\\
                $\downarrow$\\
                Accident
                }
                \end{center}
            \end{itemize}
        
        \item \textbf{Assign Conditional Probabilities}:
            \begin{itemize}
                \item Specify the probability distribution for each variable.
                \item \textbf{Example}:
                \begin{itemize}
                    \item $P(\text{Rain}) = 0.2$
                    \item $P(\text{Traffic} | \text{Rain}) = 0.8$
                    \item $P(\text{Accident} | \text{Traffic}) = 0.1$
                \end{itemize}
            \end{itemize}
        
        \item \textbf{Formularize the Joint Probability Distribution}:
            \begin{equation}
            P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^n P(X_i | \text{Parents}(X_i))
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Validating the Bayesian Network}
    \begin{block}{Validation Steps}
        \begin{itemize}
            \item Test the Bayesian network with real or simulated data to ensure accurate predictions.
            \item Adjust the model if necessary based on the validation results.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Each node's probability is directly related to its parent nodes.
            \item The Bayesian formula updates probabilities with new evidence.
            \item A well-constructed model clarifies complex relationships in uncertain domains.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Python Code Snippet}
    \begin{lstlisting}[language=Python]
from pgmpy.models import BayesianModel
from pgmpy.inference import VariableElimination

# Define the model structure
model = BayesianModel([('Rain', 'Traffic'), ('Traffic', 'Accident')])

# Define CPDs
from pgmpy.factors.discrete import TabularCPD

cpd_rain = TabularCPD(variable='Rain', variable_card=2, values=[[0.8], [0.2]])
cpd_traffic = TabularCPD(variable='Traffic', variable_card=2, values=[[0.2, 0.8], [0.8, 0.2]], 
                         evidence=['Rain'], evidence_card=[2])
cpd_accident = TabularCPD(variable='Accident', variable_card=2, values=[[0.9, 0.1], [0.1, 0.9]], 
                          evidence=['Traffic'], evidence_card=[2])

# Add CPDs and validate
model.add_cpds(cpd_rain, cpd_traffic, cpd_accident)
assert model.check_model()

# Inference
inference = VariableElimination(model)
print(inference.query(variables=['Accident'], evidence={'Rain': 1, 'Traffic': 1}))
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary of Key Steps}
        Constructing a Bayesian network involves systematic steps such as defining variables, determining relationships, creating graphs, assigning probabilities, formulating distributions, and validating the model. Mastery of this process is essential for effective inference and application of Bayesian reasoning in various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inference in Bayesian Networks - Overview}
    % Content for the overview of inference in Bayesian networks
    \begin{itemize}
        \item Inference in Bayesian networks involves updating probabilities based on new evidence.
        \item Allows drawing conclusions or making decisions in uncertain situations.
        \item Relies on the probabilistic relationships encoded in the network.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Inference}
    % Key concepts related to inference in Bayesian networks
    \begin{block}{Bayesian Network Structure}
        A directed acyclic graph (DAG) where:
        \begin{itemize}
            \item Nodes represent random variables.
            \item Edges denote conditional dependencies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Prior and Posterior Probability}
        \begin{itemize}
            \item \textbf{Prior Probability:} Initial assumption before observing any evidence.
            \item \textbf{Posterior Probability:} Updated probability after observing new evidence.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Inference Works}
    % Steps informing how inference works in Bayesian networks
    \begin{enumerate}
        \item \textbf{Model Construction:} Build a Bayesian network reflecting relationships among variables.
        \item \textbf{Evidence Insertion:} Introduce new evidence to update beliefs about variables.
        \item \textbf{Updating Beliefs:} Use Bayes' theorem to compute posterior probabilities.
    \end{enumerate}

    \begin{equation}
    P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
    \end{equation}
    \begin{itemize}
        \item \( P(H|E) \) = posterior probability of hypothesis $H$ given evidence $E$.
        \item \( P(E|H) \) = likelihood of observing evidence $E$ given hypothesis $H$.
        \item \( P(H) \) = prior probability of hypothesis $H$.
        \item \( P(E) \) = marginal likelihood of evidence $E$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms for Inference - Overview}
    \begin{block}{Overview}
        In Bayesian networks, inference is the process of updating the probabilities of certain variables based on new evidence. This slide covers two common algorithms used for this purpose: \textbf{Variable Elimination} and \textbf{Belief Propagation}. Both methods help derive posterior probabilities, but they do so in different ways.
    \end{block}

    \begin{itemize}
        \item Understand how Variable Elimination and Belief Propagation operate within Bayesian networks.
        \item Explore situations where each algorithm is advantageous.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms for Inference - Variable Elimination}
    \begin{block}{1. Variable Elimination}
        \textbf{Concept:}  
        Variable Elimination is a method used to compute the marginal probability of a variable by systematically eliminating other variables through summation.

        \textbf{Process:}
        \begin{enumerate}
            \item Identify the Query: Choose the variable for which you want to find the marginal probability.
            \item Enumerate Factors: Create a set of factors (probability distributions) associated with each variable.
            \item Eliminate Variables: Sum out all irrelevant variables.
            \item Normalization: Normalize the resulting factor.
        \end{enumerate}
        
        \textbf{Example:}  
        To find \( P(B | E) \):
        \[
        P(B | E) = \sum_{C} P(B, C | A, E)
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms for Inference - Belief Propagation}
    \begin{block}{2. Belief Propagation}
        \textbf{Concept:}  
        Belief Propagation updates beliefs iteratively until convergence.

        \textbf{Process:}
        \begin{enumerate}
            \item Initialization: Start with initial beliefs based on prior probabilities.
            \item Message Passing: Each node sends messages to neighbors:
            \[
            m_{Y \leftarrow X} = \sum_{Z} P(X | Z) \cdot m_{Z \leftarrow X}
            \]
            \item Update Beliefs: Each node updates its belief using incoming messages.
            \item Iteration: Repeat until beliefs stabilize.
        \end{enumerate}

        \textbf{Key Points:}
        \begin{itemize}
            \item Variable Elimination is better for smaller networks.
            \item Belief Propagation is efficient for larger, interconnected networks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Introduction}
    Bayes' theorem provides a mathematical framework for updating probabilities as new evidence becomes available. 
    While Bayesian networks effectively model complex uncertainties, there are several challenges and limitations associated with using them.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Key Challenges}
    \begin{enumerate}
        \item \textbf{Computational Complexity} 
        \begin{itemize}
            \item Analyzing Bayesian networks can become computationally expensive, especially with large networks.
            \item \textit{Example}: Inference tasks like calculating marginal probabilities can have exponential time complexity.
        \end{itemize}
        
        \item \textbf{Data Requirements} 
        \begin{itemize}
            \item Sufficient data is required to accurately estimate prior and conditional probabilities.
            \item \textit{Example}: Lack of data for a particular disease can lead to unreliable probability estimates in medical diagnosis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Additional Challenges}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Model Specification} 
        \begin{itemize}
            \item Constructing the network structure can be sensitive and subjective.
            \item \textit{Example}: Mis-specifying dependencies can lead to inaccurate conclusions.
        \end{itemize}
        
        \item \textbf{Overfitting} 
        \begin{itemize}
            \item Bayesian networks can become overfitted if too complex relative to the data.
            \item \textit{Example}: A network with too many parameters may perform poorly on unseen data.
        \end{itemize}
        
        \item \textbf{Scalability} 
        \begin{itemize}
            \item As the number of variables increases, managing dependencies becomes more complex.
            \item \textit{Example}: Modeling interactions in large-scale applications like social networks can be impractical.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Interpretability and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{5}
        \item \textbf{Interpretability} 
        \begin{itemize}
            \item The complexity of networks can make them difficult to interpret for non-experts.
            \item \textit{Example}: Stakeholders may struggle to understand implications of complex networks.
        \end{itemize}
    \end{enumerate}
    
    \vspace{1em}
    \textbf{Conclusion:} While Bayesian networks are powerful tools for reasoning under uncertainty, awareness of their limitations is crucial for effective application. Solutions may include simplifying models, employing approximate inference methods, and ensuring data quality.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Understand the computational demands when designing Bayesian networks.
        \item Ensure adequate data to establish prior and conditional probabilities.
        \item Carefully consider model structure to avoid mis-specification and overfitting.
        \item Strive for a balance between complexity and interpretability for end-users.
    \end{itemize}
    
    \vspace{1em}
    By acknowledging these challenges, one can better navigate the intricacies of probabilistic reasoning and enhance the application of Bayesian methods in various fields.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Bayesian and Non-Bayesian Approaches}
    
    \begin{block}{Key Learning Objectives}
        \begin{itemize}
            \item Distinguish between Bayesian and frequentist statistical methods.
            \item Highlight strengths and weaknesses of each approach.
            \item Illustrate practical applications of both methodologies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Defining the Approaches}

    \begin{block}{Bayesian Methods}
        \textbf{Definition:} Bayesian statistics incorporates prior beliefs (prior probabilities) and updates these beliefs in light of new evidence (likelihood) to provide posterior probabilities.
        \begin{equation}
            P(H|D) = \frac{P(D|H) \cdot P(H)}{P(D)}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( P(H|D) \) = posterior probability
            \item \( P(D|H) \) = likelihood
            \item \( P(H) \) = prior probability
            \item \( P(D) \) = marginal likelihood
        \end{itemize}
    \end{block}

    \begin{block}{Frequentist Methods}
        \textbf{Definition:} Frequentist statistics focuses on the long-run frequency of events. Parameters are fixed and estimations are made without incorporating prior beliefs.
        \begin{itemize}
            \item Confidence Intervals: Provide a range of values with a specified probability.
            \item Hypothesis Testing: Involves null and alternative hypotheses, using p-values for statistical significance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences}

    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Aspect} & \textbf{Bayesian Approach} & \textbf{Frequentist Approach} \\
            \hline
            Interpretation & Probability is subjective (belief update) & Probability is objective (long-run frequency) \\
            \hline
            Parameters & Can incorporate prior distributions & Treats parameters as fixed values \\
            \hline
            Data Usage & Uses all available evidence; updates with new data & Focuses only on the data at hand \\
            \hline
            Computation & Often computationally intensive (e.g., MCMC) & Simpler calculations, analytical solutions common \\
            \hline
            Decision Making & Direct probabilistic statement about hypotheses & Relies on thresholds (e.g., p < 0.05) for decisions \\
            \hline
        \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}

    \begin{block}{Example of Bayesian Use}
        In medical diagnostics, Bayesian methods can update the probability of a disease given a positive test result, factoring in both the test's accuracy and prior prevalence of the disease.
    \end{block}

    \begin{block}{Example of Frequentist Use}
        In quality control, a manufacturer tests a sample of products, calculating the proportion of defects to determine if the production process meets specifications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Insights}

    \begin{block}{When to Use Bayesian}
        \begin{itemize}
            \item When prior information is available and relevant.
            \item Problems requiring a flexible modeling approach and continuous updating.
        \end{itemize}
    \end{block}

    \begin{block}{When to Use Frequentist}
        \begin{itemize}
            \item When dealing with large samples where the law of large numbers applies.
            \item Need for straightforward interpretations and simpler calculations.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Both Bayesian and frequentist approaches have their strengths and limitations. The choice depends on the analysis context, available data, and research questions. Understanding these differences is crucial for effective statistical reasoning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies in Bayesian Networks}
    \begin{block}{Introduction to Bayesian Networks}
        Bayesian networks are powerful probabilistic models that represent a set of variables and their conditional dependencies via a directed acyclic graph (DAG).
        They allow reasoning under uncertainty and updating beliefs based on new evidence.
        This presentation explores various case studies illustrating their effectiveness across industries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Medical Diagnosis}
    \begin{itemize}
        \item \textbf{Industry}: Healthcare
        \item \textbf{Example}: Diagnosis of Diseases
    \end{itemize}
    \begin{block}{Details}
        In the medical field, Bayesian networks help diagnose diseases based on symptoms and medical history. For instance, they can model relationships between symptoms like fever, cough, and exposure history to infer probabilities for conditions such as influenza, pneumonia, or COVID-19.
    \end{block}
    \begin{itemize}
        \item Enables clinicians to update disease probabilities as new symptoms arise.
        \item Identifies the most likely conditions through inference mechanisms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Fraud Detection}
    \begin{itemize}
        \item \textbf{Industry}: Finance
        \item \textbf{Example}: Credit Card Fraud Detection
    \end{itemize}
    \begin{block}{Details}
        Financial institutions utilize Bayesian networks to detect fraudulent transactions. By analyzing historical data, they adjust the probability of a transaction being fraudulent based on features like amount, location, and transaction history.
    \end{block}
    \begin{itemize}
        \item Adaptive learning to identify emerging fraud patterns.
        \item Real-time fraud probability calculations lead to timely actions.
    \end{itemize}
    \begin{equation}
        P(Fraud | Transaction) = \frac{P(Transaction | Fraud) \cdot P(Fraud)}{P(Transaction)}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 3: Predictive Maintenance}
    \begin{itemize}
        \item \textbf{Industry}: Manufacturing
        \item \textbf{Example}: Equipment Failure Prediction
    \end{itemize}
    \begin{block}{Details}
        In manufacturing, Bayesian networks predict equipment failures, allowing for timely maintenance. By evaluating sensor data, they infer the likelihood of equipment parts wearing out or failing.
    \end{block}
    \begin{itemize}
        \item Reduces downtime and maintenance costs through predictive insights.
        \item Models incorporate prior maintenance data for improved predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Summary}
    \begin{block}{Conclusion}
        Bayesian networks exhibit significant versatility across domains. They harness uncertainties to make informed decisions, enhancing operations, driving efficacy, and reducing risks.
    \end{block}
    \begin{itemize}
        \item \textbf{Applications}: Healthcare, Finance (Fraud Detection), Manufacturing (Predictive Maintenance)
        \item \textbf{Advantages}: Real-time updating, adaptive learning, informed decision-making.
    \end{itemize}
    \begin{block}{Call to Action}
        Encourage students to explore these case studies further and consider how Bayesian networks can be implemented in additional fields or applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    Probabilistic reasoning is an essential component of artificial intelligence (AI) that enables systems to handle uncertainty and make informed decisions based on partial data. As AI technology evolves, several trends and advancements are shaping the future of probabilistic reasoning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Part 1}
    \begin{enumerate}
        \item \textbf{Bayesian Networks Expansion}
        \begin{itemize}
            \item Bayesian networks will continue to grow in complexity, enabling the modeling of more elaborate systems.
            \item \textit{Example}: Used in healthcare to predict patient outcomes based on a network of symptoms and diseases.
        \end{itemize}
        
        \item \textbf{Integration with Deep Learning}
        \begin{itemize}
            \item The convergence of probabilistic reasoning with deep learning techniques is opening new avenues.
            \item \textit{Example}: Variational autoencoders (VAEs) leverage probabilistic models to capture data distributions, facilitating generative tasks in AI.
        \end{itemize}
        
        \item \textbf{Real-time Decision Making}
        \begin{itemize}
            \item Advances in computational power will allow faster probabilistic inference, enabling real-time decision-making in dynamic environments.
            \item \textit{Illustration}: Autonomous vehicles utilize probabilistic reasoning for obstacle detection and navigation.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering from the previous frame
        \item \textbf{Reinforcement Learning Enhancements}
        \begin{itemize}
            \item Incorporating probabilistic models in reinforcement learning frameworks can enhance exploration strategies and decision-making under uncertainty.
            \item \textit{Example}: Probabilistic graphical models yield smarter policies for complex tasks, like robotic manipulation.
        \end{itemize}

        \item \textbf{Explainability and Trustworthiness}
        \begin{itemize}
            \item As AI systems become more integrated into everyday life, ensuring their decisions are explainable through probabilistic reasoning is critical.
            \item \textit{Illustration}: Probabilistic reasoning can provide a rationale, such as quantifying the confidence in predictions, which is vital in fields like finance and healthcare.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends}
    \begin{itemize}
        \item \textbf{Integration with Quantum Computing}: Potential to revolutionize how we handle large-scale probabilistic computations.
        \item \textbf{AI-enhanced Data Analysis}: Automated identification of patterns and correlations in vast datasets using probabilistic methods.
        \item \textbf{Collaborative AI Systems}: Systems that utilize shared probabilistic models to improve collective learning and decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The future of probabilistic reasoning in AI is bright, with advancements promising to enhance decision-making, improve model accuracy, and foster trust in AI systems. Continuous exploration of these trends will be essential as we navigate the complexities of an increasingly AI-driven world.
\end{frame}

\begin{frame}[fragile]
    \frametitle{References for Further Reading}
    \begin{itemize}
        \item Murphy, K. P. (2012). \textit{Machine Learning: A Probabilistic Perspective}. MIT Press.
        \item Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Introduction to Probabilistic Reasoning}
    \begin{block}{Overview}
        Probabilistic reasoning is essential in artificial intelligence (AI). It allows machines to make informed decisions when faced with uncertain information.
    \end{block}
    \begin{itemize}
        \item Enables improved predictions and classifications.
        \item Supports decision-making under uncertainty.
        \item Utilizes mathematical frameworks for reasoning about outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Key Concepts Recap}
    \begin{enumerate}
        \item \textbf{Probability Basics:}
            \begin{itemize}
                \item Quantifies uncertainty (0 to 1).
                \item Example: Probability of rolling a three on a six-sided die is \(\frac{1}{6}\).
            \end{itemize}
        \item \textbf{Bayes' Theorem:}
            \begin{itemize}
                \item Relates conditional and marginal probabilities:
                \[
                P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
                \]
                \item Application: Spam filtering - determining if a message is spam given certain words.
            \end{itemize}
        \item \textbf{Random Variables:}
            \begin{itemize}
                \item Variables taking multiple values with associated probabilities.
                \item Types: Discrete (finite outcomes) and Continuous (infinite outcomes).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Continuing Key Concepts}
    \begin{enumerate}[resume]
        \item \textbf{Probability Distributions:}
            \begin{itemize}
                \item Describe how probabilities are distributed.
                \item Common Examples:
                    \begin{itemize}
                        \item Normal Distribution: Bell-shaped curve in statistics.
                        \item Bernoulli Distribution: Represents binary outcomes (success/failure).
                    \end{itemize}
            \end{itemize}
        \item \textbf{Inference in AI:}
            \begin{itemize}
                \item Draw conclusions from data using probabilistic models.
                \item Applications: Machine Learning models like Naive Bayes classifiers.
            \end{itemize}
        \item \textbf{Decision Making under Uncertainty:}
            \begin{itemize}
                \item Evaluate possible actions and expected outcomes via probabilistic models.
                \item Example: Real-time decision-making in autonomous vehicles.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Relevance and Final Points}
    \begin{block}{Relevance to AI}
        \begin{itemize}
            \item Natural Language Processing: Understanding linguistic complexities.
            \item Computer Vision: Categorizing and predicting objects with uncertainty.
            \item Robotics: Enhanced navigation by evaluating environmental uncertainties.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Mastery of uncertainty concepts is crucial for robust AI systems.
            \item Probabilistic models facilitate adaptive learning from complex datasets.
            \item Essential for various AI applications and advancements.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}