\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Deep Learning Fundamentals]{Week 15: Deep Learning Fundamentals}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Deep Learning}
    
    \begin{block}{Overview}
        Deep learning is a subset of machine learning using algorithms inspired by the brain's structure, called artificial neural networks. It automatically learns data representations through complex patterns in large datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Deep Learning}
    
    \begin{enumerate}
        \item \textbf{High Performance:}
        \begin{itemize}
            \item Achieves state-of-the-art results in image recognition, natural language processing, and speech recognition.
            \item Example: Convolutional Neural Networks (CNNs) for classifying millions of images from the ImageNet dataset.
        \end{itemize}
        
        \item \textbf{Handling Big Data:}
        \begin{itemize}
            \item Efficiently analyzes and learns from vast datasets generated by internet and sensor technologies.
        \end{itemize}
        
        \item \textbf{Automating Feature Engineering:}
        \begin{itemize}
            \item Automatically discovers relevant features for classification, reducing the need for manual extraction.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relationship with Artificial Intelligence (AI)}
    
    \begin{block}{AI Overview}
        Artificial intelligence includes systems performing tasks like reasoning, learning, and understanding natural language.
    \end{block}
    
    \begin{block}{Deep Learning as a Key Component}
        Deep learning, a vital technique in AI, enables hierarchical learning from raw data, improving accuracy across various applications. It contrasts with rule-based systems and optimization algorithms emphasizing end-to-end learning.
    \end{block}
    
    \begin{block}{Key Points}
        - Models consist of multiple layers for multi-level abstraction.
        - Popular frameworks include TensorFlow, Keras, and PyTorch to simplify model building and training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Neural Networks? - Overview}
    \begin{itemize}
        \item Neural Networks are computational models inspired by the human brain.
        \item They are designed to recognize patterns and solve complex problems.
        \item Composed of interconnected nodes called neurons, organized into layers.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Neural Networks? - Key Concepts}
    \begin{enumerate}
        \item \textbf{Neurons}:
            \begin{itemize}
                \item The basic unit of a neural network, similar to a biological neuron.
                \item Each neuron receives input, processes it through an activation function, and produces an output.
                \item \textbf{Activation Functions}:
                    \begin{itemize}
                        \item Examples: Sigmoid, ReLU (Rectified Linear Unit), Tanh.
                        \item Determines whether a neuron will "fire."
                    \end{itemize}
                \item \textbf{Output Example}:
                    \begin{equation}
                        \text{output} = \text{activation\_function}(w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n + b)
                    \end{equation}
            \end{itemize}
        \item \textbf{Layers}:
            \begin{itemize}
                \item \textbf{Input Layer}: Receives raw data.
                \item \textbf{Hidden Layers}: Intermediate layers for processing (multiple in deep networks).
                \item \textbf{Output Layer}: Produces the final output. 
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Neural Networks? - Types and Applications}
    \begin{enumerate}
        \setcounter{enumi}{2}  % Continue enumeration
        \item \textbf{Types of Neural Networks}:
            \begin{itemize}
                \item Feedforward Neural Networks: One-way information flow.
                \item Convolutional Neural Networks (CNNs): Used for image data to detect spatial hierarchies.
                \item Recurrent Neural Networks (RNNs): Designed for temporal data, maintaining memory of previous inputs.
            \end{itemize}
        \item \textbf{Practical Example}:
            \begin{itemize}
                \item **Image Recognition**: Training on thousands of images of cats and dogs.
                \item Each image passes through layers to extract features (edges, shapes) before categorization.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Neural Networks? - Key Points and Conclusion}
    \begin{itemize}
        \item Neural networks mimic human brain information processing.
        \item Architecture and activation functions influence performance.
        \item Essential for designing effective AI models.
    \end{itemize}
    \begin{block}{Conclusion}
        Neural networks are crucial in modern deep learning applications, mastering complex data patterns used in fields like computer vision and natural language processing.
    \end{block}
    \textit{Prepare for the next slide on forward propagation and backpropagation processes.}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Neural Networks Work}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the concepts of forward propagation and backpropagation in neural networks.
            \item Recognize how these processes contribute to the learning of a neural network.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Network Functioning}
    Neural networks mimic how human brains process information, composed of layers of interconnected nodes (neurons) that analyze data and make predictions or classifications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation}
    \begin{block}{Definition}
        The process where input data is passed through the layers of the network to generate an output.
    \end{block}
    
    \begin{block}{How It Works}
        \begin{enumerate}
            \item \textbf{Input Layer}: Accepts input features (e.g., pixel values).
            \item \textbf{Weights and Biases}: Each connection has a weight and each neuron has a bias.
            \item \textbf{Activation Function}: The result is passed through an activation function (e.g., Sigmoid, ReLU).
            \item \textbf{Propagation Through Layers}: Outputs become inputs for the next layer until reaching the output layer.
        \end{enumerate}
    \end{block}

    \begin{block}{Formula}
        \begin{equation}
            z = \sum (w_i \cdot x_i) + b
        \end{equation}
        \begin{equation}
            a = \text{Activation}(z)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( w_i \): weights
            \item \( x_i \): input features
            \item \( b \): bias
            \item \( a \): output after activation
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Forward Propagation}
    For an input feature vector:
    \[
    x = [0.5, 1.5], \quad w = [0.4, 0.6], \quad b = 0.2
    \]

    The calculation becomes:
    \[
    z = (0.4 \times 0.5) + (0.6 \times 1.5) + 0.2 = 1.25
    \]
    
    Using a ReLU activation function:
    \[
    a = \max(0, 1.25) = 1.25
    \]
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation}
    \begin{block}{Definition}
        The process of updating weights and biases based on the error of predictions made.
    \end{block}
    
    \begin{block}{How It Works}
        \begin{enumerate}
            \item \textbf{Calculate Loss}: Determine the difference between predicted output and actual target.
            \item \textbf{Gradient of Loss}: Compute gradients of the loss indicating direction for minimizing loss.
            \item \textbf{Update Weights and Biases}: Use gradient descent to adjust weights and biases.
        \end{enumerate}
    \end{block}

    \begin{block}{Formula}
        \begin{equation}
            w_{new} = w_{old} - \eta \cdot \frac{\partial L}{\partial w}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( \eta \): learning rate
            \item \( L \): loss function
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Backpropagation}
    If the gradient of loss with respect to a weight is \( 0.1 \) and the learning rate is \( 0.01 \):
    \[
    w_{new} = w_{old} - 0.01 \cdot 0.1 = w_{old} - 0.001
    \]
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Forward Propagation} enables predictions based on input data.
        \item \textbf{Backpropagation} allows the model to learn from mistakes, refining weights for improved accuracy.
        \item Both processes are cyclic, continuing over multiple iterations until satisfactory performance is achieved.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Forward propagation and backpropagation are foundational processes in neural networks, enabling them to learn complex patterns in data effectively. Understanding these mechanisms is crucial for those interested in deep learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives}
    \begin{itemize}
        \item Understand the purpose and importance of activation functions in neural networks.
        \item Learn about common activation functions: Sigmoid, ReLU, and Softmax.
        \item Analyze the advantages and disadvantages of each activation function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Activation Functions?}
    Activation functions are mathematical equations that determine the output of a neural network's node (or neuron). They play a crucial role in introducing non-linearities into the model, enabling the network to learn complex patterns in the data.

    \begin{block}{Key Roles of Activation Functions}
        \begin{itemize}
            \item \textbf{Non-linearity:} Allow the model to capture complex relationships.
            \item \textbf{Output control:} Scale the output to a specific range.
            \item \textbf{Gradient propagation:} Ensure effective learning during backpropagation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Activation Functions}
    \begin{enumerate}
        \item \textbf{Sigmoid Function}
        \begin{itemize}
            \item \textbf{Formula:} 
            \[
            \sigma(x) = \frac{1}{1 + e^{-x}}
            \]
            \item \textbf{Characteristics:} Range: (0, 1); Shapes output like an S-curve.
            \item \textbf{Use case:} Binary classification problems (e.g., logistic regression).
            \item \textbf{Pros:} Smooth and continuous gradient.
            \item \textbf{Cons:} Can cause vanishing gradient problems for extreme input values.
        \end{itemize}

        \item \textbf{ReLU (Rectified Linear Unit)}
        \begin{itemize}
            \item \textbf{Formula:} 
            \[
            \text{ReLU}(x) = \max(0, x)
            \]
            \item \textbf{Characteristics:} Output is 0 for negative values and equal to the input for positive values.
            \item \textbf{Use case:} Widely used in hidden layers of deep networks.
            \item \textbf{Pros:} Computationally efficient, reduces the likelihood of vanishing gradients.
            \item \textbf{Cons:} "Dying ReLU" issue, where neurons stop learning.
        \end{itemize}

        \item \textbf{Softmax Function}
        \begin{itemize}
            \item \textbf{Formula:} 
            \[
            \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
            \]
            \item \textbf{Characteristics:} Converts raw logits into probabilities that sum to 1.
            \item \textbf{Use case:} Output layer for multi-class classification.
            \item \textbf{Pros:} Clear interpretation of outputs as probabilities.
            \item \textbf{Cons:} Sensitive to outlier values.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visual Summary and Conclusion}
    \begin{itemize}
        \item \textbf{Sigmoid:} Outputs limited between 0 and 1.
        \item \textbf{ReLU:} Linear for positive inputs, zero for negative inputs.
        \item \textbf{Softmax:} Normalizes outputs into a probability distribution.
    \end{itemize}

    \begin{block}{Conclusion}
        Activation functions are essential components of neural networks that alter the outputs of nodes to solve complex problems. Choosing the right activation function can profoundly impact model performance and learning efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example}
    \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

# Example of using different activation functions in a neural network
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.sigmoid = nn.Sigmoid()
        self.fc2 = nn.Linear(5, 2)
        self.relu = nn.ReLU()  # Using ReLU
        self.fc3 = nn.Linear(2, 3)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.sigmoid(x)  # Applying Sigmoid
        x = self.relu(x)     # Applying ReLU
        x = self.fc2(x)
        return nn.Softmax(dim=1)(self.fc3(x))  # Applying Softmax

model = SimpleNN()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks}
    \begin{block}{Overview}
        Neural networks are powerful computational models inspired by the human brain, used to identify patterns in data. 
        We focus on three primary types:
        \begin{itemize}
            \item Feedforward Neural Networks (FNN)
            \item Convolutional Neural Networks (CNN)
            \item Recurrent Neural Networks (RNN)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedforward Neural Networks (FNN)}
    \begin{block}{Definition}
        The simplest type of neural network where information moves in one direction—from input to output.
    \end{block}
    \begin{itemize}
        \item \textbf{Structure}:
            \begin{itemize}
                \item Input Layer: Takes external input features.
                \item Hidden Layer(s): Processes inputs through weights and activation functions.
                \item Output Layer: Produces the final output.
            \end{itemize}
        \item \textbf{Example}: Predicting house prices.
    \end{itemize}
    \begin{block}{Key Points}
        \begin{itemize}
            \item No cycles; output of one layer is input to the next.
            \item Activation functions (e.g., ReLU, Sigmoid) introduce non-linearity.
        \end{itemize}
    \end{block}
    \begin{equation}
        y = f\left( \sum_{i=1}^{n} w_i x_i + b \right)
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Convolutional Neural Networks (CNN)}
    \begin{block}{Definition}
        A class of deep neural networks designed for analyzing visual imagery.
    \end{block}
    \begin{itemize}
        \item \textbf{Structure}:
            \begin{itemize}
                \item Convolutional Layers: Detect local patterns.
                \item Pooling Layers: Reduce dimensionality while preserving features.
                \item Fully Connected Layers: Flatten output to connect to final nodes.
            \end{itemize}
        \item \textbf{Example}: Image classification (e.g., recognizing cats vs. dogs).
    \end{itemize}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Preserves spatial relationship through local connectivity.
            \item Uses filters (kernels) that slide over input to extract features.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNN)}
    \begin{block}{Definition}
        A class of neural networks suited for sequential data with connections that loop back, allowing memory of past inputs.
    \end{block}
    \begin{itemize}
        \item \textbf{Structure}: Neurons process sequences and pass information through time.
        \item \textbf{Example}: Text generation and language modeling.
    \end{itemize}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Ideal for sequential data like time series.
            \item LSTM units mitigate issues like long-term dependencies.
        \end{itemize}
    \end{block}
    \begin{equation}
        h_t = f(W_h h_{t-1} + W_x x_t + b)
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        Understanding distinct types of neural networks is crucial for selecting the appropriate architecture for a specific problem:
        \begin{itemize}
            \item FNNs are versatile
            \item CNNs excel in image processing
            \item RNNs handle sequential data
        \end{itemize}
    \end{block}
    \begin{block}{Next Steps}
        In the following slide, we will explore popular deep learning frameworks that facilitate the building and training of these neural networks.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Learning Objectives}
    \begin{itemize}
        \item Understand the role of deep learning frameworks in AI and machine learning.
        \item Compare popular frameworks: TensorFlow and PyTorch.
        \item Identify use cases and features of each framework.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What are Deep Learning Frameworks?}
    Deep Learning Frameworks are software libraries that aid in building, training, 
    and deploying deep learning models. They provide essential tools and abstractions 
    to simplify the complexities involved in deep learning tasks.
\end{frame}

\begin{frame}
    \frametitle{Key Frameworks - TensorFlow}
    \begin{block}{Overview}
        Developed by Google, TensorFlow is an open-source framework that excels 
        in building large-scale machine learning models.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Flexibility}: Supports both high-level APIs (like Keras) 
                  and low-level APIs for detailed customization.
            \item \textbf{Deployment}: Optimized for production use, suitable 
                  for applications ranging from mobile devices to cloud servers.
            \item \textbf{Visualization}: Integrated with TensorBoard for tracking 
                  and visualizing model training.
        \end{itemize}
        \item \textbf{Use Cases}:
        \begin{itemize}
            \item Image and speech recognition
            \item Natural language processing (NLP)
            \item Time-series forecasting
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{TensorFlow Example Code}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(10, activation='softmax')
])
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Frameworks - PyTorch}
    \begin{block}{Overview}
        An open-source library developed by Facebook, PyTorch is known for its 
        dynamic computation graph and ease of use.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Dynamic Graphing}: Allows changes to the network 
                  architecture during runtime, offering greater flexibility.
            \item \textbf{Pythonic Nature}: More intuitive to use, closely 
                  resembling standard Python coding practices.
            \item \textbf{Community Support}: Rapidly growing ecosystem with 
                  extensive libraries and tutorials.
        \end{itemize}
        \item \textbf{Use Cases}:
        \begin{itemize}
            \item Research and experimentation in academia
            \item Natural Language Processing (NLP)
            \item Computer Vision applications
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PyTorch Example Code}
    \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleModel()
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{TensorFlow} is often preferred for production settings due 
              to its deployment capabilities.
        \item \textbf{PyTorch} is favored in research and prototyping due to 
              its flexibility and ease of use.
        \item Both frameworks have rich ecosystems, with tools and libraries 
              that complement deep learning tasks.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Transition to Next Slide}
    Having a solid understanding of these frameworks enables you to choose the 
    right tools for specific tasks as you delve into deep learning applications. 
    This foundational knowledge sets the stage for exploring model training techniques 
    in the next chapter.
    
    In our next discussion, we will delve into \textbf{Training Deep Learning Models}, 
    exploring the critical steps in data preprocessing, dataset splitting, and 
    hyperparameter optimization to enhance model performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Deep Learning Models}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the training process of deep learning models.
            \item Grasp the significance of data preprocessing and dataset management.
            \item Learn the importance of hyperparameter optimization for model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. The Training Process}
    Training a deep learning model involves feeding it with data to learn patterns, allowing it to make predictions. The training process typically follows these steps:
    
    \begin{enumerate}
        \item \textbf{Initialization:} Set initial weights for the model's parameters.
        \item \textbf{Feed Forward:} Pass the input data through the network.
        \item \textbf{Loss Calculation:} Compute the loss (error) using a loss function.
        \item \textbf{Backpropagation:} Update the weights using gradients of the loss.
        \item \textbf{Iteration:} Repeat for a specified number of epochs.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Data Preprocessing}
    Data must be prepared before training to enhance model efficiency and accuracy. This can include:
    
    \begin{itemize}
        \item \textbf{Normalization/Standardization:} 
        Rescale data to a standard range (e.g., [0, 1]) or to have a mean of 0 and a standard deviation of 1.
        \begin{equation}
            X_{\text{scaled}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
        \end{equation}
        
        \item \textbf{Data Augmentation:} Generate variations of the dataset to improve model robustness (e.g., rotating images).

        \item \textbf{Handling Missing Values:} Remove data, fill values with mean/median, or use models to predict missing values.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Splitting Datasets}
    To evaluate a model's performance accurately, datasets should be divided into:
    
    \begin{itemize}
        \item \textbf{Training Set:} Used to train the model.
        \item \textbf{Validation Set:} Used to tune hyperparameters and avoid overfitting.
        \item \textbf{Test Set:} Used to assess the final model performance on unseen data.
    \end{itemize}
    
    \textbf{Common Splitting Techniques:}
    
    \begin{itemize}
        \item \textbf{Random Split:} Randomly divides data into subsets.
        \item \textbf{Stratified Split:} Ensures proportional representation of classes.
    \end{itemize}
    
    \textbf{Example Split (70/15/15):}
    \begin{itemize}
        \item Training: 70\%
        \item Validation: 15\%
        \item Test: 15\%
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Hyperparameter Optimization}
    Hyperparameters can be adjusted to optimize model performance. Common hyperparameters include:
    
    \begin{itemize}
        \item \textbf{Learning Rate:} Influences how weights are updated.
        \item \textbf{Batch Size:} Number of samples processed before the model is updated.
        \item \textbf{Number of Epochs:} Times the learning algorithm iterates through the entire dataset.
    \end{itemize}
    
    \textbf{Optimization Techniques:}
    
    \begin{itemize}
        \item \textbf{Grid Search:} Exhaustively tests a set of parameters.
        \item \textbf{Random Search:} Randomly selects parameters from a specified range.
        \item \textbf{Bayesian Optimization:} A probabilistic model to find optimal hyperparameters efficiently.
    \end{itemize}
    
    \textbf{Example Code:}
    \begin{lstlisting}[language=Python]
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Effective data preprocessing is foundational for building robust models.
        \item Proper dataset splitting is critical to avoid overfitting and ensure reliable model validation.
        \item Hyperparameter tuning can significantly influence model accuracy and performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Learning - Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the diverse applications of deep learning across multiple fields.
            \item Explore specific examples, illustrating the impact of deep learning technologies.
            \item Analyze how deep learning is reshaping industries and enhancing everyday tasks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Learning - Computer Vision}
    \begin{block}{Computer Vision}
        \begin{itemize}
            \item \textbf{Definition}: A field of computer science that enables machines to interpret and make decisions based on visual data.
            \item \textbf{Applications}:
            \begin{itemize}
                \item \textbf{Image Recognition}: Models like Convolutional Neural Networks (CNNs) classify images with high accuracy. For instance, Google's image search identifies objects in photos.
                \item \textbf{Facial Recognition}: Used in security for identity verification (e.g., Apple's Face ID).
                \item \textbf{Autonomous Vehicles}: Algorithms process visual data from cameras, enabling navigation and obstacle detection.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Learning - NLP and Healthcare}
    \begin{block}{Natural Language Processing (NLP)}
        \begin{itemize}
            \item \textbf{Definition}: A technology that allows computers to understand, interpret, and respond to human languages.
            \item \textbf{Applications}:
            \begin{itemize}
                \item \textbf{Chatbots and Virtual Assistants}: Tools like Siri and Alexa engage in meaningful conversations.
                \item \textbf{Sentiment Analysis}: Analyzing customer feedback on social media using deep learning (e.g., TensorFlow models processing Twitter data).
                \item \textbf{Machine Translation}: Models such as BERT provide improved translation accuracy (e.g., Google Translate).
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Healthcare}
        \begin{itemize}
            \item \textbf{Definition}: Utilizing deep learning in medical data analysis to improve patient outcomes.
            \item \textbf{Applications}:
            \begin{itemize}
                \item \textbf{Medical Imaging}: Analyzing X-rays and MRIs for early disease detection, improving diagnostic accuracy (e.g., CNNs classifying tumor types).
                \item \textbf{Predictive Analytics}: Predicting health risks by analyzing patient data over time (e.g., likelihood of diabetes).
                \item \textbf{Genomics}: Interpreting complex genetic data for personalized medicine (e.g., RNNs for DNA sequence analysis).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Real-World Impact}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Impact}: Deep learning is crucial for innovations, enhancing efficiency and accuracy in decision-making.
            \item \textbf{Interdisciplinary Nature}: The applications span multiple disciplines, highlighting the versatility and importance of deep learning.
            \item \textbf{Future Trends}: Anticipate growth in areas like augmented reality (AR), smart robotics, and deeper AI integration into everyday life.
        \end{itemize}
    \end{block}

    \begin{block}{Examples of Real-World Impact}
        In healthcare, studies show that deep learning models can match diagnostic performance with human experts. 
        For example, a CNN detected breast cancer in mammograms with 94\% accuracy, catalyzing AI-assisted diagnostics.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Challenges in Deep Learning - Introduction}
    \begin{itemize}
        \item Deep learning has advanced significantly, but it faces several critical challenges.
        \item Understanding these challenges is essential for developing robust models and ensuring successful deployment.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Challenges in Deep Learning - Key Aspects}
    \begin{enumerate}
        \item \textbf{Overfitting}
            \begin{itemize}
                \item Occurs when a model learns the training data too well, capturing noise over patterns.
                \item Example: A neural network for digit classification may perform well on training data but poorly on unknown data.
                \item \textbf{Solution:} Use regularization, dropout, and cross-validation.
            \end{itemize}
        \item \textbf{Computational Cost}
            \begin{itemize}
                \item requires significant resources, including powerful GPUs.
                \item Example: Training models like ResNet may take days or weeks.
                \item \textbf{Solution:} Model pruning, quantization, and transfer learning can help reduce cost.
            \end{itemize}
        \item \textbf{Data Requirements}
            \begin{itemize}
                \item Large amounts of labeled data are necessary for effective learning.
                \item Example: Insufficient labeled data in medical imaging can lead to poor performance.
                \item \textbf{Solution:} Techniques like data augmentation and active learning are beneficial.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Deep Learning - Key Points and Code Snippet}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Challenges significantly impact model performance and deployment.
            \item Addressing overfitting is critical for generalization.
            \item High computational costs must be considered in model design.
            \item Data curation and preprocessing are crucial for effective training.
        \end{itemize}
    \end{block}
    
    \begin{block}{Overfitting Indicator: Validation vs. Training Loss}
        \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

def plot_loss(training_loss, validation_loss):
    epochs = range(1, len(training_loss) + 1)
    plt.plot(epochs, training_loss, label='Training Loss')
    plt.plot(epochs, validation_loss, label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    Deep learning raises critical ethical questions that need to be addressed for responsible usage. 
    This discussion focuses on two central ethical issues:
    \begin{itemize}
        \item \textbf{Bias}
        \item \textbf{Transparency}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Bias}
    \textbf{Bias in Deep Learning}
    \begin{itemize}
        \item \textbf{Definition}: Systematic favoritism or discrimination in data, algorithms, or outcomes.
        \item \textbf{Causes}:
        \begin{itemize}
            \item \textbf{Data-driven Bias}:
            \begin{itemize}
                \item If a model is trained on biased datasets, it reinforces existing biases.
                \item \textit{Example}: A facial recognition system poorly performs on darker skin tones if trained mainly on lighter-skinned images.
            \end{itemize}
            \item \textbf{Algorithmic Bias}:
            \begin{itemize}
                \item Choices in modeling can introduce bias.
                \item \textit{Example}: A hiring algorithm favoring certain traits can disadvantage other demographics.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Transparency}
    \textbf{Transparency in Deep Learning}
    \begin{itemize}
        \item \textbf{Definition}: Clarity on how models make decisions or predictions.
        \item \textbf{Importance}:
        \begin{itemize}
            \item \textbf{Accountability}: Stakeholders must understand model behavior.
            \item \textbf{Explainability}: Crucial in high-stakes fields, e.g., healthcare.
            \item \textit{Example}: Predicting recidivism rates requires understandable reasons for predictions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Real-World Examples}
    \textbf{1. Healthcare}
    \begin{itemize}
        \item AI tools in diagnostics must represent demographic diversity to avoid unequal outcomes.
        \item \textit{Illustration}: A model trained on a single demographic may overlook critical signals in others.
    \end{itemize}
    
    \textbf{2. Justice System}
    \begin{itemize}
        \item Risk assessment algorithms need to avoid unfair targeting of specific groups.
        \item \textit{Illustration}: Biased historical data can influence disproportionate sentencing recommendations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Frameworks and Guidelines}
    \begin{itemize}
        \item \textbf{Fairness}: Outputs should be equitable across demographics.
        \item \textbf{Accountability}: Developers liable for outcomes of their systems.
        \item \textbf{Transparency Tools}:
        \begin{itemize}
            \item \textbf{LIME} (Local Interpretable Model-agnostic Explanations): Interprets black-box models.
            \item \textbf{SHAP} (SHapley Additive exPlanations): A game-theoretic approach for explaining outputs.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Conclusion}
    \begin{itemize}
        \item Addressing bias and ensuring transparency are prerequisites for ethical deep learning.
        \item Robust ethical guidelines enhance trust in AI systems.
        \item Active engagement with ethical considerations is necessary to mitigate potential harms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Deep Learning}
    \begin{block}{Introduction}
        Deep learning, a subset of machine learning, continues to evolve rapidly. Understanding emerging trends such as unsupervised learning, transfer learning, and explainability is crucial for leveraging deep learning in real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Unsupervised Learning}
    \begin{block}{Explanation}
        Unsupervised learning involves training models on data without labeled outputs. The model learns to identify patterns and structures within the data, making it essential for tasks where labeled data is scarce.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Applications:} Clustering (e.g., customer segmentation), anomaly detection, and representation learning.
        \item \textbf{Techniques:} Autoencoders, GANs (Generative Adversarial Networks).
    \end{itemize}
    
    \begin{block}{Example}
        \textbf{Clustering:} Using k-means clustering to group similar items (e.g., grouping customer purchase behaviors).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Transfer Learning}
    \begin{block}{Explanation}
        Transfer learning allows models to leverage knowledge gained in one task and apply it to another, often related, task. This is especially useful in scenarios where limited data is available for a new task.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Efficiency:} Reduces training time and resource requirements.
        \item \textbf{Applications:} Common in image and text classification tasks where pre-trained models (e.g., ResNet, BERT) are fine-tuned on specific datasets.
    \end{itemize}
    
    \begin{block}{Example}
        Using a pre-trained image classification model to identify specific objects in images from a different domain with minimal additional training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Explainability in Deep Learning}
    \begin{block}{Explanation}
        As deep learning models become more complex, understanding their decisions is critical. Explainability aims to make the behavior of models more transparent and trustable, especially in sensitive applications (e.g., healthcare, finance).
    \end{block}
    
    \begin{itemize}
        \item \textbf{Importance:} Mitigates bias and supports ethical AI practices, aligning with previous discussions on ethical implications in deep learning.
        \item \textbf{Techniques:} Design methods such as LIME (Local Interpretable Model-agnostic Explanations) for model interpretation.
    \end{itemize}
    
    \begin{block}{Example}
        Using LIME to explain a model's prediction on whether an email is spam or not, highlighting the most influential words.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Staying updated on these trends—unsupervised learning, transfer learning, and explainability—enables practitioners to harness deep learning more effectively. As technology progresses, understanding these concepts will become increasingly important.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Table of Key Trends}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Trend} & \textbf{Description} & \textbf{Example} \\ \hline
            Unsupervised Learning & Models learn from unlabeled data & Customer segmentation \\ \hline
            Transfer Learning & Applying knowledge from one task to another & Fine-tuning a pre-trained model for specific tasks \\ \hline
            Explainability & Making model decisions more transparent & Using LIME for interpreting predictions \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}
  \frametitle{Learning Objectives}
  \begin{itemize}
    \item Understand the basic components of a neural network.
    \item Learn how to implement a simple neural network using Python and a popular framework (e.g., TensorFlow/Keras).
    \item Gain insights into model training, evaluation, and practical applications.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{What is a Neural Network?}
  \begin{block}{Definition}
    A neural network is a collection of algorithms that recognizes underlying relationships in a set of data, mimicking the way the human brain operates.
  \end{block}
  \begin{itemize}
    \item \textbf{Neurons:} Basic units that receive input, process it, and transfer output.
    \item \textbf{Layers:}
    \begin{itemize}
      \item \textbf{Input Layer:} Where data enters the network.
      \item \textbf{Hidden Layers:} Intermediate layers where computation is performed.
      \item \textbf{Output Layer:} Produces the final result.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Building a Simple Neural Network}
  \textbf{Framework:} TensorFlow/Keras

  \textbf{Step-by-Step Walkthrough:}

  \begin{enumerate}
    \item \textbf{Import Libraries}
    \begin{lstlisting}[language=Python]
import numpy as np
import tensorflow as tf
from tensorflow import keras
    \end{lstlisting}

    \item \textbf{Define the Model}
    \begin{lstlisting}[language=Python]
model = keras.Sequential([
    keras.layers.Dense(10, activation='relu', input_shape=(input_size,)),
    keras.layers.Dense(1, activation='sigmoid')  # binary classification
])
    \end{lstlisting}
    \begin{itemize}
      \item \textbf{Dense Layer:} Fully connected layer.
      \item \textbf{Activation Functions:}
      \begin{itemize}
        \item ReLU: Introduces non-linearity.
        \item Sigmoid: Used for binary classification.
      \end{itemize}
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Training and Evaluating the Model}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Compile the Model}
    \begin{lstlisting}[language=Python]
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
    \end{lstlisting}

    \item \textbf{Prepare Data}
    \begin{lstlisting}[language=Python]
X_train = np.random.rand(1000, input_size)  # Example input features
y_train = (X_train.sum(axis=1) > 0.5).astype(int)  # Binary labels
    \end{lstlisting}

    \item \textbf{Train the Model}
    \begin{lstlisting}[language=Python]
model.fit(X_train, y_train, epochs=10, batch_size=32)
    \end{lstlisting}

    \item \textbf{Evaluate the Model}
    \begin{lstlisting}[language=Python]
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Model accuracy: {accuracy * 100:.2f}%")
    \end{lstlisting}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Key Points and Applications}
  \begin{itemize}
    \item Neural networks are versatile tools for tasks such as classification and regression.
    \item Choices of activation functions, optimizers, and loss functions are crucial for effective training.
    \item Training data impacts performance: ensure preprocessing and splitting into training and testing sets.
  \end{itemize}

  \textbf{Applications:}
  \begin{itemize}
    \item Image Recognition
    \item Natural Language Processing
    \item Fraud Detection
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics in Deep Learning - Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand key evaluation metrics for deep learning models.
            \item Differentiate between accuracy, precision, recall, and F1 score.
            \item Apply these metrics to assess the performance of models in binary classification tasks.
        \end{itemize}
    \end{block}

    \begin{block}{Introduction}
        Evaluating deep learning models is crucial to ensure their predictive performance. This presentation focuses on four foundational metrics used in classification tasks:
        \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
            \item F1 Score
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics - Accuracy and Precision}
    \begin{block}{1. Accuracy}
        \textbf{Definition:} Measures the overall correctness of the model's predictions, defined as:
        \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
        \end{equation}

        \textbf{Example:} For a dataset of 100 instances with 80 correctly identified (70 True Positives, 10 True Negatives) and 20 misclassified:
        \begin{equation}
            \text{Accuracy} = \frac{70 + 10}{100} = 0.80 \text{ or } 80\%
        \end{equation}
    \end{block}
    
    \begin{block}{2. Precision}
        \textbf{Definition:} Evaluates the accuracy of positive predictions, defined as:
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \end{equation}

        \textbf{Example:} Given 70 True Positives and 15 False Positives:
        \begin{equation}
            \text{Precision} = \frac{70}{70 + 15} = \frac{70}{85} \approx 0.82 \text{ or } 82\%
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics - Recall and F1 Score}
    \begin{block}{3. Recall}
        \textbf{Definition:} Measures the ability of a model to find all relevant cases (actual positives), defined as:
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}

        \textbf{Example:} If there are 70 True Positives and 5 False Negatives:
        \begin{equation}
            \text{Recall} = \frac{70}{70 + 5} = \frac{70}{75} \approx 0.93 \text{ or } 93\%
        \end{equation}
    \end{block}

    \begin{block}{4. F1 Score}
        \textbf{Definition:} Harmonic mean of Precision and Recall, defined as:
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}

        \textbf{Example:} With Precision (82%) and Recall (93%):
        \begin{equation}
            \text{F1 Score} \approx 0.87 \text{ or } 87\%
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Lab Activity}
    \begin{block}{Description}
        Interactive lab session where students will implement a simple deep learning model and analyze its performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objectives}
    \begin{itemize}
        \item Students will implement a simple deep learning model.
        \item Students will analyze model performance using appropriate evaluation metrics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Deep Learning Models}
    \begin{itemize}
        \item Deep learning models are a subset of machine learning techniques using neural networks with many layers to learn from data.
    \end{itemize}
    \begin{block}{Key Terminology}
        \begin{itemize}
            \item \textbf{Neural Network:} A network of nodes that mimic the human brain's function.
            \item \textbf{Layers:} Composed of interconnected nodes, where data transformation occurs.
            \item \textbf{Training Data:} The dataset used to teach the model patterns and features.
            \item \textbf{Evaluation Metrics:} Quantitative measures used to assess model performance (e.g., accuracy, precision).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity Steps - Setup}
    \begin{enumerate}
        \item Set Up Your Environment:
        \begin{itemize}
            \item Ensure you have Python installed along with libraries such as TensorFlow or PyTorch.
            \item Suggested installation command:
            \begin{lstlisting}[language=bash]
pip install tensorflow
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity Steps - Data Handling}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item Load Dataset:
        \begin{itemize}
            \item Use the classic MNIST dataset, which consists of hand-written digits.
            \item Code snippet to load data:
            \begin{lstlisting}[language=python]
from tensorflow.keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
            \end{lstlisting}
        \end{itemize}
        
        \item Preprocess Data:
        \begin{itemize}
            \item Normalize pixel values to the range of 0 to 1 for better model performance.
            \item Code example:
            \begin{lstlisting}[language=python]
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity Steps - Build and Compile Model}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item Build the Model:
        \begin{itemize}
            \item Create a simple sequential model with input, hidden, and output layers.
            \item Code example:
            \begin{lstlisting}[language=python]
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
            \end{lstlisting}
        \end{itemize}

        \item Compile the Model:
        \begin{itemize}
            \item Use an appropriate optimizer and loss function.
            \item Code example:
            \begin{lstlisting}[language=python]
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity Steps - Train and Evaluate Model}
    \begin{enumerate}
        \setcounter{enumi}{5}
        \item Train the Model:
        \begin{itemize}
            \item Train the model on the training dataset.
            \item Code example:
            \begin{lstlisting}[language=python]
model.fit(x_train, y_train, epochs=5)
            \end{lstlisting}
        \end{itemize}
        
        \item Evaluate the Model:
        \begin{itemize}
            \item Assess the model using the test dataset and review metrics.
            \item Code example:
            \begin{lstlisting}[language=python]
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_accuracy:.2f}')
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Model evaluation is crucial for understanding performance.
        \item Different metrics highlight various aspects of model quality (e.g., accuracy vs. F1 score).
        \item Use visualizations to interpret model predictions vs. true labels - confusion matrices can be valuable tools.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    \begin{block}{Collaborative Project Overview}
        Prepare to apply deep learning techniques to real-world problems, utilizing the knowledge and experience gained in this lab!
    \end{block}
    \begin{block}{Conclusion}
        By engaging in this lab, you gain hands-on experience implementing models and evaluating their performance, preparing you for more complex machine learning challenges in the future. Enjoy exploring the power of deep learning!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Collaborative Project Overview}
    
    \begin{block}{Objectives}
        \begin{itemize}
            \item Understand the importance of teamwork in deep learning applications.
            \item Identify real-world problems that can be addressed using deep learning techniques.
            \item Develop a clear project plan and relevant milestones for your group project.
        \end{itemize}
    \end{block}
    
    \begin{block}{Introduction}
        This collaborative project allows you to apply deep learning techniques to explore and solve real-world problems while working in groups.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Scope}
    
    \begin{enumerate}
        \item \textbf{Problem Selection}:
        \begin{itemize}
            \item Choose a real-world problem to address using deep learning, such as:
            \begin{itemize}
                \item Image Classification (e.g., identifying objects in photographs)
                \item Natural Language Processing (e.g., sentiment analysis on social media)
                \item Time Series Forecasting (e.g., predicting stock prices)
            \end{itemize}
            \item Example: Create a sentiment analysis tool to assess public opinion on climate change using Twitter data.
        \end{itemize}
        
        \item \textbf{Team Roles}:
        \begin{itemize}
            \item Data Scientist
            \item Model Architect
            \item Analyst
        \end{itemize}

        \item \textbf{Methodologies}:
        \begin{itemize}
            \item Use popular deep learning frameworks like TensorFlow/Keras and PyTorch.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}

    \begin{block}{Simple Neural Network in Keras}
    \begin{lstlisting}[language=Python]
from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(input_dim,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Project Milestones}
        \begin{itemize}
            \item Week 1-2: Team Formation and Problem Selection
            \item Week 3-4: Data Gathering and Preprocessing
            \item Week 5-6: Model Design and Implementation
            \item Week 7: Testing and Evaluation of Model
            \item Week 8: Presentation Preparation and Final Submission
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Points Summary}
    
    \begin{itemize}
        \item \textbf{Definition of Deep Learning}:
        \begin{itemize}
            \item Subset of machine learning utilizing deep neural networks.
            \item Models and understands complex patterns in large datasets.
        \end{itemize}
        
        \item \textbf{Architecture of Neural Networks}:
        \begin{itemize}
            \item \textbf{Input Layer}: Receives data.
            \item \textbf{Hidden Layers}: Perform computations via neurons with activation functions.
            \item \textbf{Output Layer}: Produces final output.
        \end{itemize}
        
        \item \textbf{Common Architectures}:
        \begin{itemize}
            \item \textbf{CNNs}: Used for image processing.
            \item \textbf{RNNs}: Best for sequential data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Process and Loss Functions}
    
    \begin{block}{Learning Process}
        \begin{itemize}
            \item \textbf{Forward Pass}: Input data flows through the network.
            \item \textbf{Backward Pass (Backpropagation)}: Weights are adjusted based on error, optimizing the network via gradient descent.
        \end{itemize}
    \end{block}
    
    \begin{block}{Loss Functions}
        \begin{itemize}
            \item Measures network performance.
            \item \textbf{Common Types}:
            \begin{itemize}
                \item Mean Squared Error (MSE) for regression.
                \item Cross-Entropy Loss for classification.
            \end{itemize}
        \end{itemize}
        \begin{equation}
            \text{Cross-Entropy} = -\sum_{i=1}^{N} y_i \log(p_i)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Deep Learning and Q\&A}
    
    \begin{itemize}
        \item \textbf{Importance of Data Preparation}:
        \begin{itemize}
            \item Quality and quantity of data impact performance.
            \item Data augmentation increases robustness (e.g., flipping images).
        \end{itemize}

        \item \textbf{Future Trends}:
        \begin{itemize}
            \item Exploration of unsupervised and semi-supervised learning.
            \item Integration of deep learning in diverse fields—autonomous vehicles, healthcare, finance.
        \end{itemize}
        
        \item \textbf{Open Floor for Questions}:
        \begin{itemize}
            \item Encourage questions about algorithms, implementations, or data preparation.
        \end{itemize}
    \end{itemize}
\end{frame}


\end{document}