\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Decision Making: MDPs]{Week 11: Decision Making: Markov Decision Processes}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes (MDPs)}
    \begin{block}{Overview of MDPs}
        A Markov Decision Process (MDP) is a mathematical framework utilized for modeling decision-making scenarios where outcomes are partly random and partly under the control of a decision-maker. MDPs are relevant to various fields, including artificial intelligence (AI), robotics, economics, and operations research.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{States (S)}: Represents all possible situations or configurations of the system. \\
        \textit{Example:} In a chess game, each arrangement of pieces corresponds to a state.
        
        \item \textbf{Actions (A)}: Represents all possible moves or decisions available at a given state. \\
        \textit{Example:} In chess, a player can move a piece in various ways based on the current state.
        
        \item \textbf{Transition Model (T)}: Defines probabilities of moving from one state to another given a specific action. \\
        \textit{Example:} From state A with action X, you may move to state B with a probability of 0.7.
        
        \item \textbf{Rewards (R)}: Assigns a numerical value to each state-action pair, reflecting the immediate benefit of taking an action. \\
        \textit{Example:} A reward of +10 for checkmate or -10 for losing a piece.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance and Relevance of MDPs}
    \begin{block}{Significance in Decision Making}
        \begin{itemize}
            \item \textbf{Structured Representation}: MDPs formalize complex decision-making problems.
            \item \textbf{Optimal Policy Determination}: Enable the creation of a strategy that maximizes cumulative rewards.
            \item \textbf{Dynamic Programming}: Widely applied due to their recursive properties.
        \end{itemize}
    \end{block}

    \begin{block}{Relevance to AI}
        \begin{itemize}
            \item \textbf{Reinforcement Learning}: MDPs are foundational for reinforcement learning algorithms.
            \item \textbf{Autonomous Systems}: Critical in training agents such as robots and self-driving cars for informed decision-making.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Application in AI}
    Imagine training an AI agent in a grid-based environment:
    \begin{itemize}
        \item \textbf{States}: Each cell in the grid represents a state.
        \item \textbf{Actions}: Move Up, Down, Left, Right.
        \item \textbf{Transition Model}: Probabilities may define the outcome of movements based on obstacles.
        \item \textbf{Rewards}: Positive points for reaching goals and negative scores for traps.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding MDPs is crucial for anyone venturing into decision-making processes in AI. 
    \begin{itemize}
        \item MDPs model decision-making under uncertainty.
        \item Essential components: States, Actions, Transition Model, Rewards.
        \item Vital for reinforcement learning and AI applications in uncertain environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Markov Decision Process?}
    A \textbf{Markov Decision Process (MDP)} is a mathematical framework for modeling decision-making where outcomes are partly random and partly controllable. 
    \begin{itemize}
        \item MDPs enable formalizing sequential decision problems.
        \item They are crucial for ensuring optimal decision-making over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs - States and Actions}
    \begin{enumerate}
        \item \textbf{States (S)}: All possible situations for the decision maker.
        \begin{itemize}
            \item Example: In a grid world, each cell represents a state.
        \end{itemize}

        \item \textbf{Actions (A)}: All possible actions the agent can take.
        \begin{itemize}
            \item Example: In a grid world, actions may include moving 'up', 'down', 'left', 'right'.
        \end{itemize}

    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs - Transition Model and Rewards}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Transition Model (P)}: Probability function depicting how state changes.
        \begin{itemize}
            \item Defined as $P(s' | s, a)$: probability of moving to state $s'$ from $s$ via action $a$.
            \item Example: 80\% chance of moving right, 20\% remaining in the current state.
        \end{itemize}

        \item \textbf{Rewards (R)}: Numerical value assigned to each state or state-action pair.
        \begin{itemize}
            \item Objective: Maximize cumulative reward over time.
            \item Example: +10 for reaching a goal, -5 for falling into a trap.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points and Conclusion}
    \begin{itemize}
        \item MDPs blend probability and control, essential in reinforcement learning.
        \item Scenarios decomposed into states, actions, transition models, and rewards.
        \item MDP knowledge is vital for reinforcement learning algorithms like Q-learning.
    \end{itemize}
    
    \textbf{Conclusion:} MDPs underpin numerous AI applications, including robotics, game design, and resource management.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - Overview}
    \begin{block}{Markov Decision Processes (MDPs)}
        MDPs provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. Understanding their key components is essential for designing effective algorithms for planning and learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - States and Actions}
    \begin{enumerate}
        \item \textbf{States (S)}
        \begin{itemize}
            \item Definition: Represent all possible situations for an agent.
            \item Example: Each configuration in chess or positions of a robotic vacuum.
        \end{itemize}
        
        \item \textbf{Actions (A)}
        \begin{itemize}
            \item Definition: Choices available to an agent in each state.
            \item Example: In grid navigation, actions such as "move up" or "move right." 
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - Transition Probabilities and Rewards}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Transition Probabilities (P)}
        \begin{itemize}
            \item Definition: Likelihood of moving from one state to another given an action.
            \item Mathematical Representation: $P(s'|s,a)$.
            \item Example: 70\% chance of moving right in a grid and 30\% of moving up.
        \end{itemize}

        \item \textbf{Rewards (R)}
        \begin{itemize}
            \item Definition: Feedback on the desirability of a state or action.
            \item Mathematical Representation: $R(s,a,s')$.
            \item Example: $+10$ for cleaning a dirty spot, $-5$ for hitting a wall. 
        \end{itemize}

        \item \textbf{Mathematical Formulation}
        \begin{itemize}
            \item An MDP can be defined as a 5-tuple $(S, A, P, R, \gamma)$.
            \item Components include: states, actions, transitions, rewards, and the discount factor $\gamma$.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space - Overview}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{State Space Definition}: The set of all possible states (S) in which an agent can find itself within an MDP.
            \item \textbf{Discrete vs. Continuous State Spaces}:
                \begin{itemize}
                    \item Discrete: Finite or countable set of states.
                    \item Continuous: Uncountable set of states represented in a continuous range.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space - Discrete vs. Continuous}
    \begin{block}{Discrete State Space}
        \begin{itemize}
            \item A finite or countable set of distinct states.
            \item \textbf{Example}: A chessboard with 64 positions represents a discrete state space.
        \end{itemize}
    \end{block}
    
    \begin{block}{Continuous State Space}
        \begin{itemize}
            \item An uncountable set of states.
            \item \textbf{Example}: A car's speed range from 0 to 100 km/h.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space - Implications in MDP Formulations}
    \begin{block}{Modeling Complexity}
        \begin{itemize}
            \item Discrete spaces: Easier to model, allowing straightforward state/action enumeration.
            \item Continuous spaces: Requires approximation methods (e.g., discretization).
        \end{itemize}
    \end{block}

    \begin{block}{Computational Efficiency}
        \begin{itemize}
            \item MDP algorithms often scale better with discrete states.
            \item Continuous state spaces can lead to challenges, requiring techniques like neural networks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space - Examples and Code Snippet}
    \begin{block}{Examples in Robotics}
        \begin{itemize}
            \item \textbf{Discrete}: A robot navigating a grid where each cell represents a state.
            \item \textbf{Continuous}: A robot moving in a physical space with a continuously changing pose.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item The choice of state space can significantly affect MDP design and performance.
            \item Understanding the state's nature is critical for selecting MDP algorithms.
        \end{itemize}
    \end{block}
    
    \begin{lstlisting}[language=Python, caption=Code Example: Defining Discrete State Space]
# Pseudo-code for defining a simple discrete state space
states = ['State1', 'State2', 'State3']
actions = ['Action1', 'Action2']

# Defining transition probabilities
transition_probs = {
    'State1': {'Action1': 'State2', 'Action2': 'State3'},
    'State2': {'Action1': 'State1', 'Action2': 'State3'},
    'State3': {'Action1': 'State1', 'Action2': 'State2'},
}
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Action Space - Overview}
    \begin{block}{Definition}
        The action space in a Markov Decision Process (MDP) refers to the set of all possible actions that an agent can take when in a particular state. The interaction between the agent's actions and the environment determines the outcome of the decision-making process.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Action Space - Key Concepts}
    \begin{itemize}
        \item \textbf{Discrete vs. Continuous Action Space:}
            \begin{itemize}
                \item \textbf{Discrete Action Space:} A finite set of actions (e.g., moves in chess).
                \item \textbf{Continuous Action Space:} An infinite set of possible actions (e.g., steering angle in self-driving cars).
            \end{itemize}
        \item \textbf{Impact on Policy Determination:}
            \begin{itemize}
                \item A \textbf{policy} defines action choice for each state.
                \item Can be \textbf{deterministic} or \textbf{stochastic}.
                \item Larger action spaces can complicate policy complexity.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Action Space - Examples and Formulas}
    \begin{block}{Examples of Action Space}
        \begin{itemize}
            \item \textbf{Gridworld:} 
                \begin{itemize}
                    \item State Space: Cells (e.g., (0,0), (1,0)).
                    \item Action Space: Moves {Up, Down, Left, Right}.
                \end{itemize}
            \item \textbf{Robot Navigation:} 
                \begin{itemize}
                    \item State Space: Robot's position and orientation.
                    \item Action Space: {Move Forward, Turn Left, Turn Right, Stop}.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Formula}
        The expected value of taking action \(a\) in state \(s\) is:
        \begin{equation}
            Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s')
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Action Space - Conclusion}
    \begin{block}{Summary}
        Understanding the action space is crucial for formulating effective policies in MDPs. The agent's choice of action affects immediate rewards and future states, shaping the decision-making process.
    \end{block}

    \begin{block}{Key Takeaway}
        The design of the action space fundamentally influences strategies and outcomes in decision-making scenarios modeled by MDPs. Consideration of both discrete and continuous action spaces is essential for developing efficient policies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Model - Learning Objectives}
    \begin{itemize}
        \item Understand the concept of transition probability in Markov Decision Processes (MDPs).
        \item Recognize how state changes are modeled.
        \item Explore applications of transition models in decision-making scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Model - What is it?}
    In the context of \textbf{Markov Decision Processes (MDPs)}, the transition model is a critical component that defines how an agent transitions from one state to another after taking an action. 

    It is represented by the \textbf{transition probability function}, often denoted as \( P(s' | s, a) \), which describes the probability of moving to state \( s' \) given the current state \( s \) and action \( a \).
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{States \( (s) \)}: Represent all possible situations the agent can be in.
            \item \textbf{Actions \( (a) \)}: Choices the agent can make that affect the environment and state changes.
            \item \textbf{Next State \( (s') \)}: The state the agent arrives at after executing action \( a \).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Model - Probability Function}
    The transition function \( P(s' | s, a) \) is defined mathematically as:

    \begin{equation}
        P(s' | s, a) = \text{Prob}(S_{t+1} = s' | S_t = s, A_t = a)
    \end{equation}
    
    This means the probability of being in state \( s' \) at time \( t+1 \), given that the current state at time \( t \) is \( s \) and the action taken is \( a \).

    \begin{block}{Illustration}
        Imagine a simple grid world where an agent can move in four directions (up, down, left, right):
        \begin{itemize}
            \item \textbf{States}: Each cell in the grid is a state (e.g., \( s_1, s_2, \ldots, s_n \)).
            \item \textbf{Actions}: The set of actions \{up, down, left, right\}.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of the Transition Model}
    \begin{itemize}
        \item Defines how reality responds to actions, helping evaluate policies.
        \item Essential for calculating expected future rewards through dynamic programming methods such as Value Iteration and Policy Iteration.
    \end{itemize}
    
    \begin{block}{Example Application}
        In robotics, the transition model is vital for path planning. If a robot chooses to move forward, the transition probabilities guide its expected positioning within the environment, influencing its navigation and decision-making process.
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding the transition model allows us to predict outcomes of actions and supports the formulation of optimal policies to maximize cumulative rewards in various applications, from gaming to automated systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Function - Overview}
    \begin{block}{What is a Reward Function?}
        In the context of Markov Decision Processes (MDPs), the \textbf{Reward Function} quantifies the immediate benefit an agent receives after transitioning from one state to another due to an action. 
    \end{block}
    \begin{equation}
        R: S \times A \rightarrow \mathbb{R}
    \end{equation}
    Where:
    \begin{itemize}
        \item **S** = Set of all states
        \item **A** = Set of all actions
        \item **R(s, a)** = Reward received after taking action **a** in state **s**
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Function - Importance}
    \begin{block}{Importance of the Reward Function}
        The reward function is critical in guiding the learning process of an agent:
    \end{block}
    \begin{enumerate}
        \item \textbf{Agents’ Decision Making:} Influences the choices made by the agent. Higher rewards lead to repeated actions, while lower/negative rewards discourage them.
        \item \textbf{Learning Objective:} The ultimate goal is to maximize cumulative reward over time. Effective reward function design is crucial for favorable outcomes.
        \item \textbf{Policy Improvement:} Rewards help inform the value of states and actions, enabling policy refinement – the mapping from states to actions maximizing expected rewards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Function - Example and Key Points}
    \begin{block}{Example}
        Consider a robot navigating a maze:
        \begin{itemize}
            \item Reward of +10 for reaching the exit.
            \item Reward of -1 for each move to encourage finding the shortest path.
            \item Reward of -5 for hitting a wall.
        \end{itemize}
        This feedback helps the robot learn to navigate efficiently over time.
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Immediate vs Long-Term Reward:} Distinction between immediate rewards and those leading to long-term goals.
            \item \textbf{Exploration vs Exploitation:} Balancing exploration of new actions with exploitation of known beneficial actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Function - Summary}
    \begin{block}{Summary}
        The reward function is fundamental in MDPs, providing essential feedback for agents in uncertain environments. 
        Careful design and tuning are vital for successful learning and optimal policy development.
    \end{block}
    \begin{block}{Additional Notes}
        When creating a reward function, consider issues like \textbf{reward hacking}, where agents might find unintended ways to maximize rewards, leading to undesirable behaviors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Part 1}
    \begin{block}{What is a Policy in the Context of MDPs?}
        In the framework of Markov Decision Processes (MDPs), a \textbf{policy} is a crucial component that defines the strategy used for decision-making within a given state.
    \end{block}
    
    Formally, a policy is a mapping from states of the environment to actions. It can be:
    
    \begin{itemize}
        \item \textbf{Deterministic Policy}:
            \[
            \pi(s) = a
            \]
            For each state \( s \), a deterministic policy \( \pi \) selects a specific action \( a \).
        
        \item \textbf{Stochastic Policy}:
            \[
            \pi(a|s) = P(A = a | S = s)
            \]
            For each state \( s \), a stochastic policy defines a probability distribution over actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Part 2}
    \begin{block}{How Policies Influence Decision Making}
        \begin{enumerate}
            \item \textbf{Decision Guidance}:
                  The policy directly influences which actions are taken based on the current state, guiding the decision-making process.
                  
            \item \textbf{Optimization Objective}:
                  The goal is to find an optimal policy \( \pi^* \) that maximizes the expected sum of future rewards:
                  \[
                  \text{Maximize } E\left[\sum_{t=0}^{T} r_t | \pi\right]
                  \]
                  
            \item \textbf{Exploration vs. Exploitation}:
                  A good policy balances exploration (trying new actions) and exploitation (selecting known rewarding actions).
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Part 3}
    \begin{block}{Example}
        Imagine a robot navigating a maze:
        \begin{itemize}
            \item A \textbf{deterministic policy} might dictate that if the robot is at position \( (3, 3) \), it should always move left to position \( (3, 2) \).
            \item A \textbf{stochastic policy} might allow for probabilities of moving left (70%), moving right (20%), and staying put (10%) in state \( (3, 3) \).
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Policy Definition: Central to decision-making in MDPs.
            \item Types: Deterministic vs. Stochastic.
            \item Learning Impact: Aids in maximizing rewards.
            \item Role: Integral for effective navigation through states.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Function}
    \begin{block}{Understanding Value Functions in MDPs}
        \begin{itemize}
            \item Define and differentiate between state-value functions and action-value functions.
            \item Explain their role in evaluating policies within a Markov Decision Process (MDP).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Value Functions}
    \begin{block}{Value Function}
        In MDPs, a value function quantifies the expected return of being in a specific state or taking an action. 
    \end{block}
    
    \begin{block}{State-Value Function (V)}
        \begin{itemize}
            \item \textbf{Definition}: Denoted as \( V(s) \), it represents the expected return starting from state \( s \) and following policy \( \pi \).
            \item \textbf{Formula}:
            \begin{equation}
            V(s) = \mathbb{E}_{\pi} [R_t | S_t = s] = \sum_{a \in A} \pi(a|s) \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V(s') \right]
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Action-Value Function (Q)}
    \begin{block}{Action-Value Function (Q)}
        \begin{itemize}
            \item \textbf{Definition}: Denoted as \( Q(s, a) \), it indicates the expected return for taking action \( a \) in state \( s \) and then following policy \( \pi \).
            \item \textbf{Formula}:
            \begin{equation}
            Q(s, a) = \mathbb{E}_{\pi} [R_t | S_t = s, A_t = a] = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V(s') \right]
            \end{equation}
        \end{itemize}
    \end{block}
    
    \begin{block}{Role in Evaluating Policies}
        \begin{itemize}
            \item Value functions are essential for assessing policy quality.
            \item An optimal policy maximizes expected value from all states.
            \item They guide policy improvement by selecting higher value actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visual and Practical Applications}
    \begin{block}{Visual Representation}
        \begin{itemize}
            \item A flow diagram could illustrate state transitions with various actions leading to subsequent states and rewards.
            \item A table mapping states to their state-value and action-value functions could be useful.
        \end{itemize}
    \end{block}

    \begin{block}{Practical Application}
        \begin{itemize}
            \item In reinforcement learning, value functions are foundational for algorithms like Q-learning and Actor-Critic methods.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding value functions is crucial in navigating MDPs and in evaluating and refining policies over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations}
    \begin{block}{Introduction}
        The \textbf{Bellman equations} are crucial in \textbf{Markov Decision Processes (MDPs)} and form the backbone of \textbf{dynamic programming}. These equations provide a recursive method for evaluating the value of a policy, which is instrumental for making optimal decisions in uncertain environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Value Function}: Represents the expected return from a state, depending on future decisions.
        
            \begin{itemize}
                \item \textbf{State-Value Function (V)}:
                \begin{equation}
                    V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s'} P(s'|s, a) \left[R(s, a, s') + \gamma V^\pi(s')\right]
                \end{equation}
                
                \item \textbf{Action-Value Function (Q)}:
                \begin{equation}
                    Q^\pi(s, a) = \sum_{s'} P(s'|s, a) \left[R(s, a, s') + \gamma V^\pi(s')\right]
                \end{equation}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Bellman Equation}
    The Bellman equation applies to the state-value function as follows:
    
    \begin{equation}
        V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') \right)
    \end{equation}

    \begin{block}{Meaning}
        The value of a state \( s \) is the maximum expected return among all actions \( a \), considering immediate and discounted future rewards. The parameter \( \gamma \) (discount factor) regulates the value of future benefits (0 ≤ \( \gamma \) < 1).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Bellman Equations}
    
    \begin{enumerate}
        \item \textbf{Optimal Policy Determination}: Enables recursive calculation for determining optimal policies.
        \item \textbf{Dynamic Programming Foundation}: Forms the basis for algorithms such as \textbf{Value Iteration} and \textbf{Policy Iteration}, creating systematic methods to achieve optimal solutions.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Conclusion}
    
    \textbf{Example:} In a grid world, an agent can move in four possible directions to reach a target. Using the Bellman equation, we can evaluate the value of each state, considering potential rewards and derive optimal actions.

    \begin{block}{Conclusion}
        Understanding the Bellman equations is essential for effective MDP solutions. They break complex decision-making into simpler parts, paving the way for more sophisticated algorithms like Value Iteration.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Method - Overview}
    \begin{block}{Overview}
        The Value Iteration Method is a fundamental algorithm for computing the optimal policy in Markov Decision Processes (MDPs).
    \end{block}
    \begin{itemize}
        \item \textbf{Learning Objectives:}
        \begin{enumerate}
            \item Understand the concept of value iteration in the context of MDPs.
            \item Learn the step-by-step algorithm for value iteration.
            \item Apply the concept through a simple example.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Value Function and Bellman Equation}
    \begin{block}{Value Function}
        The \textbf{value function} \( V(s) \) estimates the maximum expected reward from state \( s \) under a policy. The goal is to find the optimal value function \( V^*(s) \).
    \end{block}
    
    \begin{block}{Bellman Optimality Equation}
        The relationship is given by the Bellman equation:
        \begin{equation}
            V^*(s) = \max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V^*(s')]
        \end{equation}
        where:
        \begin{itemize}
            \item \( P(s' | s, a) \): Transition probability from state \( s \) to \( s' \) given action \( a \).
            \item \( R(s, a, s') \): Reward received for transitioning from state \( s \) to \( s' \).
            \item \( \gamma \): Discount factor (0 ≤ \( \gamma \) < 1).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of the Value Iteration Algorithm}
    \begin{enumerate}
        \item \textbf{Initialization:} Set \( V(s) = 0 \) for all states \( s \) in the state space \( S \).
        
        \item \textbf{Value Update:}
        \begin{equation}
            V_{\text{new}}(s) = \max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V(s')]
        \end{equation}
        Update \( V(s) \leftarrow V_{\text{new}}(s) \).
        
        \item \textbf{Convergence Check:}
        If \( |V_{\text{new}}(s) - V(s)| < \epsilon \) for all states \( s \), then stop; otherwise, repeat from step 2.
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The value iteration method guarantees convergence to the optimal value function.
            \item The choice of discount factor \( \gamma \) affects future rewards' valuation.
            \item Value iteration is effective for finite state spaces.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Method}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the concept of policy iteration in Markov Decision Processes (MDPs).
            \item Differentiate between value iteration and policy iteration.
            \item Learn the steps involved in the policy iteration method.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Policy Iteration?}
    \begin{block}{Definition}
        Policy Iteration is an alternative approach to the Value Iteration method for finding an optimal policy in Markov Decision Processes (MDPs). It involves iteratively evaluating a policy and then improving it until the policy converges to the optimal one.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Policy (\(\pi\))}: A mapping from states to actions, indicating the action to take in each state.
        \item \textbf{Value Function (\(V\))}: The value function, \( V(s) \), estimates the expected return (cumulative future rewards) when starting from state \(s\) under a specific policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of Policy Iteration}
    \begin{enumerate}
        \item \textbf{Initialize the Policy}: Start with an arbitrary policy \( \pi \).
        \item \textbf{Policy Evaluation}:
            \begin{equation}
                V^\pi(s) = R(s) + \gamma \sum_{s'} P(s'|s, \pi(s)) V^\pi(s')
            \end{equation}
            \begin{itemize}
                \item \( R(s) \): Reward received from state \( s \)
                \item \( \gamma \): Discount factor (0 < \(\gamma\) < 1)
                \item \( P(s'|s, \pi(s)) \): Transition probability to state \( s' \) when taking action \( \pi(s) \)
            \end{itemize}
        \item \textbf{Policy Improvement}:
            \begin{equation}
                \pi'(s) = \text{arg max}_{a} \left( R(s) + \gamma \sum_{s'} P(s'|s, a) V^\pi(s') \right)
            \end{equation}
        \item \textbf{Check for Convergence}: If \( \pi' \) does not change, stop. Otherwise, set \( \pi = \pi' \) and repeat.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Policy Iteration}
    \begin{itemize}
        \item \textbf{Initialization}: Start with a simple grid world with three states, A, B, C, with policy "stay" in each state.
        \item \textbf{Policy Evaluation}: Calculate the value for each state based on immediate rewards and transitions.
        \item \textbf{Policy Improvement}: Re-evaluate actions based on computed values and adjust the policy.
        \item \textbf{Convergence Check}: Continue until the policy stabilizes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    \begin{itemize}
        \item Policy Iteration is often faster than Value Iteration in practice because it evaluates whole policies rather than individual states at each iteration.
        \item The systematic approach of policy evaluation and improvement facilitates convergence to the optimal policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{Summary}
        Policy Iteration provides a robust framework for solving MDPs by evaluating and refining policies iteratively. This method is particularly effective when the state and action spaces are moderate in size, allowing for practical computations.
        \\
        By understanding and implementing the Policy Iteration Method, you will gain deeper insights into decision-making processes and optimal strategies within dynamic environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Overview}
    \begin{itemize}
        \item Understanding Markov Decision Processes (MDPs)
        \item Real-world applications in:
        \begin{itemize}
            \item Robotics
            \item Automated systems
            \item Reinforcement learning
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Markov Decision Processes (MDPs)}
    \begin{block}{Key Components of MDPs}
        \begin{itemize}
            \item \textbf{States (S)}: Set of possible scenarios for the agent.
            \item \textbf{Actions (A)}: Set of actions available to the agent.
            \item \textbf{Transition Probabilities (P)}: Probabilities of moving between states.
            \item \textbf{Rewards (R)}: Immediate rewards received after transitions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs in Robotics}
    \begin{block}{1. Robot Navigation}
        \begin{itemize}
            \item MDPs are employed in path planning for autonomous navigation.
            \item \textbf{Example}: A robot navigating a maze optimally decides its path.
        \end{itemize}
    \end{block}

    \begin{block}{2. Reinforcement Learning in Robotics}
        \begin{itemize}
            \item Robots learn optimal behaviors via trial and error.
            \item \textbf{Illustration}: A robotic arm improves its object-picking strategy based on rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs in Automated Systems}
    \begin{block}{1. Inventory Management}
        \begin{itemize}
            \item Optimize stock levels to minimize costs while maintaining service.
            \item \textbf{Example}: Predicting stock depletion and ordering inventory.
        \end{itemize}
    \end{block}

    \begin{block}{2. Energy Management in Smart Grids}
        \begin{itemize}
            \item MDPs manage energy distribution adapting to demand and supply.
            \item \textbf{Application}: Adjusting energy sources based on expected demand.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs in Reinforcement Learning}
    \begin{block}{1. Game Playing}
        \begin{itemize}
            \item MDPs are fundamental in game algorithms, enabling agents to learn strategies.
            \item \textbf{Example}: In games, agents refine strategies based on experiences.
        \end{itemize}
    \end{block}

    \begin{block}{2. Personalized Recommendations}
        \begin{itemize}
            \item MDPs optimize user engagement through adaptive recommendations.
            \item \textbf{Illustration}: Streaming services suggest content based on viewing history.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion on MDPs}
    \begin{itemize}
        \item MDPs provide a framework for decision-making under uncertainty.
        \item Applications span across various fields: robotics, automated systems, reinforcement learning.
        \item Understanding MDPs is vital for advancements in AI and automation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Overview}
    \begin{itemize}
        \item MDPs face several significant challenges.
        \item Key issues include:
        \begin{itemize}
            \item Complexity and Scalability
            \item Curse of Dimensionality
            \item Partial Observability
            \item Stationarity of Models
            \item Computation of Optimal Policies
            \item Reward Definition and Delays
            \item Action Space Explosion
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Complexity and Scalability}
    \begin{itemize}
        \item \textbf{Exponential State Space}:
        \begin{itemize}
            \item MDPs can be infeasible as states and actions increase exponentially.
            \item Example: 
            \begin{itemize}
                \item A 3x3 grid has 9 states, a 4x4 has 16, and a 5x5 has 25, leading to computational overload.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Curse of Dimensionality and Partial Observability}
    \begin{itemize}
        \item \textbf{Curse of Dimensionality}:
        \begin{itemize}
            \item Increasing states result in a vast number of state-action pairs, complicating value function evaluations.
        \end{itemize}
        
        \item \textbf{Partial Observability}:
        \begin{itemize}
            \item MDPs assume full observability; however, real-life scenarios often deal with uncertainties.
            \item Example: In poker, players make decisions without knowing others' cards.
            \item \textbf{Solution}: Use Partially Observable Markov Decision Processes (POMDPs), adding complexity.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stationarity and Computation of Optimal Policies}
    \begin{itemize}
        \item \textbf{Stationarity of Models}:
        \begin{itemize}
            \item MDPs assume constant transition probabilities and rewards, which is often not the case.
            \item Example: In stock trading, strategies must adapt to dynamic market conditions.
        \end{itemize}
        
        \item \textbf{Computation of Optimal Policies}:
        \begin{itemize}
            \item Optimal policies can be found using methods like Policy Iteration or Value Iteration.
            \item These methods may be inefficient and resource-intensive.
            \item \textbf{Key Method}: Approximate methods, like Q-learning, can be solutions at the cost of precision.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Definition and Action Space}
    \begin{itemize}
        \item \textbf{Reward Definition and Delays}:
        \begin{itemize}
            \item Poorly defined rewards can lead to suboptimal policies.
            \item Example: Rewarding only final outcomes rather than individual actions may lead to erratic agent behavior.
        \end{itemize}

        \item \textbf{Action Space Explosion}:
        \begin{itemize}
            \item Large or continuous action spaces require sophisticated methods like function approximation.
            \item Example: Autonomous vehicles face numerous potential actions (e.g., accelerate, brake, steer).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Formulas}
    \begin{itemize}
        \item Despite their theoretical elegance, MDPs face practical challenges.
        \item Awareness of these limitations is crucial for real-world implementations.
    \end{itemize}
    
    \begin{block}{Key Formulas}
        \begin{equation}
            V(s) = \max_a \left[ R(s,a) + \gamma \sum P(s'|s,a)V(s') \right]
        \end{equation}
        The Bellman Equation for MDPs formulates value functions recursively.
    \end{block}
    
    \begin{block}{Engagement Tip}
        Encourage students to experiment with MDP implementations in various scenarios to illustrate successes and failures in decision-making contexts.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 1}
    \textbf{Summary of Key Takeaways Regarding Markov Decision Processes (MDPs) and Their Role in AI Decision Making}
    
    \begin{itemize}
        \item \textbf{Understanding MDPs}
        \begin{itemize}
            \item MDPs offer a framework for modeling decision-making with random outcomes.
            \item Key components include:
            \begin{itemize}
                \item States (\(S\))
                \item Actions (\(A\))
                \item Transition Model (\(P\))
                \item Reward Function (\(R\))
                \item Discount Factor (\(\gamma\))
            \end{itemize}
        \end{itemize}
        
        \item \textbf{MDPs in AI Decision Making}
        \begin{itemize}
            \item Foundation for many reinforcement learning algorithms.
            \item Vital for sequential decision making in various fields.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 2}
    \textbf{Key Concepts in MDPs}
    
    \begin{itemize}
        \item \textbf{Value Functions}:
        \begin{itemize}
            \item State Value Function (\(V\)):
            \begin{equation}
            V(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t) \mid s_0 = s \right]
            \end{equation}
            \item Action Value Function (\(Q\)):
            \begin{equation}
            Q(s, a) = \mathbb{E} \left[ R(s, a) + \gamma V(s') \right]
            \end{equation}
        \end{itemize}
        \item \textbf{Optimal Policy}: Maximizes expected reward for decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 3}
    \textbf{Practical Applications and Challenges of MDPs}
    
    \begin{itemize}
        \item \textbf{Applications}:
        \begin{itemize}
            \item Game AI (e.g., chess, Go)
            \item Robotics (efficient path planning)
            \item Healthcare (optimizing treatment strategies)
        \end{itemize}
        
        \item \textbf{Challenges}:
        \begin{itemize}
            \item Scalability issues with increasing states and actions.
            \item Difficulty in accurately modeling real-world scenarios.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    Markov Decision Processes are essential for modeling complex decision-making in AI.
    
    \begin{itemize}
        \item They offer structure for understanding dynamics, rewards, and trade-offs in decision-making.
        \item Balance between flexibility and complexity when applying MDPs in real-world problems.
    \end{itemize}
    
    \textbf{Next Up:} Explore further reading and resources for a deeper understanding of MDPs and their applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Part 1}
    \textbf{Understanding MDPs: A Deeper Dive}
    
    To enhance your grasp of Markov Decision Processes (MDPs), we recommend the following readings and resources. Each item is tailored to help deepen your understanding of the core concepts, applications, and mathematical foundations of MDPs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Part 2}
    \textbf{Recommended Readings}
    
    \begin{enumerate}
        \item \textbf{``Reinforcement Learning: An Introduction'' by Richard S. Sutton and Andrew G. Barto}
        \begin{itemize}
            \item \textit{Overview:} Foundational understanding of reinforcement learning principles, including MDPs.
            \item \textit{Key Sections:}
            \begin{itemize}
                \item Chapter 3: Key concepts of MDPs, reward structures, and policies.
                \item Chapter 4: Value functions and Bellman equations.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{``Dynamic Programming and Optimal Control'' by Dimitri P. Bertsekas}
        \begin{itemize}
            \item \textit{Overview:} Delves into dynamic programming approaches essential for solving MDPs.
            \item \textit{Key Sections:}
            \begin{itemize}
                \item Volume 1, Chapter 3: Principles of optimal control and optimality equations.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{``Markov Decision Processes: Discrete Stochastic Dynamic Programming'' by Martin L. Puterman}
        \begin{itemize}
            \item \textit{Overview:} Detailed examination of MDPs with applications in various fields.
            \item \textit{Key Sections:}
            \begin{itemize}
                \item Chapters on algorithms for solving MDPs, including policy iteration and value iteration.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Part 3}
    \textbf{Online Resources}

    \begin{enumerate}
        \item \textbf{Coursera Course: ``Reinforcement Learning''}
        \begin{itemize}
            \item \textit{Description:} Covers fundamental concepts and techniques in reinforcement learning, including MDPs. For beginners and refreshers.
            \item \textit{Link:} \url{https://www.coursera.org/learn/reinforcement-learning}
        \end{itemize}
        
        \item \textbf{OpenAI Gym}
        \begin{itemize}
            \item \textit{Description:} A toolkit for developing and comparing reinforcement learning algorithms, with environments modeled as MDPs.
            \item \textit{Link:} \url{https://gym.openai.com/}
        \end{itemize}
    \end{enumerate}

    \textbf{Key Concepts to Explore}
    \begin{itemize}
        \item States (S): Possible configurations of the environment, where decisions are made.
        \item Actions (A): Choices available to the agent affecting the state.
        \item Transition Model (P): Probability of moving from one state to another given an action.
        \item Reward (R): Feedback received after an action taken in a state, guiding the learning process.
    \end{itemize}
\end{frame}


\end{document}