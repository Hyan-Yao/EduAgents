\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 4: Support Vector Machines}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Support Vector Machines (SVM)}
    \begin{block}{Overview of Support Vector Machines (SVM)}
        Support Vector Machines (SVM) are a supervised learning algorithm used primarily for classification, but they can also accommodate regression tasks. The aim is to find a hyperplane that best separates the data into distinct classes in high-dimensional space. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Support Vector Machines (SVM) - Part 2}
    \begin{itemize}
        \item \textbf{What are Support Vector Machines?}
            \begin{itemize}
                \item They aim to find a hyperplane that optimally separates data.
                \item "Support vectors" are the closest data points, crucial for determining the hyperplane's position.
            \end{itemize}
        
        \item \textbf{Importance in Machine Learning}
            \begin{enumerate}
                \item Effective in high-dimensional spaces; useful in image recognition, text classification.
                \item Versatile for linear and non-linear classification using kernel tricks.
                \item Robust against overfitting as they consider only support vectors.
            \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Brief History of SVM Development}
    \begin{itemize}
        \item \textbf{Early Development:}
            \begin{itemize}
                \item Introduced in 1963 by Vladimir Vapnik and Alexey Chervonenkis.
            \end{itemize}
        
        \item \textbf{Breakthrough in the 1990s:}
            \begin{itemize}
                \item Gained popularity due to a solid theoretical foundation.
                \item 1995 paper by Cortes and Vapnik introduced the kernel trick for handling non-linear data.
            \end{itemize}

        \item \textbf{Recent Advancements:}
            \begin{itemize}
                \item Widely used in bioinformatics, text classification, and image processing.
            \end{itemize}
        
        \item \textbf{Conclusion:}
            \begin{itemize}
                \item SVMs are fundamental in machine learning for classifying complex datasets.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamental Concepts of SVM - Hyperplanes}
    \begin{itemize}
        \item \textbf{Hyperplanes}
        \begin{itemize}
            \item \textbf{Definition:} A hyperplane is a decision boundary that separates different classes in a feature space.
            \item \textbf{In Two Dimensions:} A hyperplane is a line dividing the 2D plane into two parts, each containing points from one class.
            \item \textbf{Mathematical Representation:}
            \begin{equation}
                w \cdot x + b = 0
            \end{equation}
            where \( w \) is the weight vector, \( x \) is the feature vector, and \( b \) is the bias term.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamental Concepts of SVM - Support Vectors and Decision Boundaries}
    \begin{itemize}
        \item \textbf{Support Vectors}
        \begin{itemize}
            \item \textbf{Definition:} Support vectors are the closest data points to the hyperplane that influence its position and orientation.
            \item \textbf{Importance:} Only support vectors define the optimal hyperplane; non-support vector points do not affect classification.
            \item \textbf{Visualization:} In a 2D plot, support vectors lie on the edge of the margin.
        \end{itemize}

        \item \textbf{Decision Boundaries}
        \begin{itemize}
            \item \textbf{Definition:} The decision boundary is the hyperplane separating different classes in the dataset.
            \item \textbf{Goal of SVM:} To find the optimal decision boundary that maximizes the margin between classes.
            \item \textbf{Visualization Example:} Separation of two classes in a 2D space using a line (hyperplane) with maximal distance from the closest points (support vectors).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamental Concepts of SVM - Key Points and Formulas}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Maximizing Margin:} SVMs aim for the widest possible margin, enhancing robustness and generalization.
            \item \textbf{Non-linearity:} SVMs can utilize kernel functions for non-linearly separable data.
        \end{itemize}

        \item \textbf{Formulas}
        \begin{equation}
            M = \frac{2}{\|w\|}
        \end{equation}
        where \( M \) is the margin.

        \item \textbf{Example in Python:}
        \begin{lstlisting}[language=Python]
from sklearn import datasets
from sklearn.svm import SVC
import matplotlib.pyplot as plt

# Load dataset
X, y = datasets.make_blobs(n_samples=50, centers=2, random_state=6)

# Fit an SVM model
model = SVC(kernel='linear')
model.fit(X, y)

# Plotting
plt.scatter(X[:, 0], X[:, 1], c=y)
# Plotting the hyperplane
plt.axis('equal')
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear SVM - Overview}
    \begin{block}{Understanding Linear Support Vector Machines}
        A Linear Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification tasks. Its primary goal is to find the optimal hyperplane that best separates two classes of data in a high-dimensional feature space.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear SVM - Key Terminologies}
    \begin{itemize}
        \item \textbf{Hyperplane:} A flat affine subspace that divides the space into two halves. 
        \item \textbf{Linearly Separable Data:} A dataset is linearly separable if a hyperplane can separate the classes without misclassifications.
        \item \textbf{Support Vectors:} Closest data points to the hyperplane that determine its position. Removing them changes the hyperplane.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear SVM - How It Works}
    \begin{enumerate}
        \item \textbf{Defining the Hyperplane:} 
        \[
        w^T x + b = 0
        \]
        where \( w \) is the weight vector, \( b \) is the bias, and \( x \) is the input vector.
        
        \item \textbf{Finding the Optimal Hyperplane:} Maximize the margin defined as:
        \[
        \text{Margin} = \frac{2}{\|w\|}
        \]
        
        \item \textbf{Optimization Problem:} 
        \[
        \text{Minimize } \frac{1}{2} \|w\|^2 
        \]
        subject to:
        \[
        y_i (w^T x_i + b) \geq 1 \quad \forall i
        \]
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear SVM - Example and Key Points}
    \begin{block}{Example}
        Consider a dataset:
        \begin{itemize}
            \item Class 1: \((2, 3)\), \((3, 5)\)
            \item Class 2: \((5, 2)\), \((6, 1)\)
        \end{itemize}
        A linear SVM seeks a line to separate these points while maximizing the margin.
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Linear SVMs can effectively classify only linearly separable data.
            \item Support vectors are crucial in defining the hyperplane's position.
            \item Linear SVMs are efficient for high-dimensional datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear SVM - Conclusion}
    Linear SVMs are powerful for classification in linearly separable scenarios. Understanding their mechanics sets the stage for discussions on non-linear cases with soft margin SVMs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Soft Margin SVM - Introduction}
    \begin{itemize}
        \item **Objective**: Soft Margin SVM extends SVMs to handle noisy or non-linearly separable data.
        \item Allows for some misclassifications, enhancing robustness and handling outliers effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Soft Margin SVM - Key Concepts}
    \begin{enumerate}
        \item **Margin**: 
        \begin{itemize}
            \item Aim is to maximize the distance between closest points (hard-margin case).
        \end{itemize}
        
        \item **Soft Margin**:
        \begin{itemize}
            \item Flexibility through allowing points within the margin or misclassifications.
            \item Controlled by the trade-off parameter **C**.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Soft Margin SVM - Trade-off Parameter C}
    \begin{block}{Definition of C}
        The parameter **C** determines the penalty for misclassification:
        \begin{itemize}
            \item Small value of **C**: Wider margin and more misclassifications.
            \item Large value of **C**: Prioritizes correct classification with tight margins.
        \end{itemize}
    \end{block}

    \begin{block}{Behavior of C}
        \begin{itemize}
            \item **C → 0**: Focus on maximizing the margin, allowing many misclassifications.
            \item **C → ∞**: Increases penalty for misclassification, leading to fewer mistakes but potentially narrowing the margin.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Soft Margin SVM - Mathematical Formulation}
    \begin{equation}
        \text{minimize} \quad \frac{1}{2} \| \mathbf{w} \|^2 + C \sum_{i=1}^{n} \xi_i
    \end{equation}
    
    Where:
    \begin{itemize}
        \item \( \mathbf{w} \) is the weight vector defining the hyperplane.
        \item \( \xi_i \) are slack variables for misclassified instances.
        \item \( C \) is the penalty term.
    \end{itemize}

    The constraints for classification are:
    \begin{equation}
        y_i(\mathbf{w} \cdot \mathbf{x_i} + b) \geq 1 - \xi_i, \quad \text{for all } i
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Soft Margin SVM - Practical Example}
    \begin{itemize}
        \item **With Hard Margin SVM**: 
        \begin{itemize}
            \item may struggle to find an optimal hyperplane with overlapping classes.
        \end{itemize}
        
        \item **With Soft Margin SVM (C=1)**:
        \begin{itemize}
            \item Can tolerate misclassifications.
            \item Balances margin and overall classification performance for a robust model.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Soft Margin SVM - Key Takeaways}
    \begin{itemize}
        \item Essential for handling real-world noisy data.
        \item The parameter **C** is crucial for model behavior regarding misclassifications.
        \item Understanding and tuning **C** can significantly improve model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Kernel Trick - Introduction}
    % Overview of the Kernel Trick
    \begin{itemize}
        \item The kernel trick is a technique used in Support Vector Machines (SVM) to operate in higher-dimensional space.
        \item It allows us to compute inner products in a transformed space without explicitly transforming the data.
        \item This method enables the SVM to handle non-linearly separable data effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Kernel Trick - Significance}
    % Importance of the Kernel Trick
    \begin{itemize}
        \item \textbf{Handling Non-Linearly Separable Data:} 
            \begin{itemize}
                \item Many datasets cannot be separated by a simple linear boundary.
                \item Higher-dimensional transformation helps in finding linear boundaries in new representations.
            \end{itemize}

        \item \textbf{Computational Efficiency:} 
            \begin{itemize}
                \item Direct mapping to high dimensions is computationally expensive.
                \item Kernel functions allow efficient calculations without high-dimensional coordinates.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Kernel Trick - How It Works}
    % Explanation of the Kernel Function
    \begin{itemize}
        \item \textbf{Kernel Function Definition:}  
            \begin{equation}
            K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)
            \end{equation}
            where $\phi$ is the mapping to higher-dimensional space.
        
        \item \textbf{SVM Optimization:} 
            \begin{itemize}
                \item The SVM minimizes the loss function based on margins using kernel functions.
                \item Actual transformation is unnecessary, saving computational resources.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Kernel Trick - Example}
    % Example of the Kernel Trick
    \begin{itemize}
        \item \textbf{Consider a Simple Scenario:} 
            \begin{itemize}
                \item Trying to separate blue and red points that form concentric circles.
                \item Linear SVM fails due to non-linearity.
                \item By applying a kernel (e.g., Gaussian kernel), we can create a feature space where points can be separated linearly.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Kernel Trick - Key Points}
    % Summary of Key Points
    \begin{itemize}
        \item The kernel trick enables SVMs to handle complex data distributions effectively.
        \item Provides a way to compute high-dimensional representations without explicit transformations.
        \item Different kernel functions can model various types of relationships in data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Kernel Trick - Common Kernels}
    % Overview of Common Kernels
    \begin{itemize}
        \item \textbf{Polynomial Kernel}
        \item \textbf{Gaussian (RBF) Kernel}
        \item \textbf{Sigmoid Kernel}
    \end{itemize}
    By understanding the kernel trick, we bridge the gap between linear separation capabilities of SVMs and the complexities of real-world, non-linear datasets.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Kernel Functions}
    \begin{block}{Overview of Kernel Functions}
        In Support Vector Machines (SVM), the choice of kernel function is crucial as it defines the decision boundary for classification. 
        A kernel function transforms the input data into a higher-dimensional space where it is easier to separate the classes with a hyperplane.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Kernel Functions - Part 1}
    \begin{block}{1. Polynomial Kernel}
        \textbf{Definition:}
        The polynomial kernel computes the similarity between two vectors in a feature space with polynomial interactions:
        \begin{equation}
        K(x, y) = (x \cdot y + c)^d
        \end{equation}
        where:
        \begin{itemize}
            \item \( x \) and \( y \) are input vectors,
            \item \( c \) is a constant term that controls the influence of higher-order terms,
            \item \( d \) is the degree of the polynomial.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        For \( d = 2 \) and \( c = 1 \):
        \begin{equation}
        K(x, y) = (x \cdot y + 1)^2
        \end{equation}
        This kernel captures interactions between input features, useful for non-linear relationships.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Kernel Functions - Part 2}
    \begin{block}{2. Gaussian (RBF) Kernel}
        \textbf{Definition:}
        The Radial Basis Function (RBF) kernel measures the distance between input points and is defined as:
        \begin{equation}
        K(x, y) = e^{-\frac{\| x - y \|^2}{2\sigma^2}}
        \end{equation}
        where \( \sigma \) is the bandwidth parameter that controls the kernel's spread.
        
        \textbf{Key Points:}
        \begin{itemize}
            \item Represents complex decision boundaries.
            \item Reacts to both distance and feature scale.
        \end{itemize}
        
        \textbf{Example:}
        If \( x \) and \( y \) are close, \( K(x, y) \) approaches 1. As distance increases, \( K(x, y) \) approaches 0.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Kernel Functions - Part 3}
    \begin{block}{3. Sigmoid Kernel}
        \textbf{Definition:}
        The sigmoid kernel resembles the activation function used in neural networks:
        \begin{equation}
        K(x, y) = \tanh(\alpha x \cdot y + c)
        \end{equation}
        where \( \alpha \) is the scaling factor and \( c \) is a constant that shifts the function.
        
        \textbf{Key Points:}
        \begin{itemize}
            \item Emulates neural network behavior.
            \item Sensitive to the parameters \( \alpha \) and \( c \).
        \end{itemize}
        
        \textbf{Example:}
        If \( \alpha = 0.1 \) and \( c = 1 \):
        \begin{equation}
        K(x, y) = \tanh(0.1(x \cdot y) + 1)
        \end{equation}
    \end{block}
    
    \begin{block}{Summary}
        \begin{itemize}
            \item Polynomial kernel captures polynomial relationships.
            \item Gaussian (RBF) kernel allows flexible decision boundaries based on distance.
            \item Sigmoid kernel mimics neural networks and provides non-linear separations.
        \end{itemize}
        Select the appropriate kernel based on dataset characteristics and desired model complexity.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SVM Algorithm Steps}
    \begin{block}{Overview}
        Step-by-step explanation of the SVM algorithm process from data input to model training and prediction.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 1: Data Input}
    \begin{itemize}
        \item Collect and prepare your dataset.
        \item Ensure pre-processing:
        \begin{itemize}
            \item Handle missing values.
            \item Encode categorical variables.
            \item Standardize numerical features.
        \end{itemize}
        \item \textbf{Example:} Convert height to meters and weight to kilograms if needed.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 2: Choose the Kernel Function}
    \begin{itemize}
        \item Select an appropriate kernel function to map the original feature space into a higher-dimensional space.
        \item Common kernel choices:
        \begin{itemize}
            \item \textbf{Linear Kernel:} For linearly separable data.
            \item \textbf{Polynomial Kernel:} For non-linear relationships.
            \item \textbf{Gaussian (RBF) Kernel:} For complex boundaries among data points.
        \end{itemize}
        \item \textbf{Key Point:} The choice of kernel impacts classification effectiveness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 3: Parameter Initialization}
    \begin{itemize}
        \item Specify hyperparameters for SVM:
        \begin{itemize}
            \item \textbf{C (Regularization Parameter):} Controls trade-off between margin maximization and classification error.
            \item \textbf{Gamma (for RBF kernel):} Influences the impact of a training example.
        \end{itemize}
        \item Formula for decision boundary:
        \begin{equation}
            f(x) = \sum_{i=1}^{n} \alpha_i y_i K(x_i, x) + b
        \end{equation}
        \item Where \( \alpha \) = Lagrange multipliers, \( y \) = class labels, \( K \) = kernel function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 4: Training the Model}
    \begin{itemize}
        \item Train the SVM with training data:
        \begin{itemize}
            \item Algorithm maximizes the margin between two classes.
        \end{itemize}
        \item Solves a convex optimization problem to minimize a cost function.
        \item \textbf{Illustration:} Visualize a 2D plot showing class separation and the optimal hyperplane.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 5: Prediction}
    \begin{itemize}
        \item Once trained, SVM classifies new data points.
        \item Use decision function to predict unseen instances using the same kernel.
        \item \textbf{Example:} A new observation in a certain region defined by the hyperplane is classified accordingly.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 6: Model Evaluation}
    \begin{itemize}
        \item Assess model performance:
        \begin{itemize}
            \item Metrics: Accuracy, precision, recall, F1 score.
            \item Cross-validation to ensure generalization.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item SVM effectively handles both linear and non-linear classification problems using kernel functions.
        \item Hyperparameter selection is crucial for optimal model performance.
        \item Regularization is important to prevent overfitting.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet for Python SVM}
    \begin{lstlisting}[language=Python]
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Load dataset
data = datasets.load_iris()
X = data.data
y = data.target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Initialize SVM classifier with RBF kernel
model = SVC(kernel='rbf', C=1.0, gamma='scale')

# Train model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate model
print(classification_report(y_test, predictions))
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Parameter Tuning in SVM - Overview}
    \begin{block}{Understanding Hyperparameters in Support Vector Machines (SVM)}
        The performance of an SVM model greatly depends on the right choice of hyperparameters. Key parameters include:
    \end{block}
    \begin{itemize}
        \item Kernel Choice
        \item C (Regularization Parameter)
        \item Gamma ($\gamma$)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Parameter Tuning in SVM - Key Hyperparameters}
    \begin{block}{Key Hyperparameters}
        \begin{enumerate}
            \item \textbf{Kernel Choice}:
                \begin{itemize}
                    \item \textbf{Linear Kernel}: \( K(x_i, x_j) = x_i^T x_j \) 
                    \item \textbf{Polynomial Kernel}: \( K(x_i, x_j) = (x_i^T x_j + c)^d \)
                    \item \textbf{RBF Kernel}: \( K(x_i, x_j) = e^{-\gamma \|x_i - x_j\|^2} \)
                \end{itemize}
            \item \textbf{C (Regularization Parameter)}:
                \begin{itemize}
                    \item Controls trade-off between margin maximization and classification error.
                    \item High C may lead to overfitting; Low C allows misclassifications for better generalization.
                \end{itemize}
            \item \textbf{Gamma ($\gamma$)}:
                \begin{itemize}
                    \item Influences the reach of a single training example.
                    \item Low $\gamma$ results in smoother boundaries; High $\gamma$ can lead to overfitting.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Parameter Tuning in SVM - Example and Conclusion}
    \begin{block}{Example Scenario}
        \begin{itemize}
            \item Dataset: Binary classification with linear and non-linear distributions.
            \begin{itemize}
                \item Linear Kernel with High C works for linearly separable data.
                \item RBF Kernel with High $\gamma$ may lead to overfitting in noisy datasets.
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Tuning hyperparameters in SVM is crucial for model performance. Balancing C, kernel type, and $\gamma$ can lead to robust models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Parameter Tuning in SVM - Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn import svm
from sklearn.model_selection import GridSearchCV

# Create a support vector classifier
svc = svm.SVC()

# Define parameter grid
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [0.001, 0.01, 0.1, 1],
    'kernel': ['linear', 'rbf', 'poly']
}

# Perform Grid Search
grid_search = GridSearchCV(svc, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Best parameters found
print(grid_search.best_params_)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for SVM - Introduction}
    Evaluating the performance of Support Vector Machines (SVM) is crucial to understanding how well your model is making predictions. The primary metrics used for evaluation include:
    \begin{itemize}
        \item \textbf{Accuracy}
        \item \textbf{Precision}
        \item \textbf{Recall}
        \item \textbf{F1 Score}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for SVM - Key Metrics}
    \textbf{Accuracy}:
    \begin{itemize}
        \item \textbf{Definition}: The ratio of correctly predicted instances to the total instances.
        \item \textbf{Formula}:  
        \[
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \]
    \end{itemize}
    
    \textbf{Precision}:
    \begin{itemize}
        \item \textbf{Definition}: Tells us how many of the predicted positive cases were actually positive.
        \item \textbf{Formula}:  
        \[
        \text{Precision} = \frac{TP}{TP + FP}
        \]
    \end{itemize}
    
    \textbf{Recall} (Sensitivity):
    \begin{itemize}
        \item \textbf{Definition}: Measures how many actual positive cases were captured by the model.
        \item \textbf{Formula}:  
        \[
        \text{Recall} = \frac{TP}{TP + FN}
        \]
    \end{itemize}
    
    \textbf{F1 Score}:
    \begin{itemize}
        \item \textbf{Definition}: The harmonic mean of precision and recall, useful for class imbalanced data.
        \item \textbf{Formula}:  
        \[
        \text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for SVM - Example Calculation}
    Assume we evaluated an SVM model with the following confusion matrix:

    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
        \hline
        \textbf{Actual Positive} & 50 (TP) & 10 (FN) \\
        \hline
        \textbf{Actual Negative} & 5 (FP) & 100 (TN) \\
        \hline
    \end{tabular}
    \end{center}

    From this matrix, we calculate:
    
    \begin{itemize}
        \item \textbf{Accuracy}:  
        \[
        \text{Accuracy} = \frac{50 + 100}{50 + 10 + 5 + 100} = \frac{150}{165} \approx 0.909 \ (90.9\%)
        \]
        
        \item \textbf{Precision}:  
        \[
        \text{Precision} = \frac{50}{50 + 5} = \frac{50}{55} \approx 0.909 \ (90.9\%)
        \]
        
        \item \textbf{Recall}:  
        \[
        \text{Recall} = \frac{50}{50 + 10} = \frac{50}{60} \approx 0.833 \ (83.3\%)
        \]

        \item \textbf{F1 Score}:  
        \[
        \text{F1} = 2 \times \frac{0.909 \times 0.833}{0.909 + 0.833} \approx 0.869 \ (86.9\%)
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for SVM - Cross-Validation Techniques}
    Cross-validation helps in assessing model robustness and avoiding overfitting. Techniques include:
    
    \begin{itemize}
        \item \textbf{K-Fold Cross-Validation}: Split data into 'k' subsets. Train on 'k-1' folds and validate on the last fold, averaging results for a robust estimate.
        
        \item \textbf{Stratified K-Fold}: A variation of K-Fold that maintains the proportion of classes in each fold, particularly helpful for imbalanced datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for SVM - Key Points to Remember}
    \begin{itemize}
        \item Use multiple evaluation metrics to get a comprehensive view of the model's performance.
        \item Precision and Recall are particularly important for imbalanced datasets.
        \item Cross-validation is vital for ensuring the model's generalizability.
    \end{itemize}
    
    Using these metrics will provide a holistic view of your SVM model's performance and help inform any necessary adjustments or improvements.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of SVM - Introduction}
    \begin{block}{Introduction to Support Vector Machines (SVM)}
        Support Vector Machines (SVM) are powerful supervised learning models utilized for classification and regression tasks. They find optimal hyperplanes to separate data points while maximizing the margin between different classes, making them highly effective across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of SVM - Text Classification}
    \begin{block}{Text Classification}
        \begin{itemize}
            \item \textbf{Definition}: Categorizing text into predefined categories.
            \item \textbf{Examples}:
            \begin{itemize}
                \item \textbf{Spam Detection}: Classifying emails as 'spam' or 'not spam' based on word frequency.
                \item \textbf{Sentiment Analysis}: Identifying sentiment in customer reviews (positive, negative, neutral).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of SVM - Image Recognition and Bioinformatics}
    \begin{block}{Image Recognition}
        \begin{itemize}
            \item \textbf{Definition}: Classifying images by learning to distinguish features associated with different classes.
            \item \textbf{Examples}:
            \begin{itemize}
                \item \textbf{Face Detection}: Identifying and locating human faces in images.
                \item \textbf{Object Recognition}: Distinguishing objects like cars and animals based on features.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Bioinformatics}
        \begin{itemize}
            \item \textbf{Definition}: Analyzing biological data and making predictions about genetic sequences or protein structures.
            \item \textbf{Examples}:
            \begin{itemize}
                \item \textbf{Gene Classification}: Predicting gene function by classifying DNA sequences.
                \item \textbf{Cancer Detection}: Classifying tumor cells as benign or malignant based on gene expression.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Flexibility}: Different kernel functions (linear, polynomial, RBF) allow handling of non-linear data.
            \item \textbf{Robustness}: Performs well in high-dimensional spaces; often cases where features exceed samples.
            \item \textbf{Generalization}: Focus on support vectors helps SVM generalize well to unseen data.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        SVMs are versatile across various applications, enhancing data-driven decision-making from natural language processing to medical diagnostics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet - SVM for Text Classification}
    \begin{lstlisting}[language=Python]
from sklearn import SVC
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import make_pipeline

# Example: Text Classification with SVM
data = [("I love this product!", 1), ("Worst purchase ever", 0)]
texts, labels = zip(*data)

# Create a pipeline that creates a vector representation and trains an SVM model
model = make_pipeline(CountVectorizer(), SVC(kernel='linear'))
model.fit(texts, labels)

# Predicting sentiment
prediction = model.predict(["This is a great service!"])
print(prediction)  # Output: [1]
    \end{lstlisting}
\end{frame}


\end{document}