\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 2: Supervised Learning Fundamentals}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction - Part 1}
    \begin{block}{Introduction}
        Supervised Learning is a fundamental aspect of machine learning that involves training models on labeled datasets. This chapter will equip you with the foundational concepts, techniques, and practical applications that are critical to understanding supervised learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction - Part 2}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Definition}: 
                Supervised learning refers to a class of algorithms that learn from labeled training data, where input data is paired with expected output labels.
            \item \textbf{Process}: 
                The process can be broken down into three main steps:
                \begin{itemize}
                    \item \textbf{Training}: Using a labeled dataset to train the model.
                    \item \textbf{Validation}: Assessing the model's performance on unseen data.
                    \item \textbf{Testing}: Evaluating the model's accuracy and performance.
                \end{itemize}
                
            \item \textbf{Types of Supervised Learning}:
                \begin{itemize}
                    \item \textbf{Classification}: Assigning categories to data points.
                    \item \textbf{Regression}: Predicting continuous output values.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction - Part 3}
    \begin{block}{Importance of Supervised Learning}
        \begin{itemize}
            \item Real-world applications in finance, healthcare, and marketing.
            \item A basis for further learning in advanced topics like unsupervised learning and deep learning.
        \end{itemize}
    \end{block}
    
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Classification Example}: Predicting loan default based on credit history and income.
            \item \textbf{Regression Example}: Forecasting future temperatures based on historical data.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The difference between classification and regression.
            \item The significance of labeled data in training.
            \item The iterative nature of developing a supervised learning model.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{block}{Supervised Learning Definition}
        Supervised learning is a type of machine learning where an algorithm is trained on labeled data. This involves providing the model with input-output pairs (features and target labels) so that it learns to predict output from new input data.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Point:} Labeled Data - Each training example includes both the input features (X) and the output label (Y).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Training & Evaluation}
    \begin{itemize}
        \item \textbf{Training Data vs. Test Data:}
        \begin{itemize}
            \item \textbf{Training Data:} The dataset used to train the model.
            \item \textbf{Test Data:} Separate data used to evaluate the model's performance after training.
        \end{itemize}

        \item \textbf{Model Evaluation Metrics:}
        \begin{itemize}
            \item \textbf{Accuracy:} The proportion of correct predictions made by the model.
            \item \textbf{Precision:} The ratio of true positive predictions to the sum of true positives and false positives.
            \item \textbf{Recall:} The ratio of true positive predictions to the sum of true positives and false negatives.
            \item \textbf{F1 Score:} The harmonic mean of precision and recall, useful when dealing with imbalanced datasets.
        \end{itemize}
        
        \item \textbf{Overfitting \& Underfitting:}
        \begin{itemize}
            \item \textbf{Overfitting:} The model learns the training data too well, leading to poor generalization.
            \item \textbf{Underfitting:} The model is too simple and fails to capture the underlying trend.
        \end{itemize}
        \item \textbf{Key Point:} Balancing the complexity of the model is crucial to ensure it generalizes well.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Supervised Learning Algorithms}
    \begin{itemize}
        \item \textbf{Linear Regression:} Predicting a continuous output. Example: Predicting house prices based on features like size and location.
        \item \textbf{Logistic Regression:} Used for binary classification tasks. Example: Spam detection in emails.
        \item \textbf{Decision Trees:} Model data using a tree-like structure. Example: Classifying types of flowers based on petal lengths and widths.
        \item \textbf{Support Vector Machines (SVM):} Classifies data by finding the best hyperplane that separates different classes.
        \item \textbf{Neural Networks:} Recognize patterns and used in tasks like image recognition and natural language processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Implementations}
    \begin{lstlisting}[language=Python]
# Example of a simple linear regression using Python's scikit-learn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Sample data (X: features, y: target)
X = [[1], [2], [3], [4], [5]]
y = [2, 3, 4, 5, 6]

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Predicting and evaluating
predictions = model.predict(X_test)
mse = mean_squared_error(y_test, predictions)

print("Mean Squared Error:", mse)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Discussion}
    \begin{itemize}
        \item Understanding the fundamentals of supervised learning is essential for building effective machine learning applications.
        \item Key concepts to remember include:
        \begin{itemize}
            \item Importance of labeled data
            \item Various evaluation metrics
            \item Balancing model complexity
            \item Algorithms available
        \end{itemize}
    \end{itemize}
    \begin{block}{Discussion Prompt}
        Consider a problem you would like to solve using supervised learning. What data would you need, and which algorithm would you choose?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary of Key Concepts in Supervised Learning}
    \begin{itemize}
        \item Supervised learning involves training models on labeled data.
        \item The process includes:
        \begin{itemize}
            \item Data Collection
            \item Training Phase
            \item Evaluation Phase
            \item Prediction Phase
        \end{itemize}
        \item Two major classifications: Regression and Classification.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Important Algorithms}
    \begin{itemize}
        \item \textbf{Linear Regression}: Assumes linear relationship.
        \begin{equation}
            \hat{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
        \end{equation}
        \item \textbf{Logistic Regression}: Used for binary classification, predicting probabilities.
        \item \textbf{Decision Trees}: Flowchart-like structure for classification and regression.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Evaluation Metrics and Final Thoughts}
    \begin{itemize}
        \item \textbf{Evaluation Metrics}:
        \begin{itemize}
            \item \textbf{Accuracy}:
            \begin{equation}
                \text{Accuracy} = \frac{\text{Correct predictions}}{\text{Total predictions}}
            \end{equation}
            \item \textbf{Confusion Matrix}: Evaluates classification performance.
        \end{itemize}
        \item \textbf{Key Points to Remember}:
        \begin{itemize}
            \item Relies on labeled data, distinct from unsupervised learning.
            \item Performance depends on data quality and quantity.
            \item Feature selection and preprocessing improve accuracy.
        \end{itemize}
        \item \textbf{Final Thoughts}: Mastery of these fundamentals is crucial for effective algorithm development.
    \end{itemize}
\end{frame}


\end{document}