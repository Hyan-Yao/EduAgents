\frametitle{Key Evaluation Metrics - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Resume numbering
        \item **Recall (Sensitivity)**
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted positive observations to all actual positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Example}: If there are 70 actual positives and the model identifies 50 correctly, recall = 50 / (50 + 20) = 0.714 or 71.4\%.
        \end{itemize}

        \item **F1 Score**
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of precision and recall, balancing both metrics.
            \item \textbf{Formula}:
            \begin{equation}
                \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Example}: If precision is 0.83 and recall is 0.71, F1 Score = 2 * (0.83 * 0.71) / (0.83 + 0.71) â‰ˆ 0.765 or 76.5\%.
        \end{itemize}

        \item **ROC Curve and AUC**
        \begin{itemize}
            \item \textbf{Definition}: The ROC curve plots the true positive rate against the false positive rate. AUC quantifies the overall performance.
            \item \textbf{Interpretation}: AUC value of 0.5 indicates no discrimination (random guessing) while 1.0 indicates perfect discrimination.
            \item \textbf{Example}: An AUC of 0.9 suggests excellent model performance.
        \end{itemize}
    \end{enumerate}
