\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title{Chapter 6: Evaluating Models}
\author[J. Smith]{John Smith, Ph.D.}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation - Overview}
    \begin{block}{Overview}
        Model evaluation is a critical step in the machine learning lifecycle. It allows us to measure how well our models predict or classify data by comparing their outputs against known outcomes.
    \end{block}
    \begin{block}{Importance}
        \begin{itemize}
            \item Performance Assessment: Evaluate model effectiveness before deployment.
            \item Comparative Analysis: Compare different models to select the best one.
            \item Model Improvement: Identify areas for tuning and enhancement.
            \item Trust and Reliability: Build confidence in model robustness.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation - Objectives}
    \begin{block}{Objectives of Model Evaluation}
        \begin{itemize}
            \item Establish Baselines: Define performance metrics that models must exceed.
            \item Identify Overfitting/Underfitting: Avoid complex or overly simple models.
            \item Select Evaluation Metrics: Choose metrics suited to the task type.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation - Example and Key Points}
    \begin{block}{Illustrative Example}
        Consider a model predicting loan defaults:
        \begin{itemize}
            \item Split data into training and testing sets.
            \item Evaluate predictions using unseen data.
            \item Use metrics like accuracy, precision, and recall.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Evaluation isn't just about accuracy; understanding various metrics is crucial.
            \item Different problems require different evaluation strategies (e.g., prioritize recall in healthcare).
            \item Continuous evaluation adapts models to new data, vital in dynamic environments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics}
    In machine learning, evaluation metrics are essential for assessing model performance, especially in classification tasks. We will explore:
    \begin{itemize}
        \item Accuracy
        \item Precision
        \item Recall
        \item F1-Score
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Evaluation Metrics}
    \begin{block}{Key Metrics}
        These metrics provide meaningful insights into how well a model is performing on a given task:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Accuracy}
    \begin{itemize}
        \item \textbf{Definition}: Proportion of correct predictions out of total predictions.
        \item \textbf{Formula}:
            \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives (TP)} + \text{True Negatives (TN)}}{\text{Total Predictions} (TP + TN + FP + FN)}
            \end{equation}
        \item \textbf{Example}: 
            If 80 predictions of cats are correct and 20 predictions of dogs are wrong, the accuracy is 
            \( \frac{80}{100} = 0.8 \) or 80\%.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Precision}
    \begin{itemize}
        \item \textbf{Definition}: Number of true positive predictions versus total positive predictions.
        \item \textbf{Formula}:
            \begin{equation}
            \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
            \end{equation}
        \item \textbf{Example}: 
            If a model predicts 70 cats (50 correct), precision is 
            \( \frac{50}{70} \approx 0.71 \) or 71\%.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Recall}
    \begin{itemize}
        \item \textbf{Definition}: Ability of the model to find all relevant cases (true positives).
        \item \textbf{Formula}:
            \begin{equation}
            \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
            \end{equation}
        \item \textbf{Example}: 
            If there are 100 cats and the model correctly identifies 50, recall is 
            \( \frac{50}{100} = 0.5 \) or 50\%.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. F1-Score}
    \begin{itemize}
        \item \textbf{Definition}: Harmonic mean of precision and recall, balancing both.
        \item \textbf{Formula}:
            \begin{equation}
            \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
        \item \textbf{Example}: 
            If precision is 0.71 and recall is 0.5, F1-score is 
            \( 2 \times \frac{0.71 \times 0.5}{0.71 + 0.5} \approx 0.58 \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Context Matters}: Choose the right metric based on the problem, e.g., medical diagnosis vs. spam detection.
        \item \textbf{Trade-offs}: High precision often means lower recall and vice versa. The F1-score helps in finding a balance.
        \item \textbf{Use Cases}: Precision is important when false positives are costly (e.g., fraud detection), recall is vital when false negatives are critical (e.g., disease screening).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Concluding Thought}
    Continuous evaluation of models using these metrics is essential for enhancing their effectiveness and ensuring they meet desired objectives in real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Overview}
    % Introduction to the confusion matrix in model evaluation
    
    A **confusion matrix** is a table used to evaluate the performance of a classification model. It summarizes the predictions made by the model against the actual values (ground truth) in a structured format. 
    \begin{itemize}
        \item Visualizes model performance per class
        \item Essential for model assessment
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Structure}
    % Overview of confusion matrix structure
    
    For a binary classification problem (e.g., spam detection), a confusion matrix is structured as follows:
    
    \begin{table}[h]
        \centering
        \begin{tabular}{|l|c|c|}
            \hline
            & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
            \hline
            \textbf{Actual Positive} & \text{TP (True Positive)} & \text{FN (False Negative)} \\
            \hline
            \textbf{Actual Negative} & \text{FP (False Positive)} & \text{TN (True Negative)} \\
            \hline
        \end{tabular}
    \end{table}
    
    \begin{itemize}
        \item **True Positive (TP)**: Correctly predicted positive
        \item **True Negative (TN)**: Correctly predicted negative
        \item **False Positive (FP)**: Incorrectly predicted positive
        \item **False Negative (FN)**: Incorrectly predicted negative
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Metrics}
    % Important metrics derived from the confusion matrix
    
    The confusion matrix allows us to derive several key metrics:
    
    \begin{itemize}
        \item **Accuracy**:
        \begin{equation}
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        
        \item **Precision**:
        \begin{equation}
            \text{Precision} = \frac{TP}{TP + FP}
        \end{equation}
        
        \item **Recall** (Sensitivity):
        \begin{equation}
            \text{Recall} = \frac{TP}{TP + FN}
        \end{equation}
        
        \item **F1 Score**:
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{itemize}
    
    Understanding these metrics guides model improvement and assessment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation Techniques}
    \begin{block}{Introduction to Cross-Validation}
        Cross-validation is a statistical method used to estimate the skill of machine learning models. It helps us understand how our model performs on unseen data by splitting our dataset into subsets, or folds, which provides a more accurate assessment of model efficacy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Cross-Validation?}
    \begin{itemize}
        \item \textbf{Robust Evaluation:} 
            \begin{itemize}
                \item Minimizes overfitting and underfitting by testing on various data subsets.
            \end{itemize}
        \item \textbf{Data Efficiency:} 
            \begin{itemize}
                \item Maximizes the use of available data by creating multiple training and testing datasets.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Cross-Validation Techniques}
    \begin{enumerate}
        \item \textbf{K-Fold Cross-Validation:}
            \begin{itemize}
                \item Split into K equal subsets. Train on K-1 folds, test on 1. Repeat K times.
                \item Example: For K=5 with 100 points, train on 80, test on 20, repeat 5 times.
            \end{itemize}
        \item \textbf{Stratified K-Fold Cross-Validation:}
            \begin{itemize}
                \item Similar to K-fold but preserves the percentage of samples per class.
                \item Example: For 90-10 class split, each fold maintains the same split.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Cross-Validation Techniques (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Leave-One-Out Cross-Validation (LOOCV):}
            \begin{itemize}
                \item Each observation is left out once as a validation set.
                \item Example: With 100 points, train on 99, test on the 1 left out.
            \end{itemize}
        \item \textbf{Randomized Cross-Validation:}
            \begin{itemize}
                \item Random samples selected for training/testing instead of strict folds.
                \item Example: Randomly choose 70\% for training, 30\% for testing multiple times.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices}
    \begin{itemize}
        \item \textbf{Choose an appropriate K:} A common choice is K=10.
        \item \textbf{Use stratified folds in classification tasks:} To handle class imbalances.
        \item \textbf{Check for data leakage:} Ensure testing sets remain unseen during training.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Thought-Provoking Questions}
    \begin{itemize}
        \item How does your choice of cross-validation technique impact the evaluation of your model?
        \item What real-world implications can arise from overfitting versus underfitting in your model's predictions?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Cross-validation is a vital technique for validating machine learning models. By rigorously testing our models, we can ensure reliability and accuracy when making predictions on new data. Embracing these techniques brings us closer to deploying effective models in real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting}
    % Introduction to the concepts of overfitting and underfitting in machine learning models.

    \begin{block}{Overview}
        Overfitting occurs when a model learns the training data too well, capturing noise and not generalizing to new data. In contrast, underfitting happens when a model is too simple to capture the data's underlying patterns. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Overfitting}

    \begin{itemize}
        \item \textbf{Definition:} 
        Overfitting means a model performs well on training data but poorly on unseen data.
        \item \textbf{Analogy:} 
        Like a student who memorizes answers rather than understanding concepts.
        \item \textbf{Key Indicators:}
        \begin{itemize}
            \item High training accuracy.
            \item Low validation/test accuracy.
        \end{itemize}
    \end{itemize}

    \begin{block}{Example of Overfitting}
        A model trained on cat and dog images that concentrates on specific features like fur patterns will struggle with new images of cats and dogs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Underfitting}

    \begin{itemize}
        \item \textbf{Definition:} 
        Underfitting occurs when a model is too simplistic to capture underlying data patterns.
        \item \textbf{Analogy:} 
        Like a student studying only a small portion of a subject.
        \item \textbf{Key Indicators:}
        \begin{itemize}
            \item Low accuracy on both training and validation/test data.
        \end{itemize}
    \end{itemize}

    \begin{block}{Example of Underfitting}
        A linear regression model predicting a nonlinear relationship would misrepresent the data if it fits a straight line to a complex curve.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Balancing Model Performance}

    \begin{itemize}
        \item \textbf{Visualizing Performance:} 
        Learning curves help assess model performance, plotting training and validation accuracy/loss over epochs.
        
        \item \textbf{Strategies to Combat Overfitting:}
        \begin{itemize}
            \item Use regularization (e.g., L1 and L2).
            \item Pruning in decision trees.
            \item Employ cross-validation.
        \end{itemize}
        
        \item \textbf{Strategies to Combat Underfitting:}
        \begin{itemize}
            \item Increase model complexity through additional features or algorithms.
            \item Reduce regularization.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Conclusion}
        Strive for a model that generalizes well, balancing complexity and simplicity for effective evaluation and selection.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}

    \begin{lstlisting}[language=Python]
# Example: Model evaluation with a decision tree classifier
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load your dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = DecisionTreeClassifier(max_depth=3)  # Control overfitting/underfitting
model.fit(X_train, y_train)

# Evaluate the model
train_preds = model.predict(X_train)
test_preds = model.predict(X_test)

print(f'Training Accuracy: {accuracy_score(y_train, train_preds):.2f}')
print(f'Test Accuracy: {accuracy_score(y_test, test_preds):.2f}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Strategies - Introduction}
    \begin{block}{Introduction to Model Selection}
        In data science, selecting the right model is crucial for accurate predictions and insights. Various strategies help evaluate which model performs best based on its intended use and the data at hand. Understanding the balance between complexity and performance is essential.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Strategies - Evaluation Criteria}
    \begin{block}{Key Evaluation Criteria}
        When selecting a model, consider the following criteria:
        \begin{enumerate}
            \item \textbf{Accuracy}: 
            \begin{equation}
                \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}}
            \end{equation}
            \item \textbf{Precision}:
            \begin{equation}
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Recall}:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{F1 Score}:
            \begin{equation}
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Strategies - Strategies}
    \begin{block}{Strategies for Model Selection}
        Consider these methods to enhance your model selection process:
        \begin{itemize}
            \item \textbf{Cross-Validation}: Split data into subsets; train on one and validate on another to ensure generalizability.
            \item \textbf{Grid Search and Random Search}: Techniques for hyperparameter tuning to find the best model configurations efficiently.
            \item \textbf{Ensemble Methods}: Utilize approaches like AdaBoost and Bagging to improve model performance by combining multiple algorithms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Strategies - Context and Conclusion}
    \begin{block}{Importance of Context and Application}
        The best model depends on:
        \begin{itemize}
            \item \textbf{Real-World Applicability}: Is the model usable in practice?
            \item \textbf{Domain Constraints}: Is interpretability required (e.g., healthcare vs finance)?
            \item \textbf{Volume of Data}: Some models excel in larger datasets.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Effective model selection balances metrics and application context, leading to informed decisions that boost predictive modeling efforts.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Strategies - Key Points Recap}
    \begin{block}{Key Points Recap}
        \begin{itemize}
            \item Focus on accuracy, precision, recall, and F1 Score as metrics.
            \item Use cross-validation for robust model evaluation.
            \item Incorporate ensemble methods for improved performance.
            \item Consider domain context in your model choice.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Context in Evaluation}
    \begin{block}{Summary}
        Evaluating models requires a thorough understanding of their context, including the application, dataset characteristics, user needs, and the operating environment. Proper evaluation aligns metrics with the specific goals and conditions of the model's deployment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Context in Model Evaluation}
    \begin{itemize}
        \item Evaluating models involves more than just accuracy or error rates.
        \item Context factors include:
        \begin{itemize}
            \item Real-world application
            \item Dataset characteristics
            \item User needs
            \item Environment of operation
        \end{itemize}
        \item Ensures models are effective and relevant.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points in Evaluation}
    \begin{enumerate}
        \item \textbf{Define the Objective:} Clarify the model's purpose (e.g., sentiment analysis, fraud detection).
        \item \textbf{Nature of the Data:} Consider data distribution, static vs dynamic data, and presence of outliers.
        \item \textbf{Stakeholder Needs:} Identify end user perspectives and prioritize relevant metrics.
        \item \textbf{Environment Considerations:} Determine if the model is for real-time or batch processing and adjust evaluation metrics accordingly.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example: Credit Scoring Model}
    \begin{itemize}
        \item \textbf{Objective:} Predict likelihood of borrower defaulting on a loan.
        \item \textbf{Data:} Historical loan data including features like credit history, income, and employment status.
        \item \textbf{Stakeholder Needs:} Minimize risk by prioritizing precision and recall due to implications of false negatives.
    \end{itemize}
    \begin{block}{Conclusion}
        When evaluating such models, consider economic and societal implications for better outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    To effectively evaluate a model:
    \begin{itemize}
        \item Align evaluation metrics with intended use and context.
        \item Stay adaptable; context can change over time.
        \item Engage stakeholders to identify critical metrics.
    \end{itemize}
    \begin{block}{Impact}
        Recognizing the importance of context leads to more robust, effective models, enhancing user satisfaction and real-world applicability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Case Studies}
    % Understanding model evaluation through examples

    \begin{block}{Significance of Model Evaluation}
        Model evaluation is crucial for ensuring predictive models are effective and reliable in real-world scenarios. 
        This presentation showcases successful real-world case studies that highlight the importance of thorough model evaluation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Predicting Hospital Readmissions}
    
    \begin{itemize}
        \item \textbf{Context}: A healthcare provider aimed to reduce readmissions for chronic heart failure patients.
        
        \item \textbf{Model Used}: Logistic regression model to predict 30-day readmission likelihood.
        
        \item \textbf{Evaluation Approach}:
        \begin{itemize}
            \item Metrics: Accuracy, precision, recall, F1 score.
            \item Cross-validation: k-fold cross-validation to ensure robustness.
        \end{itemize}
        
        \item \textbf{Outcome}:
        \begin{itemize}
            \item Identified high-risk patients leading to targeted interventions.
            \item Achieved a 15\% reduction in readmission rates.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Credit Scoring for Loan Approvals}
    
    \begin{itemize}
        \item \textbf{Context}: A bank wanted to improve loan approval processes while minimizing defaults.
        
        \item \textbf{Model Used}: Random forest classifier to manage non-linear relationships effectively.
        
        \item \textbf{Evaluation Approach}:
        \begin{itemize}
            \item Metrics: ROC-AUC curve for performance measurement.
            \item Threshold Adjustment: Balanced false positives and approvals for good candidates.
        \end{itemize}
        
        \item \textbf{Outcome}:
        \begin{itemize}
            \item Improved loan processing by 25\% while keeping default rates below 5\%.
            \item Enhanced customer satisfaction through quicker decisions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{enumerate}
        \item \textbf{Importance of Context}: A model's evaluation is dependent on understanding the specific context of its application.
        
        \item \textbf{Diverse Metrics}: Selection of evaluation metrics should reflect the model's objectives and impacts.
        
        \item \textbf{Iterative Evaluation}: Iteration is essential for model improvement based on evaluation findings.
        
        \item \textbf{Stakeholder Engagement}: Engaging stakeholders is crucial to ensure practical applicability and address concerns.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    \begin{block}{Final Thoughts}
        These case studies show that effective model evaluations not only enhance predictive power but also lead to significant advancements across various sectors, from healthcare to finance.
        Consider the implications of our models and the assessment of their value in real-world applications.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Ethical Considerations in Evaluating Machine Learning Models}
    \begin{itemize}
        \item Ethical considerations are crucial when evaluating machine learning models.
        \item Understanding bias and fairness contributes to reliable outcomes.
        \item Key concepts include:
        \begin{itemize}
            \item Bias in Machine Learning
            \item Fairness in Model Evaluation
            \item Metrics for Evaluating Fairness and Bias
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Bias in Machine Learning Models}
    \begin{block}{Definition}
        Bias refers to systematic errors in predictions that may lead to unfair treatment of certain groups.
    \end{block}
    \begin{itemize}
        \item Sources of Bias:
        \begin{itemize}
            \item Data Bias (e.g., underrepresentation)
            \item Algorithm Bias (e.g., favoring specific outcomes)
        \end{itemize}
        \item \textbf{Example:} A facial recognition system may inaccurately identify darker-skinned individuals if trained primarily on lighter-skinned images.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Fairness in Model Evaluation}
    \begin{block}{Definition}
        Fairness means that a model should perform well across different demographic groups.
    \end{block}
    \begin{itemize}
        \item Types of Fairness:
        \begin{itemize}
            \item Demographic Parity: Equal positive outcomes across diverse groups.
            \item Equal Opportunity: Equal chances of favorable outcomes once eligible for selection.
        \end{itemize}
        \item \textbf{Example:} A hiring algorithm favoring specific demographics may raise fairness concerns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Fairness and Bias}
    \begin{itemize}
        \item Key Metrics for Evaluation:
        \begin{itemize}
            \item Confusion Matrix: Analyze outcomes across groups.
            \item Statistical Parity Difference: Selection rate differences between groups.
            \item Equalized Odds: Equal true positive and false positive rates across groups.
        \end{itemize}
    \end{itemize}
    \begin{block}{Example Code: Statistical Parity}
        \begin{lstlisting}[language=Python]
def statistical_parity(selected_group, total_group):
    return (selected_group / total_group) * 100

female_selected = 70
male_selected = 30
female_total = 100
male_total = 100

female_parity = statistical_parity(female_selected, female_total)
male_parity = statistical_parity(male_selected, male_total)

print(f"Female Selection Rate: {female_parity}%")
print(f"Male Selection Rate: {male_parity}%")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Ethical evaluation of models is vital for societal trust.
        \item Aim to identify and mitigate bias throughout the evaluation process.
        \item The impact of bias can perpetuate inequalities.
    \end{itemize}
    \begin{block}{Conclusion}
        Incorporating ethical considerations is essential for building trustworthy AI systems. Prioritize fairness and minimize bias for equitable solutions.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Discussion Questions}
    \begin{enumerate}
        \item How can we actively identify bias in our datasets?
        \item What measures can enhance fairness in model outcomes?
        \item Can fairness and accuracy always coexist in model evaluation?
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Key Points}
    \begin{block}{Model Evaluation Definition}
    Model evaluation is the process of assessing a machine learning model's performance and effectiveness using various metrics and methods. It's critical for ensuring models perform well on unseen data.
    \end{block}

    \begin{itemize}
        \item \textbf{Performance Metrics:}
        \begin{itemize}
            \item Accuracy, Precision, and Recall
            \item AUC-ROC for model comparisons
        \end{itemize}
        
        \item \textbf{Overfitting vs. Underfitting:}
        \begin{itemize}
            \item Overfitting: Capturing noise instead of trends
            \item Underfitting: Too simple to grasp data's complexity
        \end{itemize}
        
        \item \textbf{Cross-Validation:}
        Uses data subsets for robust performance evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Ethical Considerations}
    \begin{itemize}
        \item \textbf{Ethical Considerations:}
        \begin{itemize}
            \item Assessing models for bias and fairness
            \item Example: Hiring algorithms favoring certain demographics
        \end{itemize}
        
        \item \textbf{Final Thoughts:}
        \begin{itemize}
            \item Model evaluation is a continuous process.
            \item Results interpretation is crucial for decision-making.
            \item Emerging technologies require adaptable evaluation strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Reflective Questions}
    \begin{block}{Reflective Questions}
        \begin{enumerate}
            \item How might biases in training data affect model outcomes, and what steps can be taken to mitigate them?
            \item What combination of metrics would you choose to evaluate a model in specific applications, like healthcare or finance?
            \item How can iterative evaluation improve the performance and reliability of machine learning models?
        \end{enumerate}
    \end{block}

    \begin{block}{Key Takeaway}
        Effective model evaluation is fundamental not only for technical development but also for aligning machine learning initiatives with ethical standards. Comprehensive evaluation strategies foster trust and efficacy in machine learning solutions.
    \end{block}
\end{frame}


\end{document}