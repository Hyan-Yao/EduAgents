\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{Overview of Data Preprocessing in Data Mining}
        Data preprocessing is a crucial step that serves as the foundation for extracting meaningful patterns and insights from raw data. 
        In this section, we explore its importance, challenges, and impact on the quality of data analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Data Preprocessing?}
    \begin{itemize}
        \item A series of steps performed on raw data to prepare it for analysis.
        \item Involves:
        \begin{itemize}
            \item Cleaning
            \item Transforming
            \item Organizing
        \end{itemize}
        \item Ensures data is accurate, consistent, and usable.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Improves Data Quality}
        \begin{itemize}
            \item Addresses errors, duplicates, missing values, and outliers.
            \item \textit{Example:} Duplicate entries in customer reviews can skew sentiment analysis.
        \end{itemize}
        
        \item \textbf{Enhances Model Performance}
        \begin{itemize}
            \item Proper preprocessing leads to better prediction accuracy.
            \item \textit{Illustration:} Clean data yields a more effective machine learning model.
        \end{itemize}
        
        \item \textbf{Facilitates Data Integration}
        \begin{itemize}
            \item Merges datasets from multiple sources in compatible formats.
            \item \textit{Example:} Standardizing currency formats for regional sales data.
        \end{itemize}
        
        \item \textbf{Reduces Complexity}
        \begin{itemize}
            \item Simplifying data allows for clearer analysis.
            \item \textit{Key Point:} Techniques like PCA condense data while preserving essential information.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Data Preprocessing}
    \begin{itemize}
        \item \textbf{Data Cleaning:} 
        \begin{itemize}
            \item Identifying and rectifying inaccuracies or inconsistencies.
        \end{itemize}
        
        \item \textbf{Data Transformation:}
        \begin{itemize}
            \item Converting data into suitable formats, normalizing, and scaling.
        \end{itemize}
        
        \item \textbf{Data Reduction:}
        \begin{itemize}
            \item Reducing data size through feature selection or aggregation.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Data preprocessing is vital for ensuring high-quality data suitable for analysis. 
        The steps taken during this phase significantly influence insights and the effectiveness of predictive modeling.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Cleaning - Introduction}
    \begin{block}{Definition}
        Data cleaning is a crucial step in the data preprocessing stage of data mining. It involves identifying and correcting errors or inconsistencies in datasets to improve data quality, enhancing the accuracy of analyses and insights derived from the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Cleaning - Understanding Data Errors}
    \begin{itemize}
        \item \textbf{Human Entry Errors}: Mistakes made while inputting data (e.g., typos, incorrect formats).
        \item \textbf{Systematic Errors}: Issues arising from the collection process (e.g., faulty sensors or systems).
        \item \textbf{Missing Data}: Absence of values due to lost information or incomplete data entry.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Cleaning - Techniques}
    \begin{enumerate}
        \item \textbf{Removing Duplicates}
            \begin{itemize}
                \item Identify and eliminate duplicate entries in the dataset to ensure each record is unique.
            \end{itemize}
        \item \textbf{Handling Missing Values}
            \begin{itemize}
                \item Address missing values using strategies such as imputation or removal of incomplete records.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Data Cleaning - Code Examples}
    \begin{block}{Removing Duplicates}
        \begin{lstlisting}[language=Python]
import pandas as pd

# Load data
data = pd.read_csv('customers.csv')
# Remove duplicates
data_cleaned = data.drop_duplicates()
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Handling Missing Values}
        \begin{lstlisting}[language=Python]
# Fill missing values with mean
data['age'].fillna(data['age'].mean(), inplace=True)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Cleaning - More Techniques}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Correcting Data Types}
            \begin{itemize}
                \item Ensure that data types are appropriate for intended analyses.
            \end{itemize}
        \item \textbf{Standardizing Formats}
            \begin{itemize}
                \item Make data consistent in terms of units or formats.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Data Cleaning - More Code Examples}
    \begin{block}{Correcting Data Types}
        \begin{lstlisting}[language=Python]
# Convert string to datetime
data['date'] = pd.to_datetime(data['date'], errors='coerce')
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Standardizing Formats}
        \begin{lstlisting}[language=Python]
# A function to standardize phone numbers
def standardize_phone(phone):
    return re.sub('[^0-9]', '', phone)  # Keep only digits

data['phone'] = data['phone'].apply(standardize_phone)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Cleaning - Validation and Conclusion}
    \begin{itemize}
        \item \textbf{Validating Data}: Check for accuracy and adherence to defined business rules.
        \item \textbf{Key Points to Remember}:
            \begin{itemize}
                \item Quality data is essential for better insights and decision-making.
                \item Data cleaning is often an iterative process.
                \item Maintain documentation of the cleaning processes for transparency.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation - Overview}
    \begin{block}{What is Data Transformation?}
        Data transformation is the process of converting data into a format suitable for analysis. This step is crucial in data preprocessing, as the format and scale of data can significantly impact analysis outcomes.
    \end{block}
    
    \begin{block}{Importance of Data Transformation}
        \begin{itemize}
            \item \textbf{Enhances Accuracy:} Leads to improved accuracy in predictive modeling.
            \item \textbf{Improves Model Performance:} Contributes to better convergence rates for optimization algorithms.
            \item \textbf{Facilitates Comparison:} Allows easier comparison between variables on a similar scale.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation - Methods}
    \begin{block}{Common Methods}
        \begin{enumerate}
            \item \textbf{Normalization}
                \begin{itemize}
                    \item \textbf{Definition:} Scales data to fit within a specific range, typically 0 to 1 or -1 to 1.
                    \item \textbf{When to Use:} Ideal for datasets that do not follow a Gaussian distribution.
                    \item \textbf{Formula:}
                    \begin{equation}
                    X' = \frac{X - \text{min}(X)}{\text{max}(X) - \text{min}(X)}
                    \end{equation}
                    \item \textbf{Example:} A score of 75 from a range of 50 to 100 becomes:
                    \begin{equation}
                    75' = \frac{75-50}{100-50} = 0.5
                    \end{equation}
                \end{itemize}
                
            \item \textbf{Scaling (Standardization)}
                \begin{itemize}
                    \item \textbf{Definition:} Transforms data to have a mean of 0 and standard deviation of 1.
                    \item \textbf{When to Use:} Useful for normally distributed data.
                    \item \textbf{Formula:}
                    \begin{equation}
                    X' = \frac{X - \mu}{\sigma}
                    \end{equation}
                    where \( \mu \) is the mean and \( \sigma \) is the standard deviation.
                    \item \textbf{Example:} A value of 120 with mean 100 and SD 15 transforms as:
                    \begin{equation}
                    120' = \frac{120 - 100}{15} = 1.33
                    \end{equation}
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation - Key Points and Code}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Choosing the Right Method:} Depends on data distribution and analysis requirements.
            \item \textbf{Impact on Machine Learning:} Algorithms based on distance metrics require normalized or scaled data.
            \item \textbf{Implementing in Python:} Libraries like \texttt{scikit-learn} offer easy functions for these transformations.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Code Snippet (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Sample Data
data = [[50], [75], [100], [200]]

# Normalization
scaler_norm = MinMaxScaler()
normalized_data = scaler_norm.fit_transform(data)

# Standardization
scaler_std = StandardScaler()
standardized_data = scaler_std.fit_transform(data)
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Conclusion}
        Data transformation is essential in preprocessing, influencing analytical outcomes and model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - Introduction}
    \begin{block}{Overview}
        Data Reduction refers to the process of reducing the volume of a dataset while producing the same or similar analytical results. This is crucial in data preprocessing as it helps manage large datasets while still retaining their most significant characteristics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - Importance}
    \begin{itemize}
        \item \textbf{Efficiency}: Smaller datasets enable faster processing and analysis.
        \item \textbf{Storage}: Reduces storage costs and resource usage.
        \item \textbf{Noise Reduction}: Helps in eliminating redundant data, enhancing model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - Overview}
    \begin{enumerate}
        \item Dimensionality Reduction
        \item Feature Selection
        \item Sampling
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - Dimensionality Reduction}
    \begin{block}{Definition}
        Reduces the number of features (dimensions) in a dataset while preserving essential relationships.
    \end{block}
    \begin{itemize}
        \item \textbf{Principal Component Analysis (PCA)}:
            \begin{itemize}
                \item Transforms original features into principal components which are linear combinations of the original features.
                \item Formula for PCA:
                \begin{equation}
                    Z = XW
                \end{equation}
                where \( Z \) is the new feature set, \( X \) is the original data matrix, and \( W \) is the matrix of eigenvectors.
            \end{itemize}
        \item \textbf{t-SNE (t-distributed Stochastic Neighbor Embedding)}:
            \begin{itemize}
                \item A non-linear technique ideal for visualizing high-dimensional datasets in a lower-dimensional space (typically 2D).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - Feature Selection and Sampling}
    \begin{itemize}
        \item \textbf{Feature Selection}:
            \begin{itemize}
                \item Involves selecting a subset of relevant features for use in model construction.
                \item \textbf{Methods}:
                \begin{itemize}
                    \item Filter Methods: Use statistical tests (e.g., Chi-square test).
                    \item Wrapper Methods: Use a predictive model to score feature subsets (e.g., Recursive Feature Elimination).
                    \item Embedded Methods: Incorporate feature selection as part of model training (e.g., Lasso regression).
                \end{itemize}
            \end{itemize}
        \item \textbf{Sampling}:
            \begin{itemize}
                \item Reduces dataset size by selecting a representative subset of the data.
                \item \textbf{Types of Sampling}:
                \begin{itemize}
                    \item Random Sampling: Selects data points randomly.
                    \item Stratified Sampling: Ensures specific sub-groups (strata) are represented.
                \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - Key Points}
    \begin{itemize}
        \item Techniques are essential for improving computational efficiency while preserving important data characteristics.
        \item Dimensionality Reduction and Feature Selection are the most commonly used techniques.
        \item Choosing the right technique depends on the dataset and the specific objectives of the analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - PCA Example}
    \begin{block}{Illustration}
        Imagine a cloud of points in a 3D space that represents complex data. PCA helps reduce this space to 2D while retaining the points' distribution, effectively cutting out excess data while maintaining the essence of the data intact.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Reduction Techniques - Conclusion}
    \begin{block}{Conclusion}
        Data Reduction Techniques are fundamental in preprocessing steps to handle large datasets effectively. Understanding and applying these methods can drastically enhance data analysis capabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data - Introduction}
    \begin{block}{Introduction to Missing Data}
        Missing data occurs when values are absent in a dataset. Incomplete data can lead to biased analysis or unreliable models, making handling missing values crucial.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data - Key Concepts}
    \begin{itemize}
        \item \textbf{Types of Missing Data:}
            \begin{enumerate}
                \item \textbf{Missing Completely at Random (MCAR):} The absence of a value is independent of any other data.
                \item \textbf{Missing at Random (MAR):} The missingness is related to the observed data but not to the missing data itself.
                \item \textbf{Not Missing at Random (NMAR):} The missingness is related to the missing data (e.g., high-income earners may not disclose their income).
            \end{enumerate}
        \item \textbf{Impact of Missing Data:}
            \begin{itemize}
                \item Introduces bias.
                \item Decreases statistical power.
                \item Reduces the validity of conclusions.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Handling Missing Data - Strategies}
    \begin{block}{Strategies for Handling Missing Data}
        \begin{enumerate}
            \item \textbf{Deletion Methods:}
                \begin{itemize}
                    \item \textbf{Listwise Deletion:} Remove any records with missing values.
                    \item \textbf{Pairwise Deletion:} Uses all available data without removing entire records.
                \end{itemize}
            \item \textbf{Imputation Methods:}
                \begin{itemize}
                    \item \textbf{Mean/Median/Mode Imputation:} Replace missing values with relevant statistics.
                    \item \textbf{K-Nearest Neighbors (KNN):} Impute using values from similar entries.
                    \begin{lstlisting}[language=Python]
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=3)
imputed_data = imputer.fit_transform(data_with_missing_values)
                    \end{lstlisting}
                    \item \textbf{Multiple Imputation:} Create multiple datasets to account for uncertainty.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data - Conclusion and Key Points}
    \begin{itemize}
        \item Understand the type of missing data before selecting a strategy.
        \item Deletion methods can lead to loss of information while imputation can introduce bias if not done correctly.
        \item Consider the nature and impact of missing values on your analysis goals.
        \item Documentation of how missing data is handled is essential for reproducibility.
    \end{itemize}
    \begin{block}{Conclusion}
        Effectively handling missing data is a fundamental step in data preprocessing, aiming to maintain integrity while allowing for meaningful analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Encoding - Overview}
    \frametitle{Data Encoding}
    
    \begin{itemize}
        \item Data encoding is essential for preparing categorical variables for machine learning algorithms.
        \item Many algorithms require numerical input, making encoding crucial.
        \item Proper encoding can enhance model performance by creating meaningful representations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Encoding - Techniques}
    \frametitle{Common Encoding Techniques}

    \begin{enumerate}
        \item \textbf{Label Encoding}
            \begin{itemize}
                \item Converts categories into unique integers.
                \item \textit{Example:} Color → Integer: Red → 0, Green → 1, Blue → 2.
                \item Best for ordinal categories.
            \end{itemize}
        
        \item \textbf{One-Hot Encoding}
            \begin{itemize}
                \item Creates binary columns for each category.
                \item \textit{Example:}
                \begin{verbatim}
                Color_Red Color_Green Color_Blue
                     1           0           0
                     0           1           0
                     0           0           1
                \end{verbatim}
                \item Ideal for nominal categories.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Data Encoding - Advanced Techniques}
    \frametitle{Advanced Encoding Techniques}

    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Binary Encoding}
            \begin{itemize}
                \item Combines label and one-hot encoding.
                \item Categories are converted to binary code.
                \item \textit{Example:} 
                \begin{verbatim}
                Color → Binary Code
                Red   → 00
                Green → 01
                Blue  → 10
                \end{verbatim}
            \end{itemize}
        
        \item \textbf{Target Encoding}
            \begin{itemize}
                \item Replaces categories with the mean/median of the target variable for that category.
                \item \textit{Example:} 
                \begin{verbatim}
                Color  | Sales | Target Encoding
                -------------------------------
                Red    | 200   | 250
                Green  | 300   | 300
                Blue   | 250   | 275
                \end{verbatim}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Data Encoding - Key Takeaways}
    \frametitle{Key Points to Remember}
    
    \begin{itemize}
        \item Choose encoding based on the type of categorical variable (ordinal vs. nominal).
        \item Avoid over-encoding to prevent the curse of dimensionality.
        \item Encoding is crucial in the preprocessing pipeline and impacts model effectiveness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Encoding - Practical Implementation}
    \frametitle{Practical Implementation}

    Below is a simple code snippet using \texttt{pandas} for one-hot encoding:

    \begin{lstlisting}[language=Python]
import pandas as pd

# Sample DataFrame
data = {'Color': ['Red', 'Green', 'Blue', 'Green']}
df = pd.DataFrame(data)

# One-hot Encoding
encoded_df = pd.get_dummies(df, columns=['Color'], prefix='Color')
print(encoded_df)
    \end{lstlisting}

    \textit{Expected Output:}
    \begin{verbatim}
   Color_Blue  Color_Green  Color_Red
0           0            0          1
1           0            1          0
2           1            0          0
3           0            1          0
    \end{verbatim}
\end{frame}

\begin{frame}[fragile]{Data Integration - Introduction}
    \begin{block}{What is Data Integration?}
        Data Integration is the process of combining data from different sources to create a unified dataset. This is essential in data preprocessing as it ensures that the data used in analysis and model building is complete and consistent.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Integration - Importance}
    \begin{itemize}
        \item \textbf{Comprehensive Insights:} Merging diverse datasets provides a holistic view, enabling better analysis and decision-making.
        \item \textbf{Data Completeness:} It fills in gaps by utilizing information from various databases.
        \item \textbf{Consistency:} Harmonizes different data formats, structures, and interpretations across sources.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Integration - Common Methods}
    \begin{itemize}
        \item \textbf{Manual Integration:} Combining data by hand, often suited for small datasets. 
        \item \textbf{ETL (Extract, Transform, Load):} 
        \begin{itemize}
            \item \textbf{Extract:} Data from various sources (databases, APIs, flat files).
            \item \textbf{Transform:} Ensure compatibility (normalization, data type conversion).
            \item \textbf{Load:} Into a destination database.
        \end{itemize}
        \item \textbf{Data Warehousing:} Centralizing data using technologies like Amazon Redshift, Google BigQuery, or Snowflake.
        \item \textbf{APIs:} Automating data collection (e.g., REST APIs for web services).
        \item \textbf{Data Lakes:} Storing raw and unstructured data for exploratory analysis (e.g., Hadoop, Amazon S3).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Integration - Challenges}
    \begin{itemize}
        \item \textbf{Data Quality:} Inconsistent formatting or missing values.
            \begin{itemize}
                \item \textbf{Solution:} Implement data cleaning to standardize formats.
            \end{itemize}
        \item \textbf{Schema Mismatch:} Compatibility issues due to different data models. 
            \begin{itemize}
                \item \textbf{Solution:} Develop a unified schema.
            \end{itemize}
        \item \textbf{Handling Duplicates:} Merging can lead to duplicate entries.
            \begin{itemize}
                \item \textbf{Solution:} Use deduplication techniques (e.g., clustering or hashing).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Integration - Key Points}
    \begin{itemize}
        \item Assess data quality and structure before integration.
        \item Use automation (ETL tools, APIs) to save time and reduce errors.
        \item Continuously monitor integrated datasets for accuracy and consistency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Integration - Example Illustration}
    \begin{block}{Integration Process using ETL}
        Consider two datasets:
        \begin{itemize}
            \item A customer database (names and contact information in CSV format).
            \item A purchase history database (transaction details in a SQL database).
        \end{itemize}
        
        \textbf{Steps:}
        \begin{enumerate}
            \item \textbf{Extract:} Read customer data from CSV.
            \begin{lstlisting}[language=python]
import pandas as pd
customer_data = pd.read_csv('customers.csv')
            \end{lstlisting}
            \item \textbf{Transform:} Ensure consistent date formats.
            \item \textbf{Load:} Into a unified database for analysis.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Integration - Conclusion}
    Effective data integration is a cornerstone of successful data analysis and machine learning. By using the right methods and addressing challenges thoughtfully, we can create cohesive datasets that drive insights and improve decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Preprocessing - Overview}
    \begin{itemize}
        \item Data preprocessing is a crucial step in the machine learning pipeline.
        \item Significantly influences model performance.
        \item Summary of key practices to ensure data is clean, formatted, and ready for analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Preprocessing - Data Cleaning}
    \begin{enumerate}
        \item \textbf{Missing Values}
            \begin{itemize}
                \item Identify and handle missing data points.
                \item Options include:
                    \begin{itemize}
                        \item \textbf{Imputation}: Replace missing values with means, medians, etc.
                        \item \textbf{Removals}: Remove records or variables with excessive missing data.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Outlier Detection}
            \begin{itemize}
                \item Identify and treat outliers that can skew results.
                \item Common methods include Z-scores or IQR (Interquartile Range).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Preprocessing - Data Transformation}
    \begin{enumerate}
        \item \textbf{Normalization \& Standardization}
            \begin{itemize}
                \item Scale numerical features for consistent units.
                \item \textbf{Normalization}: Scale values to a range of 0 to 1.
                \begin{equation}
                    X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}
                \end{equation}
                \item \textbf{Standardization}: Transform features to have a mean of 0 and a standard deviation of 1.
                \begin{equation}
                    X_{std} = \frac{X - \mu}{\sigma}
                \end{equation}
            \end{itemize}
        \item \textbf{Encoding Categorical Variables}
            \begin{itemize}
                \item Convert categorical variables into numerical format.
                \item Example: One-hot encoding for feature `Color` with values `Red`, `Green`, and `Blue`.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Preprocessing - Feature Selection \& Data Splitting}
    \begin{enumerate}
        \item \textbf{Feature Selection}
            \begin{itemize}
                \item Identify and retain only the most relevant features.
                \item Techniques include Recursive Feature Elimination (RFE) or feature importance scores.
                \item Dimensionality Reduction: Use methods like PCA (Principal Component Analysis).
            \end{itemize}
        \item \textbf{Data Splitting}
            \begin{itemize}
                \item Always divide dataset into training and testing sets (e.g., 80/20 split).
                \item Implement k-fold cross-validation for reliable model evaluation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Preprocessing - Key Points}
    \begin{itemize}
        \item \textbf{Thorough Data Cleaning}: Leads to more accurate models.
        \item \textbf{Appropriate Feature Engineering}: Tailor preprocessing to specific data and model needs.
        \item \textbf{Iterative Process}: Be prepared to revisit steps based on model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Preprocessing - Conclusion}
    \begin{itemize}
        \item Adhering to these best practices enhances model's ability to learn from data.
        \item Results in improved performance and reliability.
        \item Quality of your model is directly tied to the quality of your data.
    \end{itemize}
\end{frame}


\end{document}