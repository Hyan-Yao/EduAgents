\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 2: Machine Learning Basics}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Machine Learning}
    \begin{block}{Overview of Machine Learning as a Subset of Artificial Intelligence}
        Machine Learning (ML) is a branch of Artificial Intelligence (AI) that focuses on enabling computers to learn and make predictions or decisions 
        without being explicitly programmed for each task. ML algorithms use data to improve their performance over time, akin to how humans learn from experience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Purpose and Significance of Machine Learning}
    \begin{itemize}
        \item \textbf{Purpose of Machine Learning:}
        \begin{itemize}
            \item \textbf{Autonomous Learning:} Enables systems to identify patterns and make decisions based on data without manual input.
            \item \textbf{Predictive Insights:} Analyzes past data to forecast future outcomes in various sectors.
        \end{itemize}
        
        \item \textbf{Key Significance:}
        \begin{enumerate}
            \item \textbf{Efficiency:} Automates complex analyses that are time-consuming for humans.
            \item \textbf{Scalability:} Handles vast amounts of data suitable for diverse applications.
            \item \textbf{Continuous Improvement:} Increases accuracy and effectiveness as more data is provided.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Key Points}
    \begin{block}{Examples of Machine Learning Applications}
        \begin{itemize}
            \item \textbf{Spam Detection:} Email services use ML to identify spam based on user behavior and incoming message patterns.
            \item \textbf{Image Recognition:} Platforms like Google Photos recognize and categorize images by visual content.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        - \textbf{Subset of AI:} ML is not synonymous with AI; it focuses on pattern recognition and prediction.
        - \textbf{Data-Driven:} Quality data is the foundation of effective ML solutions.
        - \textbf{Application Everywhere:} Used across various sectors, enhancing functionalities in finance, healthcare, marketing, and entertainment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 1}
    \begin{block}{Definition of Machine Learning}
        \begin{itemize}
            \item Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn from data.
            \item It identifies patterns and makes decisions with minimal human intervention.
            \item Unlike traditional programming, ML relies on algorithms that learn from examples.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 2}
    \begin{block}{Role in Creating Predictive Models}
        \begin{itemize}
            \item Predictive modeling is a significant application of ML.
            \item Algorithms analyze historical data to predict future outcomes.
            \item Example: Loan approval models based on past borrower behavior to predict defaults.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 3}
    \begin{block}{Distinction from Traditional Programming}
        \begin{itemize}
            \item \textbf{Traditional Programming:}
            \begin{itemize}
                \item Explicitly programmed rules and logic.
                \item Example: "If customer has bought more than \$1000, apply 10\% discount."
            \end{itemize}

            \item \textbf{Machine Learning:}
            \begin{itemize}
                \item Learns and adapts from data patterns through training.
                \item Example: A model trained on past purchase data identifies factors of customer loyalty.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 4}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item ML uses past data to inform adaptable decisions.
            \item Can handle complex, nonlinear datasets compared to traditional programming.
            \item Improved performance over time as data grows.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 5}
    \begin{block}{Conceptual Illustration}
        \begin{itemize}
            \item \textbf{Traditional Programming:} A chef specifies every step in the cooking process.
            \item \textbf{Machine Learning:} A chef learns through feedback and adapts based on taste preferences.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Conclusion}
    \begin{block}{Conclusion}
        Machine learning represents a fundamental shift from traditional programming, enabling systems to learn from data.
        This capability allows for predictive analysis, personalization, automation, and intelligent decision-making across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Formula of a Machine Learning Model Prediction}
    \begin{equation}
        \text{Prediction} = \text{Model Parameters} \cdot \text{Input Features}
    \end{equation}
    \begin{itemize}
        \item Where “Model Parameters” are learned from training data.
        \item “Input Features” represent new data inputs for predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Machine Learning - Overview}
    \begin{block}{Introduction}
        Machine Learning (ML) is a subset of artificial intelligence that focuses on algorithms that enable computers to learn from data. 
        ML is broadly categorized into three types:
        \begin{itemize}
            \item Supervised Learning
            \item Unsupervised Learning
            \item Reinforcement Learning
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning}
    \begin{block}{Definition}
        Supervised Learning involves training a model on a labeled dataset where each training example is paired with an output label.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item Labeled Data: Requires labeled training data.
            \item Objective: Minimize prediction error.
        \end{itemize}
        \item \textbf{Common Algorithms:} Linear Regression, Decision Trees, Support Vector Machines, Neural Networks.
    \end{itemize}
    \begin{block}{Example}
        \textbf{Email Classification (Spam Detection):} Model learns to classify emails as “spam” or “not spam”.
    \end{block}
    \begin{block}{Formula Example}
        For linear regression, the prediction can be expressed as:
        \begin{equation}
            y = mx + b
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Unsupervised Learning}
    \begin{block}{Definition}
        Unsupervised Learning deals with unlabeled data, aiming to discover the natural structure in the data.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item Unlabeled Data: No labeled outputs.
            \item Objective: Identify patterns or groupings.
        \end{itemize}
        \item \textbf{Common Algorithms:} K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA).
    \end{itemize}
    \begin{block}{Example}
        \textbf{Customer Segmentation:} E-commerce uses it to segment customers based on purchasing behavior.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning}
    \begin{block}{Definition}
        Reinforcement Learning (RL) involves an agent learning to make decisions by taking actions in an environment to maximize cumulative rewards.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item Agent: The decision-maker.
            \item Environment: The context of operations.
            \item Reward Feedback: Feedback after actions.
            \item Objective: Maximize long-term rewards.
        \end{itemize}
        \item \textbf{Common Algorithms:} Q-Learning, Deep Q-Networks (DQN), Policy Gradients.
    \end{itemize}
    \begin{block}{Example}
        \textbf{Game Playing (e.g., Chess, Go):} RL agents learn strategies by receiving rewards or penalties from game outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    \begin{itemize}
        \item \textbf{Supervised Learning:} Uses labeled data to predict outcomes.
        \item \textbf{Unsupervised Learning:} Discovers patterns in unlabeled data.
        \item \textbf{Reinforcement Learning:} Learns strategies through rewards and penalties.
    \end{itemize}
    By understanding these fundamental types, you'll be better prepared to select the appropriate approach for specific tasks in your ML journey.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Machine Learning}
    \begin{block}{Introduction to Popular Algorithms}
        Machine Learning (ML) relies on various algorithms to analyze data and make predictions or decisions. This slide focuses on four key algorithms commonly used in both supervised and unsupervised learning:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Linear Regression}
    \begin{block}{Concept}
        Linear regression is a statistical method to model the relationship between a dependent variable ($y$) and one or more independent variables ($x$). It assumes a linear relationship.
    \end{block}
    
    \begin{block}{Equation}
        \begin{equation}
            y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_nx_n + \epsilon
        \end{equation}
        \begin{itemize}
            \item $y$: predicted outcome
            \item $\beta_0$: intercept
            \item $\beta_1, \beta_2, \ldots, \beta_n$: coefficients for each variable
            \item $\epsilon$: error term
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Predicting house prices based on features like size, location, and age.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Decision Trees}
    \begin{block}{Concept}
        A decision tree is a flowchart-like structure that makes decisions based on asking a series of questions. Each internal node represents a test on an attribute, each branch represents the outcome of that test, and each leaf node represents a class label (or regression output).
    \end{block}

    \begin{block}{Example}
        Deciding whether to play outside based on weather conditions:
        \begin{itemize}
            \item Is it raining?
            \begin{itemize}
                \item Yes: Stay indoors
                \item No: Check temperature
                \begin{itemize}
                    \item Is it too hot?
                    \begin{itemize}
                        \item Yes: Stay indoors
                        \item No: Go outside
                    \end{itemize}
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Key Point}
        Decision trees are easy to interpret and visualize but can be prone to overfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Support Vector Machines (SVM)}
    \begin{block}{Concept}
        SVM is a supervised learning algorithm that finds the hyperplane that best separates different classes in the data. It works well in high-dimensional spaces and with clear margin of separation.
    \end{block}

    \begin{block}{Visualization}
        Imagine a 2D plane where data points of two classes lie on either side. The SVM seeks the hyperplane that maximizes the margin between these classes.
    \end{block}

    \begin{block}{Example}
        Classifying emails as spam or not spam based on features like word frequency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Neural Networks}
    \begin{block}{Concept}
        Neural networks are inspired by the structure of the human brain and consist of layers of interconnected nodes (neurons). These algorithms are particularly powerful for complex problems like image and speech recognition.
    \end{block}

    \begin{block}{Basic Structure}
        \begin{itemize}
            \item \textbf{Input Layer}: Receives the input data.
            \item \textbf{Hidden Layers}: Transform the inputs through weighted connections.
            \item \textbf{Output Layer}: Produces the final prediction.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Identifying handwritten digits by analyzing pixel data from images.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{itemize}
        \item \textbf{Linear Regression} is straightforward for predicting continuous values.
        \item \textbf{Decision Trees} provide intuitive decision-making models but can lead to overfitting.
        \item \textbf{SVM} excels in classification tasks with clear class separations.
        \item \textbf{Neural Networks} tackle complex, nonlinear relationships but require significant compute resources.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Choosing the right algorithm depends on the nature of your data and the specific problem you wish to solve.
        \item Each algorithm has strengths and weaknesses, so understanding them is crucial for effective model building.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Building a Predictive Model}
    \begin{block}{Overview}
        A predictive model is a mathematical representation that uses historical data to predict future outcomes.
        This slide outlines a systematic approach to building such a model, ensuring clarity and effectiveness at each stage.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Steps to Build a Predictive Model - Part 1}
    \begin{enumerate}
        \item \textbf{Data Collection}
            \begin{itemize}
                \item \textbf{Definition}: Gathering relevant data from various sources.
                \item \textbf{Sources}: Databases, CSV files, APIs, web scraping.
                \item \textbf{Example}: For predicting house prices, collect data on past sales, prices, square footage, location, etc.
            \end{itemize}

        \item \textbf{Data Preprocessing}
            \begin{itemize}
                \item \textbf{Importance}: Ensures your data is clean and suitable for modeling.
                \item \textbf{Key Techniques}:
                    \begin{itemize}
                        \item Handling Missing Values: Imputation or deletion.
                        \item Normalization or Standardization: Scaling features to mean 0, std. dev. 1.
                        \item Feature Selection: Choosing relevant variables to improve performance.
                    \end{itemize}
                \item \textbf{Example}: Removing houses with missing square footage.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Build a Predictive Model - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Model Training}
            \begin{itemize}
                \item \textbf{Process}: Using a machine learning algorithm on your training dataset.
                \item \textbf{Common Algorithms}: Linear Regression, Decision Trees, Support Vector Machines, Neural Networks.
                \item \textbf{Example Code Snippet}:
                \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Assuming data is in a DataFrame called df
X = df[['square_footage', 'bedrooms', 'bathrooms']]
y = df['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Model Evaluation}
            \begin{itemize}
                \item \textbf{Objective}: Assess model performance using a validation dataset.
                \item \textbf{Metrics}: Mean Absolute Error (MAE), Mean Squared Error (MSE).
                \item \textbf{Example Code Snippet}:
                \begin{lstlisting}[language=Python]
from sklearn.metrics import mean_squared_error

predictions = model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Each step is crucial for developing a successful predictive model.
        \item Preprocessing directly impacts model prediction quality.
        \item Evaluation metrics provide insights into model performance and necessary adjustments.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Building a predictive model requires thoughtful execution of each step, fostering a cycle of iteration and improvement based on data and prediction outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Overview}
    \begin{block}{Overview}
        Data preprocessing is a crucial step in the machine learning pipeline, often determining the model's performance. 
        It involves preparing raw data to enhance quality and ensure suitability for analysis.
    \end{block}
    
    \begin{itemize}
        \item Importance of data quality
        \item Essential preprocessing techniques
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Handling Missing Values}
    \begin{block}{Handling Missing Values}
        Missing values can skew results and lead to inaccurate predictions. It's essential to handle these appropriately.
    \end{block}

    \begin{itemize}
        \item \textbf{Deletion:} Remove records with missing values.
        \item \textbf{Imputation:} Fill in missing values using statistical methods.
        \begin{itemize}
            \item Mean/Median Imputation
            \item Forward/Backward Fill
        \end{itemize}
    \end{itemize}
    
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load dataset
data = pd.read_csv('data.csv')

# Impute missing values
data.fillna(data.mean(), inplace=True)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Normalization and Feature Selection}
    \begin{block}{Normalization}
        Normalization rescales values to a common range, improving model training.
    \end{block}

    \begin{itemize}
        \item \textbf{Min-Max Scaling:}
        \begin{equation}
        X_{\text{norm}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
        \end{equation}
        
        \item \textbf{Z-score Normalization:}
        \begin{equation}
        Z = \frac{(X - \mu)}{\sigma}
        \end{equation}
        
        \item \textbf{Feature Selection:} Identifying and selecting the most relevant features.
        \begin{itemize}
            \item Filter, Wrapper, and Embedded methods.
            \item Example: Relevant features in predicting housing prices.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Conclusion}
        Effective data preprocessing is vital for machine learning success.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Overview}
    Evaluating the performance of machine learning models is crucial to ensuring they perform accurately and effectively in real-world applications. Different metrics provide insights into various aspects of a model’s performance. This slide discusses five key evaluation metrics:
    \begin{itemize}
        \item Accuracy
        \item Precision
        \item Recall
        \item F1 Score
        \item ROC-AUC
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Key Definitions}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item Definition: Proportion of correct predictions to total predictions.
                \item Formula: 
                \[
                \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
                \]
            \end{itemize}
        \item \textbf{Precision}
            \begin{itemize}
                \item Definition: Ratio of correctly predicted positives to total predicted positives.
                \item Formula: 
                \[
                \text{Precision} = \frac{TP}{TP + FP}
                \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Continued Definitions}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Recall (Sensitivity)}
            \begin{itemize}
                \item Definition: Ratio of correctly predicted positives to all actual positives.
                \item Formula: 
                \[
                \text{Recall} = \frac{TP}{TP + FN}
                \]
            \end{itemize}
        \item \textbf{F1 Score}
            \begin{itemize}
                \item Definition: Harmonic mean of precision and recall.
                \item Formula: 
                \[
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                \]
            \end{itemize}
        \item \textbf{ROC-AUC}
            \begin{itemize}
                \item Definition: Graphical representation of model's ability to distinguish between classes.
                \item Range: AUC is between 0 and 1.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Key Points and Conclusion}
    \begin{itemize}
        \item Accuracy alone can be misleading in cases of imbalanced datasets.
        \item Precision and Recall provide deeper insights on positive class handling.
        \item F1 Score balances precision and recall, especially in binary classification.
        \item ROC-AUC is a comprehensive measure for model comparison.
    \end{itemize}
    \textbf{Conclusion:} Understanding these evaluation metrics is essential for accurately assessing machine learning models. Use a combination of metrics for a holistic view of performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Machine Learning - Overview}
    \begin{itemize}
        \item The challenges faced in machine learning can significantly impact model performance.
        \item Key topics we will cover:
        \begin{enumerate}
            \item Overfitting
            \item Underfitting
            \item Dataset Biases
            \item Interpretability
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Overfitting}
    \begin{block}{1. Overfitting}
        \textbf{Explanation:}  
        Overfitting occurs when a model learns the training data too well, capturing noise along with the underlying patterns. This results in high accuracy on training data but poor performance on the test set.
    \end{block}

    \textbf{Example Illustration:}  
    - Fitting a polynomial curve to a straight line with random noise can demonstrate overfitting.

    \textbf{Key Point:}  
    Use techniques like cross-validation, regularization (L1, L2), and simplifying models to mitigate overfitting.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Underfitting}
    \begin{block}{2. Underfitting}
        \textbf{Explanation:}  
        Underfitting happens when a model is too simple to capture the underlying trend of the data, leading to poor performance on both training and test datasets.
    \end{block}

    \textbf{Example Illustration:}  
    - A linear regression model trying to fit a quadratic relationship would exhibit underfitting.

    \textbf{Key Point:}  
    Choosing more complex models, increasing feature representation, or reducing regularization can help address underfitting.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Dataset Biases and Interpretability}
    \begin{block}{3. Dataset Biases}
        \textbf{Explanation:}  
        Data biases arise when the training data is not representative of the actual task, leading to skewed results.
    \end{block}

    \textbf{Example Illustration:}  
    - A facial recognition system trained predominantly on images from a single ethnicity may perform poorly on other groups.

    \textbf{Key Point:}  
    Diverse and representative datasets are critical; data augmentation and careful pre-processing can aid in reducing biases.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Interpretability and Summary}
    \begin{block}{4. Interpretability}
        \textbf{Explanation:}  
        Interpretability refers to how well a human can understand the cause of a model's decisions. Complex models may act as "black boxes."
    \end{block}

    \textbf{Example Illustration:}  
    - A healthcare model predicting diseases without clear rationale may hinder clinician trust.

    \textbf{Key Point:}  
    Using simpler models, local explanation techniques (like LIME), and visualizing model predictions can enhance interpretability.
    
    \vspace{0.2cm}
    \textbf{Summary of Challenges:}
    \begin{itemize}
        \item Overfitting: Poor generalization on test data.
        \item Underfitting: Inadequate complexity leading to poor performance.
        \item Dataset Biases: Non-representative data results in skewed outputs.
        \item Interpretability: Complexity hinders understanding.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges - Formula}
    \begin{block}{Formula for Regularization}
        To mitigate overfitting, the loss function can be adapted:
        \begin{equation}
        L(\theta) = \text{Loss}(\theta) + \lambda R(\theta)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( L \): Total loss
            \item \( R(\theta) \): Regularization term (e.g., L1 or L2)
            \item \( \lambda \): Regularization parameter
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Machine Learning - Introduction}
    \begin{itemize}
        \item Machine learning (ML) technologies are increasingly integrated into our lives.
        \item Understanding ethical implications is crucial.
        \item Primary areas of concern:
        \begin{itemize}
            \item Bias
            \item Privacy
            \item Accountability
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Machine Learning - Bias}
    \begin{block}{Bias}
        \begin{itemize}
            \item Definition: Systematic prejudice in algorithm results.
            \item Types of Bias:
            \begin{itemize}
                \item Data Bias: Unrepresentative training data.
                \item Algorithmic Bias: Decisions in model-building process.
            \end{itemize}
            \item Example: A hiring algorithm may favor candidates from a specific gender.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Ensure diverse and representative datasets.
            \item Regularly audit models for bias and take corrective actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Machine Learning - Privacy}
    \begin{block}{Privacy}
        \begin{itemize}
            \item Definition: Issues arising from sensitive personal information in training data.
            \item Concerns:
            \begin{itemize}
                \item Data leakage during training or sharing.
                \item Inadequate anonymization leading to re-identification.
            \end{itemize}
            \item Example: A facial recognition system trained without consent violates privacy rights.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Implement rigorous data anonymization.
            \item Follow legal frameworks (e.g., GDPR) governing data usage.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Machine Learning - Accountability}
    \begin{block}{Accountability}
        \begin{itemize}
            \item Definition: Responsibility for outcomes of ML algorithms.
            \item Concerns:
            \begin{itemize}
                \item Clarity on responsibility for decisions made by autonomous systems.
                \item Complexity of tracing decision-making processes (the "black box" problem).
            \end{itemize}
            \item Example: Liability of developers vs. organizations vs. medical personnel in healthcare inaccuracies.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Maintain transparency in algorithms.
            \item Establish clear guidelines for accountability in ML applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Machine Learning - Conclusion}
    \begin{itemize}
        \item Proactively addressing ethical considerations fosters trust.
        \item Ensures fairness and protects individual rights.
        \item Prioritizing ethical practices enables responsible use of ML technologies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Machine Learning - Additional Note}
    \begin{itemize}
        \item Upcoming session will focus on hands-on learning with AI tools.
        \item Emphasizing incorporation of ethical considerations into practical applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Learning with AI Tools - Introduction}
    In this session, we will explore three powerful machine learning libraries:
    \begin{itemize}
        \item \textbf{TensorFlow}
        \item \textbf{Keras}
        \item \textbf{Scikit-Learn}
    \end{itemize}
    Understanding these tools is essential for building, training, and deploying machine learning models effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Learning with AI Tools - Key Tools}
    \begin{enumerate}
        \item \textbf{TensorFlow}
        \begin{itemize}
            \item Overview: An open-source library developed by Google for numerical computation using data flow graphs.
            \item Key Features:
            \begin{itemize}
                \item Supports both CPU and GPU computing.
                \item Flexible and portable.
            \end{itemize}
            \item Use Case Example: Building deep learning models for image classification.
        \end{itemize}

        \item \textbf{Keras}
        \begin{itemize}
            \item Overview: A high-level neural networks API running on top of TensorFlow.
            \item Key Features:
            \begin{itemize}
                \item User-friendly and modular.
                \item Supports multiple backend engines.
            \end{itemize}
            \item Use Case Example: Quick prototyping of neural networks.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Learning with AI Tools - Code Snippets}
    \textbf{TensorFlow Code Snippet (Creating a model):}
    \begin{lstlisting}[language=Python]
    import tensorflow as tf
    from tensorflow.keras import layers, models
    
    model = models.Sequential()
    model.add(layers.Dense(64, activation='relu', input_shape=(input_shape,)))
    model.add(layers.Dense(10, activation='softmax'))
    
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    \end{lstlisting}
    
    \textbf{Keras Code Snippet (Training a model):}
    \begin{lstlisting}[language=Python]
    model.fit(training_data, training_labels, epochs=10, batch_size=32)
    \end{lstlisting}
\end{frame}


\end{document}