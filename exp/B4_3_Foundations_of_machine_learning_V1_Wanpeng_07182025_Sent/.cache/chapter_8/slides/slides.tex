\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Chapter 8: Introduction to Neural Networks]{Chapter 8: Introduction to Neural Networks}
\author[J. Smith]{John Smith, Ph.D.}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks}
    \begin{block}{Overview}
        Neural networks are computational models inspired by the human brain, designed to recognize patterns and make decisions based on data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What Are Neural Networks?}
    \begin{itemize}
        \item \textbf{Neurons}: Basic units that receive input, process data, and pass on the output.
        \item \textbf{Weights}: Parameters that adjust the strength of connections between neurons, optimized during training.
        \item \textbf{Activation Function}: Determines if neurons should "fire" based on input; common functions include:
        \begin{itemize}
            \item \textbf{Sigmoid}: \( f(x) = \frac{1}{1 + e^{-x}} \)
            \item \textbf{ReLU}: \( f(x) = \max(0, x) \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of a Neural Network}
    \begin{itemize}
        \item \textbf{Input Layer}: First layer that receives input data.
        \item \textbf{Hidden Layers}: Intermediate layers for processing through weighted connections and activation functions.
        \item \textbf{Output Layer}: Last layer that provides final results, often through classification or regression outputs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Machine Learning}
    \begin{itemize}
        \item \textbf{Pattern Recognition}: Effective in identifying patterns in large datasets; suited for tasks like:
        \begin{enumerate}
            \item \textbf{Image Recognition}: E.g., Convolutional Neural Networks (CNNs) classify images (e.g., distinguishing cats from dogs).
            \item \textbf{Speech Recognition}: E.g., Recurrent Neural Networks (RNNs) process sequential data like audio signals for accurate speech recognition.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Universal Approximation Theorem}: A feedforward neural network with at least one hidden layer can approximate any continuous function.
        \item \textbf{Training Process}: Neural networks learn through backpropagation, adjusting weights to minimize prediction error.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Neural networks are foundational to modern machine learning, enabling advancements across various fields such as healthcare, finance, and autonomous systems. Their ability to learn complex patterns from vast amounts of data makes them a powerful tool in todayâ€™s data-driven world. Understanding their structure and significance allows for appreciation of their applications and impact on technology.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Perceptron?}
    \begin{block}{Definition}
        A perceptron is a fundamental building block of neural networks, introduced by Frank Rosenblatt in 1958. It simulates a single neuron in the human brain and classifies input data into binary outputs based on linear decision boundaries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of a Perceptron}
    \begin{enumerate}
        \item \textbf{Inputs ($x_1, x_2, \ldots, x_k$)}: Features of the input data, such as pixel intensity or numerical values.
        
        \item \textbf{Weights ($w_1, w_2, \ldots, w_k$)}: Each weight signifies the importance of the corresponding input. Initially, weights can be randomly assigned.
        
        \item \textbf{Bias ($b$)}: Allows the model to shift the decision boundary, improving performance by adjusting the output independently of input values.
        
        \item \textbf{Activation Function}: Determines output based on the weighted sum. A common choice is the step function, yielding an output of 1 if the weighted sum exceeds a threshold, and 0 otherwise.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Function of a Perceptron}
    The perceptron computes a weighted sum of the inputs and biases:

    \begin{equation}
        z = w_1x_1 + w_2x_2 + \ldots + w_kx_k + b
    \end{equation}

    It determines the output \(y\) as follows:

    \begin{equation}
        y = 
        \begin{cases} 
        1 & \text{if } z > 0 \\ 
        0 & \text{otherwise} 
        \end{cases}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Perceptron}
    \textbf{Binary Classification Problem:} Predict whether an email is spam (1) or not (0) based on:

    \begin{itemize}
        \item $x_1$: Presence of the word "sale"
        \item $x_2$: Presence of the word "free"
    \end{itemize}

    \textbf{Assuming weights:}
    \begin{itemize}
        \item $w_1 = 2$ (for "sale")
        \item $w_2 = 1$ (for "free")
        \item Bias $b = -2$
    \end{itemize}

    \textbf{Calculations:}
    \begin{itemize}
        \item If both words are present: 
        \[
        z = 2(1) + 1(1) - 2 = 1 \Rightarrow y = 1 \,(\text{Spam})
        \]
        
        \item If neither word is present: 
        \[
        z = 2(0) + 1(0) - 2 = -2 \Rightarrow y = 0 \,(\text{Not Spam})
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    \begin{itemize}
        \item The perceptron is the simplest form of a neural network, capable of linear classification.
        \item It serves as a foundation for more complex neural network architectures.
        \item Weights significantly influence output classification.
    \end{itemize}
    \begin{block}{Summary}
        The perceptron is critical for understanding neural networks, distinguishing between two classes based on input characteristics through linear combinations and activation functions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Perceptron Learning Algorithm - Overview}
    \begin{block}{Overview}
        The Perceptron Learning Algorithm is foundational in understanding how a single-layer perceptron learns from data. 
        This algorithm adjusts the weights of the perceptron based on its predictions and the desired outputs, enabling 
        it to make better decisions over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Perceptron Learning Algorithm - Key Components}
    \begin{itemize}
        \item \textbf{Inputs and Weights:}
        \begin{itemize}
            \item A perceptron receives multiple inputs, each associated with a weight (\(w_i\)).
            \item The weighted sum of inputs is calculated as:
            \[
            z = \sum_{i=1}^{n} w_i x_i + b
            \]
            where \(b\) is the bias term.
        \end{itemize}

        \item \textbf{Activation Function:}
        \begin{itemize}
            \item Uses a step function to determine output:
            \[
            y =
            \begin{cases} 
            1 & \text{if } z \geq 0 \\
            0 & \text{if } z < 0 
            \end{cases}
            \]
            \item Introduces a threshold for decision-making.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Perceptron Learning Algorithm - Learning Process}
    \begin{enumerate}
        \item \textbf{Initialization:}
        \begin{itemize}
            \item Initialize weights (\(w_i\)) and bias (\(b\)) to small random values.
        \end{itemize}
        
        \item \textbf{Feedforward Step:}
        \begin{itemize}
            \item Compute the output using the weighted sum and activation function.
        \end{itemize}

        \item \textbf{Weight Update Rule:}
        \begin{equation}
            w_i \leftarrow w_i + \alpha (t - y) x_i
        \end{equation}
        \begin{itemize}
            \item \(\alpha\) is the learning rate controlling the size of the weight update.
        \end{itemize}

        \item \textbf{Iteration:}
        \begin{itemize}
            \item Repeat the feedforward and weight update steps for multiple epochs.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Perceptrons - Introduction}
    \begin{itemize}
        \item Perceptrons are the simplest form of neural networks.
        \item They consist of a single layer of output nodes connected to input features.
        \item While foundational in machine learning, they have notable limitations in handling complex problems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Perceptrons - Key Limitations}
    \begin{enumerate}
        \item \textbf{Inability to Handle Non-Linearly Separable Data}
        \begin{itemize}
            \item Defined as data that cannot be perfectly classified by a linear boundary.
            \item Example: The XOR function demonstrates this limitation.
        \end{itemize}

        \item \textbf{Limited to Binary Classification}
        \begin{itemize}
            \item A perceptron can only output two distinct classes.
            \item Fails in multi-class classification without additional mechanisms.
        \end{itemize}

        \item \textbf{Sensitivity to Input Feature Scaling}
        \begin{itemize}
            \item Performance is heavily influenced by the scale of input features.
            \item Non-normalized features result in poor convergence during training.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Perceptrons - Challenges and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Gradient Descent Limitations}
        \begin{itemize}
            \item Uses a simple gradient descent approach.
            \item Can get stuck in local minima with slow convergence on complex problems.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Non-linearity makes perceptrons unsuitable for many complex tasks.
            \item Transition to Multi-Layer Perceptrons (MLPs) is necessary for advanced applications.
            \item Serve as a stepping stone to future neural network advancements.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding perceptron limitations is vital for recognizing neural network design advancements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Multi-Layer Perceptrons (MLPs)}
    Multi-Layer Perceptrons (MLPs) represent a significant advancement over simple perceptrons, addressing their major limitations. 
    \begin{itemize}
        \item While a perceptron can solve only linearly separable problems, MLPs can model complex, non-linear relationships.
        \item MLPs incorporate multiple layers of neurons, enhancing their ability to learn intricate patterns in data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of MLPs}
    \begin{enumerate}
        \item \textbf{Definition}:
        \begin{itemize}
            \item An MLP is a type of artificial neural network with:
            \begin{itemize}
                \item **Input Layer**: Receives initial data.
                \item **Hidden Layers**: Intermediate layers that process inputs.
                \item **Output Layer**: Produces final outputs.
            \end{itemize}
        \end{itemize}

        \item \textbf{Overcoming Limitations}:
        \begin{itemize}
            \item MLPs use activation functions (e.g., Sigmoid, ReLU) for non-linear mappings.
            \begin{equation}
                y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
            \end{equation}
            \item MLPs create deep networks, allowing hierarchical representation of data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Applications of MLPs}
    \begin{itemize}
        \item \textbf{Image Recognition}:
        \begin{itemize}
            \item MLPs identify objects by capturing multiple-level features (e.g., edges, shapes).
        \end{itemize}
        
        \item \textbf{Natural Language Processing}:
        \begin{itemize}
            \item MLPs help understand sentiment by detecting complex patterns in text.
        \end{itemize}
    \end{itemize}

    \textbf{Illustration of MLP Structure:}
    \begin{verbatim}
    Input Layer                    Hidden Layer                      Output Layer
        O                             O     O     O                     O
        O ------> O ------> O ------> O        O --------> O
        O                             O     O     O                     O
    \end{verbatim}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of an MLP - Introduction}
    \begin{block}{Multi-Layer Perceptron (MLP)}
        A Multi-Layer Perceptron (MLP) is a type of neural network composed of multiple layers (neurons) that transform input data into output predictions. 
    \end{block}
    
    \begin{itemize}
        \item MLP consists of three main types of layers:
            \begin{itemize}
                \item \textbf{Input Layer}
                \item \textbf{Hidden Layers}
                \item \textbf{Output Layer}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of an MLP - Components}
    \begin{block}{Input Layer}
        \begin{itemize}
            \item Receives input features.
            \item Each neuron represents a feature.
            \item For example: 
              \begin{itemize}
                \item 28x28 pixel images lead to 784 neurons.
              \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Hidden Layers}
        \begin{itemize}
            \item One or more layers where computations occur.
            \item Can apply non-linear transformations.
            \item Example: Two hidden layers with:
              \begin{itemize}
                \item 128 neurons, and
                \item 64 neurons.
              \end{itemize}
        \end{itemize}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of an MLP - Output Layer and Key Points}
    \begin{block}{Output Layer}
        \begin{itemize}
            \item Produces the final output.
            \item Number of neurons correlates to classification classes.
            \item Example: MLP for digit recognition has 10 neurons for digits (0-9).
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Feedforward Process:} 
              \begin{itemize}
                \item Information flows from input to output layer without cycles.
              \end{itemize}
            \item \textbf{Weights and Biases:} 
              \begin{itemize}
                \item Connections have weights adjusted during training; each neuron has a bias.
              \end{itemize}
            \item \textbf{Activation Functions:} 
              \begin{itemize}
                \item Non-linear functions like Sigmoid, Tanh, and ReLU applied at neurons.
              \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions in MLPs}
    \begin{block}{Overview of Activation Functions}
        Activation functions introduce non-linearity to Multi-Layer Perceptrons (MLPs), enabling the model to learn complex patterns. Three widely-used functions are:
        \begin{itemize}
            \item Sigmoid
            \item Tanh
            \item ReLU (Rectified Linear Unit)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Sigmoid Function}
    \begin{block}{Definition}
        The sigmoid function maps any real-valued number into a value between 0 and 1:
        \begin{equation}
            \sigma(x) = \frac{1}{1 + e^{-x}}
        \end{equation}
    \end{block}
    
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item Range: (0, 1)
            \item Output: Useful for binary classification
            \item Derivatives: Faces the vanishing gradient problem
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        For \( x = 0 \):
        \begin{equation}
            \sigma(0) = \frac{1}{1 + e^{0}} = 0.5
        \end{equation}
    \end{block}
    
    \begin{block}{Visual Representation}
        The sigmoid curve is S-shaped, approaching 1 for large positive \( x \) and 0 for large negative \( x \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Hyperbolic Tangent (Tanh)}
    \begin{block}{Definition}
        The tanh function outputs values in the range of -1 to 1:
        \begin{equation}
            \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
        \end{equation}
    \end{block}

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item Range: (-1, 1)
            \item Output: Centers data around zero, improving convergence
            \item Derivatives: Better than sigmoid but also suffers from the vanishing gradient problem
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        For \( x = 0 \):
        \begin{equation}
            \tanh(0) = 0
        \end{equation}
    \end{block}
    
    \begin{block}{Visual Representation}
        The tanh curve is S-shaped, crossing the origin, approaching -1 for negative \( x \) and 1 for positive \( x \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Rectified Linear Unit (ReLU)}
    \begin{block}{Definition}
        ReLU is defined as:
        \begin{equation}
            f(x) = \max(0, x)
        \end{equation}
    \end{block}

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item Range: [0, âˆž)
            \item Output: Preferred in hidden layers of deep networks
            \item Derivatives: Constant gradient of 1 for positive inputs; potential issue of "dying ReLU"
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        For \( x = 2 \):
        \begin{equation}
            \text{ReLU}(2) = 2
        \end{equation}
        For \( x = -3 \):
        \begin{equation}
            \text{ReLU}(-3) = 0
        \end{equation}
    \end{block}

    \begin{block}{Visual Representation}
        The ReLU function is linear for positive inputs and flat for negative inputs, creating a two-segment line.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary \& Key Points}
    \begin{block}{Essential Takeaways}
        \begin{itemize}
            \item Activation functions introduce non-linearity, crucial for learning complex functions.
            \item Choosing the right function:
                \begin{itemize}
                    \item Sigmoid: Traditional for binary classification
                    \item Tanh: Improves convergence
                    \item ReLU: Efficient for deep learning
                \end{itemize}
            \item Sigmoid and tanh are prone to vanishing gradient; ReLU can lead to inactive neurons.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula Overview}
        \begin{itemize}
            \item Sigmoid: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)
            \item Tanh: \( \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \)
            \item ReLU: \( \text{ReLU}(x) = \max(0, x) \)
        \end{itemize}
    \end{block}
    
    Understanding these activation functions is key to effectively deploying MLPs and improving model performance during training.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training an MLP - What is Backpropagation?}
    \begin{block}{Backpropagation}
        Backpropagation is the key algorithm for training Multi-Layer Perceptrons (MLPs). 
        It adjusts the weights of the connections in the network to minimize the output error. 
    \end{block}
    \begin{itemize}
        \item \textbf{Forward Pass:}
        \begin{itemize}
            \item Input data is passed through the network, producing an output.
            \item This output is compared to the target (ground truth) to calculate the error.
        \end{itemize}
        \item \textbf{Backward Pass:}
        \begin{itemize}
            \item The error is propagated backward through the network.
            \item Gradients of the loss function with respect to each weight are computed using the chain rule.
            \item The weights are updated to minimize the loss (error).
        \end{itemize}
   \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training an MLP - Key Formula}
    The weight update can be expressed as:
    \begin{equation}
        w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w}
    \end{equation}
    Where:
    \begin{itemize}
        \item $w$ = weight
        \item $\eta$ = learning rate
        \item $L$ = loss function (how far the output is from the target)
    \end{itemize}

    \begin{block}{Example}
        Suppose we start with a guess that a house will be worth \$300,000 (our MLP's output) and the actual price is \$350,000. 
        Backpropagation helps us adjust the weights so that the model's output is closer to \$350,000.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training an MLP - Loss Functions and Optimization}
    \begin{itemize}
        \item \textbf{Loss Functions:}
        \begin{itemize}
            \item \textbf{Mean Squared Error (MSE):}
            \begin{equation}
                L(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
            \item \textbf{Cross-Entropy Loss:}
            \begin{equation}
                L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \cdot \log(\hat{y}_i)
            \end{equation}
            Where $C$ is the number of classes.
        \end{itemize}
        \item \textbf{Optimization Process:}
        \begin{itemize}
            \item \textbf{Stochastic Gradient Descent (SGD):} Updates weights using small batches of data.
            \item \textbf{Adam Optimizer:} Dynamically adjusts the learning rate based on first and second moments of the gradients.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks}
    \begin{block}{Overview}
        Neural networks have revolutionized various fields by enabling machines to perform tasks that usually require human intelligence. Here, we explore three major applications: \textbf{Image Recognition}, \textbf{Natural Language Processing (NLP)}, and \textbf{Health Diagnostics}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Image Recognition}
    \begin{block}{Description}
        Image recognition utilizes neural networks to identify and classify objects within images. Convolutional Neural Networks (CNNs) are especially suited for this task.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Example:}
        \begin{itemize}
            \item Facial Recognition: Applications like Facebook's photo tagging and security systems use CNNs to detect and recognize faces from images uploaded by users.
        \end{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Handles complex visual data with high accuracy.
            \item Works by detecting features, such as edges, shapes, and facial structures.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Natural Language Processing (NLP)}
    \begin{block}{Description}
        NLP focuses on the interaction between computers and humans through natural language. Recurrent Neural Networks (RNNs) and Transformers are commonly used for tasks like text classification, sentiment analysis, and machine translation.
    \end{block}

    \begin{itemize}
        \item \textbf{Example:}
        \begin{itemize}
            \item Chatbots and Virtual Assistants: Services like Siri and Alexa use NLP to understand and respond to user queries in conversational language.
        \end{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Understands context, grammar, and semantics.
            \item Capable of generating coherent text output based on input.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Health Diagnostics}
    \begin{block}{Description}
        Neural networks assist in analyzing medical data to support decision-making in diagnostics and treatment planning. They process vast amounts of data from medical imaging, patient records, and genetic information.
    \end{block}

    \begin{itemize}
        \item \textbf{Example:}
        \begin{itemize}
            \item Radiology: Deep learning models are employed to detect anomalies in X-ray and MRI scans, aiding radiologists in identifying conditions like tumors with improved accuracy.
        \end{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Enhances diagnostic speed and precision.
            \item Supports personalized medicine by analyzing genetic profiles.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Summary}
        Neural networks are pivotal in transforming tasks across various domains by mimicking human cognitive functions. By leveraging powerful architectures like CNNs for vision, RNNs and Transformers for language, and predictive models for health, the applications are vast and impactful.
    \end{block}

    \begin{block}{Conclusion}
        Neural networks stand at the forefront of AI advancements, continuously pushing the boundaries of what machines can achieve in recognizing images, processing language, and improving health outcomes. Understanding these applications will be essential in grasping the potential of neural networks in our everyday lives.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Points Summary of Neural Networks}
    \begin{enumerate}
        \item \textbf{Definition and Basic Concept}:
        \begin{itemize}
            \item Neural networks are computational models inspired by the human brain. 
            \item They consist of layers of interconnected nodes (neurons) that process data and learn from it.
        \end{itemize}
        
        \item \textbf{Structure of Neural Networks}:
        \begin{itemize}
            \item **Layers**: Three types - Input Layer, Hidden Layers, and Output Layer.
            \item **Weights**: Each connection has a weight adjusted during training.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Learning Process and Applications}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Learning Process}:
        \begin{itemize}
            \item **Training**: Neural networks learn to minimize prediction error using datasets.
            \item **Cost Function**: Measures prediction accuracy; the goal is to minimize it.
            \item **Activation Functions**: Non-linear functions (ReLU, Sigmoid, Tanh) that enable learning complex patterns.
        \end{itemize}
        
        \item \textbf{Applications in Real Life}:
        \begin{itemize}
            \item **Image Recognition**: Classifying objects in images.
            \item **Natural Language Processing (NLP)**: Understanding and generating human language.
            \item **Health Diagnostics**: Analyzing medical images and data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Challenges and Future Directions}
    \begin{enumerate}
        \setcounter{enumi}{4} % Continue numbering
        \item \textbf{Challenges and Future Directions}:
        \begin{itemize}
            \item **Overfitting**: Models that learn training data too well; mitigated by dropout and regularization.
            \item **Data Requirements**: Require large labeled datasets for effective training.
            \item **Advancements**: Unsupervised learning and transfer learning for more robust applications.
        \end{itemize}
        
        \item \textbf{Conclusion Statement}:
        \begin{itemize}
            \item Neural networks are transformative in machine learning, capable of solving complex problems.
            \item Understanding their structure and applications helps harness their potential for innovation.
        \end{itemize}
    \end{enumerate}
\end{frame}


\end{document}