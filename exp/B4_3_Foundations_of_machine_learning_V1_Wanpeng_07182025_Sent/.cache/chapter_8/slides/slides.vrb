\frametitle{3. Rectified Linear Unit (ReLU)}
    \begin{block}{Definition}
        ReLU is defined as:
        \begin{equation}
            f(x) = \max(0, x)
        \end{equation}
    \end{block}

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item Range: [0, âˆž)
            \item Output: Preferred in hidden layers of deep networks
            \item Derivatives: Constant gradient of 1 for positive inputs; potential issue of "dying ReLU"
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        For \( x = 2 \):
        \begin{equation}
            \text{ReLU}(2) = 2
        \end{equation}
        For \( x = -3 \):
        \begin{equation}
            \text{ReLU}(-3) = 0
        \end{equation}
    \end{block}

    \begin{block}{Visual Representation}
        The ReLU function is linear for positive inputs and flat for negative inputs, creating a two-segment line.
    \end{block}
