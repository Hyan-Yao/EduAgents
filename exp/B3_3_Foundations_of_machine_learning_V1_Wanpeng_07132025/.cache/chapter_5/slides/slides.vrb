\frametitle{Model Evaluation Metrics - Precision and Recall}
    \begin{block}{2. Precision}
        \begin{itemize}
            \item \textbf{Definition}: Measures the correctness of positive predictions.
            \item \textbf{Formula}:
            \[
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \]
            \item \textbf{Example}:
            \begin{itemize}
                \item If a model predicts 70 instances as positive (50 correct, 20 wrong):
                \[
                \text{Precision} = \frac{50}{50 + 20} = \frac{50}{70} \approx 0.71 \quad (71\%)
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{3. Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the model's ability to identify all relevant instances.
            \item \textbf{Formula}:
            \[
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \]
            \item \textbf{Example}:
            \begin{itemize}
                \item If there are 80 actual positive cases and the model correctly identifies 50:
                \[
                \text{Recall} = \frac{50}{50 + 30} = \frac{50}{80} = 0.625 \quad (62.5\%)
                \end{itemize}
        \end{itemize}
    \end{block}
