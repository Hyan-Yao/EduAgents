\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Chapter 8: Practical Skills]{Chapter 8: Practical Skills: TensorFlow / PyTorch}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Practical Skills in AI}
    \begin{block}{Overview}
        This chapter explores the foundational practical skills in Artificial Intelligence (AI) through hands-on implementation using two of the most popular deep learning frameworks: \textbf{TensorFlow} and \textbf{PyTorch}.
    \end{block}
    \begin{itemize}
        \item Essential for budding AI practitioners to master the basics.
        \item Focus on implementing basic AI models for practical applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{TensorFlow:}
            \begin{itemize}
                \item Developed by Google, open-source framework for building and deploying ML models.
                \item \textbf{Core Component:} Tensors – multi-dimensional arrays for computations.
            \end{itemize}

        \item \textbf{PyTorch:}
            \begin{itemize}
                \item Created by Facebook, known for dynamic computation graph.
                \item \textbf{Core Component:} Autograd – automatic differentiation for gradient computation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications and Importance}
    \begin{block}{Importance of Practical Skills}
        \begin{itemize}
            \item \textbf{Real-World Applications:} AI solutions in various industries (e.g., healthcare, finance, autonomous systems).
            \item \textbf{Hands-On Learning:} Solidifies understanding through implementation of models.
        \end{itemize}
    \end{block}
    \begin{block}{Examples of AI Models}
        \begin{itemize}
            \item \textbf{Linear Regression:}
                \begin{lstlisting}[language=Python]
import torch
from torch import nn, optim

model = nn.Linear(1, 1)  # Simple linear model
criterion = nn.MSELoss()  # Loss function
optimizer = optim.SGD(model.parameters(), lr=0.01)
                \end{lstlisting}

            \item \textbf{Convolutional Neural Network (CNN):}
                \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(1, activation='sigmoid')
])
                \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    \begin{itemize}
        \item Understand core concepts related to TensorFlow and PyTorch.
        \item Familiarity with key framework features.
        \item Skills in model development, training, and evaluation.
        \item Explore real-world applications and hands-on implementation.
        \item Emphasize iterative learning and practical skills development.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Core Concepts}
    \begin{enumerate}
        \item \textbf{Understand Core Concepts}
        \begin{itemize}
            \item \textbf{Tensor Operations}: Learn the significance of tensors in deep learning, akin to arrays in numpy.
            \item \textbf{Example}: A 2D Tensor represents an image; each pixel corresponds to an element of the tensor.
        \end{itemize}
        
        \item \textbf{Familiarity with Framework Features}
        \begin{itemize}
            \item \textbf{TensorFlow}: Insights into features like TensorFlow Hub and Keras API.
            \item \textbf{PyTorch}: Explore dynamic computation graphs for easy debugging.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Model Development}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Model Development \& Training}
        \begin{itemize}
            \item Building neural networks in both frameworks with practical examples.
        \end{itemize}
        
        \begin{block}{TensorFlow Code Snippet}
        \begin{lstlisting}[language=Python]
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])
        \end{lstlisting}
        \end{block}
        
        \begin{block}{PyTorch Code Snippet}
        \begin{lstlisting}[language=Python]
class NeuralNet(nn.Module):
    def __init__(self):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)
        \end{lstlisting}
        \end{block}

        \item \textbf{Model Evaluation \& Optimization}
        \begin{itemize}
            \item Evaluate using accuracy, precision, recall, and F1 score.
            \item Familiarize with optimization techniques like Adam and SGD.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of TensorFlow}
    \begin{block}{What is TensorFlow?}
        TensorFlow is an open-source machine learning framework developed by Google Brain Team. It provides a flexible and efficient framework for developers to build and train machine learning models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of TensorFlow}
    \begin{itemize}
        \item \textbf{Flexible Architecture:}
        \begin{itemize}
            \item Supports deployment across various platforms (CPU, GPU, TPUs).
            \item Easy model building via high-level APIs (like Keras) and low-level APIs for customization.
        \end{itemize}
        
        \item \textbf{Powerful Ecosystem:}
        \begin{itemize}
            \item Includes TensorFlow Extended (TFX) for ML pipelines, TensorFlow Lite for mobile devices, and TensorFlow.js for web browsers.
        \end{itemize}
        
        \item \textbf{Automatic Differentiation:}
        \begin{itemize}
            \item Utilizes autograd for automatic gradient calculation, simplifying algorithms like backpropagation.
        \end{itemize}
        
        \item \textbf{Scalability:}
        \begin{itemize}
            \item Handles large datasets and complex models across distributed systems efficiently.
        \end{itemize}

        \item \textbf{Community and Documentation:}
        \begin{itemize}
            \item Strong community support and extensive documentation for developers.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of TensorFlow in AI}
    \begin{itemize}
        \item \textbf{Computer Vision:}
        \begin{itemize}
            \item Image classification, object detection, and image generation utilizing models such as Convolutional Neural Networks (CNNs).
        \end{itemize}
        
        \item \textbf{Natural Language Processing (NLP):}
        \begin{itemize}
            \item Tasks include text classification, sentiment analysis, and machine translation using RNNs and Transformers.
        \end{itemize}
        
        \item \textbf{Reinforcement Learning:}
        \begin{itemize}
            \item Algorithm development for gaming, robotics, and optimization problems.
        \end{itemize}

        \item \textbf{Healthcare:}
        \begin{itemize}
            \item Analyzing medical images, predicting outcomes, and assisting in diagnoses.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Building a Simple Neural Network with TensorFlow}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow.keras import layers, models

# Define the model
model = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(input_dimension,)),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

# Summary of the model
model.summary()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    TensorFlow is a versatile and powerful framework supporting a diverse range of AI applications. Its ability to manage anything from simple linear regression to complex deep learning architectures makes it essential in the machine learning toolkit. 

    In the next segment, we will explore practical aspects, including basic operations such as tensors and computational graphs.
\end{frame}

\begin{frame}
    \frametitle{Basic Operations in TensorFlow}
    \begin{block}{Introduction}
        TensorFlow is an open-source machine learning framework developed by Google. It allows users to build and train models using data. This presentation covers basic operations, including tensors, computational graphs, and essential functions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Tensors}
    \begin{block}{Definition}
        A tensor is a multi-dimensional array fundamental to TensorFlow, representing:
    \end{block}
    \begin{itemize}
        \item \textbf{Scalar (0D)}: A single value (e.g., \(5\)).
        \item \textbf{Vector (1D)}: An array of values (e.g., \([1, 2, 3]\)).
        \item \textbf{Matrix (2D)}: An array of vectors (e.g., \([[1, 2], [3, 4]]\)).
        \item \textbf{Tensor (nD)}: Generalization of scalars, vectors, and matrices.
    \end{itemize}
    \begin{block}{Example}
        \begin{lstlisting}[language=Python]
import tensorflow as tf

# Creating a scalar
scalar = tf.constant(5)
# Creating a vector
vector = tf.constant([1, 2, 3])
# Creating a matrix
matrix = tf.constant([[1, 2], [3, 4]])
# Creating a 3D tensor
tensor_3d = tf.constant([[[1], [2]], [[3], [4]]])

print("Scalar:", scalar)
print("Vector:", vector)
print("Matrix:", matrix)
print("3D Tensor:", tensor_3d)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Computational Graphs}
    \begin{block}{Definition}
        A computational graph represents mathematical operations and their dependencies.
    \end{block}
    \begin{itemize}
        \item \textbf{Nodes}: Represent operations (e.g., addition, multiplication).
        \item \textbf{Edges}: Represent the tensors (inputs/outputs).
        \item Graphs are built dynamically for efficient computation.
    \end{itemize}
    \begin{block}{Example}
        \begin{lstlisting}[language=Python]
# Define a simple computation graph
a = tf.constant(2)
b = tf.constant(3)
c = tf.add(a, b)  # Operation node

print("Graph result: ", c.numpy())  # Outputs 5
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Basic TensorFlow Functions}
    \begin{block}{Essential Operations}
        Basic operations in TensorFlow include:
    \end{block}
    \begin{itemize}
        \item \textbf{Addition}: \( \text{tf.add}(tensor1, tensor2) \)
        \item \textbf{Multiplication}: \( \text{tf.multiply}(tensor1, tensor2) \)
        \item \textbf{Matrix Multiplication}: \( \text{tf.matmul}(matrix1, matrix2) \)
        \item \textbf{Activation Functions}: (e.g., ReLU, sigmoid).
    \end{itemize}
    \begin{block}{Example of Basic Operations}
        \begin{lstlisting}[language=Python]
# Basic tensor operations
x = tf.constant([1, 2, 3])
y = tf.constant([4, 5, 6])

addition_result = tf.add(x, y)
multiplication_result = tf.multiply(x, y)

print("Addition result:", addition_result.numpy())  # Outputs [5, 7, 9]
print("Multiplication result:", multiplication_result.numpy())  # Outputs [4, 10, 18]
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Tensors are the core data structures in TensorFlow, representing data of various dimensions.
        \item Computational graphs facilitate the execution of tensor operations, optimizing performance and memory usage.
        \item Understanding basic operations is crucial for effective model construction and training.
    \end{itemize}
    \begin{block}{Conclusion}
        Mastering these basic operations in TensorFlow is essential for building robust machine learning applications. Explore similar concepts in PyTorch for a comprehensive understanding of different frameworks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of PyTorch}
    % Introduction to the PyTorch framework, its features, and its applications in AI.
    PyTorch is an open-source machine learning library developed by Facebook's AI Research lab. It excels in deep learning and AI due to its flexibility and ease of use.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of PyTorch}
    \begin{enumerate}
        \item \textbf{Dynamic Computation Graph:}
        \begin{itemize}
            \item Adjusts on-the-fly, ideal for variable input lengths.
        \end{itemize}
        
        \item \textbf{Tensors:}
        \begin{itemize}
            \item Fundamental data structure, akin to NumPy arrays, supporting GPU operations.
        \end{itemize}
        
        \item \textbf{Automatic Differentiation (Autograd):}
        \begin{itemize}
            \item Simplifies backpropagation with automatic gradient computation.
        \end{itemize}

        \item \textbf{Extensive Library Support:}
        \begin{itemize}
            \item A wide range of libraries for vision, NLP, graph learning, etc.
        \end{itemize}
        
        \item \textbf{Community and Resources:}
        \begin{itemize}
            \item Active community with tutorials, forums, and documentation.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in AI}
    \begin{itemize}
        \item \textbf{Computer Vision:} 
        \begin{itemize}
            \item Image classification, object detection, and image generation.
        \end{itemize}

        \item \textbf{Natural Language Processing:}
        \begin{itemize}
            \item Language translation, sentiment analysis, chatbots.
        \end{itemize}

        \item \textbf{Reinforcement Learning:}
        \begin{itemize}
            \item Training agents through environment interaction.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Points}
    \begin{block}{Summary}
        PyTorch simplifies building and training deep learning models with dynamic computation graphs and efficient tensor operations. Community support and libraries boost its utility in various AI applications.
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Dynamic graphs for flexible model design.
            \item Intuitive syntax and ease of use.
            \item Strong support for GPU acceleration.
            \item Robust autograd system for backpropagation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of PyTorch Basics}
    \begin{itemize}
        \item PyTorch is an open-source machine learning library.
        \item Utilizes tensors, similar to NumPy arrays but with additional capabilities.
        \item Supports GPU acceleration for efficient computation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Tensor Initialization}
    \begin{block}{Common Initialization Methods}
        \begin{itemize}
            \item \textbf{From NumPy Array:}
            \begin{lstlisting}[language=Python]
import numpy as np
import torch

np_array = np.array([[1, 2], [3, 4]])
tensor_from_numpy = torch.from_numpy(np_array)
            \end{lstlisting}
            
            \item \textbf{Creating from Python Lists:}
            \begin{lstlisting}[language=Python]
list_array = [[1, 2], [3, 4]]
tensor_from_list = torch.tensor(list_array)
            \end{lstlisting}

            \item \textbf{Zeros, Ones, Random Values:}
            \begin{lstlisting}[language=Python]
zeros_tensor = torch.zeros((2, 2))   # 2x2 tensor of zeros
ones_tensor = torch.ones((2, 2))      # 2x2 tensor of ones
random_tensor = torch.rand((2, 2))    # 2x2 tensor with random values
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Tensor Manipulation}
    \begin{block}{Common Operations}
        \begin{itemize}
            \item \textbf{Shape Manipulation:}
            \begin{lstlisting}[language=Python]
tensor = torch.tensor([[1, 2], [3, 4]])
reshaped_tensor = tensor.view(4)  # Reshape to a 1D tensor
            \end{lstlisting}

            \item \textbf{Slicing:}
            \begin{lstlisting}[language=Python]
sliced_tensor = tensor[0, :]  # Slicing the first row
            \end{lstlisting}

            \item \textbf{Mathematical Operations:}
            \begin{lstlisting}[language=Python]
tensor_a = torch.tensor([[1, 2], [3, 4]])
tensor_b = torch.tensor([[5, 6], [7, 8]])
sum_tensor = tensor_a + tensor_b  # Result: [[6, 8], [10, 12]]
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Automatic Differentiation}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item Tensors can track gradients for backpropagation:
            \begin{lstlisting}[language=Python]
tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True)
            \end{lstlisting}

            \item \textbf{Performing Operations:}
            \begin{lstlisting}[language=Python]
y = tensor * 2  # Simple operation
z = y.mean()    # Compute mean
            \end{lstlisting}

            \item \textbf{Calculating Gradients:}
            \begin{lstlisting}[language=Python]
z.backward()           # Computes gradients of z with respect to the tensor
print(tensor.grad)     # Displays gradients
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item Tensors are the core data structure in PyTorch, enabling efficient computation.
        \item Tensor manipulations are fundamental for preparing data for machine learning models.
        \item Automatic differentiation facilitates easy gradient computation for model training.
        \item This foundation prepares you for more complex topics in machine learning and deep learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building an AI Model with TensorFlow}
    \begin{block}{Overview}
        Step-by-step process of creating a simple AI model using TensorFlow:
        \begin{itemize}
            \item Data Preparation
            \item Model Building
            \item Training the Model
            \item Making Predictions
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 1: Data Preparation}
    \begin{block}{Concept}
        Data preparation is the foundation of any machine learning model, involving cleaning, organizing, and splitting data.
    \end{block}
    \begin{block}{Example}
        Using the MNIST dataset of handwritten digits:
        \begin{itemize}
            \item Normalize pixel values (0-255) to [0, 1].
            \item Split into 60,000 training samples and 10,000 test samples.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# Load the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the data
x_train, x_test = x_train / 255.0, x_test / 255.0

# Reshape data to fit the model
x_train = x_train.reshape(-1, 28, 28, 1)  # Adding channel dimension for CNN
x_test = x_test.reshape(-1, 28, 28, 1)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 2: Model Building}
    \begin{block}{Concept}
        Constructing a neural network involves specifying the architecture.
    \end{block}
    \begin{block}{Key Layers}
        \begin{itemize}
            \item Convolutional Layers: Extract features from images.
            \item Pooling Layer: Reduce dimensionality.
            \item Dense Layer: Make predictions.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')  # 10 output classes
])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 3: Compile the Model}
    \begin{block}{Concept}
        Compiling the model defines the loss function, optimizer, and metrics for evaluation.
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Loss Function: Measures performance.
            \item Optimizer: Adjusts weights to minimize loss.
            \item Metrics: Evaluate model performance.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 4: Training the Model}
    \begin{block}{Concept}
        Training: The process where the model learns from data.
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Epoch: One complete pass through the training data.
            \item Batch Size: Number of samples processed before weight update.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 5: Evaluate and Make Predictions}
    \begin{block}{Concept}
        After training, check the model’s performance on the test dataset.
    \end{block}
    \begin{lstlisting}[language=Python]
test_loss, test_acc = model.evaluate(x_test, y_test)
print('Test accuracy:', test_acc)

# Making predictions
predictions = model.predict(x_test)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{itemize}
        \item Data Preparation: Critical to model success.
        \item Model Architecture: Essential for feature extraction and prediction.
        \item Training Process: Understanding epochs and batch size is vital.
        \item Evaluation: Validate performance before deployment.
    \end{itemize}
    \begin{block}{Formula for Loss Function}
        \begin{equation}
            \text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} \log(p(y_i | x_i))
        \end{equation}
        Where $N$ is the number of samples and $p(y_i | x_i)$ is the predicted probability of class $y_i$ given input $x_i$.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building an AI Model with PyTorch - Overview}
    \begin{block}{Step-by-Step Process}
        Creating an AI model with PyTorch involves several key stages:
        \begin{itemize}
            \item Data Preparation
            \item Model Building
            \item Training
            \item Evaluation
        \end{itemize}
        Let's break these down.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building an AI Model with PyTorch - Data Preparation}
    \begin{block}{1. Data Preparation}
        \begin{itemize}
            \item \textbf{Data Loading:}
            Use PyTorch's \texttt{torchvision} library for loading image datasets.
            \begin{lstlisting}[language=Python]
from torchvision import datasets, transforms
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST(root='/data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
            \end{lstlisting}

            \item \textbf{Normalizing Data:}
            Normalization is crucial to improve model convergence.
            \begin{lstlisting}[language=Python]
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Mean and Std Dev
])
            \end{lstlisting}
        \end{itemize}
    \end{block}

    \begin{block}{Key Point}
        Data needs to be appropriately transformed (e.g., normalization) to enhance model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building an AI Model with PyTorch - Model Building}
    \begin{block}{2. Model Building}
        \begin{itemize}
            \item \textbf{Define the Neural Network:}
            Create a simple feedforward neural network.
            \begin{lstlisting}[language=Python]
import torch.nn as nn
import torch.nn.functional as F

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)  # Input layer
        self.fc2 = nn.Linear(128, 64)      # Hidden layer
        self.fc3 = nn.Linear(64, 10)       # Output layer

    def forward(self, x):
        x = x.view(-1, 28*28)  # Flatten the input
        x = F.relu(self.fc1(x)) # Activation function
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.log_softmax(x, dim=1)  # Softmax for multi-class classification
            \end{lstlisting}
            \item \textbf{Example:}
            This model takes 28x28 pixel images (like MNIST) and outputs class probabilities across 10 classes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building an AI Model with PyTorch - Training and Evaluation}
    \begin{block}{3. Training the Model}
        \begin{itemize}
            \item \textbf{Set Loss Function and Optimizer:}
            \begin{lstlisting}[language=Python]
import torch.optim as optim

model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
            \end{lstlisting}
            
            \item \textbf{Training Loop:}
            \begin{lstlisting}[language=Python]
for epoch in range(5):  # Training for 5 epochs
    for data, target in train_loader:
        optimizer.zero_grad()      # Clear gradients
        output = model(data)      # Forward pass
        loss = criterion(output, target)  # Compute loss
        loss.backward()           # Backward pass
        optimizer.step()          # Update weights
            \end{lstlisting}
        \end{itemize}
        
        \begin{block}{Key Point}
            Training involves iteratively adjusting model parameters based on the loss computed from predictions.
        \end{block}
    \end{block}

    \begin{block}{4. Evaluation}
        \begin{itemize}
            \item \textbf{Model Performance:}
            Validate your model on unseen data and calculate metrics like accuracy.
            \begin{lstlisting}[language=Python]
with torch.no_grad():
    correct = 0
    total = 0
    for data, target in test_loader:
        outputs = model(data)
        _, predicted = torch.max(outputs.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
    print(f'Accuracy: {100 * correct / total}%')
            \end{lstlisting}
            \item \textbf{Example:}
            A predicted accuracy of 98% on a test set indicates a solid benchmark for a basic MNIST classifier.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building an AI Model with PyTorch - Summary}
    \begin{block}{Summary}
        By following these steps – data preparation, model building, training, and evaluation – you can create a robust AI model using PyTorch. 
        This emphasizes the importance of each stage in developing effective machine learning solutions.
    \end{block}

    \begin{block}{Additional Resources}
        \begin{itemize}
            \item \texttt{PyTorch Documentation:} \url{https://pytorch.org/docs/stable/index.html}
            \item \texttt{MNIST Dataset:} \url{http://yann.lecun.com/exdb/mnist/}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TensorFlow and PyTorch - Overview}
    \begin{block}{Overview}
        TensorFlow and PyTorch are two leading frameworks for building and deploying machine learning models. 
        They cater to various needs in research and industry, having distinct functionalities, strengths, and user experiences.
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Overview of differences and similarities.
            \item When to use each framework.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TensorFlow and PyTorch - Key Differences}
    \begin{enumerate}
        \item \textbf{Computational Graphs:}
            \begin{itemize}
                \item \textbf{TensorFlow:} Utilizes static graphs (v1.x); Eager execution in v2.x.
                \item \textbf{PyTorch:} Dynamic graphs allow real-time changes during execution.
            \end{itemize}
        
        \item \textbf{Syntax and Usability:}
            \begin{itemize}
                \item \textbf{TensorFlow:} More complex syntax, requires boilerplate code.
                \item \textbf{PyTorch:} Pythonic syntax, enhancing readability and learnability.
            \end{itemize}
        
        \item \textbf{Ecosystem and Community:}
            \begin{itemize}
                \item \textbf{TensorFlow:} Larger ecosystem with support from Google.
                \item \textbf{PyTorch:} Rapidly growing community, strong emphasis on research from Facebook.
            \end{itemize}
        
        \item \textbf{Performance:}
            \begin{itemize}
                \item \textbf{TensorFlow:} Optimized for production environments.
                \item \textbf{PyTorch:} Faster in research phase but may need optimizations for production.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TensorFlow and PyTorch - Key Similarities and Use Cases}
    \begin{block}{Key Similarities}
        \begin{itemize}
            \item Support for GPU acceleration.
            \item Robust support for deep learning components.
            \item Interoperability with libraries like Keras and ONNX.
        \end{itemize}
    \end{block}

    \begin{block}{Use Cases}
        \begin{itemize}
            \item \textbf{TensorFlow:} Best for production-level applications, scalability, and multi-disciplinary collaboration.
            \item \textbf{PyTorch:} Preferred for research, rapid prototyping, and tasks needing frequent adjustments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TensorFlow and PyTorch - Code Examples}
    \begin{block}{TensorFlow Example}
        \begin{lstlisting}[language=Python]
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        \end{lstlisting}
    \end{block}

    \begin{block}{PyTorch Example}
        \begin{lstlisting}[language=Python]
import torch.nn as nn
import torch.optim as optim

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TensorFlow and PyTorch - Conclusion}
    \begin{block}{Conclusion}
        Choosing between TensorFlow and PyTorch depends on:
        \begin{itemize}
            \item Project needs and workflow preferences.
            \item Understand differences and use cases to make informed decisions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges and Troubleshooting in TensorFlow and PyTorch}
    \begin{block}{Common Issues Faced}
        \begin{itemize}
            \item \textbf{Installation Problems:} Errors due to incompatible versions.
            \item \textbf{Memory Issues:} Out-of-memory (OOM) errors when training models.
            \item \textbf{Model Convergence:} Models failing to converge or producing high loss.
            \item \textbf{Debugging Dynamic Computation Graphs (PyTorch):} Difficulty in tracing errors.
            \item \textbf{Eager Execution vs. Graph Execution (TensorFlow):} Confusion over execution modes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Troubleshooting Methods}
    \begin{block}{Installation Issues}
        \begin{itemize}
            \item Ensure compatibility between TensorFlow/PyTorch and dependencies.
            \item Use \texttt{pip} or \texttt{conda}:
            \begin{lstlisting}[language=bash]
pip install tensorflow==2.x.x
pip install torch==1.x.x
            \end{lstlisting}
        \end{itemize}
    \end{block}
    
    \begin{block}{Memory Management}
        \begin{itemize}
            \item Use \texttt{torch.cuda.empty\_cache()} or set memory growth for TensorFlow:
            \begin{lstlisting}[language=python]
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Convergence and Debugging}
    \begin{block}{Model Convergence}
        \begin{itemize}
            \item Check learning rate settings; implement schedulers.
            \item Example for using Adam optimizer in TensorFlow:
            \begin{lstlisting}[language=python]
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
            \end{lstlisting}
        \end{itemize}
    \end{block}
    
    \begin{block}{Debugging in PyTorch}
        \begin{itemize}
            \item Use \texttt{print()} statements for tensor shapes.
            \item Enable anomaly detection:
            \begin{lstlisting}[language=python]
torch.autograd.set_detect_anomaly(True)
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Execution Modes and Key Points}
    \begin{block}{Understanding Execution Modes in TensorFlow}
        \begin{itemize}
            \item Use \texttt{tf.function} for optimizing execution,
            \item Familiarize yourself with eager execution for easy debugging.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Always check library versions for compatibility.
            \item Employ debugging tools throughout the model building process.
            \item Mindful adjustment of hyperparameters is crucial.
            \item Embrace PyTorch's dynamic nature for flexibility.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        The process of troubleshooting in TensorFlow and PyTorch is integral to successful model development. A systematic approach enhances your ability to overcome common challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of AI Models - Overview}
    \begin{itemize}
        \item AI models using TensorFlow and PyTorch are revolutionizing industries.
        \item These frameworks facilitate development, training, and deployment of deep learning models.
        \item Focus on key applications in various domains.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of AI Models - Key Applications}
    \begin{enumerate}
        \item \textbf{Image Recognition \& Computer Vision}
            \begin{itemize}
                \item AI models process visual data for healthcare and automotive sectors.
                \item \textit{Example}: CNNs for medical imaging analysis (e.g., tumor detection).
            \end{itemize}
        
        \item \textbf{Natural Language Processing (NLP)}
            \begin{itemize}
                \item AI models understand and generate human language.
                \item \textit{Example}: Transformer models like BERT or GPT.
            \end{itemize}
        
        \item \textbf{Predictive Analytics}
            \begin{itemize}
                \item Forecasting trends based on historical data for better decision-making.
                \item \textit{Example}: AI in retail for sales forecasting.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of AI Models - Continued}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Autonomous Systems}
            \begin{itemize}
                \item AI powers self-driving technology using sensor data.
                \item \textit{Example}: Tesla and Waymo using deep reinforcement learning.
            \end{itemize}
        
        \item \textbf{Recommendation Systems}
            \begin{itemize}
                \item AI analyzes user behavior for tailored recommendations.
                \item \textit{Example}: E-commerce platforms using filtering methods.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of AI Models - Conclusion}
    \begin{itemize}
        \item AI models are transforming industry operations via algorithms and data.
        \item Diverse applications enhance efficiency and customer engagement.
        \item Understanding these concepts prepares professionals for real-world challenges.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in AI Frameworks - Overview}
    \begin{itemize}
        \item \textbf{TensorFlow} and \textbf{PyTorch} are leading frameworks for building AI models.
        \item They provide tools for deep learning, machine learning, and data flow programming.
        \item Continuous evolution is crucial to meet the demands of researchers and industries.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in AI Frameworks - Emerging Trends}
    \begin{enumerate}
        \item \textbf{Increased Focus on Scalability}
            \begin{itemize}
                \item Prioritization of scaling models to handle massive datasets.
                \item Transition from single-device training to distributed training.
            \end{itemize}
            
        \item \textbf{Integration of AutoML}
            \begin{itemize}
                \item Simplification of model selection and optimization.
                \item Tools like Google’s AutoML for non-experts in ML.
            \end{itemize}

        \item \textbf{Support for Reinforcement Learning}
            \begin{itemize}
                \item Application of RL in various domains (e.g., gaming, robotics).
                \item Integration with frameworks like OpenAI’s Gym.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in AI Frameworks - Continued Trends}
    \begin{enumerate}[resume]
        \item \textbf{Heterogeneous Computing}
            \begin{itemize}
                \item Strategy to support diverse hardware for optimized performance.
                \item Example: TensorFlow's TensorRT for NVIDIA GPUs.
            \end{itemize}

        \item \textbf{Model Interpretability and Explainability}
            \begin{itemize}
                \item Focus on understanding AI decisions for trust and compliance.
                \item Tools like LIME and SHAP are gaining importance.
            \end{itemize}

        \item \textbf{Edge AI Deployment}
            \begin{itemize}
                \item Enabling models to run on edge devices for real-time applications.
                \item Example: TensorFlow Lite for mobile systems.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in AI Frameworks - Key Takeaways}
    \begin{itemize}
        \item AI frameworks are rapidly evolving to meet scalability, integration, and interpretability challenges.
        \item Staying informed about these trends is crucial for professionals in AI and deep learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in AI Frameworks - Code Snippet Example}
    \begin{lstlisting}[language=Python]
import torch
import torch.distributed as dist

# Initialize the process group for distributed training
dist.init_process_group("nccl")

# Model parallelism example
model = MyModel().to(device)
model = torch.nn.parallel.DistributedDataParallel(model)

# Training loop
for data, target in data_loader:
    optimizer.zero_grad()
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in AI Frameworks - Conclusion}
    \begin{itemize}
        \item Future developments in frameworks like TensorFlow and PyTorch will enhance accessibility, scalability, and efficiency.
        \item Understanding these trends is essential for anyone involved in AI and deep learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways}
    \begin{enumerate}
        \item \textbf{Understanding TensorFlow and PyTorch}
        \begin{itemize}
            \item \textbf{TensorFlow}: Open-source library by Google for numerical computation and machine learning using data flow graphs.
            \item \textbf{PyTorch}: Open-source library by Facebook known for dynamic computation graphs, allowing flexibility and ease of debugging.
        \end{itemize}
        
        \item \textbf{Importance of Practical Skills}
        \begin{itemize}
            \item Hands-on experience is vital; theoretical knowledge needs to be translated into real-world applications.
            \item Developing projects enhances understanding of concepts like model building and hyperparameter tuning.
            \item Familiarity with collaboration tools (e.g., Git) and cloud platforms (e.g., Google Cloud, AWS) is crucial.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Techniques and Evaluating Models}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Key Techniques in TensorFlow/PyTorch}
        \begin{itemize}
            \item Model definition using high-level APIs:
            \begin{lstlisting}[language=python]
# TensorFlow Example
import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(10, activation='softmax')
])

# PyTorch Example
import torch.nn as nn
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Evaluating Models}
        \begin{itemize}
            \item Essential metrics: accuracy, precision, recall, F1-score.
            \item Example of accuracy calculation:
            \begin{equation}
                \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}} \times 100\%
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Future Learning and Final Thoughts}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Future Learning Paths}
        \begin{itemize}
            \item Stay updated with continuous learning platforms for advanced courses in TensorFlow, PyTorch, and AI applications.
            \item Participate in competitions (e.g., Kaggle) to refine skills.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Final Thoughts}
        \begin{itemize}
            \item Integration of practical skills with theoretical knowledge is crucial for success in AI.
            \item Engage in challenging projects to deepen understanding of AI concepts.
            \item Practical skills enhance employability and readiness for real-world challenges in AI.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}