\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Basic Title Page Information
\title[Clustering and Dimensionality Reduction]{Chapter 6: Clustering and Dimensionality Reduction}
\author{John Smith, Ph.D.}
\institute{Department of Computer Science\\ University Name}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Clustering and Dimensionality Reduction}
    \begin{block}{Overview}
        Clustering and dimensionality reduction are essential techniques in data analysis and machine learning that allow us to extract meaningful insights from complex datasets. 
    \end{block}
    \begin{itemize}
        \item An overview of both concepts and their importance.
        \item Understanding their interrelation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Part 1}
    \begin{block}{1. Clustering}
        \textbf{Definition:} Clustering is an unsupervised learning technique that groups similar data points into clusters based on their characteristics. 
        It aims to maximize intra-cluster similarity while minimizing inter-cluster similarity.
    \end{block}
    \begin{itemize}
        \item \textbf{Importance:} 
        \begin{itemize}
            \item Facilitates data exploration and pattern recognition.
            \item Identifies natural groupings in data (e.g., customer segmentation, anomaly detection).
        \end{itemize}
    \end{itemize}
    \begin{block}{Example}
        K-means clustering divides a dataset of customer purchases into distinct groups, allowing a business to tailor marketing strategies to different customer personas.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Part 2}
    \begin{block}{2. Dimensionality Reduction}
        \textbf{Definition:} Dimensionality reduction involves reducing the number of variables (dimensions) in a dataset while preserving its underlying structure.
    \end{block}
    \begin{itemize}
        \item \textbf{Importance:}
        \begin{itemize}
            \item Enhances visualization (e.g., reducing data from 100 dimensions to 2 for plotting).
            \item Improves efficiency in machine learning models by reducing complexity and computation time.
        \end{itemize}
        \item \textbf{Popular Techniques:} 
        \begin{itemize}
            \item Principal Component Analysis (PCA)
            \item t-Distributed Stochastic Neighbor Embedding (t-SNE)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interconnection}
    \begin{block}{Clustering and Dimensionality Reduction}
        Clustering often utilizes dimensionality reduction to preprocess data, making it easier to identify natural groups. 
        By reducing the dimensions, the clustering algorithm can focus on the most significant features of the data.
    \end{block}
    \begin{block}{PCA Formula Example}
        To transform data using PCA, the following key steps are performed:
    \end{block}
    \begin{enumerate}
        \item \textbf{Standardization}: Scale the data to have a mean of 0 and a variance of 1.
        \begin{equation}
            Z = \frac{(X - \mu)}{\sigma}
        \end{equation}
        \item \textbf{Covariance Matrix Calculation}: Compute the covariance matrix to identify how features vary together.
        \item \textbf{Eigenvalue Decomposition}: Determine the eigenvectors and eigenvalues to find the principal components.
        \item \textbf{Feature Reduction}: Select the top $k$ eigenvectors that correspond to the largest eigenvalues.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Definition}
    Clustering is an \textbf{unsupervised learning technique} used in data analysis that groups similar data points based on their features without having predefined labels. 
    The primary objective is to discover underlying structures within the data. Unlike supervised learning, clustering does not rely on labeled outcomes, making it widely applicable in exploratory data analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Purpose}
    \begin{itemize}
        \item \textbf{Data Exploration}: Helps in identifying inherent patterns or distributions in large datasets.
        \item \textbf{Segmentation}: Enables the division of datasets into meaningful subsets, facilitating targeted analysis.
        \item \textbf{Anomaly Detection}: Identifies outliers or unusual data points that do not conform to expected clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Applications}
    \begin{enumerate}
        \item \textbf{Market Segmentation}: Businesses use clustering to group customers into segments based on purchasing behavior, allowing for tailored marketing strategies.
        \item \textbf{Social Network Analysis}: Identifies communities within social networks by clustering users with similar interests or interactions.
        \item \textbf{Image Segmentation}: In computer vision, clustering algorithms can group similar pixels within an image, aiding in object recognition.
        \item \textbf{Recommendation Systems}: Clustering can enhance product recommendations by grouping similar user preferences and behaviors.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Key Points}
    \begin{itemize}
        \item \textbf{Unsupervised Learning}: Clustering is critical when we lack labeled data, allowing us to gain insights from unstructured data.
        \item \textbf{Similarity Measures}: Clustering relies on metrics such as \textbf{Euclidean distance} or \textbf{cosine similarity} to define the closeness between data points.
        \item Popular algorithms include \textbf{K-Means}, \textbf{Hierarchical Clustering}, and \textbf{DBSCAN}, each with unique approaches and assumptions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Algorithm}
    The \textbf{K-Means Algorithm} is a common clustering algorithm that operates using the following steps:
    \begin{enumerate}
        \item \textbf{Initialization}: Select \textbf{K} initial centroids randomly.
        \item \textbf{Assignment}: Assign each data point \(x_i\) to the nearest centroid:
        \begin{equation}
        C(i) = \arg \min_{k} \| x_i - \mu_k \|^2
        \end{equation}
        where \(C(i)\) is the cluster assignment and \(\mu_k\) represents the centroid of cluster \(k\).
        \item \textbf{Update}: Update centroids as the mean of assigned points:
        \begin{equation}
        \mu_k = \frac{1}{N_k} \sum_{x_i \in C_k} x_i
        \end{equation}
        where \(N_k\) is the number of points in cluster \(k\).
        \item \textbf{Iterate}: Repeat the assignment and update steps until convergence.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By understanding clustering, we unlock powerful techniques for revealing the structure in multidimensional data, thereby facilitating deeper insights and informed decision-making in various fields.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques}
    
    \begin{block}{Overview of Clustering Techniques}
        Clustering is a key technique in unsupervised learning, enabling the grouping of data points into clusters based on their similarities. The three common types of clustering techniques are:
    \end{block}

    \begin{enumerate}
        \item Partitioning Methods
        \item Hierarchical Methods
        \item Density-Based Methods
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Partitioning Methods}
    
    \begin{block}{Description}
        Partitioning methods divide the dataset into distinct clusters, where each object belongs to a single cluster. The most well-known algorithm is K-means.
    \end{block}

    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Requires the number of clusters (K) to be defined beforehand.
            \item Iteratively assigns data points to the nearest centroid and updates centroids based on the mean of the assigned points.
        \end{itemize}
        
        \item \textbf{Example:} In K-means, if you have customer data with two features (e.g., age and spending), you can define \( K=3 \). The algorithm assigns customers to three clusters based on similarities in age and spending patterns.
        
        \item \textbf{Mathematical Formula:} 
        \begin{equation}
            J = \sum_{k=1}^{K} \sum_{i=1}^{n} \| x_i^{(k)} - c_k \|^2
        \end{equation}
        where \( c_k \) is the centroid of cluster \( k \) and \( x_i^{(k)} \) represents data points in cluster \( k \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Hierarchical Methods \& 3. Density-Based Methods}
    
    \begin{block}{2. Hierarchical Methods}
        These methods build a hierarchy of clusters either in a bottom-up (agglomerative) or top-down (divisive) manner. They do not require the number of clusters to be specified a priori.
    \end{block}

    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Creates dendrograms (tree-like structures) to visualize how clusters are formed at various levels of similarity.
        \end{itemize}
    
        \item \textbf{Example:} Using agglomerative clustering on a set of animals based on size and habitat, we can merge smaller clusters (e.g., cats and dogs) into larger clusters (e.g., mammals).
        
        \item \textbf{Common Linkage Methods:}
        \begin{itemize}
            \item Single Linkage: Minimum distance between clusters
            \item Complete Linkage: Maximum distance between clusters
            \item Average Linkage: Average distance between clusters
        \end{itemize}
    \end{itemize}
   
    \begin{block}{3. Density-Based Methods}
        Density-based methods group together data points that are closely packed while marking points in low-density regions as outliers. DBSCAN is a prominent algorithm.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Identifies clusters of varying shapes and sizes.
            \item Requires two parameters: epsilon (\( \epsilon \)) – the radius for neighborhood search, and MinPts – the minimum number of points required to form a dense region.
        \end{itemize}
    
        \item \textbf{Example:} In geographical data, DBSCAN can identify clusters of houses based on population density.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Overview}
    \begin{block}{Overview of K-means Algorithm}
        K-means clustering is an unsupervised machine learning algorithm used to partition data into \textbf{K distinct clusters}. The goal is to:
        \begin{itemize}
            \item Minimize the variance within each cluster
            \item Maximize the variance between clusters
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - How it Works}
    \begin{enumerate}
        \item \textbf{Initialization:}
        \begin{itemize}
            \item Select \textbf{K} initial centroids randomly from the dataset.
        \end{itemize}
        \item \textbf{Assignment Step:}
        \begin{itemize}
            \item Assign each data point to the nearest centroid using a distance metric (commonly Euclidean distance).
            \item \textbf{Euclidean Distance Formula:} 
            \begin{equation}
                d(x, c) = \sqrt{\sum_{i=1}^{n} (x_i - c_i)^2}
            \end{equation}
        \end{itemize}
        \item \textbf{Update Step:}
        \begin{itemize}
            \item Calculate new centroids as the mean of assigned data points.
            \item \textbf{Centroid Calculation:}
            \begin{equation}
                c_j = \frac{1}{N_j} \sum_{x_i \in C_j} x_i
            \end{equation}
        \end{itemize}
        \item \textbf{Repeat:} Repeat Steps 2 and 3 until convergence.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Key Considerations}
    \begin{block}{Distance Measurement}
        The choice of distance metric is vital:
        \begin{itemize}
            \item \textbf{Euclidean Distance}: Best for spherical clusters.
            \item \textbf{Manhattan Distance}: Suitable for high-dimensional spaces.
        \end{itemize}
    \end{block}
    \begin{block}{Criteria for Stopping}
        \begin{itemize}
            \item Convergence of centroids (minimal change)
            \item Maximum number of iterations
            \item Monitoring inertia (sum of squared distances)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Initialization and Limitations - Overview}
    \begin{itemize}
        \item K-means Clustering influences the performance of algorithms through proper centroid initialization.
        \item Initialization methods: Random, K-means++, and others.
        \item Challenges: Local minima and sensitivity to initial conditions.
        \item Solutions: Multiple runs, K-means++ initialization, and the Elbow Method.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Initialization}
    \begin{block}{Introduction}
        K-means clustering partitions data into K clusters, relying heavily on the selection of initial centroids.
    \end{block}
    
    \begin{block}{Selecting Initial Centroids}
        \begin{itemize}
            \item \textbf{Random Initialization:} Selects K centroids randomly from the data points. 
            \item \textbf{Example:} Choosing \((2,3)\) and \((1,1)\) from points \((2,3), (5,8), (1,1)\).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Initialization}
    \begin{enumerate}
        \item \textbf{Local Minima:} The algorithm may settle at a local minimum, leading to suboptimal clustering due to poor initial centroid selection.
        
        \item \textbf{Sensitivity to Initial Conditions:} Variations in initial centroid placements can result in significant differences in clustering outcomes, especially in heterogeneous datasets.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Addressing Initialization Issues}
    \begin{enumerate}
        \item \textbf{Multiple Runs:}
            \begin{itemize}
                \item Run K-means multiple times with different initial conditions (e.g., 10-20 times) and select the best outcome based on performance measures like inertia.
            \end{itemize}
        
        \item \textbf{K-means++ Initialization:}
            \begin{itemize}
                \item \textbf{Algorithm:}
                    \begin{enumerate}
                        \item First centroid is chosen randomly from the dataset.
                        \item Subsequent centroids are chosen based on the squared distance to the closest current centroid.
                    \end{enumerate}
                \item \textbf{Advantage:} Spreads out initial centroids effectively for better convergence.
                \item \textbf{Distance Formula:}
                    \[
                    D(x) = \min_{c \in C} ||x - c||^2
                    \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Elbow Method and Summary}
    \begin{enumerate}
        \item \textbf{Elbow Method:}
            \begin{itemize}
                \item A heuristic for determining optimal K by plotting the sum of squared distances for varying K values. The 'elbow' indicates the best cluster number.
            \end{itemize}
        \item \textbf{Key Points:}
            \begin{itemize}
                \item Proper initialization enhances convergence speed and overall clustering quality.
                \item Techniques like K-means++ and multiple runs can effectively mitigate poor initialization effects.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Hierarchical Clustering}
    \begin{itemize}
        \item Hierarchical clustering is an unsupervised machine learning technique.
        \item It groups similar data points into clusters, forming a tree-like structure (dendrogram).
        \item Two main approaches: \textbf{Agglomerative} and \textbf{Divisive}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Agglomerative Clustering}
    \begin{itemize}
        \item \textbf{Bottom-Up Approach}:
            \begin{itemize}
                \item Starts with each data point as its own cluster.
                \item Merges closest pairs based on similarity until one large cluster is formed.
            \end{itemize}
        \item \textbf{Linkage Criteria}:
            \begin{itemize}
                \item \textbf{Single Linkage}: Minimum distance between members.
                \item \textbf{Complete Linkage}: Maximum distance between members.
                \item \textbf{Average Linkage}: Average distance between members.
            \end{itemize}
        \item \textbf{Example}: 
            Start with points A, B, C, D, E, merge A and B, then (AB) with C, etc.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Pseudocode for Agglomerative Clustering}
    \begin{lstlisting}[language=Python]
def agglomerative_clustering(data, num_clusters):
    clusters = [[point] for point in data]  # Start with each point as a cluster
    while len(clusters) > num_clusters:
        # Find the closest pair of clusters
        closest_pair = find_closest(clusters)
        # Merge them into a new cluster
        new_cluster = merge_clusters(closest_pair)
        clusters.remove(closest_pair[0])
        clusters.remove(closest_pair[1])
        clusters.append(new_cluster)
    return clusters
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Divisive Clustering}
    \begin{itemize}
        \item \textbf{Top-Down Approach}:
            \begin{itemize}
                \item Starts with all points in one cluster.
                \item Recursively splits into smaller clusters using an appropriate algorithm.
            \end{itemize}
        \item \textbf{Example}:
            Start with all points combined, split based on data distribution until distinct clusters are formed.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item The outcome is visualized using a dendrogram.
        \item Flexibility in choosing the number of clusters by cutting the dendrogram.
        \item The choice of distance metric (e.g., Euclidean, Manhattan) is critical.
        \item Computational complexity can be high (O(n³)), making it less suitable for very large datasets.
    \end{itemize}
    \begin{block}{Conclusion}
        Hierarchical clustering provides a detailed data structure visualization and is widely applicable from biology to marketing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrograms in Hierarchical Clustering - Overview}
    \textbf{Key Concept: What is a Dendrogram?}
    \begin{itemize}
        \item A \textbf{dendrogram} is a tree-like diagram that visually represents clusters formed through \textit{hierarchical clustering}.
        \item It shows the relationship between data points based on similarities or differences.
        \item Helps explain how clusters are formed through successive mergers or splits.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Structure}
    \begin{itemize}
        \item \textbf{Leaves:} Represent individual data points or observations.
        \item \textbf{Branches:} Indicate clusters formed by grouping data points; length reflects distance or dissimilarity.
        \item \textbf{Height:} The vertical axis shows the similarity level — lower heights indicate more similar clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting a Dendrogram}
    \begin{enumerate}
        \item \textbf{Identifying Clusters:} 
            \begin{itemize}
                \item Start from the bottom to identify clusters as they merge; low height indicates similarity.
            \end{itemize}
        \item \textbf{Deciding the Number of Clusters:} 
            \begin{itemize}
                \item Draw a horizontal line across the dendrogram; intersections with branches indicate possible clusters.
            \end{itemize}
        \item \textbf{Understanding Distance Metrics:} 
            \begin{itemize}
                \item Choice of distance metrics affects dendrogram structure (e.g., Euclidean, Manhattan).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Dataset}
    Consider three points A, B, and C with distances:
    \begin{itemize}
        \item Distance between A and B = 1
        \item Distance between A and C = 3
        \item Distance between B and C = 2
    \end{itemize}
    \textbf{Visual Representation:}
    
    The dendrogram will show:
    \begin{itemize}
        \item A and B merge first (at height 1).
        \item A merges with C next (at height 3).
        \item B is part of the same cluster at height 2.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Clustering Method Matters:} 
            \begin{itemize}
                \item Agglomerative methods: Start with points and merge.
                \item Divisive methods: Start with the whole dataset and split.
            \end{itemize}
        \item \textbf{Visual Insight:} 
            \begin{itemize}
                \item Dendrograms simplify relationships, often clearer than numerical data.
            \end{itemize}
        \item \textbf{Complexity Management:}
            \begin{itemize}
                \item Large datasets can create complex dendrograms; consider simplifying for clarity.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Additional Notes}
    \textbf{Summary:} 
    \begin{itemize}
        \item Dendrograms are essential tools in hierarchical clustering for understanding relationships.
        \item Examining dendrograms aids in determining the optimal number of clusters.
    \end{itemize}
    
    \textbf{Formulas:} Dendrogram construction uses distance metrics, e.g., 
    \begin{equation}
        d(A,B) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}
    \end{equation}
    
    \textbf{Tools:} Use Python libraries (e.g., Scikit-learn) for implementing hierarchical clustering.

    \begin{lstlisting}[language=Python]
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

data = [[A], [B], [C]]  # Replace A, B, C with actual data points
Z = linkage(data, 'ward')
dendrogram(Z)
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing K-means and Hierarchical Clustering - Part 1}
    \begin{block}{Introduction}
        Clustering is a technique used to group similar data points together. This presentation discusses two popular methods:
        \begin{itemize}
            \item K-means Clustering
            \item Hierarchical Clustering
        \end{itemize}
        We will analyze their differences, advantages, and disadvantages to help decide when to use each method.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Part 2}
    \begin{block}{Definition}
        K-means clustering is an iterative algorithm that partitions a dataset into K distinct clusters, each represented by the mean (centroid) of the points in that cluster.
    \end{block}
    
    \begin{block}{How it Works}
        \begin{enumerate}
            \item Initialize K centroids randomly.
            \item Assign each data point to the nearest centroid.
            \item Recalculate the centroids of the clusters.
            \item Repeat steps 2-3 until the centroids do not change significantly.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Advantages}
        \begin{itemize}
            \item Efficiency: Scales well with large datasets (O(n * K * i)).
            \item Simplicity: Easy to understand and implement.
            \item Speed: Fast convergence in practice.
        \end{itemize}
    \end{block}
    
    \begin{block}{Disadvantages}
        \begin{itemize}
            \item Fixed Number of Clusters: Must specify K in advance.
            \item Sensitivity to Initial Conditions: Different initial centroids can lead to different results.
            \item Shape Limitations: Assumes clusters are spherical and evenly sized.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Part 3}
    \begin{block}{Definition}
        Hierarchical clustering builds a tree of clusters (dendrogram) by either:
        \begin{itemize}
            \item Agglomerative (bottom-up approach)
            \item Divisive (top-down approach)
        \end{itemize}
    \end{block}
    
    \begin{block}{How it Works (Agglomerative)}
        \begin{enumerate}
            \item Start with each data point as its own cluster.
            \item Iteratively merge the closest pair of clusters.
            \item Continue until all points form one cluster or a desired number of clusters is achieved.
        \end{enumerate}
    \end{block}

    \begin{block}{Advantages}
        \begin{itemize}
            \item Dendrogram Visualization: Easy interpretation of clusters.
            \item No Need for Pre-specification of Clusters: Decide based on the dendrogram.
            \item Flexibility: Works well with arbitrary-shaped data.
        \end{itemize}
    \end{block}
    
    \begin{block}{Disadvantages}
        \begin{itemize}
            \item Computational Cost: More expensive than K-means (O(n^3)).
            \item Sensitivity to Noise and Outliers: Can distort results.
            \item Not Scalable: Inefficient for large datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Examples - Part 4}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Use K-means for large datasets with a known K when speed is crucial.
            \item Use Hierarchical Clustering for small to medium datasets when interpretability is needed.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example: K-means in Python}
        \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

# Sample data
data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])
kmeans = KMeans(n_clusters=2, random_state=0).fit(data)
print(kmeans.labels_)
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Understanding Dendrograms}
        Reviewing dendrograms will reinforce how hierarchical clustering visually represents data relationships.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 5}
    \begin{block}{Conclusion}
        Both K-means and hierarchical clustering have unique advantages and challenges. Understanding these can help in choosing the appropriate method for your data analysis needs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Dimensionality Reduction?}
    \textbf{Definition:} \\[10pt]
    Dimensionality reduction is a statistical technique used to reduce the number of input variables in a dataset while retaining the essential features that contribute to its variability.
    It transforms high-dimensional data into a lower-dimensional space, making it easier to visualize, analyze, and manipulate.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Dimensionality Reduction}
    \begin{enumerate}
        \item \textbf{Simplification of Datasets:}
        \begin{itemize}
            \item Complex datasets become easier to analyze by eliminating less informative features.
            \item Allows for clearer insights and interpretations.
        \end{itemize}
        
        \item \textbf{Improved Computational Efficiency:}
        \begin{itemize}
            \item Reduces the computational resources and time needed for processing high-dimensional data.
            \item Accelerates classification and clustering algorithms.
        \end{itemize}
        
        \item \textbf{Mitigation of the Curse of Dimensionality:}
        \begin{itemize}
            \item Addresses sparsity issues in high-dimensional spaces, enhancing pattern recognition by algorithms.
        \end{itemize}
        
        \item \textbf{Visualization:}
        \begin{itemize}
            \item Facilitates more effective data visualization, crucial for insightful analysis and presentations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques in Dimensionality Reduction}
    \begin{itemize}
        \item \textbf{Principal Component Analysis (PCA):}
        \begin{itemize}
            \item Converts correlated features into linearly uncorrelated features called principal components.
        \end{itemize}

        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE):}
        \begin{itemize}
            \item Focuses on preserving local neighborhood structures while reducing dimensionality.
        \end{itemize}

        \item \textbf{Linear Discriminant Analysis (LDA):}
        \begin{itemize}
            \item A supervised technique that maximizes the separation between multiple classes of data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    \begin{block}{Example: Student Behaviors Dataset}
    \begin{itemize}
        \item \textbf{Before Dimensionality Reduction:}
        \begin{itemize}
            \item 10 features complicate clustering algorithms and obscure relationships.
        \end{itemize}
        \item \textbf{After PCA Applied:}
        \begin{itemize}
            \item Data is represented in 2D space, revealing natural groupings of study habits.
            \item Enables more intuitive insights and conclusions from the dataset.
        \end{itemize}
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula for PCA}
    The first principal component can be computed as follows:
    \begin{enumerate}
        \item Standardize the data (subtract the mean and divide by the standard deviation).
        \item Calculate the covariance matrix.
        \item Compute eigenvalues and eigenvectors of the covariance matrix.
        \item Sort eigenvalues in decreasing order to identify the top 'k' eigenvectors (principal components) for projection.
    \end{enumerate}
    \begin{block}{Example Code for PCA}
    \begin{lstlisting}[language=Python]
import numpy as np
from sklearn.decomposition import PCA

# Example of applying PCA to a dataset
data = np.array([[...], [...], ...])  # Assume this is your dataset
pca = PCA(n_components=2)  # Reduce to 2 dimensions
reduced_data = pca.fit_transform(data)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{itemize}
        \item Dimensionality reduction simplifies high-dimensional data, making analysis clearer.
        \item Advances computational efficiency and helps mitigate the curse of dimensionality.
        \item Techniques like PCA and t-SNE are essential for managing complex datasets effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Dimensionality Reduction - Overview}
    Dimensionality reduction techniques are essential in data analysis as they help simplify complex datasets while retaining their essential patterns and structures. They are crucial for enhancing computational efficiency, reducing noise, and visualizing high-dimensional data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA)}
        \begin{itemize}
            \item \textbf{Definition}: PCA is a statistical procedure that transforms a dataset into a new coordinate system focused on maximizing variance capture.
            \item \textbf{Applications}: Image compression, noise reduction, feature extraction.
        \end{itemize}
        
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
        \begin{itemize}
            \item \textbf{Definition}: A non-linear technique for visualizing high-dimensional data by minimizing divergence between probability distributions.
            \item \textbf{Applications}: Visualizing word embeddings and gene expression data.
        \end{itemize}
        
        \item \textbf{Linear Discriminant Analysis (LDA)}
        \begin{itemize}
            \item \textbf{Definition}: A supervised method aimed at preserving class discriminatory information.
            \item \textbf{Applications}: Classification problems like face recognition and medical diagnosis.
        \end{itemize}

        \item \textbf{Autoencoders}
        \begin{itemize}
            \item \textbf{Definition}: Neural networks that learn efficient data representations for dimensionality reduction.
            \item \textbf{Applications}: Image denoising, anomaly detection, generative tasks.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Focus: Principal Component Analysis (PCA)}
    
    \textbf{Steps in PCA:}
    \begin{enumerate}
        \item \textbf{Standardization}: Normalize the dataset to have a mean of 0 and a variance of 1.
        \begin{equation}
            z = \frac{x - \mu}{\sigma}
        \end{equation}

        \item \textbf{Covariance Matrix Computation}: Calculate the covariance matrix for correlation examination.

        \item \textbf{Eigenvalue Decomposition}: Compute eigenvalues and eigenvectors of the covariance matrix.

        \item \textbf{Selecting Principal Components}: Choose the top k eigenvalues and corresponding eigenvectors.

        \item \textbf{Data Transformation}: Transform the dataset by projecting onto selected components.
        \begin{equation}
            Y = XW
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example}
    
    \textbf{Key Points:}
    \begin{itemize}
        \item Dimensionality reduction enhances machine learning performance by removing irrelevant features.
        \item PCA is effective for linear correlations, while other techniques may handle non-linear relationships better.
        \item Knowing when to apply a specific technique is crucial for effective data analysis.
    \end{itemize}

    \textbf{Example:}
    Given a dataset with three features (X1, X2, X3), PCA can reduce it to two principal components (PC1, PC2) by maximizing variance in the transformed feature space.
\end{frame}

\begin{frame}[fragile]
    \frametitle{References}
    \begin{itemize}
        \item \textit{Pattern Recognition and Machine Learning} by Christopher Bishop
        \item \textit{The Elements of Statistical Learning} by Trevor Hastie, Robert Tibshirani, and Jerome Friedman
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding PCA - Part 1}
    \begin{block}{What is Principal Component Analysis (PCA)?}
        Principal Component Analysis (PCA) is a powerful statistical technique used for dimensionality reduction, allowing us to identify patterns in high-dimensional data by transforming it into a lower-dimensional space. This process helps in simplifying data analysis and visualization while retaining the most important information.
    \end{block}

    \begin{block}{Key Concepts in PCA}
        \begin{itemize}
            \item \textbf{Dimensionality Reduction:} PCA aims to reduce the number of variables in a dataset while preserving as much variance (information) as possible.
            \item \textbf{Principal Components:} The new dimensions created by PCA are called principal components. They are orthogonal (uncorrelated) linear combinations of the original features.
            \item \textbf{Variance Maximization:} PCA selects components that capture the maximum variance in the data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding PCA - Part 2}
    \begin{block}{Mathematical Foundations of PCA}
        \begin{enumerate}
            \item \textbf{Data Standardization:} Center the data by subtracting the mean and scaling by the standard deviation:
            \begin{equation}
                Z = \frac{X - \mu}{\sigma}
            \end{equation}
            
            \item \textbf{Covariance Matrix:} Compute the covariance matrix \(C\):
            \begin{equation}
                C = \frac{1}{n-1} Z^TZ
            \end{equation}
            
            \item \textbf{Eigenvalues and Eigenvectors:} Compute eigenvalues (\(\lambda\)) and eigenvectors (\(\mathbf{v}\)):
            \begin{equation}
                C \mathbf{v} = \lambda \mathbf{v}
            \end{equation}

            \item \textbf{Selecting Principal Components:} Sort eigenvalues in descending order and select the top \(k\).
            
            \item \textbf{Transforming Data:} Project standardized data onto selected eigenvectors:
            \begin{equation}
                Y = Z \cdot V_k
            \end{equation}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding PCA - Part 3}
    \begin{block}{Example of PCA Application}
        Consider a dataset with 3 features: height, weight, and age. PCA can reduce these to 2 principal components by:
        \begin{itemize}
            \item Standardizing the features
            \item Computing the covariance matrix
            \item Extracting eigenvalues/eigenvectors
            \item Selecting the top components to capture maximum variance
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item PCA reduces dimensionality while maximizing variance retention.
            \item It involves linear transformations through eigenvalues and eigenvectors.
            \item Standardization of data is essential before applying PCA.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding PCA allows us to visually interpret high-dimensional data, improve machine learning models, and uncover latent structures within datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying PCA to Data - Overview}
    \begin{block}{Overview of PCA}
        Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset into a new coordinate system. 
        In this system:
        \begin{itemize}
            \item The greatest variance by any projection lies on the first coordinate (principal component).
            \item The second greatest variance lies on the second coordinate, and so on.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying PCA to Data - Steps to Apply PCA}
    \begin{enumerate}
        \item \textbf{Standardize the Data}:
            \begin{itemize}
                \item Center the data by subtracting the mean of each feature.
                \item Scale the data to have unit variance (if necessary).
                \item \textbf{Formula}: 
                \begin{equation}
                    X_{standardized} = \frac{X - \text{mean}(X)}{\text{std}(X)}
                \end{equation}
            \end{itemize}
        \item \textbf{Compute the Covariance Matrix}:
            \begin{itemize}
                \item Understand how features vary together.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Cov}(X) = \frac{1}{n-1}(X^T X)
                \end{equation}
            \end{itemize}
        \item \textbf{Calculate Eigenvalues and Eigenvectors}:
            \begin{itemize}
                \item Solve: \(\text{Cov}(X) v = \lambda v\)
                \item Where \(\lambda\) is the eigenvalue, and \(v\) is the eigenvector.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying PCA to Data - Steps Continued}
    \begin{enumerate}[resume]
        \item \textbf{Sort Eigenvalues and Eigenvectors}:
            \begin{itemize}
                \item Sort eigenvalues in descending order; sort the corresponding eigenvectors.
                \item The top \(k\) eigenvectors define the new feature space.
            \end{itemize}
        \item \textbf{Transform the Data}:
            \begin{itemize}
                \item Project original data onto the new feature space.
                \item \textbf{Formula}:
                \begin{equation}
                    X_{reduced} = X_{standardized} \cdot V_k
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying PCA to Data - Significance}
    \begin{block}{Interpretation of Results}
        \begin{itemize}
            \item \textbf{Explained Variance}: Eigenvalues indicate variance captured by each principal component.
            \item \textbf{Visualization}: Use a scree plot to visualize eigenvalues and determine effective components.
        \end{itemize}
    \end{block}

    \begin{block}{Significance of Eigenvalues and Eigenvectors}
        \begin{itemize}
            \item \textbf{Eigenvalues}: Measure variance. Higher eigenvalues capture more information.
            \item \textbf{Eigenvectors}: Define directions of principal components and indicate feature importance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying PCA to Data - Key Points}
    \begin{itemize}
        \item PCA reduces dimensionality and uncovers patterns in high-dimensional data.
        \item Proper data scaling is crucial before applying PCA.
        \item Select the number of principal components (\(k\)) based on cumulative explained variance ratio.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying PCA to Data - Example Code}
    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assume X is your dataset
X_standardized = StandardScaler().fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)  # Choose the number of components
X_reduced = pca.fit_transform(X_standardized)

# Explained variance
explained_variance = pca.explained_variance_ratio_
print("Explained Variance by each component:", explained_variance)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits and Limitations of PCA}
    \begin{block}{Overview of PCA}
        PCA (Principal Component Analysis) is a widely used statistical technique for dimensionality reduction that transforms high-dimensional data into a lower-dimensional space while preserving as much variance as possible.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of PCA}
    \begin{enumerate}
        \item \textbf{Data Visualization}
            \begin{itemize}
                \item \textbf{Simplicity:} Reduces dimensions to 2 or 3 principal components for easy visualization.
                \item \textbf{Clarity:} Reveals patterns and trends that may be obscured in high-dimensional space.
                \item \textit{Example:} Visualizing customer segmentation in retail data, showing distinct clusters based on purchasing behavior.
            \end{itemize}
        
        \item \textbf{Noise Reduction}
            \begin{itemize}
                \item \textbf{Elimination of Redundant Features:} Helps remove less informative features and noise.
                \item \textit{Example:} Refining medical datasets by focusing on significant patterns related to health conditions.
            \end{itemize}

        \item \textbf{Improved Algorithm Performance}
            \begin{itemize}
                \item \textbf{Faster Computation:} Increases speed by reducing dataset dimensionality.
                \item \textbf{Prevents Overfitting:} Using fewer dimensions aids generalization on unseen data.
            \end{itemize}

        \item \textbf{Feature Extraction}
            \begin{itemize}
                \item Derives new variables (principal components) that can serve as combined inputs for further analysis.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of PCA}
    \begin{enumerate}
        \item \textbf{Information Loss}
            \begin{itemize}
                \item \textbf{Discarding Variance:} Some components with useful information may be lost.
                \item \textit{Example:} In facial recognition systems, subtle features may be lost, impacting system performance.
            \end{itemize}

        \item \textbf{Linear Assumption}
            \begin{itemize}
                \item PCA presumes linear relationships, which may overlook non-linear patterns.
                \item \textit{Illustration:} Curved patterns in datasets may be inadequately captured.
            \end{itemize}

        \item \textbf{Scale Sensitivity}
            \begin{itemize}
                \item Sensitive to feature scales; larger range features can dominate principal components.
                \item \textbf{Standardization Formula:}
                \begin{equation}
                    Z_i = \frac{X_i - \mu}{\sigma}
                \end{equation}
                Where \(Z_i\) is the standardized score, \(X_i\) is the original value, \(\mu\) is the mean, and \(\sigma\) is the standard deviation.
            \end{itemize}

        \item \textbf{Interpretability}
            \begin{itemize}
                \item New axes (principal components) are linear combinations, making results harder to interpret.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies of Clustering and PCA}
    \begin{block}{Introduction to Clustering and PCA}
        Clustering is an unsupervised learning technique that groups similar data points based on their characteristics. 
        Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a lower-dimensional space while preserving variance. 
        Both techniques are pivotal in various domains for extracting insights from complex datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Customer Segmentation (Clustering)}
    \begin{itemize}
        \item \textbf{Domain:} Retail
        \item \textbf{Application:} Enhance Targeted Marketing Strategies
        \item \textbf{Explanation:}
            \begin{itemize}
                \item Businesses use clustering algorithms (e.g., K-means) to group customers based on purchasing behavior, demographics, and preferences.
            \end{itemize}
        \item \textbf{Example:}
            \begin{itemize}
                \item A retail company applies K-means clustering to its customer data.
                \item Result: Four distinct customer segments identified:
                        \begin{itemize}
                            \item Price-sensitive shoppers
                            \item Luxury buyers
                            \item Frequent buyers
                            \item New customers
                        \end{itemize}
            \end{itemize}
        \item \textbf{Outcome:} Tailored marketing campaigns increase engagement and sales by targeting specific segments effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Image Compression (PCA)}
    \begin{itemize}
        \item \textbf{Domain:} Computer Vision
        \item \textbf{Application:} Reduce File Size Without Significant Quality Loss
        \item \textbf{Explanation:}
            \begin{itemize}
                \item PCA reduces the dimensionality of image data while maintaining essential features.
            \end{itemize}
        \item \textbf{Example:}
            \begin{itemize}
                \item An image represented by pixels is transformed into a lower-dimensional space using PCA.
                \item Original Image: 256x256 pixels (65,536 dimensions).
                \item PCA reduces it to 50 dimensions while retaining 95\% variance.
            \end{itemize}
        \item \textbf{Outcome:} Significant reduction in file size enables faster loading times and less storage space.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Gene Expression Analysis (Clustering \& PCA)}
    \begin{itemize}
        \item \textbf{Domain:} Bioinformatics
        \item \textbf{Application:} Discover Patterns in Genetic Data
        \item \textbf{Explanation:}
            \begin{itemize}
                \item Clustering is used alongside PCA to analyze gene expression profiles from high-dimensional data.
            \end{itemize}
        \item \textbf{Example:}
            \begin{itemize}
                \item Researchers use PCA to reduce gene expression data complexity.
                \item Clustering groups samples with similar expression patterns.
            \end{itemize}
        \item \textbf{Outcome:} Identification of gene clusters related to specific diseases, potentially leading to better-targeted therapies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Effectiveness of Clustering:} Provides significant insights in customer analysis and segmentation.
            \item \textbf{PCA’s Role:} Enables efficient handling of high-dimensional data, enhancing pattern visibility without substantial information loss.
            \item \textbf{Interplay of Techniques:} Clustering and PCA often coalesce to reveal complex structures in multidimensional datasets.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Incorporating clustering and PCA provides a robust framework for analyzing and interpreting large datasets across different fields, highlighting their importance in modern data science and machine learning practices.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Summary}
        This presentation summarizes key concepts in clustering and dimensionality reduction, highlighting their significance in machine learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Clustering}
    \begin{itemize}
        \item \textbf{Definition}: Unsupervised learning method that groups objects based on similarity.
        \item \textbf{Common Algorithms}:
        \begin{itemize}
            \item \textbf{K-Means}: Partitions data into K distinct clusters based on proximity to centroids.
            \item \textbf{Hierarchical Clustering}: Builds a hierarchy of clusters via agglomerative or divisive methods.
            \item \textbf{DBSCAN}: Density-based clustering useful for irregular shapes and varying densities.
        \end{itemize}
        \item \textbf{Example}: Customer segmentation for targeted marketing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Dimensionality Reduction}
    \begin{itemize}
        \item \textbf{Definition}: Techniques to reduce the number of features while retaining essential information.
        \item \textbf{Common Techniques}:
        \begin{itemize}
            \item \textbf{PCA}: Transforms data into lower dimensions while maximizing variance.
            \item \textbf{t-SNE}: Ideal for visualizing high-dimensional data, preserving local structures.
        \end{itemize}
        \item \textbf{Example}: PCA in image processing for faster computations with minimal information loss.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance in Machine Learning}
    \begin{itemize}
        \item \textbf{Efficiency}: Reduces computational cost and enhances algorithm performance.
        \item \textbf{Visualization}: Facilitates the identification of patterns in high-dimensional data.
        \item \textbf{Noise Reduction}: Filters out uninformative features, improving model accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item The choice of \textbf{clustering technique} is crucial for accurate outcomes.
        \item \textbf{PCA} is valuable for visualizing and preprocessing data before modeling.
        \item \textbf{Challenges}: Selecting the right number of clusters in K-means or components in PCA is vital for effective analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile,plain]{Formulas and Code Snippets}
    \begin{block}{K-Means Algorithm}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans

# Assuming X is your data
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
labels = kmeans.labels_
    \end{lstlisting}
    \end{block}

    \begin{block}{PCA}
    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA

# Assuming X is your data
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)
    \end{lstlisting}
    \end{block}
\end{frame}


\end{document}