\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Chapter 4: Linear Models and Regression Analysis]{Chapter 4: Linear Models and Regression Analysis}
\author[J. Smith]{John Smith, Ph.D.}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Linear Models and Regression Analysis}
    \begin{block}{Overview}
        This chapter focuses on:
        \begin{enumerate}
            \item Linear Regression
            \item Logistic Regression
            \item Model Evaluation Techniques
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Linear Model?}
    A \textbf{linear model} describes a relationship where a dependent variable \(Y\) is expressed as a linear combination of independent variables \(X_i\):
    
    \begin{equation}
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
    \end{equation}
    
    \begin{itemize}
        \item \(Y\): Dependent variable
        \item \(X_1, X_2, \ldots, X_n\): Independent variables
        \item \(\beta_0\): Intercept (constant term)
        \item \(\beta_1, \beta_2, \ldots, \beta_n\): Coefficients affecting \(Y\)
        \item \(\epsilon\): Error term
    \end{itemize} 

    \textbf{Example:} In predicting housing prices, \(Y\) can depend on square footage, number of bedrooms, and age of the house.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear and Logistic Regression}
    \textbf{Linear Regression}:
    \begin{itemize}
        \item Models relationship between one dependent variable and independent variables.
        \item Assumptions: Normal distribution of residuals, homoscedasticity, independence.
        \item \textbf{Example Structure}:
        \begin{equation}
            \text{Sales} = b_0 + b_1(\text{Advertising}) + \epsilon
        \end{equation}
    \end{itemize}

    \textbf{Logistic Regression}:
    \begin{itemize}
        \item Used for binary classification (e.g., yes/no).
        \item Predicts probability of an event.
        \item Represents output using the sigmoid function:
        \begin{equation}
            P(Y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X)}}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Techniques}
    Evaluating the effectiveness of regression models is crucial:

    \begin{itemize}
        \item \textbf{R-squared} (\(R^2\)): 
        \begin{equation}
            R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}
        \end{equation}
        \textit{Measures how much variance in \(Y\) can be explained by \(X\).}

        \item \textbf{Cross-Validation}: Assesses how results generalize to an independent data set.
        
        \item \textbf{Confusion Matrix}: Used in logistic regression to summarize True Positives, True Negatives, False Positives, and False Negatives.
    \end{itemize}

    This chapter allows us to develop a robust understanding of these models and evaluation techniques.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Linear Regression - Introduction}
    \begin{block}{What is Linear Regression?}
        Linear regression is a statistical technique used to model and analyze the relationship between a dependent variable (denoted as \(Y\)) and one or more independent variables (denoted as \(X\)). 
        The primary purpose is to predict the value of the dependent variable based on the independent variables.
    \end{block}
    
    \begin{block}{Purpose of Linear Regression}
        \begin{itemize}
            \item \textbf{Prediction:} Estimate future outcomes based on historical trends.
            \item \textbf{Understanding Relationships:} Assess how changes in \(X\) impact \(Y\).
            \item \textbf{Determining Strength:} Evaluate the strength and significance of relationships between variables.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Linear Regression - Applications}
    \begin{block}{Applications of Linear Regression}
        \begin{itemize}
            \item \textbf{Economics:} Predicting consumer spending based on income levels.
            \item \textbf{Healthcare:} Analyzing the impact of treatment dosage on recovery rates.
            \item \textbf{Engineering:} Modeling the relationship between material properties and performance indicators.
        \end{itemize}
    \end{block}
    
    \begin{block}{Basic Model Structure}
        The simplest form of a linear regression model is represented by the equation:
        \begin{equation}
            Y = \beta_0 + \beta_1 X + \epsilon
        \end{equation}
        Where:
        \begin{itemize}
            \item \(Y\) = dependent variable
            \item \(X\) = independent variable
            \item \(\beta_0\) = intercept
            \item \(\beta_1\) = slope
            \item \(\epsilon\) = error term
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Linear Regression - Key Assumptions}
    \begin{block}{Key Assumptions of Linear Regression}
        \begin{enumerate}
            \item \textbf{Linearity:} The relationship between independent and dependent variables is linear.
            \item \textbf{Independence:} Observations are independent of one another.
            \item \textbf{Homoscedasticity:} Constant variance of error terms across all levels of \(X\).
            \item \textbf{Normality:} The residuals should be approximately normally distributed.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Summary}
        Linear regression is vital to statistics and data analysis, providing insights into relationships between variables while enabling predictions. 
        Understanding and checking the key assumptions is essential for valid model results and interpretations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematics of Linear Regression}
    \begin{block}{Linear Regression Equation}
        The **linear regression model** seeks to establish a relationship between a dependent variable \(Y\) and one or more independent variables \(X\). The basic form of a **simple linear regression** model is:
        \begin{equation}
        Y = \beta_0 + \beta_1 X + \epsilon
        \end{equation}
        \begin{itemize}
            \item \(Y\): Dependent variable (what we are trying to predict)
            \item \(X\): Independent variable (the predictor)
            \item \(\beta_0\): Intercept (value of \(Y\) when \(X = 0\))
            \item \(\beta_1\): Slope coefficient (change in \(Y\) for a one-unit change in \(X\))
            \item \(\epsilon\): Error term (captures the difference between predicted and actual \(Y\))
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components Explained}
    \begin{block}{Intercept (\(\beta_0\))}
        Represents the expected value of \(Y\) when all predictors are equal to zero. It is crucial in understanding where the regression line crosses the Y-axis.
    \end{block}
    \begin{block}{Coefficients (\(\beta_1, \beta_2, \ldots\))}
        Indicate the expected change in the dependent variable for a one-unit increase in the independent variable. For example, if \(\beta_1 = 2\), an increase of 1 in \(X\) will result in an increase of 2 in \(Y\).
    \end{block}
    \begin{block}{Error Term (\(\epsilon\))}
        Represents the unexplained variation in \(Y\) and accounts for the noise and factors that affect \(Y\) but are not included in the model. A smaller error term signifies a better fit of the model to the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example Illustration}
    \begin{itemize}
        \item The roles of the intercept and coefficients are central to interpreting the results of a linear regression analysis.
        \item Understanding the error term is essential for grasping model accuracy and reliability.
        \item In multiple regression, the equation expands as:
        \begin{equation}
        Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
        \end{equation}
        \item \textbf{Example:} Consider a study estimating the effect of study hours on test scores:
        \begin{equation}
        Y = 50 + 10X + \epsilon
        \end{equation}
        If a student studies for 5 hours (\(X = 5\)):
        \begin{equation}
        Y = 50 + 10(5) = 100
        \end{equation}
    \end{itemize}
    \begin{block}{Conclusion}
        Linear regression is a powerful tool for understanding relationships among variables across various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Prompt}
    \begin{block}{Discussion Prompt}
        How might changes in the error term influence the interpretation of the model coefficients?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Overview}
    \begin{block}{Introduction}
        Linear regression is a powerful statistical technique for predicting a continuous outcome based on predictor variables. Validity of results relies on adherence to key assumptions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Key Assumptions}
    \begin{enumerate}
        \item \textbf{Linearity}
        \item \textbf{Independence}
        \item \textbf{Homoscedasticity}
        \item \textbf{Normality}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 1: Linearity}
    \begin{itemize}
        \item \textbf{Definition:} Relationship between predictors and outcome must be linear.
        \item \textbf{Visual Representation:} A scatter plot with a straight line.
        \item \textbf{Example:} Weight prediction based on height should show consistent increase.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 2: Independence}
    \begin{itemize}
        \item \textbf{Definition:} Residuals must be independent.
        \item \textbf{Implication:} Correlated residuals can lead to underestimated standard errors.
        \item \textbf{Example:} Responses in a household income survey should not influence each other.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 3: Homoscedasticity}
    \begin{itemize}
        \item \textbf{Definition:} Variance of residuals should remain constant.
        \item \textbf{Visual Representation:} Residuals vs. fitted values should show random scatter.
        \item \textbf{Example:} Prediction errors for home prices should be consistent across home sizes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 4: Normality}
    \begin{itemize}
        \item \textbf{Definition:} Residuals should be approximately normally distributed.
        \item \textbf{Testing for Normality:} Use Q-Q plots or tests like the Shapiro-Wilk test.
        \item \textbf{Example:} The difference between predicted and actual exam scores should follow a normal distribution.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Assumptions}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Importance of assumptions for validity of regression models.
            \item Consequences of violations can include inaccurate predictions and invalid tests.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding and verifying the assumptions of linear regression is essential for building a reliable predictive model. Adhering to these assumptions enhances the interpretability of the results.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Diagnostic Code Snippet}
    \begin{lstlisting}[language=Python]
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Fit your model
model = sm.OLS(y, X).fit()
residuals = model.resid

# Check homoscedasticity
plt.scatter(model.fittedvalues, residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residuals vs Fitted Values")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.show()

# Normality test
sm.qqplot(residuals, line='45')
plt.title("Q-Q Plot of Residuals")
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Explained - Introduction}
    \begin{itemize}
        \item \textbf{Definition}: Logistic regression is a statistical method used for binary classification problems, where the outcome variable is categorical with two possible outcomes (e.g., Yes/No, 1/0).
        \item \textbf{Purpose}: It predicts the probability that a given input point belongs to a particular category.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Explained - The Logistic Function}
    \begin{block}{Formula}
        The logistic function is given by:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
    \end{block}
    Where:
    \begin{itemize}
        \item \( P(Y=1 | X) \): Probability the dependent variable \( Y \) equals 1.
        \item \( \beta_0, \beta_1, ... , \beta_n \): Coefficients learned during model fitting.
        \item \( e \): Euler's number (approx. 2.71828).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Explained - Mapping to Probabilities}
    \begin{itemize}
        \item The logistic function produces an 'S' shaped curve (sigmoid).
        \item As the input value increases:
        \begin{itemize}
            \item Probability approaches 1 (indicating class '1').
            \item As it decreases, probability approaches 0 (indicating class '0').
        \end{itemize}
        \item \textbf{Interpretation of Output}:
        \begin{itemize}
            \item If \( P(Y=1|X) > 0.5 \): Predict class '1'.
            \item If \( P(Y=1|X) < 0.5 \): Predict class '0'.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Explained - Example}
    \textbf{Scenario}: Predicting whether a student will pass (1) or fail (0) an exam based on hours studied.

    \begin{itemize}
        \item \textbf{Data}: Hours studied \( X \): [1, 2, 3, 4, 5], Pass (1) / Fail (0): [0, 0, 1, 1, 1].
        \item \textbf{Model Output}:
        \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{-(-4 + 1.5 \cdot X)}}
        \end{equation}
        \item For \( X = 3 \) hours studied:
        \begin{equation}
            P(Y=1|3) = \frac{1}{1 + e^{-(-4 + 1.5 \cdot 3)}} \approx 0.622
        \end{equation}
        This indicates a 62.2% probability the student will pass the exam.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Explained - Key Points}
    \begin{itemize}
        \item Logistic regression uses a non-linear transformation (logistic function) to model relationships in classification.
        \item Output is a probability interpreted against a threshold (commonly 0.5) to classify data points.
        \item Widely used in various fields:
        \begin{itemize}
            \item \textbf{Medicine}: Predicting disease presence.
            \item \textbf{Finance}: Credit risk assessment.
            \item \textbf{Marketing}: Customer churn prediction.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Explained - Conclusion}
    Logistic regression is a powerful statistical tool for binary classification that allows us to predict probabilities and make informed decisions based on underlying data relationships. Understanding the logistic function is crucial for interpreting model outputs effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{block}{Comparison}
        Linear regression and logistic regression are fundamental statistical methods for modeling the relationship between dependent and independent variables. They differ in applications, interpretations, and underlying assumptions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Purpose}:
        \begin{itemize}
            \item \textbf{Linear Regression}: Predicts continuous outcomes (e.g., sales revenue based on advertising spend).
            \item \textbf{Logistic Regression}: Used for binary classification (e.g., pass or fail based on study hours).
        \end{itemize}

        \item \textbf{Output}:
        \begin{itemize}
            \item \textbf{Linear Regression Formula}:
            \begin{equation}
                Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
            \end{equation}
            \item \textbf{Logistic Function}:
            \begin{equation}
                P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \ldots + \beta_nX_n)}}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpretation and Applications}
    \begin{enumerate}
        \item \textbf{Interpretation of Results}:
        \begin{itemize}
            \item \textbf{Linear Regression}: Coefficients show changes in the dependent variable. E.g., $\beta_1 = 2$ means increasing $X_1$ by 1 increases $Y$ by 2.
            \item \textbf{Logistic Regression}: Coefficients represent log odds. E.g., $\beta_1 = 0.7$ implies $\text{odds of } Y=1$ increases by $e^{0.7} \approx 2.01$.
        \end{itemize}

        \item \textbf{Applications}:
        \begin{itemize}
            \item \textbf{Linear Regression}: Market analysis, forecasting.
            \item \textbf{Logistic Regression}: Medical diagnoses, credit scoring.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Regression Models - Overview}
    \begin{itemize}
        \item Understanding model performance is crucial when building regression models.
        \item Various metrics assist in assessing model accuracy:
        \begin{itemize}
            \item R-squared (R²)
            \item Adjusted R-squared
            \item Mean Squared Error (MSE)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Regression Models - R-squared}
    \begin{block}{1. R-squared (R²)}
        \begin{itemize}
            \item \textbf{Definition:} Represents the proportion of variance in the dependent variable explained by the independent variables.
            \item \textbf{Calculation:}
            \begin{equation}
                R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
            \end{equation}
            where:
            \begin{itemize}
                \item \(SS_{res}\) = Sum of Squares of residuals
                \item \(SS_{tot}\) = Total Sum of Squares
            \end{itemize}
            \item \textbf{Interpretation:} R² ranges from 0 (no variance explained) to 1 (all variance explained).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Regression Models - Adjusted R-squared and MSE}
    \begin{block}{2. Adjusted R-squared}
        \begin{itemize}
            \item \textbf{Definition:} Adjusts R² for the number of predictors, penalizing for irrelevant ones.
            \item \textbf{Calculation:}
            \begin{equation}
                \text{Adjusted } R^2 = 1 - \left( \frac{(1-R^2)(n-1)}{n-p-1} \right)
            \end{equation}
            \item \textbf{Interpretation:} Can be lower than R² and is preferred for model comparison.
        \end{itemize}
    \end{block}

    \begin{block}{3. Mean Squared Error (MSE)}
        \begin{itemize}
            \item \textbf{Definition:} Average of the squares of the errors between predicted and actual values.
            \item \textbf{Formula:}
            \begin{equation}
                MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
            where \(y_i\) = observed values and \(\hat{y}_i\) = predicted values.
            \item \textbf{Interpretation:} Smaller MSE indicates a better model fit.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Regression Models - Conclusion}
    \begin{itemize}
        \item Use R², Adjusted R², and MSE collectively for a comprehensive evaluation.
        \item Each metric provides insights into model performance and improvements.
    \end{itemize}
    
    \begin{block}{Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import r2_score, mean_squared_error

# y_true = actual values, y_pred = predicted values
r2 = r2_score(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)

print(f"R-squared: {r2}")
print(f"Mean Squared Error: {mse}")
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics for Logistic Regression}
    \begin{block}{Introduction to Logistic Regression}
        Logistic Regression is a statistical model used to predict the probability of a binary outcome (success/failure, yes/no) based on one or more predictor variables. Unlike linear regression, logistic regression predicts class probabilities that fall between 0 and 1.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Accuracy, Precision, Recall}
    \begin{enumerate}
        \item \textbf{Accuracy:}
        \begin{itemize}
            \item Definition: The ratio of correctly predicted observations to the total observations.
            \item Formula: 
            \[
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
            \]
            \item Example: If a model predicts 80 out of 100 instances correctly, the accuracy is 80\%.
        \end{itemize}

        \item \textbf{Precision:}
        \begin{itemize}
            \item Definition: The ratio of true positive predictions to the total predicted positives.
            \item Formula:
            \[
            \text{Precision} = \frac{TP}{TP + FP}
            \]
            \item Example: If 10 predictions are positive, and 8 are correct (TP=8, FP=2), then Precision = 0.8 or 80\%.
        \end{itemize}

        \item \textbf{Recall (Sensitivity):}
        \begin{itemize}
            \item Definition: The ratio of true positive predictions to the actual positives.
            \item Formula:
            \[
            \text{Recall} = \frac{TP}{TP + FN}
            \]
            \item Example: If there are 10 actual positives and the model correctly identifies 8 (TP=8, FN=2), Recall = 0.8 or 80\%.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - F1-Score and ROC Curve}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering from previous frame.
        \item \textbf{F1-Score:}
        \begin{itemize}
            \item Definition: The harmonic mean of Precision and Recall.
            \item Formula:
            \[
            F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \]
            \item Example: If Precision = 0.8 and Recall = 0.6, then:
            \[
            F1 = 2 \times \frac{0.8 \times 0.6}{0.8 + 0.6} = 0.688
            \]
        \end{itemize}

        \item \textbf{Receiver Operating Characteristic (ROC) Curve:}
        \begin{itemize}
            \item Definition: A graphical representation of a binary classifier's performance, plotting True Positive Rate (Recall) against False Positive Rate.
            \item Area Under Curve (AUC): A scalar value summarizing the ROC curve. Higher AUC indicates better model performance.
            \item Example Interpretation: An AUC of 0.8 suggests good predictive performance compared to random guessing.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation - Overview}
    \begin{block}{Overview}
        Model evaluation is essential in the data analysis process, enabling us to assess predictive model performance on unseen data. 
    \end{block}
    \begin{itemize}
        \item Ensures model generalization: Accurate predictions on new data.
        \item Helps identify overfitting and provides a basis for model comparison.
        \item Informs stakeholders about model reliability and utility.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation - Techniques}
    \begin{block}{Common Techniques for Model Validation}
        \begin{enumerate}
            \item \textbf{Train-Test Split}
                \begin{itemize}
                    \item Divides dataset into training and testing parts.
                    \item E.g., 800 for training, 200 for testing.
                    \item Simple but results can vary based on the split.
                \end{itemize}
                
            \item \textbf{K-Fold Cross-Validation}
                \begin{itemize}
                    \item Data is divided into \(K\) subsets. 
                    \item Model trained on \(K-1\) folds, tested on the remaining fold.
                    \item E.g., For \(K=5\), model is trained 5 times – more robust estimates.
                    \item \begin{equation}
                        \text{Average Performance} = \frac{1}{K} \sum_{i=1}^{K} \text{Performance on Fold } i
                    \end{equation}
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation - Performance Metrics}
    \begin{block}{Performance Metrics to Evaluate Models}
        \begin{itemize}
            \item \textbf{R-squared}: Measures variance explained by independent variables.
            \item \textbf{Mean Absolute Error (MAE)}:
                \begin{equation}
                    MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
                \end{equation}
            \item \textbf{Root Mean Squared Error (RMSE)}:
                \begin{equation}
                    RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
                \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation - Summary}
    \begin{block}{Summary}
        Model evaluation is critical for understanding a model's efficacy and reliability. 
        \begin{itemize}
            \item Techniques: Train-Test Split, K-Fold, LOOCV
            \item Performance metrics: R-squared, MAE, RMSE
            \item Basis for decision-making in real-world applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Cross-Validation Techniques}
    \begin{block}{Definition of Cross-Validation}
        Cross-validation is a statistical method used to estimate the skill of machine learning models. 
        It involves partitioning the data into subsets, training the model on some subsets, and validating it on others. 
        This technique helps ensure that the model's performance is reliable and generalizable to unseen data.
    \end{block}
    
    \begin{block}{Importance of Cross-Validation}
        \begin{itemize}
            \item \textbf{Reliable Assessment}: Reduces overfitting risk and provides an unbiased performance estimate.
            \item \textbf{Model Selection}: Enables comparison of different models without overfitting.
            \item \textbf{Utilization of Data}: Maximizes the use of limited datasets for training and validation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Examples of Cross-Validation Methods}
    \begin{block}{1. K-Fold Cross-Validation}
        \begin{itemize}
            \item \textbf{Process}:
            \begin{enumerate}
                \item Divide the dataset into K equally-sized folds.
                \item For each fold, use K-1 folds for training and 1 fold for validation.
                \item Repeat K times, averaging performance metrics across all iterations.
            \end{enumerate}
            \item \textbf{Tip}: Typical values for K are 5 or 10, varying based on dataset size.
        \end{itemize}
        
        \begin{block}{Illustration of K-Fold}
            Example with 10 samples and K=5:
            \begin{itemize}
                \item Fold 1: Train on samples 3-10, validate on samples 1-2
                \item Fold 2: Train on samples 1, 2, 4-10, validate on sample 3
                \item \ldots
            \end{itemize}
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Cross-Validation Methods (contd.)}
    \begin{block}{2. Leave-One-Out Cross-Validation (LOOCV)}
        \begin{itemize}
            \item \textbf{Process}:
            \begin{enumerate}
                \item Use N-1 observations for training and the single observation for validation.
                \item Average performance across all N iterations.
            \end{enumerate}
            \item \textbf{Usage}: Ideal for small datasets, ensuring all data points are utilized.
        \end{itemize}
        
        \begin{block}{Illustration of LOOCV}
            Given a dataset with 5 samples:
            \begin{itemize}
                \item Iteration 1: Train on samples 2-5, validate on sample 1
                \item Iteration 2: Train on samples 1, 3-5, validate on sample 2
                \item \ldots
            \end{itemize}
        \end{block}
        
        \begin{block}{Key Points to Emphasize}
            \begin{itemize}
                \item \textbf{Bias-Variance Tradeoff}: Manages the tradeoff for a more robust model.
                \item \textbf{Computational Cost}: Increased computing time; consider techniques like stratified K-folds for imbalanced datasets.
            \end{itemize}
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet for K-Fold Cross-Validation in Python}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
import numpy as np

# Sample Data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([1, 2, 3, 4, 5])

# K-Fold Cross-validation
kf = KFold(n_splits=5)
model = LinearRegression()

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model.fit(X_train, y_train)
    print(f"Test Score: {model.score(X_test, y_test)}")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Overfitting and Underfitting - Concepts}
    \begin{block}{Understanding Concepts}
        \begin{itemize}
            \item \textbf{Overfitting}: The model learns the training data too well, capturing noise and outliers. High accuracy on training data, low on unseen data.
            \item \textbf{Underfitting}: The model is too simple, failing to capture the underlying trends, resulting in poor performance on both training and unseen data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Overfitting and Underfitting - Impact on Performance}
    \begin{block}{Impact on Model Performance}
        \begin{itemize}
            \item \textbf{Overfitting Consequences}:
            \begin{itemize}
                \item High accuracy on training data but low accuracy on validation/test data.
                \item Example: High-degree polynomial regression may fit noise rather than the true relationship.
            \end{itemize}
            \item \textbf{Underfitting Consequences}:
            \begin{itemize}
                \item Poor performance on both training and unseen data.
                \item Example: A linear regression model for quadratic data produces inaccurate trends.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Overfitting and Underfitting - Mitigation Strategies}
    \begin{block}{Strategies to Mitigate}
        \begin{itemize}
            \item \textbf{Regularization Techniques}:
            \begin{itemize}
                \item Lasso Regression (L1 Regularization): Adds a penalty equal to the absolute value of coefficients.
                \item Ridge Regression (L2 Regularization): Adds a penalty equal to the square of the coefficients.
            \end{itemize}
            \item \textbf{Simplifying the Model}: Reducing the number of predictors or the complexity to avoid capturing noise.
            \item \textbf{Cross-Validation}: Use methods like K-fold cross-validation to reliably assess model performance.
            \item \textbf{Train on More Data}: Increasing dataset size helps in better generalization.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    \begin{block}{Polynomial Regression}
        \begin{itemize}
            \item \textbf{Underfitting}: A linear model (\(y = mx + b\)) fitting non-linear data.
            \item \textbf{Overfitting}: A 10th-degree polynomial fitting noise and oscillating between data points.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=python]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Sample data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([3, 4, 2, 5, 6])

# Polynomial features
poly = PolynomialFeatures(degree=10)  # Adjust degree for overfitting
X_poly = poly.fit_transform(X)

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2)

# Fit model
model = LinearRegression().fit(X_train, y_train)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Regression Analysis}
    \begin{block}{Understanding Ethical Implications}
        The ethical implications of regression analysis involve:
        \begin{itemize}
            \item Bias in data
            \item Transparency in model interpretation
            \item Need for fair representations
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Bias in Data}
    \begin{itemize}
        \item \textbf{Definition}: Systematic errors leading to skewed outcomes.
        \item \textbf{Sources of Bias}:
        \begin{itemize}
            \item Sampling Bias: Non-representative samples (e.g., surveying only one demographic).
            \item Measurement Bias: Inaccurate data collection methods.
        \end{itemize}
        \item \textbf{Example}: Biased datasets in loan approvals can result in discriminatory practices.
        \item \textbf{Key Point}: Ensure datasets are representative to avoid biased results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Transparency in Model Interpretation}
    \begin{itemize}
        \item \textbf{Importance of Transparency}: Stakeholders should understand how models work.
        \item \textbf{Methods}:
        \begin{itemize}
            \item Explainability: Use regression coefficients to clarify predictor effects.
            \item Documentation: Provide clear data sources and methodologies.
        \end{itemize}
        \item \textbf{Example}: A healthcare provider must understand how patient data influences outcomes.
        \item \textbf{Key Point}: Models must be interpretable for effective use.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Need for Fair Representations}
    \begin{itemize}
        \item \textbf{Fairness in Modeling}: Models should not reinforce existing inequalities.
        \item \textbf{Approaches}:
        \begin{itemize}
            \item Disaggregate Analysis: Analyze the impact on different subgroups.
            \item Regular Audits: Evaluate models against fairness metrics.
        \end{itemize}
        \item \textbf{Example}: In criminal justice, ensure predictions do not target minority populations disproportionately.
        \item \textbf{Key Point}: Prioritize fairness to foster equality and justice.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Metrics}
    \begin{block}{Ethical Considerations}
        Ethical implications in regression analysis include bias, transparency, and fair representation. Addressing these enhances model credibility and societal contribution.
    \end{block}
    \begin{block}{Formula Metric}
        Consider using fairness metrics, e.g., Statistical Parity:
        \begin{equation}
        \text{Statistical Parity} = P(\text{Positive Prediction} |\text{Group 1}) - P(\text{Positive Prediction} |\text{Group 2})
        \end{equation}
    \end{block}
    \begin{block}{Key Takeaway}
        Emphasizing ethics in data science enhances integrity and meets societal standards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Regression Models - Introduction}
    \begin{block}{Introduction to Regression Analysis}
        Regression analysis is a powerful statistical tool used to model and analyze relationships between a dependent variable and one or more independent variables. 
    \end{block}
    \begin{itemize}
        \item Helps understand how predictor variables affect the outcome variable.
        \item Finds applications in various fields, making it invaluable in research and practical applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Regression Models - Case Studies}
    \begin{block}{1. Economics - Case Study: Housing Prices}
        \begin{itemize}
            \item Economists predict housing prices based on factors such as location and size.
            \item Example Model:
            \end{itemize}
            \begin{equation}
            \text{Price} = \beta_0 + \beta_1(\text{Square Footage}) + \beta_2(\text{Number of Bedrooms}) + \beta_3(\text{Location Quality}) + \epsilon
            \end{equation}
            \begin{itemize}
                \item Helps real estate agents and buyers understand market dynamics.
            \end{itemize}
    \end{block}

    \begin{block}{2. Healthcare - Case Study: Patient Health Outcomes}
        \begin{itemize}
            \item Analyzes relationship between lifestyle factors and health outcomes.
            \item Example Model:
            \end{itemize}
            \begin{equation}
            \text{P(Heart Attack)} = \frac{1}{1 + e^{-(\beta_0 + \beta_1(\text{Age}) + \beta_2(\text{Cholesterol Level}) + \beta_3(\text{Blood Pressure})}}
            \end{equation}
            \begin{itemize}
                \item Insights lead to better prevention strategies and public health policies.
            \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Regression Models - Conclusion}
    \begin{block}{3. Social Sciences - Case Study: Education and Earnings}
        \begin{itemize}
            \item Analyzes impact of education level on income.
            \item Example Model:
            \end{itemize}
            \begin{equation}
            \text{Income} = \beta_0 + \beta_1(\text{Years of Education}) + \epsilon
            \end{equation}
            \begin{itemize}
                \item Provides insights into education investments influencing earnings.
            \end{itemize}
    \end{block}

    \begin{block}{4. Business - Case Study: Marketing Effectiveness}
        \begin{itemize}
            \item Assesses impact of marketing campaigns on sales.
            \item Example Model:
            \end{itemize}
            \begin{equation}
            \text{Sales} = \beta_0 + \beta_1(\text{Online Ads}) + \beta_2(\text{TV Advertising}) + \beta_3(\text{Promotion}) + \epsilon
            \end{equation}
            \begin{itemize}
                \item Identifying effective channels helps businesses allocate resources efficiently.
            \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        \begin{itemize}
            \item Regression analysis is versatile and uncovers relationships within data.
            \item Provides meaningful conclusions impacting economies, health, social policies, and business strategies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points Part 1}
    \begin{block}{1. Key Concepts in Regression Analysis}
        \begin{itemize}
            \item \textbf{Regression Analysis}: A statistical technique to understand relationships between variables and predict the value of a dependent variable based on one or more independent variables.
            \item \textbf{Dependent and Independent Variables}:
                \begin{itemize}
                    \item \textbf{Dependent Variable (Y)}: The outcome we are trying to predict.
                    \item \textbf{Independent Variables (X)}: The predictors that influence the dependent variable.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points Part 2}
    \begin{block}{2. Types of Linear Models}
        \begin{itemize}
            \item \textbf{Simple Linear Regression}: 
                \begin{itemize}
                    \item \textbf{Formula}: $Y = \beta_0 + \beta_1 X + \epsilon$
                    \item \textbf{Example}: Predicting sales based on advertising expenditure.
                \end{itemize}
            \item \textbf{Multiple Linear Regression}: 
                \begin{itemize}
                    \item \textbf{Formula}: $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon$
                    \item \textbf{Example}: Predicting house prices based on size, location, and number of rooms.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points Part 3}
    \begin{block}{3. Model Evaluation Metrics}
        \begin{itemize}
            \item \textbf{R-squared (R²)}: Indicates the proportion of variance in the dependent variable explained by the independent variables. Ranges from 0 to 1.
            \item \textbf{Adjusted R-squared}: Adjusted for the number of predictors, providing a more accurate measure for multiple predictors.
            \item \textbf{Mean Absolute Error (MAE)}: Measures average magnitude of errors without considering their direction.
            \item \textbf{Root Mean Squared Error (RMSE)}: Measures square root of the average of squared differences between predicted and actual values, penalizing larger errors more than MAE.
        \end{itemize}
    \end{block}
    
    \begin{block}{4. Ethical Considerations}
        \begin{itemize}
            \item \textbf{Data Integrity}: Ensuring accuracy and representativeness to avoid misleading conclusions.
            \item \textbf{Bias in Prediction}: Awareness of potential biases leading to unfair treatment of certain groups.
            \item \textbf{Transparency}: Clearly communicating limitations and assumptions made during analysis.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Regression analysis is crucial across disciplines like economics and healthcare.
            \item Selecting the appropriate regression model and evaluation metrics is essential.
            \item Ethical considerations prevent misuse of statistical findings.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas Recap}
    \begin{equation}
        \text{Simple Linear Regression: } Y = \beta_0 + \beta_1 X + \epsilon
    \end{equation}

    \begin{equation}
        \text{Multiple Linear Regression: } Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
    \end{equation}
    
    \begin{block}{Example Code Snippet (Python using \texttt{statsmodels})}
    \begin{lstlisting}[language=Python]
import statsmodels.api as sm
X = df[['X1', 'X2']]  # Independent variables
Y = df['Y']           # Dependent variable
X = sm.add_constant(X)  # Adds a constant term
model = sm.OLS(Y, X).fit()
print(model.summary())
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussions - Overview}
    This slide opens the floor for an engaging dialogue about the key issues explored in Chapter 4 on linear models and regression analysis. 
    Engaging through discussion reinforces understanding and identifies areas requiring further clarification.

    \begin{block}{Key Concepts Recap}
        - **Linear Models**: Represent the relationship between independent (predictor) and dependent (response) variables.
        - **Types of Linear Regression**:
            \begin{itemize}
                \item Simple Linear Regression
                \item Multiple Linear Regression
            \end{itemize}
        - **Evaluation Metrics**:
            \begin{itemize}
                \item R-squared
                \item Adjusted R-squared
                \item Mean Squared Error (MSE)
            \end{itemize}
        - **Ethical Considerations**: Focus on data privacy, model bias, and interpretability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions}
    Here are some questions to consider that can guide our discussion:

    \begin{enumerate}
        \item What challenges do you foresee in applying linear regression to real-world data?
        \item How can we ensure that our models do not produce biased results?
        \item In what scenarios might a linear model fail to capture the complexity of the data?
        \item What alternative approaches could we consider if our data does not meet the assumptions of linear regression?
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples to Facilitate Discussion}
    To enhance our discussion, consider the following examples:

    \begin{block}{Example 1}
        Data analyzing the impact of study time on exam scores using linear regression shows a relationship. What if adding "previous exam performance" significantly changes your results?
    \end{block}

    \begin{block}{Example 2}
        A model predicting house prices based on size could overlook significant factors like neighborhood features (e.g., crime rate, proximity to schools). How might this oversight impact accuracy and ethics?
    \end{block}

    \begin{block}{Encourage Participation}
        Please share your thoughts, experiences, or questions related to these prompts. Engaging actively enhances understanding and retention of regression analysis concepts.
    \end{block}
\end{frame}


\end{document}