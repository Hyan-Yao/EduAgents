\frametitle{Key Evaluation Metrics - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of true positive predictions to total actual positives.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \end{equation}
            \item \textbf{When to Use}: Crucial when missing positive instances incurs high costs, e.g., medical diagnostics.
            \item \textbf{Example}: 25 correct identifications of 40 spam emails results in a Recall of 0.625.
        \end{itemize}

        \item \textbf{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of Precision and Recall, balancing the two metrics.
            \item \textbf{Formula}:
            \begin{equation}
            \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{When to Use}: Useful for imbalanced datasets where both Precision and Recall are necessary.
            \item \textbf{Example}: Given Precision = 0.83 and Recall = 0.625, F1 Score â‰ˆ 0.72.
        \end{itemize}
    \end{enumerate}
