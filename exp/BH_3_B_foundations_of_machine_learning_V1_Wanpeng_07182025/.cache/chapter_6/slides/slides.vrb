\frametitle{Hierarchical Clustering - Part 3}
    \begin{block}{Definition}
        Hierarchical clustering builds a tree of clusters (dendrogram) by either:
        \begin{itemize}
            \item Agglomerative (bottom-up approach)
            \item Divisive (top-down approach)
        \end{itemize}
    \end{block}

    \begin{block}{How it Works (Agglomerative)}
        \begin{enumerate}
            \item Start with each data point as its own cluster.
            \item Iteratively merge the closest pair of clusters.
            \item Continue until all points form one cluster or a desired number of clusters is achieved.
        \end{enumerate}
    \end{block}

    \begin{block}{Advantages}
        \begin{itemize}
            \item Dendrogram Visualization: Easy interpretation of clusters.
            \item No Need for Pre-specification of Clusters: Decide based on the dendrogram.
            \item Flexibility: Works well with arbitrary-shaped data.
        \end{itemize}
    \end{block}

    \begin{block}{Disadvantages}
        \begin{itemize}
            \item Computational Cost: More expensive than K-means (O(n^3)).
            \item Sensitivity to Noise and Outliers: Can distort results.
            \item Not Scalable: Inefficient for large datasets.
        \end{itemize}
    \end{block}
