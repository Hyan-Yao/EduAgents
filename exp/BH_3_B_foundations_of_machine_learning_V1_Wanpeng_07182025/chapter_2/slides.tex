\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \maketitle
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Learning Paradigms}
    In machine learning, we utilize various approaches to extract knowledge from data. Two fundamental categories are:
    \begin{block}{Supervised Learning}
        Models are trained using labeled data, where each input is paired with the correct output.
    \end{block}
    \begin{block}{Unsupervised Learning}
        Involves training on data that is not labeled; the algorithm identifies patterns independently.
    \end{block}
    Understanding these distinctions is essential for applying machine learning techniques effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Supervised Learning}
    \begin{itemize}
        \item \textbf{Definition:} Models are trained using labeled data.
        \item \textbf{Goal:} Learn a mapping from inputs to outputs to predict outcomes.
        \item \textbf{Examples:}
        \begin{itemize}
            \item \textbf{Classification:} Classifying emails as 'spam' or 'not spam'.
            \item \textbf{Regression:} Predicting house prices based on features like size and location.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Definition:} Training on unlabeled data; algorithms identify patterns without direct supervision.
        \item \textbf{Goal:} Discover inherent relationships within the data.
        \item \textbf{Examples:}
        \begin{itemize}
            \item \textbf{Clustering:} Grouping customers based on purchasing behavior.
            \item \textbf{Dimensionality Reduction:} Techniques like PCA to reduce features while preserving variance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item The core difference is labeled vs. unlabeled data.
        \item Applications:
        \begin{itemize}
            \item Supervised: classification, regression, time-series forecasting.
            \item Unsupervised: market segmentation, anomaly detection, exploratory data analysis.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Illustrative Examples}
    \begin{itemize}
        \item Both supervised and unsupervised learning are crucial in machine learning.
        \item Supervised learning is suited for prediction tasks; unsupervised learning is for structure exploration. 
    \end{itemize}
    \begin{block}{Formula for Linear Regression}
        \begin{equation}
            y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon
        \end{equation}
    \end{block}
    \begin{block}{K-Means Clustering Steps}
        \begin{itemize}
            \item Define $k$ initial cluster centroids.
            \item Assign points to the nearest centroid.
            \item Update centroids based on the mean of assigned points.
            \item Repeat until convergence.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definitions - Supervised Learning}
    \begin{block}{Supervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: A type of machine learning where the model learns from labeled data (input-output pairs).
            \item \textbf{How it Works}:
                \begin{enumerate}
                    \item Data Collection: Gather a dataset with features and labels.
                    \item Model Training: Learn mappings by minimizing prediction errors.
                    \item Prediction: Use the trained model to predict outputs for new inputs.
                \end{enumerate}
            \item \textbf{Example}: Email Spam Detection
                \begin{itemize}
                    \item Labeled data: Emails labeled as "spam" or "not spam."
                    \item Goal: Classify incoming emails based on learned patterns from labeled examples.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definitions - Unsupervised Learning}
    \begin{block}{Unsupervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: A type of machine learning where the model learns from unlabeled data, finding structures in the data autonomously.
            \item \textbf{How it Works}:
                \begin{enumerate}
                    \item Data Collection: Gather a dataset of input features without labels.
                    \item Model Training: Identify patterns, groupings, or structures in the data using algorithms.
                    \item Analysis: The model outputs insights such as clusters or association rules.
                \end{enumerate}
            \item \textbf{Example}: Customer Segmentation
                \begin{itemize}
                    \item Unlabeled data: Customer purchase histories without category labels.
                    \item Goal: Identify distinct customer groups based on behavior.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Relevant Concepts}
    \begin{block}{Key Points}
        \begin{itemize}
            \item In supervised learning, models receive feedback through labeled data.
            \item In unsupervised learning, there is no explicit feedback; models find patterns independently.
            \item Understanding these differences aids in selecting the right machine learning techniques for specific tasks.
        \end{itemize}
    \end{block}
    
    \begin{block}{Relevant Formulas}
        \textbf{Loss Function (Supervised Learning)}:
        \begin{equation}
        \text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
        \end{equation}
        \textbf{Clustering Metric (Unsupervised Learning)}:
        \begin{equation}
        \text{Silhouette Score} = \frac{b - a}{\max(a, b)}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences: Supervised vs. Unsupervised Learning}
    \begin{block}{Overview}
    Supervised and unsupervised learning are two fundamental approaches in machine learning, each with distinct characteristics and methodologies. Understanding these differences is essential for selecting the appropriate paradigm for a given problem.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences - Definitions}
    \begin{itemize}
        \item \textbf{Supervised Learning:} 
        \begin{itemize}
            \item Involves learning from \textit{labeled data} with input-output pairs.
            \item The model is trained to predict outputs based on inputs.
        \end{itemize}
        \item \textbf{Unsupervised Learning:} 
        \begin{itemize}
            \item Involves learning from \textit{unlabeled data} with only inputs provided.
            \item The model identifies patterns or groupings within the data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences - Methodology}
    \begin{block}{Supervised Learning}
    \begin{itemize}
        \item Uses a \textbf{training dataset} consisting of input-output pairs.
        \item A learning algorithm maps input to the correct output.
        \item \textbf{Example Algorithms:} Linear Regression, Decision Trees, Support Vector Machines.
    \end{itemize}
    \end{block}

    \begin{block}{Formula Example}
    \begin{equation}
        Y = f(X) + \epsilon
    \end{equation}
    Where:
    \begin{itemize}
        \item \( Y \): output variable
        \item \( X \): input variable
        \item \( f(X) \): function learned from the data
        \item \( \epsilon \): noise or error
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences - Goals and Outcomes}
    \begin{block}{Goals}
        \begin{itemize}
            \item \textbf{Supervised Learning:} 
            \begin{itemize}
                \item To predict outputs for new unseen data.
                \item To assess accuracy through metrics like Mean Squared Error (MSE).
            \end{itemize}
            \item \textbf{Unsupervised Learning:} 
            \begin{itemize}
                \item To explore data and find hidden patterns.
                \item To reduce dimensionality and summarize data.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Outcomes}
        \begin{itemize}
            \item \textbf{Supervised Learning:} 
            \begin{itemize}
                \item Produces models that classify or predict outcomes.
                \item \textit{Example Outcome:} A spam detection model for emails.
            \end{itemize}
            \item \textbf{Unsupervised Learning:} 
            \begin{itemize}
                \item Generates insights into data structure.
                \item \textit{Example Outcome:} Customer segmentation for targeted marketing.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Supervised learning requires \textbf{labeled data}, while unsupervised learning does not.
        \item Selection between supervised and unsupervised depends on the need to predict a label or explore data patterns.
        \item Understanding the goals and outcomes can guide algorithm choice and achieve desired results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Use Supervised Learning}
    \begin{block}{Understanding Supervised Learning}
        Supervised learning is a type of machine learning where a model is trained on labeled data. This involves providing the algorithm with both input data and the corresponding output, allowing it to learn the relationship between the two.
    \end{block}
    This methodology is essential when the goal is to predict outcomes based on historical data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Scenarios for Supervised Learning}
    \begin{enumerate}
        \item \textbf{Classification Tasks:}
            \begin{itemize}
                \item \textbf{Definition:} Predicting a discrete label from input features.
                \item \textbf{Example:} Email filtering (spam vs. non-spam).
                \item \textbf{Techniques:} Logistic Regression, Support Vector Machines, Decision Trees, Random Forests.
            \end{itemize}
        
        \item \textbf{Regression Tasks:}
            \begin{itemize}
                \item \textbf{Definition:} Predicting a continuous numerical outcome based on input features.
                \item \textbf{Example:} House price prediction.
                \item \textbf{Techniques:} Linear Regression, Polynomial Regression, Support Vector Regression, Neural Networks.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Choose Supervised Learning}
    \begin{itemize}
        \item You have a large amount of labeled data available.
        \item The problem involves predicting specific outcomes based on clear input-output relationships.
        \item The task can be categorized into distinct classes (for classification) or involves predicting a real-valued number (for regression).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Considerations}
    \begin{itemize}
        \item \textbf{Data Quality:} The success of supervised learning largely depends on the quality and quantity of the labeled data.
        \item \textbf{Model Evaluation:} Use metrics such as accuracy, precision, recall, and F1 scores for classification tasks, while mean absolute error (MAE) and root mean square error (RMSE) are useful for regression tasks to evaluate model efficacy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Supervised learning is a powerful technique best suited for tasks where predictive accuracy is vital, and a well-defined relationship between inputs and outputs exists. Understanding the differences in tasks (classification vs. regression) and tools will enable better decision-making for machine learning applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Sample dataset
X = [[1, 2], [2, 3], [3, 4], [4, 5]]  # features
y = [0, 0, 1, 1]                      # labels

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Initialize and train the model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy:.2f}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Unsupervised Learning}
    \begin{itemize}
        \item Unsupervised learning involves training algorithms on datasets without labeled outputs.
        \item The model identifies patterns and structures in data autonomously.
        \item Main techniques include:
        \begin{itemize}
            \item Clustering
            \item Dimensionality Reduction
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Unsupervised Learning - Clustering}
    \begin{itemize}
        \item \textbf{Clustering}:
        \begin{itemize}
            \item Definition: Grouping data points into clusters based on similarity.
            \item Common Algorithms:
            \begin{itemize}
                \item \textbf{K-Means}: Groups data into K distinct clusters.
                \begin{lstlisting}
Algorithm:
1. Choose the number of clusters, K.
2. Initialize cluster centroids randomly.
3. Assign each data point to the nearest cluster centroid.
4. Update centroid positions based on the mean of assigned points.
5. Repeat steps 3-4 until convergence.
                \end{lstlisting}
                \item \textbf{Hierarchical Clustering}: Builds a hierarchy of clusters.
            \end{itemize}
            \item Applications:
            \begin{itemize}
                \item Example: Segmenting customer data by purchasing behavior.
                \item Example: Organizing documents by topic similarity.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Unsupervised Learning - Dimensionality Reduction}
    \begin{itemize}
        \item \textbf{Dimensionality Reduction}:
        \begin{itemize}
            \item Definition: Reducing the number of features while retaining as much variance as possible.
            \item Common Algorithms:
            \begin{itemize}
                \item \textbf{Principal Component Analysis (PCA)}:
                \begin{lstlisting}
Steps:
1. Standardize the dataset (mean = 0, variance = 1).
2. Compute the covariance matrix.
3. Calculate eigenvalues and eigenvectors.
4. Sort eigenvalues and select top K eigenvectors.
5. Transform the original dataset using selected eigenvectors.
                \end{lstlisting}
                \item \textbf{t-SNE}: Effective for visualizing high-dimensional data.
            \end{itemize}
            \item Applications:
            \begin{itemize}
                \item Example: Compressing image data while preserving essential features.
                \item Example: Visualizing clusters of handwritten digits.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Use Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Exploratory Data Analysis}:
        \begin{itemize}
            \item Reveal hidden structures or relationships in data.
        \end{itemize}
        \item \textbf{Preprocessing for Supervised Learning}:
        \begin{itemize}
            \item Reduce dimensionality or identify groups to improve models.
        \end{itemize}
        \item \textbf{Anomaly Detection}:
        \begin{itemize}
            \item Identify outliers crucial for fraud detection or quality control.
        \end{itemize}
        \item \textbf{Market Segmentation}:
        \begin{itemize}
            \item Understand distinct customer groups for targeted marketing.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Unsupervised learning offers a powerful tool for discovering patterns in data.
        \item Techniques such as clustering and dimensionality reduction enhance understanding and analysis.
        \item Applications span various fields, allowing for insightful conclusions without predefined categories.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Common Algorithms in Supervised Learning - Overview}
    \begin{block}{Overview of Supervised Learning}
        Supervised learning involves training a model on a labeled dataset, meaning that each training example is paired with an output label. The goal is for the model to learn from this data so it can make predictions or classify new, unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Common Algorithms in Supervised Learning - Linear Regression}
    \begin{block}{Linear Regression}
        \begin{itemize}
            \item \textbf{Concept}: A statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (features). It establishes a linear equation of the form:
            \begin{equation}
            y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon
            \end{equation}
            where \( y \) is the predicted output, \( \beta \) represents coefficients, \( x \) is the feature, and \( \epsilon \) is the error term.
            \item \textbf{Example}: Predicting house prices based on features like size, number of bedrooms, and location.
            \item \textbf{Key Point}: Best for predicting continuous values and assumes a linear relationship.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Common Algorithms in Supervised Learning - Decision Trees and Support Vector Machines}
    \begin{block}{Decision Trees}
        \begin{itemize}
            \item \textbf{Concept}: A tree-like model used for both classification and regression tasks. It splits the data into subsets based on feature values, forming a tree structure.
            \begin{itemize}
                \item Each internal node represents a feature decision.
                \item Each leaf node represents a target label.
            \end{itemize}
            \item \textbf{Example}: Classifying whether an email is spam based on features like word frequency and sender information.
            \item \textbf{Key Point}: Easily interpretable; can handle both categorical and continuous data, but prone to overfitting if not pruned properly.
        \end{itemize}
    \end{block}

    \begin{block}{Support Vector Machines (SVM)}
        \begin{itemize}
            \item \textbf{Concept}: A classification technique that finds the hyperplane that best separates different classes in the feature space. The objective is to maximize the margin between data points of different classes.
            \item \textbf{Formula}: The decision boundary can be defined as:
            \begin{equation}
            f(x) = w^T x + b = 0
            \end{equation}
            where \( w \) is the weight vector and \( b \) is the bias term.
            \item \textbf{Example}: Classifying images of cats and dogs based on pixel values.
            \item \textbf{Key Point}: Effective in high-dimensional spaces and with clear margin of separation, but can be computationally intensive.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Common Algorithms in Supervised Learning - Summary}
    \begin{itemize}
        \item \textbf{Linear Regression}: Great for regression tasks, models linear relationships.
        \item \textbf{Decision Trees}: Intuitive and interpretable, suitable for both regression and classification, but may overfit.
        \item \textbf{Support Vector Machines}: Powerful classification tool, best for high-dimensional data but requires careful tuning of parameters.
    \end{itemize}

    \begin{block}{Conclusion}
        These supervised learning algorithms serve diverse applications in data science, helping to make predictions and classify data. Understanding their strengths and limitations allows data scientists to select the appropriate algorithm for their specific task.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms in Unsupervised Learning - Introduction}
    \begin{itemize}
        \item Unsupervised learning focuses on datasets without labeled responses.
        \item Algorithms identify patterns and structures based on input features.
        \item This presentation covers:
        \begin{itemize}
            \item K-means Clustering
            \item Hierarchical Clustering
            \item Principal Component Analysis (PCA)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms in Unsupervised Learning - K-Means Clustering}
    \begin{block}{Overview}
        K-means is a clustering algorithm partitioning data into K distinct clusters.
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Initialization}: Choose K centroids randomly.
        \item \textbf{Assignment}: Assign each point to the nearest centroid.
        \item \textbf{Update}: Recalculate centroids as the mean of cluster points.
        \item \textbf{Iterate}: Repeat until centroids stabilize.
    \end{enumerate}
    
    \begin{block}{Example}
        Group customers based on spending habits (e.g., low, medium, high spenders).
    \end{block}
    
    \begin{equation}
        J = \sum_{i=1}^{K} \sum_{j=1}^{n} ||X_j - \mu_i||^2 
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms in Unsupervised Learning - Hierarchical Clustering}
    \begin{block}{Overview}
        Builds a dendrogram representing nested clusters, which can be agglomerative or divisive.
    \end{block}

    \begin{enumerate}
        \item \textbf{Compute Distance Matrix}: Calculate distances between pairs.
        \item \textbf{Merge Closest Clusters}: Start with single points and merge iteratively.
        \item \textbf{Construct Dendrogram}: Visualize cluster merging.
    \end{enumerate}

    \begin{block}{Example}
        Group species based on genetic similarities in biology.
    \end{block}
    
    \begin{block}{Distance Metrics}
        \begin{itemize}
            \item Euclidean Distance: 
            \[ d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2} \]
            \item Manhattan Distance: 
            \[ d(x, y) = \sum_{i=1}^n |x_i - y_i| \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms in Unsupervised Learning - Principal Component Analysis (PCA)}
    \begin{block}{Overview}
        PCA is a dimensionality reduction technique that preserves variance.
    \end{block}

    \begin{enumerate}
        \item \textbf{Standardize Data}: Scale to have mean 0 and std deviation 1.
        \item \textbf{Covariance Matrix}: Understand how features vary together.
        \item \textbf{Eigenvalues and Eigenvectors}: Determine principal components.
        \item \textbf{Project Data}: Reduce dimensions using top K eigenvectors.
    \end{enumerate}

    \begin{block}{Example}
        In image processing, PCA reduces pixel numbers while preserving key features.
    \end{block}

    \begin{equation}
        \text{Variance}(\text{PC}) = \frac{\lambda_i}{\sum_{j=1}^{m} \lambda_j}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms in Unsupervised Learning - Key Points}
    \begin{itemize}
        \item Unsupervised learning methods focus on finding patterns in unlabeled data.
        \item Algorithm selection depends on:
        \begin{itemize}
            \item Data structure
            \item Size
            \item Desired outcomes
        \end{itemize}
        \item Visual understanding:
        \begin{itemize}
            \item Clustering algorithms are often visualized.
            \item PCA helps visualize high-dimensional data in lower dimensions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics for Supervised Learning}
    \begin{block}{Overview}
        In supervised learning, assessing model performance is crucial. Different metrics provide insights into model effectiveness, aiding in determining the most suitable model for specific problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics - Part 1}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted instances to total instances.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
            \end{equation}
            \item \textbf{When to Use}: Best for balanced datasets; misleading for imbalanced datasets.
            \item \textbf{Example}: 90 out of 100 emails classified correctly gives an Accuracy of 0.90.
        \end{itemize}
        
        \item \textbf{Precision}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of true positive predictions to total predicted positives.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \end{equation}
            \item \textbf{When to Use}: Important when false positives have significant costs, e.g., in fraud detection.
            \item \textbf{Example}: 25 out of 30 email predictions as spam are actually spam, Precision = 0.83.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of true positive predictions to total actual positives.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \end{equation}
            \item \textbf{When to Use}: Crucial when missing positive instances incurs high costs, e.g., medical diagnostics.
            \item \textbf{Example}: 25 correct identifications of 40 spam emails results in a Recall of 0.625.
        \end{itemize}

        \item \textbf{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of Precision and Recall, balancing the two metrics.
            \item \textbf{Formula}:
            \begin{equation}
            \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{When to Use}: Useful for imbalanced datasets where both Precision and Recall are necessary.
            \item \textbf{Example}: Given Precision = 0.83 and Recall = 0.625, F1 Score ≈ 0.72.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item \textbf{Accuracy}: Provides overall performance, but can be misleading in imbalanced datasets.
        \item \textbf{Precision}: Essential when false positives are critical.
        \item \textbf{Recall}: Vital when false negatives are more concerning.
        \item \textbf{F1 Score}: Balances Precision and Recall; particularly useful with imbalanced data.
    \end{itemize}
    These metrics aid in evaluating model effectiveness and guiding the model selection process.
\end{frame}

\begin{frame}[fragile]{Challenges in Supervised Learning - Overview}
    Supervised learning involves training models on labeled datasets. Each input data instance is paired with a correct output, allowing the model to learn from examples to predict or classify unseen data.
\end{frame}

\begin{frame}[fragile]{Challenges in Supervised Learning - Key Challenges}
    \begin{block}{Key Challenges}
        \begin{enumerate}
            \item Overfitting
            \item Need for Large Labeled Datasets
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Challenges in Supervised Learning - Overfitting}
    \begin{block}{Overfitting}
        \begin{itemize}
            \item \textbf{Definition}: Learning noise in training data leading to poor generalization.
            \item \textbf{Illustration}: Complex polynomial fits to a small dataset can curve erratically beyond the data range.
            \item \textbf{Detection}: Difference in training vs validation accuracy.
            \item \textbf{Mitigation Techniques}:
            \begin{itemize}
                \item Regularization (L1, L2)
                \item Cross-Validation
                \item Pruning for decision trees
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Challenges in Supervised Learning - Labeled Datasets}
    \begin{block}{Need for Large Labeled Datasets}
        \begin{itemize}
            \item \textbf{Explanation}: Dependence on ample labeled data; labeling is costly and labor-intensive.
            \item \textbf{Implications}: Insufficient data leads to inadequate training and poor performance.
            \item \textbf{Example}: Image classification requires thousands of labeled images.
            \item \textbf{Alternatives}:
            \begin{itemize}
                \item Data Augmentation
                \item Semi-supervised Learning
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Challenges in Supervised Learning - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Overfitting leads to high variance; counteract with cross-validation and regularization.
            \item Large labeled datasets are essential; use data augmentation and semi-supervised learning to mitigate challenges.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Challenges in Supervised Learning - Conclusion}
    Understanding overfitting and the need for large labeled datasets is vital for robust supervised learning models. Appropriate techniques enhance model performance in real-world applications.
\end{frame}

\begin{frame}[fragile]{Challenges in Supervised Learning - Additional Resources}
    \begin{block}{Code Snippets}
        \begin{lstlisting}[language=Python]
from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)  # Alpha is the regularization strength
model.fit(X_train, y_train)
        \end{lstlisting}
    
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print("Cross-validation scores:", scores)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Unsupervised Learning}
    \begin{block}{Understanding Unsupervised Learning}
        Unsupervised learning identifies patterns within input data without labeled responses, presenting several challenges that complicate meaningful insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in Unsupervised Learning}
    \begin{enumerate}
        \item \textbf{Validation of Results}
        \begin{itemize}
            \item Difficulty in performance metrics: No direct measures for correctness.
            \item Techniques include:
            \begin{itemize}
                \item Silhouette Score: Compares similarity within its cluster to other clusters.
                \item Davies-Bouldin Index: Evaluates average similarity of clusters.
                \item Example silhouette formula:
                \begin{equation}
                    \text{Silhouette}(x) = \frac{b(x) - a(x)}{\max(a(x), b(x))}
                \end{equation}
                where:
                \begin{itemize}
                    \item \( a(x) \): Average distance to other points in the same cluster.
                    \item \( b(x) \): Minimum average distance to points in the next nearest cluster.
                \end{itemize}
            \end{itemize}
        \end{itemize}
        \item \textbf{Interpretability of Results}
        \begin{itemize}
            \item Extracted patterns may be complex and hard to interpret, affecting practical applications.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continued Challenges and Examples}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{No Clear Objective Function}
        \begin{itemize}
            \item Ambiguities in learning goals can lead to subjective outcomes, such as varying cluster numbers.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Customer Segmentation:} Challenges in interpreting clusters for marketing strategies.
            \item \textbf{Anomaly Detection:} Difficulty distinguishing genuine anomalies from noise in fraud detection.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Unsupervised learning reveals hidden patterns but requires careful validation and interpretability measures to be effective.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies and Applications}
    \begin{block}{Overview}
        In this section, we explore the practical applications of both \textbf{supervised} and \textbf{unsupervised learning} across various industries, illustrating how these techniques are transforming operations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning Applications}
    \begin{itemize}
        \item \textbf{Healthcare}
        \begin{itemize}
            \item \textbf{Predictive Diagnostics:} Algorithms predict diseases using labeled datasets (e.g., patient records). 
            \item \textit{Example:} The \textbf{Framingham Heart Study} predicts cardiovascular risk factors.
        \end{itemize}
        
        \item \textbf{Finance}
        \begin{itemize}
            \item \textbf{Credit Scoring:} Models assess creditworthiness based on variables like income and past credit history.
            \item \textit{Example:} FICO scores evaluate and rank risk levels for loan approvals.
        \end{itemize}

        \item \textbf{Retail}
        \begin{itemize}
            \item \textbf{Customer Churn Prediction:} Algorithms predict customers likely to stop services based on historical data.
            \item \textit{Example:} \textbf{Netflix} analyzes viewing patterns to predict churn.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Unsupervised Learning Applications}
    \begin{itemize}
        \item \textbf{Marketing}
        \begin{itemize}
            \item \textbf{Customer Segmentation:} K-means clustering identifies different customer groups for targeted marketing.
            \item \textit{Example:} \textbf{Amazon} segments users based on purchasing behavior for personalized recommendations.
        \end{itemize}

        \item \textbf{Anomaly Detection}
        \begin{itemize}
            \item \textbf{Fraud Detection:} Models identify unusual transaction patterns indicating possible fraud.
            \item \textit{Example:} Credit card companies monitor transactions to flag atypical spending.
        \end{itemize}

        \item \textbf{Natural Language Processing (NLP)}
        \begin{itemize}
            \item \textbf{Topic Modeling:} Algorithms categorize documents into topics without labeled training data.
            \item \textit{Example:} News aggregators use topic modeling to group similar articles.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Examples}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Supervised Learning:} Requires labeled data; excels in classification and regression tasks.
            \item \textbf{Unsupervised Learning:} Does not need labeled data; suitable for exploratory analysis and discovering patterns.
        \end{itemize}
    \end{block}

    \begin{block}{Code Example}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Sample dataset
X, y = load_healthcare_data()  # Features and labels

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Model Training
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Predictions
predictions = model.predict(X_test)

# Evaluation
print(classification_report(y_test, predictions))
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding and applying supervised and unsupervised learning techniques can greatly enhance business intelligence and operational efficiency across numerous domains. Recognizing the appropriate context for these applications is key to leveraging these powerful tools.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations}
    \begin{block}{Understanding Ethical Implications}
        When deploying both \textbf{supervised} and \textbf{unsupervised learning models}, considering the ethical implications is crucial. These approaches can significantly impact society, influencing individual privacy, fairness, and overall trust in machine learning systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations}
    \begin{enumerate}
        \item \textbf{Data Privacy and Security}
            \begin{itemize}
                \item Supervised learning often requires labeled data, which can include sensitive information (e.g., health records).
                \item Unsupervised learning can expose personal information while analyzing large datasets.
                \item \textit{Example:} Use of facial recognition technology can infringe on privacy rights.
            \end{itemize}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item Data in both model types can contain biases, leading to unfair treatment.
                \item \textit{Example:} A supervised model for hiring might reflect existing biases in training data.
                \item \textit{Mitigation:} Audit datasets and apply strategies like Fairness through Awareness.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations (Continued)}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Transparency and Accountability}
            \begin{itemize}
                \item Stakeholders should understand decision-making processes.
                \item \textit{Example:} Explainable AI (XAI) techniques promote transparency in model decisions.
            \end{itemize}
        \item \textbf{Impact on Employment}
            \begin{itemize}
                \item ML models can automate jobs, raising concerns about job displacement.
                \item \textit{Example:} Unsupervised learning algorithms in manufacturing may reduce the need for human oversight.
            \end{itemize}
        \item \textbf{Informed Consent}
            \begin{itemize}
                \item Individuals should be informed about the use of their data, especially in supervised learning.
                \item \textit{Example:} Participants in health applications must consent to data usage for predictive models.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item Ethical considerations are essential to prevent harm and foster trust in machine learning.
            \item Key factors include data privacy, bias, transparency, and informed consent.
            \item Continuous commitment is required to mitigate ethical risks.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Upholding ethical standards is vital for the development and deployment of both supervised and unsupervised learning models to benefit society while minimizing harm.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Content for Engagement}
    \begin{enumerate}
        \item \textbf{Discussion Questions:}
            \begin{itemize}
                \item How can organizations ensure fairness in their machine learning models?
                \item What measures can be taken to protect the privacy of individuals?
            \end{itemize}
        \item \textbf{Suggested Reading:}
            \begin{itemize}
                \item "Weapons of Math Destruction" by Cathy O'Neil examines the dark side of big data and algorithms.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Remember}
        Encouraging ethical practices is not just about compliance; it’s about building a sustainable future that values individuals and communities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary of Key Points}
    \begin{itemize}
        \item Supervised Learning Defined
        \item Unsupervised Learning Defined
        \item Key Differences
        \item Importance of Choosing the Right Approach
        \item Ethical Considerations
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Learning Definitions}
    \begin{block}{1. Supervised Learning}
        - Involves training a model on a labeled dataset, where outcomes are known.\\
        - \textbf{Example}: Predicting house prices based on features like size, location, and number of bedrooms.
    \end{block}
    
    \begin{block}{2. Unsupervised Learning}
        - Model trains on data without labeled outcomes, identifying patterns or groupings.\\
        - \textbf{Example}: Market segmentation analyzing customer data to find natural clusters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Differences}
    \begin{itemize}
        \item \textbf{Input Data}:
            \begin{itemize}
                \item Supervised: Labeled data (e.g., classification tasks)
                \item Unsupervised: Unlabeled data (e.g., clustering tasks)
            \end{itemize}
        \item \textbf{Purpose}:
            \begin{itemize}
                \item Supervised: Predict outcomes or classify data.
                \item Unsupervised: Discover hidden patterns or structures.
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Importance of Choosing the Right Approach}
        - Influences model performance and validity of results:\\
        - Considerations: \\
            - Availability of labeled data \\
            - Problem objectives
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Ethical Considerations and Takeaways}
    \begin{block}{Ethical Considerations}
        - Both methods can carry ethical implications.\\
        - Bias in labeled data (supervised) or misinterpreting clustering results (unsupervised) require attention.
    \end{block}

    \begin{block}{Key Takeaways}
        - Understanding data and predictive goals informs selection of learning approach.
        - Each approach has distinct advantages and challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Closing Thoughts}
    As we move forward, consider these frameworks and examples. Think critically about their application to potential case studies or real-world scenarios.\\
    
    \begin{block}{Demo Code Samples}
        \textbf{Supervised Example:}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# Sample data
X = [[1, 2], [2, 3], [3, 4]]
y = [1, 2, 3]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Model training
model = RandomForestRegressor()
model.fit(X_train, y_train)
        \end{lstlisting}
        
        \textbf{Unsupervised Example:}
        \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans

# Sample data
X = [[1, 2], [1, 4], [1, 0],
     [4, 2], [4, 0], [4, 4]]

# Model training
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Objectives}
    \begin{itemize}
        \item \textbf{Clarify Concepts}: Address uncertainties regarding supervised and unsupervised learning.
        \item \textbf{Encourage Discussion}: Foster an open discussion to enhance understanding.
        \item \textbf{Connect Theory to Practice}: Relate concepts to real-world applications and scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Key Concepts}
    \begin{block}{Supervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: Training models on labeled datasets.
            \item \textbf{Examples}:
                \begin{itemize}
                    \item \textbf{Classification}: Identifying spam emails (e.g., logistic regression).
                    \item \textbf{Regression}: Predicting house prices using features (e.g., linear regression).
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Unsupervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: The model explores unlabeled data to find patterns.
            \item \textbf{Examples}:
                \begin{itemize}
                    \item \textbf{Clustering}: Segmentation of customers (e.g., k-means clustering).
                    \item \textbf{Dimensionality Reduction}: Maintaining important information (e.g., PCA).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Discussion Points}
    \begin{enumerate}
        \item \textbf{Choice of Approach}:
            \begin{itemize}
                \item How to choose between supervised and unsupervised learning?
                \item What data characteristics influence this choice?
            \end{itemize}

        \item \textbf{Techniques and Model Evaluation}:
            \begin{itemize}
                \item Common techniques and evaluation metrics (e.g., accuracy, silhouette score).
                \item Limitations or challenges of each approach.
            \end{itemize}

        \item \textbf{Real-World Applications}:
            \begin{itemize}
                \item Examples from experiences or industries.
                \item Impact on industries like healthcare and retail.
            \end{itemize}
    \end{enumerate}
\end{frame}


\end{document}