\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Approximate Dynamic Programming]{Week 8: Approximate Dynamic Programming}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Approximate Dynamic Programming}
    Approximate Dynamic Programming (ADP) is a crucial methodology in reinforcement learning and operations research.
    
    \begin{itemize}
        \item Addresses optimal decision-making in complex, multi-stage environments.
        \item Complements and extends traditional dynamic programming methods.
        \item Essential for practical applications with large state/action spaces.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Dynamic Programming?}
    \begin{block}{Dynamic Programming (DP)}
        DP is a method for solving complex problems by dividing them into simpler subproblems. It helps compute state values and derive optimal policies.
    \end{block}
    
    \begin{itemize}
        \item **Example**: In a grid world, an agent navigates between cells, with DP calculating the utility of each state for optimal policy formulation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of ADP in Reinforcement Learning}
    ADP enhances the reinforcement learning framework by addressing limitations of traditional DP:
    
    \begin{itemize}
        \item **Scalability**: Applicable to environments with large state/action spaces (e.g., robotics, finance).
        \item **Flexibility**: Incorporates various approximation techniques.
        \item **Balance**: Manages bias and variance to improve learning while controlling overfitting.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in ADP}
    \begin{enumerate}
        \item \textbf{Value Function Approximation}: Uses function approximators (linear functions, neural networks) to estimate value functions, reducing computational load.
        
        \item \textbf{Policy Approximation}: Approximates policies from a subset of states instead of representing all states, enhancing scalability.
        
        \item \textbf{Monte Carlo Methods}: Utilizes sample paths and statistical techniques to improve estimation accuracy for values and policies.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of ADP Applications}
    \begin{itemize}
        \item **Robot Navigation**: ADP facilitates navigation in complex environments with approximate state representations.
        \item **Revenue Management**: Optimizes pricing and inventory in real-time for industries such as airlines and hotels.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Snapshot}
    An essential aspect of ADP is the Bellman equation in its approximate form:
    \begin{equation}
        V(s) \approx R(s) + \gamma \max_{a} \sum_{s'} P(s' | s, a) V(s') 
    \end{equation}
    Where:
    \begin{itemize}
        \item \( V(s) \): Estimated value of state \( s \)
        \item \( R(s) \): Immediate reward for being in state \( s \)
        \item \( \gamma \): Discount factor (0 < \( \gamma \) < 1)
        \item \( P(s' | s, a) \): Transition probability to next state \( s' \) from state \( s \) after action \( a \)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Approximate Dynamic Programming bridges the gap between dynamic programming's ideal solutions and the realities of real-world decision-making processes in reinforcement learning. 

    \begin{itemize}
        \item Allows efficient approximation of solutions for intelligent systems.
        \item Facilitates learning and adaptation in complex environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Foundations of Dynamic Programming - Overview}
    \textbf{Dynamic Programming (DP)} is a powerful algorithmic technique used to solve complex problems by breaking them down into simpler subproblems. It is applicable across various fields such as mathematics, computer science, operations research, and economics, particularly for optimization problems. 
    \begin{itemize}
        \item DP stores results of subproblems to avoid redundant calculations, improving efficiency for issues with:
        \begin{itemize}
            \item Overlapping subproblems
            \item Optimal substructure
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Foundations of Dynamic Programming - Core Principles}
    \textbf{Core Principles of Dynamic Programming:}
    \begin{enumerate}
        \item \textbf{Optimal Substructure:}
        \begin{itemize}
            \item Optimal solution can be constructed from optimal solutions to subproblems.
            \item \textit{Example:} Fibonacci sequence: 
            \[
            F(n) = F(n-1) + F(n-2)
            \]
        \end{itemize}
        
        \item \textbf{Overlapping Subproblems:}
        \begin{itemize}
            \item Problems can be broken into smaller, repeating subproblems.
            \item \textit{Example:} Fibonacci numbers calculated recursively have repeated subproblems (e.g., F(n-1) and F(n-2)).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Dynamic Programming Approaches}
    \textbf{Approaches:}
    \begin{itemize}
        \item \textbf{Top-Down Approach (Memoization):} 
        \begin{itemize}
            \item Recursively solve the problem and store results to save computation time.
        \end{itemize}
        \item \textbf{Bottom-Up Approach (Tabulation):}
        \begin{itemize}
            \item Solve all subproblems from the smallest and build up to the original problem solution using a table to store results.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Relevance to Approximate Dynamic Programming}
    \textbf{Relevance to Approximate Dynamic Programming (ADP):}
    \begin{itemize}
        \item ADP extends traditional DP principles to address complex problems with large state spaces.
        \item Focuses on using function approximators (like neural networks) to estimate value functions or policies.
        \item Enables scalable solutions to problems that would be computationally infeasible with classical DP.
    \end{itemize}
    
    \textbf{Key Points:}
    \begin{itemize}
        \item DP is crucial for efficiently solving recursive problems.
        \item Identifying optimal substructure and overlapping subproblems is essential.
        \item ADP enhances these foundational principles for larger state spaces.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example Formula in Dynamic Programming}
    \textbf{Example Formula:}
    For a straightforward DP problem like the 0/1 Knapsack, the recurrence relation is:
    \begin{equation}
    V(n, W) = 
    \begin{cases} 
    V(n-1, W), & \text{if } w_n > W \\
    \max(V(n-1, W), V(n-1, W-w_n) + v_n), & \text{if } w_n \leq W 
    \end{cases}
    \end{equation}
    Where:
    \begin{itemize}
        \item \( V(n, W) \) is the maximum value achievable with a knapsack capacity \( W \) using the first \( n \) items.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Summary}
    \textbf{Summary:}
    Understanding the foundations of Dynamic Programming is vital for tackling advanced topics like Approximate Dynamic Programming. By mastering optimal substructure and overlapping subproblems, students will better appreciate the methods and applications of ADP in complex decision-making tasks in reinforcement learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Approximate Dynamic Programming Overview}
    \begin{block}{Definition of ADP}
        Approximate Dynamic Programming (ADP) refers to a set of techniques used to solve complex optimization problems where traditional dynamic programming becomes computationally infeasible due to the size of the state and action spaces. ADP employs approximation methods to estimate the value functions or policies when exact solutions are hard to find.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences between ADP and Classical Dynamic Programming}
    \begin{enumerate}
        \item \textbf{Scalability}
            \begin{itemize}
                \item \textbf{Classical Dynamic Programming}:
                    \begin{itemize}
                        \item Exact methods work well for manageable number of states and actions.
                        \item Example: Optimal policy for a small grid-world.
                    \end{itemize}
                \item \textbf{Approximate Dynamic Programming}:
                    \begin{itemize}
                        \item Designed for high-dimensional problems with large state spaces.
                        \item Example: Inventory management with thousands of items.
                    \end{itemize}
            \end{itemize}
            
        \item \textbf{Value Function Representation}
            \begin{itemize}
                \item \textbf{Classical Dynamic Programming}: Complete representation, e.g., tables for states.
                \item \textbf{Approximate Dynamic Programming}: Function approximation techniques, e.g., neural networks.
            \end{itemize}
        
        \item \textbf{Computation and Iteration}
            \begin{itemize}
                \item \textbf{Classical Dynamic Programming}: Recursive and iterative for all states and actions.
                \item \textbf{Approximate Dynamic Programming}: Employs sampling to update subsets of states.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance and Summary of ADP}
    \begin{block}{Importance of Approximate Dynamic Programming}
        \begin{itemize}
            \item \textbf{Real-World Applications}: Widely used in robotics, finance, and healthcare.
            \item \textbf{Flexibility in Use}: ADP can utilize various approximation schemes for specific needs.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        \begin{itemize}
            \item ADP is essential for large-scale decision-making.
            \item Key differences include scalability, value function representation, and computation methods.
            \item Key Points to Remember:
                \begin{itemize}
                    \item ADP addresses optimization in vast state spaces.
                    \item Approximation methods allow for efficient computation.
                    \item Real-world scenarios often require the flexibility of ADP techniques.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Approximate Dynamic Programming (ADP)}
    \begin{block}{Introduction to Key Algorithms}
        Approximate Dynamic Programming (ADP) addresses the curse of dimensionality inherent in dynamic programming. 
        Key algorithms simplify decision-making processes by approximating value functions or policy functions.
    \end{block}
    \begin{itemize}
        \item Value Function Approximation
        \item Policy Search
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Function Approximation (VFA)}
    \begin{block}{Definition}
        VFA is a technique used to estimate the value function \( V(s) \) for a large state space, where complete enumeration is infeasible.
    \end{block}
    
    \begin{block}{Purpose}
        The value function helps determine the expected return from each state \( s \) based on future reward expectations.
    \end{block}

    \begin{block}{Common Approaches}
        \begin{itemize}
            \item Linear Function Approximation: 
            \[
            V(s) \approx \theta^T \phi(s)
            \]
            \item Non-linear Approximators: Techniques like neural networks to capture complex value functions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Value Function Approximation}
    In a simple grid world where an agent can move in four directions (up, down, left, right), we can utilize feature representations (like proximity to the goal) to construct an approximate value function for each state, rather than calculating exact values.

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item VFA approximates state values.
            \item Important for efficient decision-making in high-dimensional spaces.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Search}
    \begin{block}{Definition}
        Policy Search directly seeks the best policy \( \pi(a | s) \) for action \( a \) in state \( s \), without estimating the value function explicitly.
    \end{block}
    
    \begin{block}{Purpose}
        This method aims to find the optimal policy through optimization techniques, bypassing direct value estimation.
    \end{block}

    \begin{block}{Common Techniques}
        \begin{itemize}
            \item Gradient Ascent: 
            \[
            \theta_{new} = \theta + \alpha \nabla J(\theta)
            \]
            \item Evolutionary Strategies: Inspired by natural selection for evolving policies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Policy Search}
    Consider a robotic arm learning to reach a target location. Instead of simulating every movement (and thus calculating value functions), the arm can optimize based on successful movements while adjusting actions for improved performance.

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item VFA approximates state values; Policy Search optimizes action choices directly.
            \item These methods can be complementary and often combined in practice.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding these key algorithms is essential for effectively implementing Approximate Dynamic Programming techniques. 
    As we proceed, we will explore Value Function Approximation further and its implications in real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Function Approximation - Overview}
    \begin{itemize}
        \item \textbf{Definition}: Value Function Approximation (VFA) is a method in Approximate Dynamic Programming (ADP) for estimating the value function.
        \item \textbf{Role in ADP}: VFA simplifies the computation of the value function, making it feasible for large or continuous state spaces.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Value Function Approximation}
    \begin{enumerate}
        \item \textbf{Value Function}:
        \begin{itemize}
            \item Denoted as \( V(s) \) for a state \( s \).
            \item Represents expected return from state \( s \):
            \begin{equation}
            V(s) = \mathbb{E} \left[ R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots \mid S_t = s \right]
            \end{equation}
            where \( R_t \) is the reward at time \( t \) and \( \gamma \) is the discount factor (0 ≤ \( \gamma \) < 1).
        \end{itemize}
        
        \item \textbf{Function Approximation}:
        \begin{itemize}
            \item Represented using a parameterized function:
            \begin{equation}
            V(s; \theta) = \phi(s)^T \theta
            \end{equation}
            where:
            \begin{itemize}
                \item \( \phi(s) \) is the feature vector.
                \item \( \theta \) represents the weights to optimize.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Function Approximators}
    \begin{itemize}
        \item \textbf{Linear Approximators}:
        \begin{equation}
        V(s; \theta) = \theta_0 + \theta_1 x_1 + \theta_2 x_2
        \end{equation}
        where \( x_1 \) and \( x_2 \) are features of the state \( s \).

        \item \textbf{Non-linear Approximators}:
        \begin{itemize}
            \item Neural networks can effectively model value functions in high-dimensional spaces.
        \end{itemize}

        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item VFA allows generalization of expected returns for similar states.
            \item Reduces computational burden by using fewer parameters.
            \item Careful selection of features is critical to minimize approximation error.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Approximation Methods - Introduction}
    \begin{block}{Introduction to Policy Approximation in ADP}
        In the realm of Approximate Dynamic Programming (ADP), policy approximation methods play a crucial role in enabling efficient decision-making in environments with large or continuous state and action spaces.
        These methods strive to find near-optimal policies, often by leveraging techniques from machine learning and function approximation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Approximation Methods - Overview}
    \begin{block}{Overview}
        Policy approximation methods can be broadly categorized into:
        \begin{itemize}
            \item \textbf{Direct Policy Search Methods:} 
            These methods seek to find an optimal policy directly by optimizing the policy representation (e.g., through gradient ascent on expected returns).
            \item \textbf{Policy Improvement via Value Function Approximation:} 
            This method builds on value function approximation, using an estimated value function to derive an improved policy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques for Policy Approximation}
    \begin{block}{Common Techniques}
        \begin{enumerate}
            \item \textbf{Parameterized Policies:}
            Policies can be represented using parameterized functions (e.g., neural networks or linear models).
            \begin{itemize}
                \item Example: A policy can be defined as \( \pi(a | s; \theta) \) where \( \theta \) denotes the parameters of the policy.
            \end{itemize}
            
            \item \textbf{Policy Gradient Methods:}
            These methods optimize the policy directly by calculating the gradient of expected rewards.
            \begin{equation}
                \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_t \nabla \log \pi_\theta(a_t | s_t) R(\tau) \right]
            \end{equation}
            \begin{itemize}
                \item Illustration: In reinforcement learning, using the Actor-Critic technique, the "Actor" updates the policy while the "Critic" evaluates it.
            \end{itemize}

            \item \textbf{Policy Iteration:}
            A systematic approach where a policy is repeatedly evaluated and improved until convergence.
            \begin{itemize}
                \item Example: Start with an arbitrary policy \( \pi \), evaluate it to estimate the value function \( V^{\pi} \), and then improve the policy to maximize \( V^{\pi} \).
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Policy Approximation}
    \begin{block}{Example}
        Suppose an agent is navigating a grid world. Instead of defining a policy for every grid state, we can use a neural network to generalize the policy across similar states:
        \begin{itemize}
            \item \textbf{States:} cells in the grid.
            \item \textbf{Actions:} moves (up, down, left, right).
            \item \textbf{Policy:} The output of the neural network gives the probability distribution of actions for each state.
        \end{itemize}
        Using this representation helps tackle the curse of dimensionality, allowing the agent to effectively learn from fewer experiences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Policy approximation is vital for managing large state-action spaces.
            \item Various methods help derive policies with fewer resources while remaining efficient.
            \item The choice of method can significantly influence learning speed and policy performance.
        \end{itemize}
    \end{block}
    
    \begin{block}{Summary}
        In Approximate Dynamic Programming, choosing the right policy approximation method is key to balancing efficiency and effectiveness. As we explore further in this chapter, understanding the advantages of ADP will help us appreciate the significant role these methods play in complex environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{References}
    \begin{itemize}
        \item Sutton, R.S. \& Barto, A.G. (2018). \textit{Reinforcement Learning: An Introduction.}
        \item Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. \textit{Nature.}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Approximate Dynamic Programming (ADP)}
    \begin{itemize}
        \item \textbf{Scalability:} Capable of handling large state/action spaces through approximation.
        \item \textbf{Reduced Computational Burden:} Functions approximation reduces resources and time.
        \item \textbf{Flexibility and Adaptability:} Tailored to changing environments for real-world applications.
        \item \textbf{Handling Uncertainty:} Incorporates stochastic elements for robust decision-making.
        \item \textbf{Policy and Value Function Generalization:} Generalizes across states/actions improving learning efficiency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of ADP Advantages}
    \begin{enumerate}
        \item **Scalability:** 
            \begin{itemize}
                \item In robotics, use approximations for paths instead of exhaustive calculations.
            \end{itemize}
        \item **Reduced Computational Burden:** 
            \begin{itemize}
                \item In finance, ADP optimizes asset allocations without exhaustive searches.
            \end{itemize}
        \item **Flexibility and Adaptability:** 
            \begin{itemize}
                \item In supply chain management, ADP adjusts to demand and inventory fluctuations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Uncertainty and Generalization}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item **Handling Uncertainty:** 
            \begin{itemize}
                \item In healthcare, ADP models uncertain disease progression to optimize treatment plans.
            \end{itemize}
        \item **Policy and Value Function Generalization:** 
            \begin{itemize}
                \item In game AI, ADP generalizes strategies learned from earlier games to new scenarios.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaway and Conceptual Visualization}
    \begin{block}{Key Takeaway}
        Approximate Dynamic Programming is vital for effectively solving complex decision-making problems across various fields, enhancing scalability, efficiency, and adaptability while managing uncertainties.
    \end{block}

    \begin{equation}
        V(s) \approx w^T \phi(s)
    \end{equation}
    where \( \phi(s) \) represents feature representation and \( w \) denotes weights learned.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    In summary, ADP's advantages make it an essential tool for addressing intricate decision-making scenarios efficiently. It remains a preferred choice for researchers and practitioners in dynamic and uncertain environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in ADP - Overview}
    \begin{block}{Introduction}
        Approximate Dynamic Programming (ADP) offers powerful methods for addressing complex decision-making problems. However, several challenges and limitations often arise during its implementation.
    \end{block}
    \begin{itemize}
        \item Curse of Dimensionality
        \item Approximation Errors
        \item Sample Efficiency
        \item Convergence Issues
        \item Complexity of Implementation
        \item Overfitting
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in ADP - Detailed Analysis}
    \begin{enumerate}
        \item \textbf{Curse of Dimensionality}
        \begin{itemize}
            \item As the number of state variables increases, the size of the state space grows exponentially.
            \item Example: Robot navigation in a grid world with increasing dimensions becomes computationally infeasible.
        \end{itemize}

        \item \textbf{Approximation Errors}
        \begin{itemize}
            \item ADP relies on function approximations that may introduce errors, compromising decision quality.
            \item Example: A neural network value function may not generalize well, leading to suboptimal policies.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in ADP - Continued}
    \begin{enumerate}[resume]
        \item \textbf{Sample Efficiency}
        \begin{itemize}
            \item Many ADP methods require extensive data for effective training, posing limitations in data-scarce environments.
            \item Example: Training a robotic arm may need thousands of samples to learn movement dynamics.
        \end{itemize}

        \item \textbf{Convergence Issues}
        \begin{itemize}
            \item Some ADP methods may not converge to an optimal solution if approximation or learning rate is poor.
            \item Key Point: Monitor stability and convergence to avoid oscillations or divergence.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in ADP - Final Points}
    \begin{enumerate}[resume]
        \item \textbf{Complexity of Implementation}
        \begin{itemize}
            \item ADP requires integrating various components, complicating implementation.
            \item Example: Game-playing AI must coordinate value function approximation with reinforcement algorithms.
        \end{itemize}

        \item \textbf{Overfitting}
        \begin{itemize}
            \item Overfitting can occur when models are trained too closely to specific data, reducing generalizability.
            \item Example: A traffic control system trained on one city may fail in a different city with distinct traffic patterns.
        \end{itemize}
    \end{enumerate}
    \begin{block}{Key Points}
        ADP faces challenges primarily due to approximation, data demands, and complexity of real-world environments. Awareness of these is essential for robust ADP model design.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Approximate Dynamic Programming (ADP)}
    \begin{itemize}
        \item ADP is a framework for decision-making in complex environments.
        \item It approximates value functions or policies for effective planning.
        \item Applicable in various fields such as healthcare, robotics, finance, and more.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of ADP}
    \begin{enumerate}
        \item \textbf{Healthcare Management}
            \begin{itemize}
                \item Dynamic treatment regimes optimize patient outcomes.
                \item Example: Chronic disease management through learning from patient history.
            \end{itemize}
        \item \textbf{Robotics and Autonomous Systems}
            \begin{itemize}
                \item Real-time navigation in autonomous vehicles.
                \item ADP evaluates best routes using traffic data.
            \end{itemize}
        \item \textbf{Finance and Portfolio Management}
            \begin{itemize}
                \item ADP manages assets to optimize portfolio performance.
                \item Adjustments based on changing market dynamics.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{More Applications of ADP}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Energy Systems Management}
            \begin{itemize}
                \item Efficient management of power grids with renewables.
                \item Anticipates demand and balances supply.
            \end{itemize}
        \item \textbf{Manufacturing and Supply Chain Optimization}
            \begin{itemize}
                \item Optimizes inventory and production schedules.
                \item Reduces costs and meets demand variability.
            \end{itemize}
        \item \textbf{Telecommunications}
            \begin{itemize}
                \item Enhances network traffic management.
                \item ADP optimizes bandwidth allocation dynamically.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of ADP Applications}
    \begin{itemize}
        \item ADP is transforming industries with adaptive solutions.
        \item Key sectors include healthcare, robotics, finance, energy, manufacturing, and telecommunications.
        \item It provides robust solutions to complex decision-making problems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Recap}
    \begin{block}{Value Function Approximation Formula}
        \begin{equation}
            V(x) \approx \hat{V}(x; \theta)
        \end{equation}
        where \( V(x) \) is the true value function and \( \hat{V}(x; \theta) \) is the approximated value function using parameters \( \theta \).
    \end{block}
    
    \begin{block}{Key Consideration}
        ADP is beneficial in environments with large or continuous state spaces, making traditional dynamic programming infeasible.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for ADP}
    \begin{lstlisting}[language=Python]
import numpy as np

def approximate_value_iteration(states, actions, reward_function, transition_model, gamma, theta):
    value_function = np.zeros(len(states))
    while True:
        delta = 0
        for s in range(len(states)):
            v = value_function[s]
            value_function[s] = max(sum(transition_model[s, a] * 
                                          (reward_function[s, a] + gamma * value_function[ns]) 
                                          for ns in range(len(states))) 
                                   for a in actions)
            delta = max(delta, abs(v - value_function[s]))
        if delta < theta:
            break
    return value_function
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other RL Techniques - Introduction}
    \begin{block}{Introduction to ADP and Comparisons}
        Approximate Dynamic Programming (ADP) is a class of techniques in reinforcement learning (RL) that focuses on solving complex decision-making problems by approximating optimal value functions or policies. 
        ADP can be contrasted with other popular RL techniques such as Q-Learning and Policy Gradients.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other RL Techniques - Key Comparisons}
    \begin{enumerate}
        \item \textbf{Learning Approach}:
            \begin{itemize}
                \item \textbf{ADP}: Utilizes value function approximation to generalize learning across similar states. 
                \item \textbf{Q-Learning}: Updates action-value estimates as experiences are collected using the formula:
                    \begin{equation}
                    Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]
                    \end{equation}
                \item \textbf{Policy Gradients}: Directly optimizes the policy based on action performance with:
                    \begin{equation}
                    \nabla J(\theta) = \mathbb{E} [ \nabla \log \pi_{\theta}(s, a) \cdot R ]
                    \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other RL Techniques - Summary of Key Points}
    \begin{itemize}
        \item \textbf{ADP}: Utilizes approximations for efficient learning in large state spaces.
        \item \textbf{Q-Learning}: Focuses on action-value estimation with an off-policy approach.
        \item \textbf{Policy Gradients}: Optimizes the policy directly, excelling in complex action environments.
    \end{itemize}
    \begin{block}{Closing Note}
        These distinctions are crucial for selecting the appropriate RL technique based on the nature of the problem, the available data, and computational resources.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: ADP in Robotics}
    \begin{block}{Introduction to Approximate Dynamic Programming (ADP)}
        \begin{itemize}
            \item \textbf{Definition}: ADP is a framework for decision-making in uncertain environments, especially with large state spaces.
            \item \textbf{Purpose}: To approximate the value functions guiding optimal decision-making using policy iteration and value function approximation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Application of ADP in Robotics}
    Robotics demands efficient decision-making in complex environments. ADP addresses these challenges through:
    
    \begin{enumerate}
        \item \textbf{Path Planning}:
            \begin{itemize}
                \item \textbf{Example}: A mobile robot that learns optimal paths in a dynamic environment.
                \item \textbf{Key Concepts}:
                    \begin{itemize}
                        \item \textbf{State Representation}: Unique states for each position and orientation of the robot.
                        \item \textbf{Value Function}: Evaluates state usefulness based on expected future rewards.
                    \end{itemize}
            \end{itemize}

        \item \textbf{Control of Manipulators}:
            \begin{itemize}
                \item \textbf{Example}: A robotic arm that learns actions to complete assembly tasks.
                \item \textbf{Key Concepts}:
                    \begin{itemize}
                        \item \textbf{Policy Approximation}: Mapping states to actions for achieving desired behaviors.
                        \item \textbf{Temporal Difference Learning}: Adjusts action values based on predicted vs. actual rewards.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques and Conclusion}
    \begin{block}{Key Techniques in ADP for Robotics}
        \begin{itemize}
            \item \textbf{Function Approximation}: Generalizes learning to unseen states using neural networks.
            \item \textbf{Experience Replay}: Enhances learning efficiency by retaining and replaying past experiences.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Algorithm: DDPG}
        \begin{itemize}
            \item \textbf{Use Case}: Suitable for continuous action spaces, e.g., controlling a robotic arm.
            \item \textbf{Mechanism}:
                \begin{enumerate}
                    \item Actor-Critic Method: Actor learns the policy; Critic evaluates actions.
                    \item Learning Update: Policies are updated based on experiences from memory buffers.
                \end{enumerate}
        \end{itemize}
        
        \begin{lstlisting}[language=Python]
# Pseudo-code snippet for DDPG algorithm
for episode in range(num_episodes):
    state = environment.reset()
    while not done:
        action = actor_model(state) # Actor generates action
        next_state, reward, done = environment.step(action)
        memory.append((state, action, reward, next_state, done)) # Store experience
        # Update models here using mini-batches from `memory`
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Conclusion}
        \begin{itemize}
            \item ADP enhances autonomy and adaptability of robots in diverse environments.
            \item It allows robots to learn from experience and optimize efficiency and reliability in tasks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recent Developments in Approximate Dynamic Programming (ADP) - Overview}
    \begin{itemize}
        \item ADP has seen significant advancements in theory and application.
        \item Driven by:
        \begin{itemize}
            \item Increasing complexity of real-world problems.
            \item Enhanced computational resources.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recent Advancements in ADP - Key Points}
    \begin{enumerate}
        \item \textbf{Deep Learning Integration}
        \begin{itemize}
            \item Revolutionizes the approximation of value functions and policies.
            \item Example: Deep Q-Networks (DQN) in game and robotic environments.
        \end{itemize}

        \item \textbf{Improved Optimization Algorithms}
        \begin{itemize}
            \item New methods like policy gradient approaches enhance convergence.
            \item Example: Proximal Policy Optimization (PPO) for robust learning.
        \end{itemize}

        \item \textbf{Model-Based Approaches}
        \begin{itemize}
            \item Agents learn models of environments to improve decisions.
            \item Example: Monte Carlo Tree Search (MCTS) with neural networks in AlphaGo.
        \end{itemize}

        \item \textbf{Multi-Agent Systems}
        \begin{itemize}
            \item Focuses on interactions among agents for collective outcomes.
            \item Example: Cooperative reinforcement learning for traffic light optimization.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Research Trends and Example Formulation}
    \begin{block}{Key Research Trends}
        \begin{itemize}
            \item \textbf{Sample Efficiency}: Enhancing learning with fewer interactions.
            \item \textbf{Transfer Learning}: Utilizing knowledge across different tasks.
            \item \textbf{Explainable AI}: Ensuring model interpretability in sensitive fields.
        \end{itemize}
    \end{block}

    \begin{block}{Example Formulation}
        ADP methods solve Bellman's equation:
        \begin{equation}
        V(s) = \max_{a \in A} \left( R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V(s') \right)
        \end{equation}
        where:
        \begin{itemize}
            \item \( V(s) \): Value function at state \( s \)
            \item \( R(s, a) \): Reward for action \( a \) in state \( s \)
            \item \( \gamma \): Discount factor (0 < \( \gamma \) < 1)
            \item \( P(s' | s, a) \): Transition probability to state \( s' \) given \( s \) and \( a \)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in ADP Research - Overview}
    \begin{block}{Understanding Approximate Dynamic Programming (ADP)}
        Approximate Dynamic Programming (ADP) is a powerful framework for solving complex decision-making problems where traditional dynamic programming (DP) methods become computationally impractical.
    \end{block}
    \begin{block}{Key Research Directions}
        Numerous avenues for future research remain to enhance and evolve ADP techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in ADP Research - Key Research Directions}
    \begin{enumerate}
        \item \textbf{Integration of Machine Learning Techniques}
            \begin{itemize}
                \item Combining ADP with machine learning, especially deep learning.
                \item Example: Neural networks for high-dimensional value function approximation (Deep Reinforcement Learning). 
                \item Potential Impact: Improved accuracy and efficiency.
            \end{itemize}
            
        \item \textbf{Scalability and Efficiency Improvements}
            \begin{itemize}
                \item Developing scalable methods that maintain computational efficiency.
                \item Example: Distributed computing and parallel algorithms for large models.
                \item Potential Impact: Real-time decision-making in dynamic environments.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in ADP Research - Continued}
    \begin{enumerate}[resume]
        \item \textbf{Robustness and Uncertainty Handling}
            \begin{itemize}
                \item Addressing uncertainties in model parameters.
                \item Example: Stochastic ADP algorithms evaluating policies under uncertainty.
                \item Potential Impact: Reliable decision-making frameworks.
            \end{itemize}
            
        \item \textbf{Application-Specific Customization}
            \begin{itemize}
                \item Tailoring ADP methodologies for specific domains (e.g., healthcare).
                \item Example: Specialized algorithms for optimal treatment planning.
                \item Potential Impact: Enhanced applicability across diverse fields.
            \end{itemize}
            
        \item \textbf{Novel Approximation Methods}
            \begin{itemize}
                \item Researching new approximation strategies beyond linear functions.
                \item Example: Tile coding, radial basis functions, generative models.
                \item Potential Impact: Expanding theoretical and practical ADP toolkits.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in ADP Research - Summary}
    \begin{enumerate}
        \item \textbf{Transfer Learning in ADP}
            \begin{itemize}
                \item Investigating knowledge transfer between related tasks.
                \item Example: Initializing new models with previously learned policies.
                \item Potential Impact: Efficiency gains in rapidly adapting environments.
            \end{itemize}
            
        \item \textbf{Key Takeaways}
            \begin{itemize}
                \item ADP continues to evolve through innovative technologies.
                \item Promising future applications across several sectors.
                \item Addressing scalability, robustness, and customization is crucial.
            \end{itemize}
    \end{enumerate}
    By pursuing these directions, researchers can advance Approximate Dynamic Programming significantly.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 1}
    \begin{block}{Recap of Approximate Dynamic Programming (ADP)}
        Approximate Dynamic Programming (ADP) is a framework for solving complex decision-making problems where traditional dynamic programming is infeasible. Key concepts include:
    \end{block}
    
    \begin{enumerate}
        \item Fundamentals of ADP
        \item Key Components of ADP
        \item Learning Methods
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 2}
    \begin{block}{Fundamentals of ADP}
        \begin{itemize}
            \item **Dynamic Programming (DP):** Breaks problems into simpler subproblems, requiring knowledge of the entire state space.
            \item **Approximation Techniques:** Utilizes value function and policy approximations to manage larger state spaces.
        \end{itemize}
    \end{block}

    \begin{block}{Key Components}
        \begin{itemize}
            \item **Value Function Approximation (VFA):** Estimates values using a function rather than exact calculations.
            \item **Policy Approximation:** Maps states to actions to determine the best course of action.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 3}
    \begin{block}{Key Concepts Continued}
        \begin{itemize}
            \item **Learning Methods:**
                \begin{itemize}
                    \item **Temporal Difference Learning (TD)**: Updates value functions as new data becomes available.
                    \item **Reinforcement Learning (RL)**: Agents learn to maximize rewards through trial and error.
                \end{itemize}
            \item **Benefits of ADP:**
                \begin{itemize}
                    \item Scalability and flexibility across various domains.
                \end{itemize}
            \item **Challenges:**
                \begin{itemize}
                    \item Approximation errors and computational complexity.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Takeaway}
        Understanding approximation techniques and the exploration-exploitation trade-off is essential for effective ADP strategies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions}
    \begin{block}{Overview of Approximate Dynamic Programming (ADP)}
        Approximate Dynamic Programming (ADP) is an essential approach in reinforcement learning and decision-making under uncertainty. It aims to solve complex dynamic programming problems that are computationally prohibitive due to the size of the state and action spaces. By using approximation techniques, ADP allows decision-makers to derive viable strategies in real-time applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions - Part 1}
    \begin{enumerate}
        \item \textbf{What are the key differences between Exact Dynamic Programming and Approximate Dynamic Programming?}
        \begin{itemize}
            \item Consider how computation time and storage requirements differ between the two methods.
            \item \textbf{Key Point:} Exact methods yield precise solutions but are infeasible for larger problems, whereas ADP balances accuracy and efficiency.
        \end{itemize}

        \item \textbf{In what contexts do you think Approximate Dynamic Programming is most beneficial?}
        \begin{itemize}
            \item Think about industries such as finance, robotics, or healthcare where decision-making is critical.
            \item \textbf{Example:} In robotics, ADP helps navigate environments with incomplete information.
        \end{itemize}

        \item \textbf{How does function approximation play a role in ADP?}
        \begin{itemize}
            \item Discuss methods like linear approximation, neural networks, or polynomial approximations used to represent value functions.
            \item \textbf{Key Point:} The choice of function approximation impacts performance and convergence rates.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Can you provide an example of a real-world problem where ADP has been successfully applied?}
        \begin{itemize}
            \item Consider supply chain management, where demand forecasting and inventory control are complex and dynamic.
            \item \textbf{Illustration:} ADP has been used to optimize inventory levels by approximating future demand and minimizing costs.
        \end{itemize}

        \item \textbf{What are some challenges and limitations associated with implementing ADP?}
        \begin{itemize}
            \item Reflect on issues like convergence, stability, and the tuning of approximation methods.
            \item \textbf{Key Point:} Understanding these limitations is crucial for practical applications.
        \end{itemize}

        \item \textbf{How can reinforcement learning techniques, such as Q-learning or Policy Gradient methods, integrate with ADP frameworks?}
        \begin{itemize}
            \item Explore the relationship between traditional reinforcement learning methods and the approximate approaches of ADP.
            \item \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
                \end{equation}
            \item \textbf{Key Insight:} ADP can enhance these methods by providing approximations when the state space is large.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Engagement is essential:} These questions encourage students to reflect on their understanding of ADP and relate it to practical applications.
        \item \textbf{Real-world relevance:} Discussing concrete examples helps bridge theory with practice.
        \item \textbf{Critical thinking:} Each question aims to prompt students to think critically about the implications, challenges, and future prospects of ADP.
    \end{itemize}
    \textit{Feel free to explore these questions in depth during class discussions, as they will help solidify your understanding of Approximate Dynamic Programming and its applications.}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Overview}
    \begin{block}{Recommended Books}
        \begin{enumerate}
            \item **"Reinforcement Learning: An Introduction" by Richard S. Sutton \& Andrew G. Barto**
                \begin{itemize}
                    \item Overview: Foundational text introducing key concepts and methods in reinforcement learning, including Approximate Dynamic Programming (ADP).
                    \item Key Concepts: Dynamic Programming, Temporal Difference Learning, Policy Gradient Methods.
                \end{itemize}
            \item **"Dynamic Programming and Optimal Control" by Dimitri P. Bertsekas**
                \begin{itemize}
                    \item Overview: Comprehensive examination of dynamic programming principles, focusing on applications and algorithms.
                    \item Key Concepts: Discounted and average cost problems, Markov Decision Processes, ADP techniques.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Research Papers}
    \begin{block}{Research Papers}
        \begin{enumerate}
            \item **"A Survey of Approximate Dynamic Programming" by John D. Gilmore et al.**
                \begin{itemize}
                    \item Overview: Reviews various methods and innovations in ADP, covering algorithms and computational techniques.
                    \item Takeaway: Outlines theoretical advancements and practical applications in complex environments.
                \end{itemize}
            \item **"Approximate Dynamic Programming: Solving the Curses of Dimensionality" by Warren B. Powell**
                \begin{itemize}
                    \item Overview: Discusses scaling ADP methods to high-dimensional problems, addressing practical challenges.
                    \item Key Insights: Strategies for tackling the 'curse of dimensionality' in real-world applications.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Online Courses and Software}
    \begin{block}{Online Courses and Lectures}
        \begin{enumerate}
            \item **Coursera: Reinforcement Learning Specialization**
                \begin{itemize}
                    \item Provider: University of Alberta
                    \item Link: \texttt{https://www.coursera.org/specializations/reinforcement-learning}
                    \item Description: Covers topics relevant to ADP, with hands-on programming assignments.
                \end{itemize}
            \item **edX: Fundamentals of Reinforcement Learning**
                \begin{itemize}
                    \item Provider: MIT
                    \item Link: \texttt{https://www.edx.org/course/fundamentals-of-reinforcement-learning}
                    \item Description: Self-paced course with modules covering dynamic programming and its approximations.
                \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Software and Libraries}
        \begin{enumerate}
            \item **OpenAI Gym**
                \begin{itemize}
                    \item Overview: Toolkit for developing and comparing reinforcement learning algorithms.
                    \item Example Usage:
                    \begin{lstlisting}[language=python]
import gym
env = gym.make("CartPole-v1")
state = env.reset()
                    \end{lstlisting}
                \end{itemize}
            \item **TensorFlow and PyTorch**
                \begin{itemize}
                    \item Overview: Leading machine learning frameworks for building models used with ADP techniques.
                    \item Key Features: Efficient computation, high-level APIs for model building.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}


\end{document}