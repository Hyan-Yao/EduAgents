\frametitle{Bellman Equation Formulation - State-Value Function}
  \textbf{1. State-Value Function (\( V(s) \))}:

  The state-value function, denoted as \( V(s) \), represents the expected value of being in state \( s \) and following a particular policy \( \pi \).

  \begin{block}{Bellman Equation for State-Value Function}
  \begin{equation}
  V(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')]
  \end{equation}
  \end{block}

  \begin{itemize}
    \item \( \mathcal{A} \): Action space
    \item \( \pi(a|s) \): Probability of taking action \( a \) in state \( s \)
    \item \( p(s', r | s, a) \): Transition probability of reaching state \( s' \) and receiving reward \( r \) after taking action \( a \) in state \( s \)
    \item \( \gamma \): Discount factor (0 â‰¤ \( \gamma \) < 1), representing the importance of future rewards
  \end{itemize}
