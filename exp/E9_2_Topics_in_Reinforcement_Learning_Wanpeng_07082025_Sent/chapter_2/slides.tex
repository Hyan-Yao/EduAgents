\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes (MDPs)}
    \begin{block}{What are MDPs?}
        Markov Decision Processes (MDPs) are mathematical frameworks used to model decision-making situations where outcomes are partly random and partly under the control of a decision-maker. In Reinforcement Learning (RL), they provide the foundational structure for agents to learn optimal behaviors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs}
    \begin{itemize}
        \item \textbf{States (S)}: All possible situations the agent can be in. 
        \item \textbf{Actions (A)}: All possible actions the agent can take.
        \item \textbf{Transition Probabilities (P)}: The probability of moving from one state to another given a specific action. 
        $$ P(s'|s, a) = \text{probability of reaching state } s' \text{ from } s \text{ after action } a $$
        \item \textbf{Rewards (R)}: Immediate payoff received after transitioning between states. 
        $$ R(s, a, s') = \text{reward for taking action } a \text{ in state } s \text{ resulting in state } s' $$
        \item \textbf{Discount Factor ($\gamma$)}: A value between 0 and 1 indicating the importance of future rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of MDPs in Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Structured Learning}: MDPs provide a way to model environments for agents to learn optimal policies through trial and error.
        \item \textbf{Policy Definition}: Agents can adopt deterministic or stochastic policies to decide actions at any time.
        \item \textbf{Value Functions}: MDPs enable the definition of value functions estimating how good it is to be in a particular state or perform an action.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Consider an autonomous robot navigating a maze (our MDP):
    \begin{itemize}
        \item \textbf{States (S)}: Robot positions in the maze (e.g., cell (1, 1), (1, 2), etc.)
        \item \textbf{Actions (A)}: Move Up, Down, Left, Right
        \item \textbf{Transition Probabilities (P)}: 
              \begin{itemize}
                  \item 0.8 probability to move as intended 
                  \item 0.2 probability of slipping
              \end{itemize}
        \item \textbf{Rewards (R)}: 
              \begin{itemize}
                  \item +10 for reaching the goal
                  \item -1 for each step taken
                  \item -5 for hitting a wall
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Markov Decision Processes are essential in Reinforcement Learning as they lay the groundwork for optimal decision-making in uncertain environments. Understanding MDPs is crucial for developing intelligent agents capable of learning from their interactions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are MDPs?}
    \begin{block}{Definition of Markov Decision Processes (MDPs)}
        A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. 
        MDPs describe the dynamics of environments in which an agent makes decisions sequentially over time, considering the current state and possible future states.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs}
    \begin{enumerate}
        \item \textbf{States (S)}: A set of states \( S \) representing all possible situations for the agent.
        \item \textbf{Actions (A)}: A set of actions \( A \) that the agent can take, influencing the state of the environment.
        \item \textbf{Transition Model (T)}: A probability distribution \( T(s'|s, a) \) determining the likelihood of moving to state \( s' \) given the current state \( s \) and action \( a \).
        \item \textbf{Rewards (R)}: A reward function \( R(s, a) \) assigning a numerical value based on the state \( s \) and action \( a \) taken.
        \item \textbf{Policy (π)}: A policy \( \pi(a|s) \) defines the agent's behavior as a mapping from states to actions.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example to Illustrate MDPs}
    \begin{block}{Scenario: Robotic Vacuum Cleaner}
        \begin{itemize}
            \item \textbf{States (S)}: Cleaning rooms (clean, dirty).
            \item \textbf{Actions (A)}: Move forward, turn left, turn right, clean.
            \item \textbf{Transition Model (T)}: Probability \( T(\text{clean}| \text{dirty}, \text{clean}) = 0.8 \).
            \item \textbf{Rewards (R)}: +10 for cleaning a room successfully, -1 for each move.
            \item \textbf{Policy (π)}: The robot learns to maximize its rewards by deciding which action to take in each state.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item MDPs capture the dynamics of decision-making environments effectively.
        \item The Markov property ensures that the future state depends only on the current state and action, not on past states.
        \item MDPs are foundational to understanding complex frameworks like Reinforcement Learning.
    \end{itemize}
    
    \begin{block}{Mathematical Notation}
        Transition Probabilities: \( T(s'|s,a) \) \\
        Rewards: \( R(s,a) \) \\
        Policy: \( \pi(s) = P(a|s) \)
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding MDPs is crucial for designing intelligent systems that make decisions in uncertain environments. 
        In the next slide, we will explore the individual components that form the backbone of MDPs, enhancing your understanding of this vital concept in Reinforcement Learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - Introduction}
    Markov Decision Processes (MDPs) are a mathematical framework for modeling decision-making situations where outcomes are uncertain. 
    An MDP is defined by four key components:
    
    \begin{itemize}
        \item \textbf{States}
        \item \textbf{Actions}
        \item \textbf{Rewards}
        \item \textbf{Transition Models}
    \end{itemize}
    
    Understanding these components is crucial for devising effective strategies in both artificial intelligence and operational research.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - States and Actions}
    
    \textbf{1. States}
    \begin{itemize}
        \item \textbf{Definition:} A state represents a specific configuration or situation of a system at a given time.
        \item \textbf{Key Point:} The set of all possible states is referred to as the \textbf{state space} (S).
    \end{itemize}
    
    \begin{block}{Example}
        In a chess game, each arrangement of pieces on the board represents a state. The state space encompasses all possible configurations throughout the game.
    \end{block}
    
    \textbf{2. Actions}
    \begin{itemize}
        \item \textbf{Definition:} Actions are the choices available to an agent that can affect the state of the system.
        \item \textbf{Key Point:} The set of actions available in each state is called the \textbf{action space} (A).
    \end{itemize}
    
    \begin{block}{Example}
        In a chess game, possible actions could include moving a pawn, capturing a piece, or castling, depending on the current state of the board.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - Rewards and Transition Models}
    
    \textbf{3. Rewards}
    \begin{itemize}
        \item \textbf{Definition:} A reward is a numerical value received after taking an action in a specific state, used to measure the desirability of that action.
        \item \textbf{Key Point:} The reward function indicates the immediate benefit and is denoted as \textbf{R(s, a)}, where s is the state and a is the action.
    \end{itemize}
    
    \begin{block}{Example}
        In a reinforcement learning scenario, a reward could be +10 for winning the game, -10 for losing, or +1 for making a move that gains positional advantage.
    \end{block}

    \textbf{4. Transition Models}
    \begin{itemize}
        \item \textbf{Definition:} The transition model defines the probability of moving from one state to another after taking an action, denoted as \textbf{P(s'|s, a)}.
        \item \textbf{Key Point:} Transition models capture the uncertainty inherent in actions and outcomes.
    \end{itemize}
    
    \begin{block}{Example}
        In a robotic navigation scenario, if a robot attempts to move forward, there's a probability that it will end up in a different location (s') due to obstacles.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of MDP Components}

    To summarize the four components of MDPs:
    \begin{itemize}
        \item \textbf{States (S):} Possible configurations of the environment.
        \item \textbf{Actions (A):} Available choices for the agent.
        \item \textbf{Rewards (R):} Feedback signaling the value of actions taken.
        \item \textbf{Transition Models (P):} Probabilities that represent the outcome of actions in states.
    \end{itemize}

    Understanding these components is essential for modeling and solving MDPs effectively!
    
    \textbf{Mathematical Representation:}
    \begin{itemize}
        \item \textbf{Transition Function:} 
        \begin{equation}
            P(s'|s,a) 
        \end{equation}
        Probability of moving to state s' from state s, given action a.
        
        \item \textbf{Reward Function:} 
        \begin{equation}
            R(s,a) 
        \end{equation}
        Immediate reward received after taking action a in state s.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States and Actions - Understanding State Space and Action Space}
    
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{States (S)}:
            \begin{itemize}
                \item Definition: A specific configuration or situation of the environment.
                \item Notation: Denoted as \( S \), containing states \( s_1, s_2, \ldots, s_n \).
                \item Example: In a grid world, positions like (0,0), (0,1) are states.
            \end{itemize}
            
            \item \textbf{Actions (A)}:
            \begin{itemize}
                \item Definition: The choices available to an agent defining state interactions.
                \item Notation: Denoted as \( A \), consisting of actions \( a_1, a_2, \ldots, a_m \).
                \item Example: In the grid world, actions include Up, Down, Left, Right.
            \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Relationship}
        The behavior of an agent in an MDP is defined by the state-action pairs \( (S, A) \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States and Actions - Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Deterministic vs. Stochastic Environments}:
        \begin{itemize}
            \item Deterministic: Actions lead to specific next states.
            \item Stochastic: Actions can lead to different next states with probabilities.
        \end{itemize}
        
        \item \textbf{State Representation}:
        The complexity of solving the MDP is influenced by how states are represented (discrete vs. continuous).
        
        \item \textbf{Action Selection}:
        The choice of action, influenced by the agent's policy, is crucial for optimal decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States and Actions - Transition Diagram & Formulas}
    
    \begin{block}{Transition Probability}
        If \( T(s, a, s') \) denotes the probability of moving to state \( s' \) by taking action \( a \) in state \( s \), it is defined as:
        \begin{equation}
            T(s, a, s') = P(s' \mid s, a)
        \end{equation}
    \end{block}
    
    \begin{block}{Sample Python Pseudocode}
        \begin{lstlisting}[language=Python]
# Define the State and Action spaces
states = ['s1', 's2', 's3']
actions = ['a1', 'a2']

# Function to apply an action
def transition(state, action):
    if state == 's1' and action == 'a1':
        return 's2'  # deterministic transition
    elif state == 's1' and action == 'a2':
        return 's1'  # stays in the same state

# Example usage
current_state = 's1'
next_state = transition(current_state, 'a1')
print(f"Transitioning from {current_state} to {next_state}")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Understanding Reward Signals}
    In Markov Decision Processes (MDPs), rewards are crucial signals that inform the agent about the desirability of its actions. They quantify how favorable an outcome is for the agent—essentially guiding its learning and decision-making processes.

    \begin{block}{Key Concepts}
        \begin{itemize}
            \item **Definition of Reward**: A \textbf{reward} $r(s, a)$ is a scalar value received by the agent after taking action $a$ in state $s$. Rewards provide immediate feedback on the action’s effectiveness.
            \item **Types of Rewards**:
                \begin{itemize}
                    \item Immediate Reward: Received right after executing an action.
                    \item Cumulative Reward: Total reward over time, evaluated using the concept of return, with future rewards discounted by a factor $\gamma$ (0 < $\gamma$ < 1).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Importance and Illustration}
    \begin{block}{Importance of Rewards}
        \begin{itemize}
            \item Drive the agent towards optimal behavior by reinforcing desirable actions leading to positive outcomes.
            \item The agent learns to maximize its expected cumulative reward over time.
        \end{itemize}
    \end{block}

    \begin{block}{Illustration of Reward Importance}
        Imagine a simple environment where a robot navigates a grid:
        \begin{itemize}
            \item Moves towards a goal: Receives a positive reward (+10).
            \item Moves into a wall: Incurs a penalty (-5).
            \item The robot learns the best path to the goal by favoring actions leading to higher cumulative rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Mathematical Representation}
    \begin{block}{Mathematical Representation}
        The expected return, $R_t$, from time $t$ is formulated as:
        \begin{equation}
            R_t = r(s_t, a_t) + \gamma r(s_{t+1}, a_{t+1}) + \gamma^2 r(s_{t+2}, a_{t+2}) + \ldots
        \end{equation}
        This formula emphasizes the importance of immediate rewards while still considering longer-term goals through discounting.
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Feedback Mechanism: Rewards serve as feedback that drives the learning process.
            \item Action Selection: Agents utilize rewards to evaluate and select actions yielding the best outcomes.
            \item Optimal Policy: An optimal policy derives from maximization of expected cumulative rewards, shaping the agent's decisions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Model - Introduction}
    \begin{block}{Introduction to Transition Probabilities}
        In Markov Decision Processes (MDPs), a transition model defines the dynamics of how a system changes from one state to another in response to an action taken by an agent. This model is critical because it captures the uncertainties inherent in the environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Model - Key Concepts}
    \begin{itemize}
        \item \textbf{States (S)}: Configurations or situations an agent can find itself in (e.g., locations in a grid).
        \item \textbf{Actions (A)}: Choices available to the agent that impact state transitions (e.g., move north).
        \item \textbf{Transition Probability (P)}: The likelihood of moving from one state to another after an action is taken. Formally represented as:
        \[
        P(s' | s, a)
        \]
        where:
        \begin{itemize}
            \item \( s \): current state,
            \item \( a \): action taken,
            \item \( s' \): next state.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Model - Dynamics and Example}
    \textbf{Understanding Transition Dynamics:}
    \begin{enumerate}
        \item \textbf{Deterministic vs Probabilistic Transitions:}
        \begin{itemize}
            \item \textbf{Deterministic}: Action leads to a specific state.
            \item \textbf{Probabilistic}: Action results in a probability distribution over possible next states.
        \end{itemize}
        
        \item \textbf{Example:} 
        \begin{itemize}
            \item Robot navigating a grid:
            \begin{itemize}
                \item Action "move right" from state \( S_1 \):
                \[
                P(S_2 | S_1, \text{{move right}}) = 0.8
                \]
                \[
                P(S_1 | S_1, \text{{move right}}) = 0.1
                \]
                \[
                P(S_4 | S_1, \text{{move right}}) = 0.1
                \]
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Model - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The transition model is foundational for predicting future states and planning paths.
            \item Understanding probabilities helps maximize expected future rewards.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        In summary, the transition model is critical for modeling the agent-environment interaction, providing the probabilistic backbone for decision-making in MDPs. 
        Recognizing state change dynamics allows for strategic planning and rewards maximization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discount Factor (\( \gamma \))}
    % Introduction to the discount factor and its significance in MDPs.
    \begin{block}{Understanding the Discount Factor}
        The discount factor, denoted as \( \gamma \), is crucial in Markov Decision Processes (MDPs) for evaluating future rewards versus immediate rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definition and Significance of \( \gamma \)}
    % Provide the definition and significance of the discount factor.
    \begin{itemize}
        \item \textbf{Discount Factor \( \gamma \)}: A scalar value in the range [0, 1] that discounts future rewards.
        \item \textbf{Significance of \( \gamma \)}:
        \begin{enumerate}
            \item Future rewards are valued less than immediate rewards.
            \item Higher \( \gamma \) indicates a preference for future rewards.
            \item \( \gamma < 1 \) allows for convergence of reward summation.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Formula}
    % Summary of key points about \( \gamma \) and formula presentation.
    \begin{itemize}
        \item \( \gamma = 0 \): The agent focuses solely on immediate rewards.
        \item \( \gamma = 1 \): Future rewards are equally valued as immediate rewards.
        \item \( 0 < \gamma < 1 \): A balance between immediate and future rewards.
    \end{itemize}
    
    \begin{block}{Formula}
        In MDPs, the total expected reward \( G_t \) starting from timestep \( t \) is expressed as:
        \begin{equation}
            G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \ldots
        \end{equation}
        Where:
        \begin{itemize}
            \item \( R_t \) = Reward at time \( t \)
            \item \( R_{t+1} \) = Reward at time \( t+1 \)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation of MDPs - Key Concepts}
    A Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making situations where outcomes are partly random and partly under the control of a decision maker. The formal structure of an MDP is defined by the following components:

    \begin{enumerate}
        \item \textbf{States (S)}: A finite set of states, \( S = \{s_1, s_2, \ldots, s_n\} \). Represents all possible situations the agent can find itself in.
        
        \item \textbf{Actions (A)}: A set of actions available to the agent, \( A = \{a_1, a_2, \ldots, a_m\} \). Each action leads to potential changes of states.

        \item \textbf{Transition Probability (P)}: A transition function \( P(s'|s, a) \) indicates the probability of moving to state \( s' \) given current state \( s \) and action \( a \). Formally, \( P: S \times A \times S \rightarrow [0, 1] \).

        \item \textbf{Rewards (R)}: A reward function \( R(s, a) \) provides the expected immediate reward received after taking action \( a \) in state \( s \). It can be represented as \( R: S \times A \rightarrow \mathbb{R} \).

        \item \textbf{Discount Factor ($\gamma$)}: A value \( 0 \leq \gamma < 1 \) that determines the present value of future rewards. It helps in modeling the trade-off between immediate and future rewards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation of MDPs - Representation}
    The mathematical formulation of an MDP can be succinctly summarized as:

    \begin{block}{MDP Representation}
        An MDP is represented as a tuple \( (S, A, P, R, \gamma) \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation of MDPs - Example}
    \textbf{Example of an MDP}:
    
    Consider a simple robotic navigation scenario:
    
    \begin{itemize}
        \item \textbf{States}: \( S = \{ \text{Home}, \text{Store}, \text{Park} \} \)
        
        \item \textbf{Actions}: \( A = \{ \text{Walk}, \text{Drive} \} \)

        \item \textbf{Transitions}: If at Home and action is “Drive,” \( P \) might yield:
        \begin{itemize}
            \item \( P(\text{Store} | \text{Home}, \text{Drive}) = 0.8 \)
            \item \( P(\text{Park} | \text{Home}, \text{Drive}) = 0.2 \)
        \end{itemize}

        \item \textbf{Rewards}: \( R(\text{Home}, \text{Drive}) = 10 \)

        \item \textbf{Discount Factor}: \( \gamma = 0.9 \)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimal Policy - Definition}
    \begin{block}{Definition of Optimal Policy}
        In the context of Markov Decision Processes (MDPs), an \textbf{optimal policy} is a strategy or rule that specifies the best action to take in each state to maximize the cumulative reward over time. 
        Mathematically, an optimal policy $\pi^*$ yields the highest expected value of rewards when following the policy from any given state.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimal Policy - Key Concepts}
    \begin{itemize}
        \item \textbf{Policy ($\pi$)}: A mapping from states (S) to actions (A), which can be deterministic or stochastic.
        \item \textbf{Reward (R)}: A numerical value received after taking an action in a state, aimed to maximize total expected rewards.
        \item \textbf{Value Function (V)}: Represents the expected cumulative reward obtainable from a state under a certain policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimal Policy - Formulation}
    An optimal policy can be determined using the value function equations. For a given state \( s \), the formulation is:

    \begin{equation}
        \pi^*(s) = \arg\max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]
    \end{equation}
    
    Where:
    \begin{itemize}
        \item \( \gamma \) is the discount factor, indicating the value of future rewards.
        \item \( P(s'|s,a) \) is the transition probability of moving to state \( s' \) by taking action \( a \) in state \( s \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimal Policy - Examples and Key Points}
    \begin{block}{Example}
        In a navigation scenario, an agent can:
        \begin{itemize}
            \item **States**: A (Start), B, C (Goal)
            \item **Actions**: Move Right, Move Left
            \item **Rewards**: +10 for reaching C, -1 for each move.
        \end{itemize}
        The optimal policy would recommend **"Move Right"** until reaching C.
    \end{block}
    
    \begin{itemize}
        \item An optimal policy maximizes long-term rewards while considering immediate and future gains.
        \item Finding optimal policies is crucial for effective decision-making in uncertain environments.
        \item The Bellman Equation is central to identifying optimal policies.
        \item Optimal policies can vary based on environmental dynamics and reward structures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimal Policy - Conclusion and Next Steps}
    \begin{block}{Conclusion}
        An optimal policy is crucial for decision-making in MDPs, guiding actions for the best long-term outcome. Understanding how to derive these policies through value functions equips us to tackle complex problems in various domains.
    \end{block}

    \begin{block}{Next Steps}
        Explore the relationship between optimal policies and value functions, as discussed in the next slide on \textbf{Value Functions}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions}
    \begin{block}{Introduction to Value Functions in MDPs}
        In Markov Decision Processes (MDPs), value functions help evaluate how effective policies are. They quantify the expected long-term outcomes of states and actions, which aids in decision-making under uncertainty.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Value Functions}
    \begin{enumerate}
        \item \textbf{State-Value Function (V)}:
        \begin{itemize}
            \item \textbf{Definition}: Represents the expected return from state \( s \) while following policy \( \pi \).
            \item \textbf{Mathematical Representation}:
            \begin{equation}
                V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0=s \right]
            \end{equation}
            Where \( R_t \) is the reward at time \( t \) and \( \gamma \) (0 ≤ \( \gamma \) < 1) is the discount factor.
        \end{itemize}
        
        \item \textbf{Action-Value Function (Q)}:
        \begin{itemize}
            \item \textbf{Definition}: The expected return from state \( s \) after taking action \( a \) and following policy \( \pi \).
            \item \textbf{Mathematical Representation}:
            \begin{equation}
                Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0=s, A_0=a \right]
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Purpose of Value Functions}: Evaluate and improve policies by estimating long-term rewards.
            \item \textbf{Relation to Policy Evaluation}: Both \( V(s) \) and \( Q(s, a) \) indicate which states and actions yield favorable outcomes.
            \item \textbf{Optimization}: Seek an optimal policy for maximizing expected returns using methods such as dynamic programming.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Consider a grid-world MDP where an agent can move and gain rewards:
        \begin{itemize}
            \item If in state \( s_1 \) and moving up to \( s_2 \) with +10 reward:
            \begin{itemize}
                \item \( V(s_1) \) considers rewards following the optimal policy.
                \item \( Q(s_1, \text{up}) \) includes immediate reward (+10) and expected rewards from \( s_2 \).
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding value functions is crucial for grasping MDPs. They are vital for evaluating policies and making informed decisions, enabling the development of more complex algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Slide 11: Bellman Equations}
    Bellman equations provide a recursive decomposition of value functions in a Markov Decision Process (MDP). They link the value of a given state (or action) to the values of subsequent states (or actions) reachable from it.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Bellman Equations in MDPs}
    \begin{itemize}
        \item \textbf{Dynamic Programming:} Essential for applying dynamic programming techniques in solving MDPs.
        \item \textbf{Optimal Value Function:} Helps derive the optimal value function $V^*(s)$ using expected rewards from actions and state transitions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Bellman Equations}

    \begin{block}{State-Value Function}
    \begin{equation}
    V(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a) \left[R(s,a,s') + \gamma V(s')\right]
    \end{equation}
    \end{block}
    
    \begin{block}{Action-Value Function}
    \begin{equation}
    Q(s,a) = \sum_{s' \in S} P(s'|s,a) \left[R(s,a,s') + \gamma \sum_{a' \in A} \pi(a'|s') Q(s',a')\right]
    \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Simple MDP}
    Consider a grid world where an agent moves to reach a goal:
    \begin{itemize}
        \item \textbf{States:} Each cell in the grid.
        \item \textbf{Actions:} Move up, down, left, right.
        \item \textbf{Rewards:} +10 for reaching the goal, -1 for each step taken.
    \end{itemize}
    
    \textbf{Applying Bellman Equation:} The value $V(i,j)$ of the agent in cell $(i,j)$ can be computed by averaging possible future states and corresponding rewards.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item \textbf{Recursive Nature:} Bellman equations simplify problems into smaller subproblems.
        \item \textbf{Foundation for Algorithms:} Essential for reinforcement learning and operations research algorithms.
        \item \textbf{Dynamic Programming Techniques:} Core mechanism for solving MDPs through iterative updates.
    \end{itemize}
    
    \textbf{Summary:} The Bellman equations encapsulate the strategy for determining the best actions in MDPs, linking current decisions with future outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming in MDPs - Overview}
    \begin{block}{Dynamic Programming (DP)}
        DP is a method for solving complex problems by breaking them down into simpler subproblems. 
        It is utilized in MDPs to compute policies that specify the best action in each state to maximize expected rewards over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming in MDPs - Fundamental Concepts}
    \begin{itemize}
        \item \textbf{States (S)}: All possible situations for the agent.
        \item \textbf{Actions (A)}: Moves the agent can take.
        \item \textbf{Transition Model (T)}: Probability of moving between states based on actions.
        \item \textbf{Reward (R)}: Feedback signal for actions indicating immediate benefits.
        \item \textbf{Discount Factor ($\gamma$)}: Represents the present value of future rewards (0 < $\gamma$ < 1).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming Steps in MDPs}
    \begin{enumerate}
        \item \textbf{Policy Evaluation}
            \begin{itemize}
                \item Determine value function $V^\pi(s)$ for policy $\pi$.
                \item Bellman Equation: 
                \[
                V^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'} T(s, \pi(s), s') V^\pi(s')
                \]
            \end{itemize}

        \item \textbf{Policy Improvement}
            \begin{itemize}
                \item Update policy using the current value function:
                \[
                \pi'(s) = \arg\max_a \left( R(s, a) + \gamma \sum_{s'} T(s, a, s') V(s') \right)
                \]
            \end{itemize}

        \item \textbf{Policy Iteration}
        \item \textbf{Value Iteration}
            \begin{itemize}
                \item Combined update for policy and value function:
                \[
                V_{k+1}(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} T(s, a, s') V_k(s') \right)
                \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming in MDPs - Key Points and Example}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Optimal Policy}: Maximizes expected cumulative rewards over time.
            \item \textbf{Efficient Computation}: Reuse of calculated values reduces overhead.
            \item \textbf{Convergence}: Both policy and value iterations guarantee convergence to the optimal solution.
        \end{itemize}
    \end{block}

    \begin{block}{Example Illustration}
        \begin{itemize}
            \item A grid navigation scenario as an MDP:
                \begin{itemize}
                    \item States: Each cell in a grid.
                    \item Actions: Move up, down, left, right.
                    \item Utilize DP to evaluate and improve policies to reach a target cell maximizing rewards.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming in MDPs - Conclusion}
    \begin{block}{Conclusion}
        Dynamic programming is essential for solving MDPs effectively, enabling optimal policy computation through systematic evaluation and improvement of value functions. Its iterative and reusable approach adds significant value to decision-making under uncertainty.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Introduction}
    \begin{block}{Introduction to MDPs}
        Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs are widely applicable across various fields due to their effectiveness in optimizing sequential decision problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Key Applications}
    \begin{enumerate}
        \item \textbf{Robotics:}
            \begin{itemize}
                \item \textbf{Example:} Robot Navigation
                \item MDPs help robots determine optimal paths in uncertain environments by evaluating different actions based on state probabilities.
                \item \textbf{Key Point:} MDPs enable adaptive navigation in changing environments.
            \end{itemize}

        \item \textbf{Finance:}
            \begin{itemize}
                \item \textbf{Example:} Portfolio Management
                \item Investors use MDPs for decisions like buying, holding, or selling assets to maximize returns.
                \item \textbf{Key Point:} MDPs incorporate market uncertainty and enhance strategy robustness.
            \end{itemize}

        \item \textbf{Healthcare:}
            \begin{itemize}
                \item \textbf{Example:} Treatment Planning
                \item MDPs model patient states and interventions to optimize outcomes.
                \item \textbf{Key Point:} Facilitate personalized treatment based on patient responses.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Continued Applications}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Operations Research:}
            \begin{itemize}
                \item \textbf{Example:} Inventory Control
                \item MDPs optimize inventory management by modeling stock levels and reordering decisions.
                \item \textbf{Key Point:} Balance costs and service levels through informed decision-making.
            \end{itemize}

        \item \textbf{Artificial Intelligence:}
            \begin{itemize}
                \item \textbf{Example:} Game Playing
                \item MDPs form the foundation for AI algorithms in strategic games.
                \item \textbf{Key Point:} AI can learn strategies through simulation and reinforcement learning.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Mathematical Foundation}
    \begin{block}{Mathematical Components}
        \begin{itemize}
            \item \textbf{States (S):} All possible configurations of the system.
            \item \textbf{Actions (A):} Choices available to the decision-maker.
            \item \textbf{Transition Probability (P):} Probability of moving from one state to another after taking an action.
            \item \textbf{Reward (R):} Immediate feedback from the environment based on actions taken.
        \end{itemize}
    \end{block}

    \begin{block}{Value Function}
        The expected long-term reward from state \( s \) is given by:
        \begin{equation}
            V(s) = \max_a \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V(s')]
        \end{equation}
        where \( \gamma \) is the discount factor (0 < \( \gamma \) < 1), influencing the present value of future rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Conclusion}
    \begin{block}{Conclusion}
        Markov Decision Processes are powerful tools that translate decision-making problems in various domains into a structured form. 
        By understanding and applying MDPs, professionals can enhance efficiency and efficacy across industries.
    \end{block}
    
    Next, we will explore the challenges and limitations associated with MDPs, shedding light on their practical constraints.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations of MDPs - Overview}
    \begin{itemize}
        \item Markov Decision Processes (MDPs) are key in reinforcement learning.
        \item Despite their power, MDPs face significant challenges that affect their applicability:
        \begin{itemize}
            \item Scalability
            \item Curse of dimensionality
            \item Assumption of the Markov property
            \item Modeling uncertainty
            \item Reward function specification
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of MDPs - Scalability and Curse of Dimensionality}
    \begin{enumerate}
        \item \textbf{Scalability Issues}:
        \begin{itemize}
            \item Large state and action spaces lead to increased computational requirements.
            \item \textit{Example}: In robotics, large numbers of states/actions complicate optimal policy computation.
            \item Dynamic Programming methods may become infeasible for large spaces.
        \end{itemize}
        
        \item \textbf{Curse of Dimensionality}:
        \begin{itemize}
            \item Exponential growth of the state space with additional dimensions complicates value function estimation.
            \item \textit{Example}: A grid world becomes a 3D problem by adding time as a feature, escalating complexity.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of MDPs - Markov Property, Uncertainty, and Reward Functions}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Assumption of Markov Property}:
        \begin{itemize}
            \item MDPs presume future states depend only on the current state and action.
            \item \textit{Limitation}: Real-world scenarios often exhibit dependencies on past states.
            \item \textit{Example}: In finance, past trends affect stock prices, violating the Markov assumption.
        \end{itemize}
        
        \item \textbf{Modeling Uncertainty}:
        \begin{itemize}
            \item Stochastic environments complicate transition probabilities and reward modeling.
            \item \textit{Example}: In healthcare, varying treatment outcomes among patients necessitate complex estimations.
        \end{itemize}
        
        \item \textbf{Reward Function Specification}:
        \begin{itemize}
            \item Critical to define rewards accurately; mis-specification can yield poor policies.
            \item \textit{Example}: A speed-based reward for a delivery robot could lead to unsafe decisions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 1}
    \textbf{Key Concepts in Markov Decision Processes (MDPs)}
    \begin{enumerate}
        \item \textbf{Definition of MDPs}:
        \begin{itemize}
            \item A mathematical framework for modeling decision-making with uncertain outcomes.
            \item Defined by four components:
            \begin{itemize}
                \item \textbf{States (S)}: Possible situations for the agent.
                \item \textbf{Actions (A)}: Possible actions the agent can take.
                \item \textbf{Transition Model (P)}: Probability distribution \(P(s' | s, a)\) for moving between states.
                \item \textbf{Reward Function (R)}: Function \(R(s, a)\) giving rewards after state transitions.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Importance of MDPs in Reinforcement Learning}:
        \begin{itemize}
            \item MDPs are crucial for representing problems in sequential decision-making.
            \item Formulate foundational concepts for reinforcement learning algorithms.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Optimal Policy}:
        \begin{itemize}
            \item An optimal policy \(\pi^*\) maximizes expected rewards.
            \item Critical for effective decision-making in MDPs.
        \end{itemize}

        \item \textbf{Value Function}:
        \begin{itemize}
            \item The value function \(V(s)\) estimates expected rewards from state \(s\).
            \item The action-value function \(Q(s, a)\) estimates value of taking action \(a\) in state \(s\).
        \end{itemize}
        
        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item Applications of MDPs in fields like robotics, economics, and AI.
            \item Acknowledge limitations of MDPs (finite state/action spaces, Markov property).
            \item Scalability challenges requiring advanced techniques.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Grid World}
    \textbf{Example Description:}
    \begin{itemize}
        \item Imagine a Grid World where:
        \begin{itemize}
            \item States are robot positions on a grid (e.g., (1,1), (1,2)).
            \item Actions are directional movements (up, down, left, right).
            \item Rewards: +1 for reaching goal state, -1 for hitting walls.
        \end{itemize}
        \item Task: Develop an optimal policy to maximize total rewards through the grid.
    \end{itemize}

    \textbf{Summary of MDP Importance}:
    \begin{itemize}
        \item MDPs provide a mathematical framework for reinforcement learning.
        \item Algorithms such as Value Iteration and Policy Iteration rely on MDPs for finding solutions.
        \item Mastery of MDPs is necessary for tackling complex decision-making scenarios in AI.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session on Markov Decision Processes (MDPs)}
    
    \begin{block}{Overview of MDPs}
        A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the control of a decision maker.
    \end{block}
    
    \begin{block}{Key Components of MDPs}
        \begin{itemize}
            \item \textbf{States (S)}: Various situations the agent can be in.
            \item \textbf{Actions (A)}: Choices that affect the state.
            \item \textbf{Transition Model (P)}: Probability of moving between states given an action, denoted as \( P(s' | s, a) \).
            \item \textbf{Reward (R)}: Immediate payoff after transitioning from state \( s \) to \( s' \) due to action \( a \).
            \item \textbf{Discount Factor ($\gamma$)}: Value between 0 and 1 prioritizing immediate rewards over future rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Formulation and Key Concepts}
    
    \begin{block}{MDP Representation}
        MDPs are represented as a tuple \( (S, A, P, R, \gamma) \).
    \end{block}
    
    \begin{block}{Value Function and Policy}
        \begin{itemize}
            \item \textbf{Value Function (V)}: Measures expected return from a state given a policy.
            \item \textbf{Policy ($\pi$)}: Strategy for choosing actions based on the current state.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Discussion Points}
    
    \begin{block}{Example: Grid World}
        Imagine a simple grid world where:
        \begin{itemize}
            \item \textbf{States}: Each cell in the grid.
            \item \textbf{Actions}: Move Up, Down, Left, Right.
            \item \textbf{Rewards}: 
                \begin{itemize}
                    \item +1 for the goal state
                    \item -1 for the obstacle state
                    \item 0 for empty cells
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Discussion Points}
        \begin{itemize}
            \item How do we deal with uncertainty in decision-making?
            \item Implications of the discount factor on long-term vs. short-term strategies?
            \item Applications of MDPs in various fields.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Remarks}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item MDPs provide a structured way to model decision-making problems.
            \item Balancing exploration vs. exploitation is crucial in reinforcement learning.
            \item Computational methods like Dynamic Programming aid in finding optimal policies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Final Thoughts}
        Encourage students to share questions or applications related to MDPs in their projects. Use the Q&A to clarify any complex aspects introduced earlier.
    \end{block}
\end{frame}


\end{document}