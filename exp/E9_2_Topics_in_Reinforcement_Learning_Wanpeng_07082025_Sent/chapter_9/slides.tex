\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Multi-Agent RL]{Week 9: Multi-Agent Reinforcement Learning}
\subtitle{An Overview}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Multi-Agent Reinforcement Learning}
    \begin{block}{Overview of MARL}
        Multi-Agent Reinforcement Learning (MARL) involves multiple autonomous agents interacting within a shared environment to achieve their individual or collective goals. This process enhances learning efficiency, decision-making, and adaptability, making it highly applicable to various real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Multi-Agent Systems}
    \begin{itemize}
        \item \textbf{Complex Problem Solving}:
        \begin{itemize}
            \item Many real-world tasks require collaboration or competition among agents. 
            \item Example: Traffic control systems optimize flow through communication between vehicles and traffic lights.
        \end{itemize}
        
        \item \textbf{Scalability}:
        \begin{itemize}
            \item MARL can scale to numerous agents working simultaneously, enhancing computational efficiency.
            \item Example: Robots in logistics optimize picking and packing processes in warehouses.
        \end{itemize}
        
        \item \textbf{Diversity of Actions}:
        \begin{itemize}
            \item Multiple agents can enact diverse strategies leading to innovative solutions.
            \item Example: In gaming, players learn and adapt to each other’s strategies, making the game dynamic.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications and Challenges}
    \begin{block}{Applications}
        \begin{itemize}
            \item \textbf{Robotics}: Collaborative robots in manufacturing increase throughput.
            \item \textbf{Economics and Finance}: Agents simulate market behaviors to predict trends.
            \item \textbf{Healthcare}: Agents optimize scheduling and resource allocation in hospitals.
        \end{itemize}
    \end{block}

    \begin{block}{Challenges}
        \begin{itemize}
            \item \textbf{Scalability of Learning}: Complexity increases exponentially with more agents.
            \item \textbf{Non-stationary Environments}: Agents’ actions change the environment for others, complicating strategies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example Code}
    \begin{itemize}
        \item \textbf{Inter-agent Interaction}: Performance is influenced by other agents’ actions.
        \item \textbf{Learning Dynamics}: Strategies impact stability of learning outcomes.
        \item \textbf{Exploration vs. Exploitation}: Agents balance exploring new strategies with maximizing rewards.
    \end{itemize}

    \begin{block}{Code Snippet}
    \begin{lstlisting}[language=Python]
class Agent:
    def __init__(self, id):
        self.id = id
        self.policy = initial_policy()
        
    def choose_action(self, state):
        return action_based_on_policy(self.policy, state)
    
    def update_policy(self, reward, next_state):
        self.policy = update_policy_function(self.policy, reward, next_state)

agents = [Agent(i) for i in range(num_agents)]
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Multi-Agent Systems? - Definition}
    \begin{block}{Definition}
        A Multi-Agent System (MAS) is a system composed of multiple interacting intelligent agents. 
        These agents are capable of perceiving their environment and taking actions to achieve their individual or collective goals. 
        Multi-agent systems can exhibit a range of behaviors, including cooperation, competition, or both.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Multi-Agent Systems? - Components}
    \begin{block}{Components of Multi-Agent Systems}
        \begin{itemize}
            \item Agents
            \item Environment
            \item Interactions
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agents in Multi-Agent Systems}
    \begin{block}{1. Agents}
        \begin{itemize}
            \item \textbf{Definition:} An agent is an autonomous entity that observes its environment and acts to achieve specific goals.
            \item \textbf{Types of Agents:}
                \begin{itemize}
                    \item Reactive Agents: Respond directly to changes (e.g., vending machines).
                    \item Deliberative Agents: Plan and reason about their actions (e.g., smart robots).
                \end{itemize}
            \item \textbf{Example:} In a multi-agent robotics challenge, each robot represents an agent with its own sensors, actuators, and objectives.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Environment in Multi-Agent Systems}
    \begin{block}{2. Environment}
        \begin{itemize}
            \item \textbf{Definition:} The environment is the context within which agents operate, encompassing external factors affecting agent states and actions.
            \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Static vs. Dynamic
                    \item Observable vs. Partially Observable
                \end{itemize}
            \item \textbf{Example:} In a chess game, the board and pieces form the environment where agents (players) interact.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interactions in Multi-Agent Systems}
    \begin{block}{3. Interactions}
        \begin{itemize}
            \item \textbf{Definition:} Interactions refer to how agents communicate, share information, and influence each other's actions.
            \item \textbf{Types of Interactions:}
                \begin{itemize}
                    \item Direct Interaction (e.g., messaging).
                    \item Indirect Interaction (e.g., through environment or shared resources).
                \end{itemize}
            \item \textbf{Example:} In a strategy game, players share resources and influence strategies while aiming to win.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Final Thought}
    \begin{block}{Key Points}
        \begin{itemize}
            \item MAS handle complex tasks through efficient resource sharing and coordination.
            \item Diverse applications include robotics, distributed computing, traffic management, and social simulations.
            \item Understanding agent dynamics, environment interactions, and communication is crucial for effective MAS development.
        \end{itemize}
    \end{block}
    \begin{block}{Final Thought}
        As we explore multi-agent systems further, we will delve into the specifics of interactions between agents in the next slide.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Multi-Agent Interactions - Overview}
    In multi-agent systems (MAS), interactions among agents can significantly influence system performance and outcomes. 
    These interactions typically fall into three main categories:
    \begin{itemize}
        \item \textbf{Cooperation}
        \item \textbf{Competition}
        \item \textbf{Mixed Modes of Interaction}
    \end{itemize}
    Understanding these types of interactions is fundamental to designing effective multi-agent strategies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Multi-Agent Interactions - Cooperation}
    \textbf{Definition:} Cooperation occurs when agents work towards a common goal, pooling their resources and information to achieve shared objectives.
    
    \textbf{Key Characteristics:}
    \begin{itemize}
        \item \textbf{Joint Efforts:} Agents coordinate actions and share knowledge.
        \item \textbf{Mutual Benefit:} Success depends on the collective performance rather than individual achievements.
    \end{itemize}

    \textbf{Example:} \textit{Robotic Soccer}
    Multiple robots collaborate to devise strategies to outplay an opposing team and score goals. They communicate about positioning and passing strategies to maximize their chances of winning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Multi-Agent Interactions - Competition and Mixed Modes}
    \textbf{Competition:}
    \begin{itemize}
        \item \textbf{Definition:} Competition arises when agents vie for limited resources or strive for individual goals, leading to conflicting incentives.
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item \textbf{Rivalry:} Each agent's gains may come at the expense of others.
            \item \textbf{Self-Interest:} Agents seek personal or local rewards, potentially disregarding the collective welfare.
        \end{itemize}
        \item \textbf{Example:} \textit{Auction Bidding} - Multiple bidders compete for a single item, leading to a bidding war.
    \end{itemize}

    \vspace{0.5em} % To add space before the Mixed Mode section
    \textbf{Mixed Modes of Interaction:}
    \begin{itemize}
        \item \textbf{Definition:} Combine elements of both cooperation and competition.
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item \textbf{Dynamic Roles:} Agents switch between cooperative and competitive behaviors based on the context.
            \item \textbf{Complex Strategies:} Requires adaptive decision-making.
        \end{itemize}
        \item \textbf{Example:} \textit{Multi-Agent Trading Systems} where agents analyze trends together but decide individually.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cooperative Multi-Agent Systems}
    \begin{block}{Introduction}
        Cooperative Multi-Agent Systems (CMAS) involve multiple intelligent agents collaborating to achieve a shared goal.  
        These systems emulate real-world collaborations, enabling agents to solve complex problems collectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Joint Action Learning}:
            \begin{itemize}
                \item Agents coordinate their actions to optimize outcomes.
                \item \textit{Example}: In an automated warehouse, robots collaborate to transport items efficiently.
            \end{itemize}
        
        \item \textbf{Shared Environment}:
            \begin{itemize}
                \item Agents operate in a common space, where one's action can influence others' experiences.
            \end{itemize}
        
        \item \textbf{Communication}:
            \begin{itemize}
                \item Effective communication is vital for agent coordination.
                \item \textit{Example}: Drones share locations during surveillance to avoid coverage overlaps.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies for Cooperation}
    \begin{enumerate}
        \item \textbf{Centralized Training with Decentralized Execution}:
            \begin{itemize}
                \item Agents are trained together but act independently in practice, learning from shared experiences.
            \end{itemize}
        
        \item \textbf{Cooperative Game Theory}:
            \begin{itemize}
                \item Game theory principles enhance agent interactions.
                \item Reward sharing influences cooperation strategies:
                \begin{equation}
                    R_A = \frac{E_A}{E_A + E_B} \times R_T, \quad R_B = \frac{E_B}{E_A + E_B} \times R_T
                \end{equation}
            \end{itemize}
        
        \item \textbf{Joint Intentions}:
            \begin{itemize}
                \item Agents create joint intentions to coordinate towards a mutual objective.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Cooperative Multi-Agent Systems}
    \begin{itemize}
        \item \textbf{Traffic Management}: Smart traffic lights optimizing flow and reducing congestion.
        \item \textbf{Robotic Soccer}: Robots cooperate to pass the ball and score by understanding team dynamics.
        \item \textbf{Distributed Sensor Networks}: Sensors monitor environmental changes collectively and report to a central database.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Conclusion}
        Cooperative Multi-Agent Systems underscore the significance of collaboration in addressing complex objectives. Through learned strategies and effective communication, agents can work together effectively.
    \end{block}
    \begin{itemize}
        \item Agents align on shared objectives and outcomes.
        \item Joint action learning enhances collaboration.
        \item Communication is crucial for coordinated actions.
        \item Cooperative strategies can be shaped by game theory.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Competitive Multi-Agent Systems - Overview}
    \begin{block}{Overview}
        In \textbf{Competitive Multi-Agent Systems}, agents operate in environments where they vie against one another for limited resources or rewards. The interactions among agents are often adversarial, leading to dynamic and strategically complex scenarios.
    \end{block}
    \begin{block}{Importance}
        Understanding these interactions is crucial for developing agents that can effectively compete and strategize.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Zero-Sum Games}
        \begin{itemize}
            \item A zero-sum game is a situation in which one agent's gain is exactly balanced by the losses of other agents.
            \item Example: 
            \begin{equation}
                R_A + R_B = 0
            \end{equation}
            If Agent A gains +10, Agent B incurs -10.
        \end{itemize}

        \item \textbf{Nash Equilibrium}
        \begin{itemize}
            \item Occurs when no agent can benefit from unilaterally changing their strategy, given other agents' strategies.
            \item Example: In Rock-Paper-Scissors, if both players randomize their selections, they achieve a Nash Equilibrium.
        \end{itemize}

        \item \textbf{Strategies in Competitive Settings}
        \begin{itemize}
            \item \textbf{Dominant Strategy}: Optimal regardless of opponent's move.
            \item \textbf{Mixed Strategy}: Randomized strategy to keep opponents guessing.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario: Racing Game}
    \begin{block}{Description}
        Imagine a racing game where multiple agents (cars) compete to finish first.
    \end{block}
    \begin{itemize}
        \item Each agent can accelerate, brake, or use power-ups.
        \item The gain for one agent (first place) is at the loss of others, modeled as a zero-sum game.
        \item This competitive dynamic requires agents to predict and counteract opponents' actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Code Snippet}
    \begin{block}{Conclusion}
        In competitive environments, interactions among agents are complex and influenced by strategic decision-making. 
        \begin{itemize}
            \item Understanding zero-sum dynamics helps design effective competing agents.
            \item Game theory principles are essential for anticipating opponent strategies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Supplementary Code Snippet}
    \begin{lstlisting}[language=Python]
class Agent:
    def __init__(self):
        self.strategy = None
    
    def choose_action(self):
        # Choose action based on equilibrium strategy
        return self.random_action() if not self.strategy else self.strategy

# Simulate matchup
agent_A = Agent()
agent_B = Agent()
result = simulate_race(agent_A.choose_action(), agent_B.choose_action())
print("Race Result:", result)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Game Theory Foundations - Introduction}
  \begin{block}{What is Game Theory?}
    Game Theory is a mathematical framework for analyzing situations in which players make decisions that are interdependent. This means each player’s outcome depends not only on their own choices but also on the choices of others.
  \end{block}

  \begin{itemize}
    \item \textbf{Players:} Individuals or agents making decisions.
    \item \textbf{Strategies:} Complete plans of actions a player can take.
    \item \textbf{Payoffs:} Outcomes received by each player based on combined strategies.
    \item \textbf{Games:} The scenarios where players interact, classified as cooperative or non-cooperative.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Game Theory Foundations - Types of Games}
  \begin{block}{Zero-Sum Games}
    - A scenario where one player's gain is another player's loss. \\
    - \textbf{Example:} Chess, where one victory equates to another's loss.
  \end{block}

  \begin{block}{Non-Zero-Sum Games}
    - Games where total payoff can vary, allowing for mutual benefit or loss. \\
    - \textbf{Example:} Trade negotiations that benefit both countries.
  \end{block}
  
  \begin{block}{Simultaneous vs. Sequential Games}
    \begin{itemize}
      \item \textbf{Simultaneous:} Players act at the same time (e.g., Rock-Paper-Scissors).
      \item \textbf{Sequential:} Players act one after another, aware of prior moves (e.g., chess).
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Game Theory Foundations - Nash Equilibrium}
  \begin{block}{Nash Equilibrium}
    A situation where no player can benefit by changing their strategy while the others keep theirs unchanged.
  \end{block}

  \begin{itemize}
    \item \textbf{Example:} In a market with two companies setting prices, if Company A maximizes profits considering Company B's price, both are in Nash Equilibrium.
  \end{itemize}

  \begin{equation}
    \text{Total Payoff} = P_1(S_1, S_2) + P_2(S_1, S_2)
  \end{equation}

  \begin{block}{Applications in Multi-Agent Settings}
    - Game theory provides principles to understand complex interactions in multi-agent systems (e.g., MARL).
    - Strategies might involve cooperation or competition depending on game structure.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-Agent Q-Learning - Overview}
    \begin{itemize}
        \item Extends traditional Q-Learning to multi-agent scenarios.
        \item Aims for optimal strategies in competitive or cooperative settings.
        \item Learning must account for the actions of other agents.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-Agent Q-Learning - Key Concepts}
    \begin{enumerate}
        \item **Multi-Agent Systems**: Involve multiple agents which may cooperate or compete.
        \item **Q-Learning Recap**:
            \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
            \end{equation}
            where:
            \begin{itemize}
                \item \(s\) = current state
                \item \(a\) = action taken
                \item \(r\) = reward received
                \item \(s'\) = new state
                \item \(\alpha\) = learning rate
                \item \(\gamma\) = discount factor
            \end{itemize}
        \item **Extensions**: Learning becomes non-stationary due to interactions with other agents.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-Agent Q-Learning - Key Challenges and Opportunities}
    \begin{block}{Key Challenges}
        \begin{enumerate}
            \item **Non-Stationarity**: Dynamics change as multiple agents learn simultaneously.
            \item **Exploration vs. Exploitation**: Balance is needed in various scenarios.
            \item **Coordination**: Required in cooperative settings.
            \item **Credit Assignment**: Difficulty in determining contributions to rewards.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Opportunities}
        \begin{enumerate}
            \item **Robust Strategies**: Development of strategies under competitive pressures.
            \item **Emergent Behavior**: New strategies may enhance overall performance.
            \item **Improved Learning Efficiency**: Information sharing leads to faster convergence.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Communication in Multi-Agent Systems}
  \begin{block}{Understanding Communication}
    Communication in Multi-Agent Systems (MAS) refers to the exchange of information between agents, allowing them to coordinate their actions effectively.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Importance of Communication}
  \begin{itemize}
    \item \textbf{Role:} Enhances collaboration in cooperative environments.
    \item Assists agents in making strategic decisions in competitive scenarios.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Types of Communication}
  \begin{enumerate}
    \item \textbf{Cooperative Communication}
      \begin{itemize}
        \item \textbf{Goal:} Achieve a common objective benefiting all agents.
        \item \textbf{Example:} Robots in a search and rescue mission share victim locations and coordinate search patterns.
      \end{itemize}
    
    \item \textbf{Competitive Communication}
      \begin{itemize}
        \item \textbf{Goal:} Gain an advantage over other agents.
        \item \textbf{Example:} Soccer players create strategies or mislead the opposing team through communication.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Communication Strategies}
  \begin{itemize}
    \item \textbf{Direct Communication:} Agents explicitly share messages.
      \begin{example}
        \item Agent A informs Agent B that its task is complete.
      \end{example}
    
    \item \textbf{Indirect Communication:} Information is inferred through observations.
      \begin{example}
        \item Agent B observes Agent A moving towards a target and infers an intention to attack.
      \end{example}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Communication Mechanisms}
  \begin{itemize}
    \item \textbf{Verbal Communication:} Structured messages based on human languages or predefined signals.
    \item \textbf{Non-Verbal Communication:} Agents signal intent through actions or environmental changes.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Communication}
  \begin{itemize}
    \item \textbf{Noise and Miscommunication:} Distorted messages can lead to ineffective collaboration.
    \item \textbf{State Inference:} Agents may need to infer others' states and intentions based on limited information.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Efficiency:} Communication must optimize time and resources; over-communication can lead to delays.
    \item \textbf{Reliability:} Ensuring messages are received and understood accurately is critical.
    \item \textbf{Scalability:} Communication protocols should effectively scale with the number of agents.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  Communication is fundamental in multi-agent systems, influencing the success and efficiency of interactions in both cooperative and competitive environments. Designing effective communication strategies is essential for enhancing multi-agent learning outcomes.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Engagement}
  \begin{itemize}
    \item Think of real-world scenarios where communication among agents is crucial (e.g., robots, sports teams).
    \item Discuss how advancements in Natural Language Processing (NLP) might influence future communication strategies in multi-agent systems.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Multi-Agent Reinforcement Learning - Introduction}
    Multi-Agent Reinforcement Learning (MARL) presents unique challenges that differ from traditional single-agent learning. As multiple agents operate in a shared environment, they must navigate complexities that arise from their interactions. Understanding these challenges is crucial for developing efficient MARL applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Multi-Agent Reinforcement Learning - Key Challenges}
    \begin{enumerate}
        \item \textbf{Non-Stationarity}
        \begin{itemize}
            \item Definition: The environment becomes non-stationary for each agent due to the actions of other agents.
            \item Example: In soccer, a player's decision to pass alters strategies of teammates and opponents.
            \item Impact: Agents struggle to learn optimal policies because their environment is constantly changing.
        \end{itemize}

        \item \textbf{Scalability}
        \begin{itemize}
            \item Definition: Increasing the number of agents leads to exponentially growing complexity.
            \item Example: Traffic management with many autonomous vehicles requires substantial coordination.
            \item Impact: Algorithms face inefficiency, longer training times, and difficulties scaling beyond few agents.
        \end{itemize}

        \item \textbf{Coordination}
        \begin{itemize}
            \item Definition: Essential for achieving common goals but can be difficult to implement.
            \item Example: A team of robots moving crates risks collisions without proper coordination.
            \item Impact: Poor coordination results in suboptimal outcomes, emphasizing the need for reliable communication.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Multi-Agent Reinforcement Learning - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Learning Dynamics}: Agents must adapt continuously due to dynamic changes in the environment.
            \item \textbf{Communication Importance}: Sharing observations and intentions can mitigate non-stationarity and coordination issues.
            \item \textbf{Algorithm Design}: New algorithms are essential to manage growing complexity in agent-dense environments.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding MARL challenges like non-stationarity, scalability, and coordination enhances theoretical knowledge and guides the development of practical solutions. Addressing these challenges is vital for successful real-world applications of MARL.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Multi-Agent Reinforcement Learning - Additional Resources}
    \begin{itemize}
        \item \textbf{Reading}: Papers on MARL algorithms, such as Deep Q-Networks in cooperative settings.
        \item \textbf{Tools}: OpenAI Gym for simulating multi-agent environments.
    \end{itemize}

    \centering
    Feel free to reach out if you have any questions or need further clarification on these concepts!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies of Multi-Agent Applications}
    \begin{block}{Introduction to Multi-Agent Reinforcement Learning (MARL)}
        Multi-Agent Reinforcement Learning involves multiple agents interacting within an environment, learning to maximize their own rewards while coordinating with others.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Robotics}
    \begin{itemize}
        \item \textbf{Application}: Autonomous Robots
        \item \textbf{Example}: Swarm Robotics
        \begin{itemize}
            \item \textbf{Description}: Groups of robots collaborate on tasks, e.g., search and rescue or environmental monitoring.
            \item \textbf{Key Learning Points}:
            \begin{itemize}
                \item Each robot adapts its behavior using MARL based on others' actions.
                \item Decentralized training improves scalability and robustness.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Game AI}
    \begin{itemize}
        \item \textbf{Application}: Multi-Agent Games
        \item \textbf{Example}: StarCraft II with DeepMind's AlphaStar
        \begin{itemize}
            \item \textbf{Description}: AlphaStar uses MARL for managing real-time strategy gameplay.
            \item \textbf{Key Learning Points}:
            \begin{itemize}
                \item Agents strategize against human players by simulating numerous games.
                \item Each agent learns to optimize moves and predict opponent strategies.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Traffic Management}
    \begin{itemize}
        \item \textbf{Application}: Intelligent Transportation Systems
        \item \textbf{Example}: Traffic Signal Control with Reinforcement Learning
        \begin{itemize}
            \item \textbf{Description}: Signals learn to optimize traffic flow based on real-time conditions.
            \item \textbf{Key Learning Points}:
            \begin{itemize}
                \item Signals communicate to reduce overall congestion effectively.
                \item The system adapts to changing traffic patterns, improving travel time and lowering emissions.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Challenges}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Collaboration \& Competition}: Agents must work together while competing for resources.
            \item \textbf{Adaptability}: MARL systems adapt over time, ideal for dynamic environments.
            \item \textbf{Real-World Impact}: Applications demonstrate the versatility and potential of MARL.
        \end{itemize}
    \end{block}
    
    \begin{block}{Potential Challenges}
        \begin{itemize}
            \item \textbf{Non-stationarity}: Policies are continually changing, complicating fixed strategies.
            \item \textbf{Scalability}: Complexity increases as the number of agents grows.
            \item \textbf{Coordination}: Effective communication among agents is crucial for success.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Single-Agent Systems - Overview}
    \begin{block}{Introduction to Reinforcement Learning (RL)}
        Reinforcement Learning is a domain of machine learning where agents learn to make decisions by taking actions in an environment to maximize cumulative rewards. 
        In this context, we can categorize RL into two main types:
        \begin{itemize}
            \item \textbf{Single-Agent Reinforcement Learning (SARL)}
            \item \textbf{Multi-Agent Reinforcement Learning (MARL)}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences Between Single-Agent and Multi-Agent Systems}
    \begin{enumerate}
        \item \textbf{Number of Agents}:
            \begin{itemize}
                \item Single-Agent RL: One agent interacting with the environment, decisions based on its own observations and experiences.
                \item Multi-Agent RL: Multiple agents interacting, can cooperate or compete, influencing each other and the environment.
            \end{itemize}
        \item \textbf{Complexity of Environment}:
            \begin{itemize}
                \item Single-Agent: Simpler dynamics, dependent on the agent's actions only.
                \item Multi-Agent: More complex dynamics due to multi-agent interactions requiring consideration of others' actions.
            \end{itemize}
        \item \textbf{Learning Paradigms}:
            \begin{itemize}
                \item Single-Agent: Techniques include Q-learning and Policy Gradients.
                \item Multi-Agent: Techniques include Cooperative Learning, Multi-Agent Q-Learning, etc.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Benefits of Multi-Agent RL}
    \begin{block}{Challenges}
        \begin{itemize}
            \item \textbf{Non-Stationarity}: Changing dynamics due to each agent's learning process.
            \item \textbf{Scalability}: Increased complexity as the number of agents grows.
            \item \textbf{Communication}: Need for coordination and information sharing among agents.
        \end{itemize}
    \end{block}

    \begin{block}{Benefits}
        \begin{itemize}
            \item \textbf{Efficiency}: Task division leads to faster learning and execution.
            \item \textbf{Robustness}: Resilience to failures, as other agents can take over tasks.
            \item \textbf{Rich Interactions}: Model social dynamics for realistic learning scenarios.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example: Traffic Management}
    \begin{itemize}
        \item \textbf{Single-Agent RL Example}: An agent controls a traffic signal based on historical data to optimize traffic flow.
        \item \textbf{Multi-Agent RL Example}: Multiple traffic signals (agents) coordinate their timings to reduce congestion effectively, requiring real-time communication and strategy adjustments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Conclusion}
        Understanding the distinctions between SARL and MARL is crucial for developing robust AI applications. As task complexity increases, multi-agent systems offer significant advantages but also introduce extra challenges.
    \end{block}

    \begin{itemize}
        \item \textbf{Single-Agent}: Simpler dynamics, focused on individual learning.
        \item \textbf{Multi-Agent}: Complex interactions, requiring cooperative/competitive strategies.
        \item Consider the environment, task complexity, and desired behavior when choosing SARL or MARL.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Future Directions in Multi-Agent RL Research}
  \begin{block}{Overview}
    In the rapidly evolving field of Multi-Agent Reinforcement Learning (MARL), several emerging trends and research directions are shaping the future landscape. Understanding these can provide insight into potential improvements in AI systems and their applications.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Research Areas in MARL - Part 1}
  \begin{enumerate}
    \item \textbf{Communication and Coordination}
      \begin{itemize}
        \item \textit{Concept}: Enhancing collective performance through agent communication.
        \item \textit{Example}: Robots in a warehouse optimizing pickup and delivery.
        \item \textit{Key Point}: Effective communication leads to improved teamwork.
      \end{itemize}
    
    \item \textbf{Emergent Behaviors}
      \begin{itemize}
        \item \textit{Concept}: Complex behaviors arise from simple interactions without centralized control.
        \item \textit{Example}: Flocking behavior in birds.
        \item \textit{Key Point}: Understanding these behaviors can inform resilient system designs.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Research Areas in MARL - Part 2}
  \begin{enumerate}[resume]
    \item \textbf{Scalability and Efficiency}
      \begin{itemize}
        \item \textit{Concept}: Algorithms that scale effectively with more agents.
        \item \textit{Example}: Decentralized training allowing agents to learn from fewer resources.
        \item \textit{Key Point}: Essential for applications like autonomous vehicles.
      \end{itemize}

    \item \textbf{Multi-Agent Cooperative Learning}
      \begin{itemize}
        \item \textit{Concept}: Algorithms that promote cooperation among agents.
        \item \textit{Example}: Agents working together in cooperative games.
        \item \textit{Key Point}: Cooperation leads to better-performing solutions than competition.
      \end{itemize}
    
    \item \textbf{Incorporating Human-Centric Elements}
      \begin{itemize}
        \item \textit{Concept}: Integrating human decision-making and preferences.
        \item \textit{Example}: Human-guided robots adapting based on feedback.
        \item \textit{Key Point}: Improving acceptance and performance in collaborative AI systems.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Research Areas in MARL - Part 3}
  \begin{itemize}
    \item \textbf{Ethical Considerations and Safety}
      \begin{itemize}
        \item \textit{Concept}: Addressing ethical implications and ensuring safety in operations.
        \item \textit{Example}: Developing agents that follow ethical guidelines in healthcare.
        \item \textit{Key Point}: Ethical frameworks are crucial for sensitive deployment scenarios.
      \end{itemize}
    
    \item \textbf{Conclusion}
      \begin{itemize}
        \item The future of MARL holds significant breakthroughs by focusing on:
        \begin{itemize}
          \item Communication
          \item Emergent behaviors
          \item Scalability
          \item Cooperation
          \item Human factors
          \item Ethics
        \end{itemize}
        \item Enhancing efficacy and applicability across various domains.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways from Multi-Agent Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Definition and Importance}:
        Multi-Agent Reinforcement Learning (MARL) focuses on how multiple agents interact in shared environments to achieve goals.
        
        \item \textbf{Collaboration vs. Competition}:
        MARL incorporates both collaborative strategies (e.g., robotic teams) and competitive scenarios (e.g., games).
        
        \item \textbf{Communication and Coordination}:
        Agents that effectively communicate can significantly improve performance and learning efficiency.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Challenges and Applications}
    \begin{enumerate}
        \item \textbf{Scalability and Challenges}:
        \begin{itemize}
            \item Non-stationarity as agents update policies continuously.
            \item Credit assignment becomes complex in multi-agent environments.
        \end{itemize}

        \item \textbf{Application Areas}:
        MARL finds use in:
        \begin{itemize}
            \item Autonomous vehicle fleets.
            \item Robotics, such as multi-robot systems.
            \item Game playing examples (e.g., AlphaStar in StarCraft II).
            \item Economics and social sciences for market simulations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Future Directions}
    \begin{block}{Future Directions}
        \begin{itemize}
            \item Exploration of robust algorithms for dynamic environments.
            \item Enhancement of communication protocols among agents.
            \item Development of decentralized learning approaches.
        \end{itemize}
    \end{block}

    \textbf{Conclusion Summary:}
    MARL is an evolving field addressing challenges and opportunities from multi-agent systems, paving the way for efficient algorithms and applications across various domains.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A}
    Open floor for questions and discussion about multi-agent systems and their applications in reinforcement learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Multi-Agent Reinforcement Learning (MARL)}
    \begin{block}{Definition}
        Multi-Agent Reinforcement Learning (MARL) is a subset of reinforcement learning that involves multiple agents interacting within an environment. Each agent aims to maximize its own reward while considering the behavior and strategies of other agents.
    \end{block}

    \begin{itemize}
        \item \textbf{Agent}: An entity that perceives its environment and makes decisions/actions based on that perception.
        \item \textbf{Environment}: The world in which agents operate. This can be a simple grid or a complex setting like a game or a robotic system.
        \item \textbf{Policies}: Strategies used by agents to determine their actions based on the current state of the environment.
        \item \textbf{Rewards}: Feedback received by agents based on their actions; the goal is to maximize cumulative rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Examples}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Cooperation vs. Competition}: Understanding the dynamics helps in designing effective algorithms.
            \item \textbf{Communication}: Agents can share information to improve their decision-making. This aspect is crucial in cooperative scenarios.
            \item \textbf{Scalability}: As the number of agents increases, coordination and communication strategies need to be optimized to ensure efficiency.
        \end{itemize}
    \end{block}

    \begin{block}{Examples of Applications}
        \begin{itemize}
            \item \textbf{Robotics}: Teams of robots working together to accomplish tasks such as search and rescue missions.
            \item \textbf{Game Theory}: Multi-agent games where players (agents) strategize against one another, like in poker or competitive video games.
            \item \textbf{Traffic Systems}: Autonomous vehicles navigating together on roads, optimizing flow and minimizing accidents.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Discussion Topics}
    \begin{block}{Q-Learning in MARL}
        The Q-value (action-value) function can be extended for multiple agents with the update rule:

        \begin{equation}
            Q(s, a) \gets Q(s, a) + \alpha [r + \gamma \max_{a'}Q(s', a') - Q(s, a)]
        \end{equation}
        Where \( s \) denotes state, \( a \) denotes action, \( r \) is the reward, \( \alpha \) is the learning rate, \( \gamma \) is the discount factor, and the agent \( i \)'s policies may affect \( Q \) values.
    \end{block}

    \begin{block}{Discussion Topics}
        \begin{itemize}
            \item What are the biggest challenges you've observed in MARL?
            \item How can agents effectively balance exploration and exploitation in a multi-agent context?
            \item In what ways can reinforcement learning benefit from real-time communication between agents?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing}
    The floor is open for questions! Consider your observations and experiences with MARL, and let’s discuss how theory translates into practice. What insights or experiences do you have to share?
\end{frame}

\begin{frame}
    \frametitle{Overview of Multi-Agent Reinforcement Learning (MARL)}
    Multi-Agent Reinforcement Learning (MARL) is an evolving field that studies how multiple agents can learn and collaborate (or compete) in a shared environment. 
    \begin{itemize}
        \item Enhances understanding of MARL concepts.
        \item Divergent resources available for depth in knowledge and skills.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Recommended Readings}
    \begin{enumerate}
        \item \textbf{Books}:
        \begin{itemize}
            \item \textit{"Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto}
            \begin{itemize}
                \item Covers basics of reinforcement learning; discusses MARL later.
            \end{itemize}
            \item \textit{"Multi-Agent Reinforcement Learning: A Survey" by Busoniu et al.}
            \begin{itemize}
                \item Overview of methodologies and challenges in MARL.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Research Papers}:
        \begin{itemize}
            \item \textit{"Cooperative Deep Reinforcement Learning: A New Approach"}
            \item \textit{"Independent Learners in Multiagent Systems"}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Online Courses and Tools}
    \begin{itemize}
        \item \textbf{Coursera}: 
        \begin{itemize}
            \item \textit{"Deep Learning Specialization by Andrew Ng"}
        \end{itemize}
        
        \item \textbf{edX}:
        \begin{itemize}
            \item \textit{"Advanced Reinforcement Learning"}
        \end{itemize}

        \item \textbf{Useful Tools and Libraries}:
        \begin{itemize}
            \item \textit{OpenAI Gym}
            \item \textit{Stable Baselines3}
            \item \textit{PettingZoo}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Problem: Simulating Cooperative Agents}
    Develop a simple scenario using OpenAI Gym where two agents work together to achieve a common reward.
    \begin{lstlisting}[language=Python]
import gym
from stable_baselines3 import PPO

# Create environment
env = gym.make('CartPole-v1')

# Initialize agent
model = PPO("MlpPolicy", env, verbose=1)

# Train the agent
model.learn(total_timesteps=10000)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Collaboration vs. Competition}: Understand agent dynamics in collaborative and competitive settings.
        \item \textbf{Scalability}: Explore how algorithms adapt as more agents are introduced.
        \item \textbf{Real-World Applications}: Applications in robotics, finance, and healthcare.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Enhanced understanding of Multi-Agent Reinforcement Learning through suggested readings, courses, and tools will greatly benefit your academic journey. 
    \begin{itemize}
        \item Engage with these resources to deepen your insights and practical skills in this dynamic field.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assignment Overview - Multi-Agent Reinforcement Learning}
    \begin{block}{Objectives}
        The upcoming assignment will deepen your understanding of Multi-Agent Reinforcement Learning (MARL) by focusing on the following key objectives:
        \begin{enumerate}
            \item \textbf{Implementing MARL Algorithms}: Code and evaluate at least two different MARL algorithms, such as Independent Q-Learning (IQL) and Cooperative Deep Q-Networks (CDQN).
            \item \textbf{Evaluating Agent Performance}: Analyze the performance of the agents in various environments, highlighting the impact of cooperation and competition.
            \item \textbf{Reflecting on Learning Outcomes}: Write a reflective report summarizing your findings, challenges faced, and potential improvements.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assignment Details}
    \begin{block}{Submission Format}
        \begin{itemize}
            \item Code submissions should be organized in a Jupyter Notebook or Python script (.py).
            \item Include a README file that explains how to run your code and any dependencies.
            \item The reflective report should be a separate PDF document no longer than 3 pages.
        \end{itemize}
    \end{block}
    
    \begin{block}{Due Date}
        The assignment is due at the end of Week 10; late submissions will incur a penalty unless previously discussed.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case and Key Points}
    \begin{block}{Example Use Case}
        Consider a scenario where two agents are learning to complete a maze in which they can either cooperate or compete. You will:
        \begin{itemize}
            \item Implement IQL, where each agent learns independently.
            \item Implement CDQN, where both can benefit from shared experiences.
        \end{itemize}
        By tracking their performance, you will assess scenarios where cooperation yields higher rewards compared to competition.
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Collaboration vs. Competition}: Understanding how agents interact is crucial in MARL settings.
            \item \textbf{Algorithm Selection}: Different environments may require you to choose different algorithms based on your specific goals.
            \item \textbf{Data Analysis}: Visualization (e.g., plotting rewards over time) will be essential for your report—use libraries like Matplotlib for clarity.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}