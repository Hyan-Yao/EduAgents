\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Reinforcement Learning in Games]{Week 11: Reinforcement Learning in Games}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Reinforcement Learning (RL) in Game Development}
    
    \begin{block}{What is Reinforcement Learning?}
        Reinforcement Learning is a subset of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Agent}: Learner or decision-maker.
        \item \textbf{Environment}: The space in which the agent operates.
        \item \textbf{Actions}: Choices the agent can make.
        \item \textbf{Rewards}: Feedback from the environment that helps the agent learn.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Game Theory}
    
    \begin{block}{What is Game Theory?}
        A mathematical framework used for analyzing competitive situations where the outcome for each participant depends on the actions of all involved parties.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Players}: Individuals or groups making decisions.
        \item \textbf{Strategies}: Plans of action players choose.
        \item \textbf{Payoff}: The outcome of a particular strategy, which can be beneficial (winning) or detrimental (losing).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relationship between Game Theory and Reinforcement Learning}
    
    \begin{block}{Integration Points}
        \begin{itemize}
            \item \textbf{Real-time Decision Making}: In games, agents must often make decisions in real-time based on the actions of other players, necessitating the application of Game Theory concepts.
            \item \textbf{Strategy and Learning}: Just like players devise strategies based on their knowledge of competitors, RL agents learn optimal strategies over time through exploration and exploitation of the environment.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example of Application}
        \begin{itemize}
            \item In games like “StarCraft” or “Dota,” where multiple agents (both human and AI) compete, RL techniques can be applied to allow agents to learn from their interactions, adapting their strategies based on gameplay dynamics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Interdependency}: Both RL and Game Theory focus on decision-making in competitive scenarios.
        \item \textbf{Adaptation and Learning}: Reinforcement Learning allows agents to adapt strategies in response to the actions of other players, leveraging concepts from Game Theory.
        \item \textbf{Practical Relevance}: Understanding this relationship is crucial for developing intelligent game agents capable of competing in complex environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula for Markov Decision Process}
    
    \begin{block}{Markov Decision Process (MDP) Framework}
        \begin{itemize}
            \item \textbf{States} ($S$)
            \item \textbf{Actions} ($A$)
            \item \textbf{Transition Function} ($P$)
            \item \textbf{Reward Function} ($R$)
        \end{itemize}
    \end{block}
    
    \begin{equation}
    Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q(s', a')
    \end{equation}
    Where:
    \begin{itemize}
        \item $Q(s, a)$: Expected utility of action $a$ in state $s$.
        \item $\gamma$: Discount factor for future rewards.
        \item $P(s'|s, a)$: Probability of reaching state $s'$ from state $s$ after action $a$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Objectives of This Week's Lesson - Overview}
    This week’s lesson focuses on understanding how Reinforcement Learning (RL) is applied in gaming, alongside foundational concepts in game theory. 
    We will examine how these concepts interact and contribute to the dynamics of gameplay and player strategies.
\end{frame}

\begin{frame}[fragile]{Objectives of This Week's Lesson - Learning Objectives}
    \begin{enumerate}
        \item \textbf{Understanding Reinforcement Learning in Game Development}
            \begin{itemize}
                \item \textbf{Definition of RL}: Reinforcement Learning is an area of machine learning where agents learn by taking actions to maximize cumulative rewards.
                \item \textbf{Application in Games}: RL is used to train AI opponents, develop adaptive gameplay mechanics, personalize player experiences, and enhance NPC behaviors.
                \item \textbf{Example}: In AlphaGo, reinforcement learning enabled AI to master Go by self-play and learning from outcomes.
            \end{itemize}
            
        \item \textbf{Exploring Game Theory Concepts}
            \begin{itemize}
                \item \textbf{Basic Principles}: Game Theory studies strategic interactions among rational decision-makers, offering insights into competitive and cooperative behavior.
                \item \textbf{Key Concepts}:
                    \begin{itemize}
                        \item \textbf{Nash Equilibrium}: A situation where no player can benefit by changing their strategy while others keep theirs unchanged.
                        \item \textbf{Zero-Sum Games}: A scenario where one player's gain is balanced by the losses of others, e.g., chess.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Objectives of This Week's Lesson - Integration and Key Takeaways}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue enumeration
        \item \textbf{Integration of RL and Game Theory}
            \begin{itemize}
                \item \textbf{Interplay of Strategies}: RL agents learn and adapt strategies based on other players' actions as outlined by game theory.
                \item \textbf{Collaborative and Competitive Learning}: RL can be applied in both cooperative (multi-agent systems) and competitive (adversarial) settings.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item RL and Game Theory are interrelated fields impacting modern game development.
            \item Understanding these topics enhances the ability to create engaging game environments that adapt to player actions.
        \end{itemize}
    \end{block}

    \textbf{Conclusion}: By week's end, students will grasp how RL influences game dynamics and the strategic frameworks game theory provides for player interactions.
\end{frame}

\begin{frame}[fragile]{Reinforcement Learning Basics - Key Concepts}
    \begin{block}{Key Concepts in Reinforcement Learning}
        \begin{enumerate}
            \item \textbf{Agents}
            \begin{itemize}
                \item \textbf{Definition}: An entity that makes decisions to achieve a goal.
                \item \textbf{Example}: The player controlled character in a game.
            \end{itemize}

            \item \textbf{Environments} 
            \begin{itemize}
                \item \textbf{Definition}: Everything the agent interacts with, including world state.
                \item \textbf{Example}: The chess board and pieces in a chess game.
            \end{itemize}

            \item \textbf{Rewards} 
            \begin{itemize}
                \item \textbf{Definition}: A feedback signal after an action evaluating its success.
                \item \textbf{Example}: Scoring points for winning a round in a game.
            \end{itemize}

            \item \textbf{Policy}
            \begin{itemize}
                \item \textbf{Definition}: A strategy for the agent to decide next actions.
                \item \textbf{Types}:
                \begin{itemize}
                    \item \textbf{Deterministic}: Specific action for each state.
                    \item \textbf{Stochastic}: Probability distribution over actions per state.
                \end{itemize}
                \item \textbf{Example}: Racing game's policy for acceleration and braking.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Reinforcement Learning Basics - Key Points and Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Reinforcement learning is iterative; agents learn and improve policies.
            \item Interplay between exploration and exploitation is crucial.
            \item Applications span various domains, notably in game development.
        \end{itemize}
    \end{block}

    \begin{block}{Illustrative Example}
        \textit{Consider a simple grid world:}
        \begin{itemize}
            \item \textbf{Agent}: The robot navigating the grid.
            \item \textbf{Environment}: The grid itself, including the goal and obstacles.
            \item \textbf{Actions}: Move up, down, left, or right.
            \item \textbf{Rewards}: +10 for reaching the goal, -1 for hitting obstacles.
        \end{itemize}
        The robot learns over time through trial and error, adjusting its policy.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Reinforcement Learning Basics - Formulas}
    \begin{block}{Cumulative Reward}
        \begin{equation}
        R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
        \end{equation}
        Where:
        \begin{itemize}
            \item \( R_t \): Total reward at time \( t \)
            \item \( \gamma \) (0 ≤ \( \gamma \) < 1): Discount factor prioritizing immediate rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Game Theory Fundamentals}
    \begin{block}{Introduction to Game Theory}
        Game Theory is a mathematical framework for analyzing situations with interdependent decision-makers. It helps understand strategic interactions among rational players.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Definitions}
    \begin{enumerate}
        \item \textbf{Game}: A set of players, rules, strategies, and payoffs.
        \item \textbf{Players}: Decision-makers (e.g., individuals, groups, organizations).
        \item \textbf{Strategies}: Plans of action that players may adopt.
        \item \textbf{Payoffs}: Outcomes based on chosen strategies.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Games}
    \begin{itemize}
        \item \textbf{Players}: Can be multiple, from two in simple games to many in complex scenarios.
        \item \textbf{Strategies}: Players may have various strategies, e.g.:
            \begin{itemize}
                \item Different opening moves in chess
                \item Pricing strategies in economics
            \end{itemize}
        \item \textbf{Payoffs}: Represent utility or benefits. Types include:
            \begin{itemize}
                \item Numerical (e.g., profit)
                \item Qualitative (e.g., satisfaction)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Games}
    \begin{itemize}
        \item \textbf{Cooperative vs. Non-Cooperative}: 
            \begin{itemize}
                \item Cooperative games allow for binding commitments.
                \item Non-cooperative games do not allow for cooperation or binding agreements.
            \end{itemize}
        \item \textbf{Zero-sum vs. Non-Zero-sum}:
            \begin{itemize}
                \item Zero-sum: One player's gain is another's loss (e.g., poker).
                \item Non-zero-sum: Both can benefit or both can suffer (e.g., trade negotiations).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Game Theory}
    \begin{itemize}
        \item \textbf{Strategy Optimization}: Helps players optimize strategies by anticipating others.
        \item \textbf{Predicting Outcomes}: Predicts outcomes of strategic interactions.
        \item \textbf{Conflict Resolution}: Identifies equilibria to facilitate negotiation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: The Prisoner’s Dilemma}
    \begin{itemize}
        \item Two criminals, interrogated separately, choose to:
            \begin{itemize}
                \item \textbf{Cooperate} (remain silent)
                \item \textbf{Defect} (betray the other)
            \end{itemize}
        \item Outcomes:
            \begin{itemize}
                \item Both cooperate: light sentences.
                \item One defects, the other cooperates: defector goes free, cooperator gets heavy sentence.
                \item Both defect: moderate sentences.
            \end{itemize}
        \item Illustrates how rational strategies can lead to suboptimal outcomes (mutual defection).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Rational Decision-Making}: Players aim to maximize payoffs.
        \item \textbf{Equilibria}: Concepts like Nash Equilibrium predict stable strategies.
        \item \textbf{Applications}: Beyond games, applies to economics, politics, social sciences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas (Optional)}
    \begin{block}{Nash Equilibrium}
        A set of strategies where no player can benefit by changing their strategy unilaterally:
        \begin{equation}
            \begin{aligned}
            & U_1(s_1^\ast, s_2^\ast) \geq U_1(s_1, s_2^\ast) \text{ for all } s_1 \\
            & U_2(s_1^\ast, s_2^\ast) \geq U_2(s_1^\ast, s_2) \text{ for all } s_2
            \end{aligned}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Relationship Between Game Theory and Reinforcement Learning - Introduction}
    \begin{block}{Game Theory}
        The study of mathematical models of strategic interactions among rational decision-makers. 
        Key components include:
        \begin{itemize}
            \item Players
            \item Strategies
            \item Payoffs
        \end{itemize}
    \end{block}
    
    \begin{block}{Reinforcement Learning (RL)}
        A subfield of machine learning focused on how agents take actions in an environment to maximize cumulative rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Interconnection Between Game Theory and RL}
    \begin{enumerate}
        \item \textbf{Guiding RL Algorithm Design}
            \begin{itemize}
                \item Game theory aids in designing RL algorithms for multi-agent environments.
                \item Concepts like Nash Equilibrium and Pareto Efficiency help create optimal strategies.
            \end{itemize}
        \item \textbf{Informed Strategy Learning}
            \begin{itemize}
                \item RL can utilize game-theoretic ideas for adapting strategies against other agents.
                \item Agents can predict opponent behavior using Nash Equilibria.
            \end{itemize}
        \item \textbf{Simulating Game Environments}
            \begin{itemize}
                \item RL algorithms model environments as games, using game-theoretic frameworks.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Examples Illustrating the Relationship}
    \begin{enumerate}
        \item \textbf{Nash Equilibrium}
            \begin{itemize}
                \item In games like Prisoner's Dilemma, RL explores strategies to converge to Nash Equilibrium.
                \item Optimal strategies ensure no player can benefit by changing their strategy unilaterally.
            \end{itemize}
        \item \textbf{Zero-Sum Games}
            \begin{itemize}
                \item RL learns optimal mixed strategies in zero-sum games.
                \item Strategies adapt based on the opponent’s behavior while incorporating game-theoretic solutions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Key Points and Conclusion}
    \begin{itemize}
        \item Game theory enriches RL algorithm design with principles of rational decision-making.
        \item Effective multi-agent RL requires understanding strategic interactions.
        \item Mixed strategies and equilibria guide the design and evaluation of RL agents.
    \end{itemize}
    
    In conclusion, the interplay between game theory and reinforcement learning leads to more refined algorithms that navigate the complexities of multi-agent environments.
\end{frame}

\begin{frame}[fragile]{Code Snippet: Q-learning in a Zero-Sum Game}
    \begin{lstlisting}[language=Python]
# Simple pseudo-code for Q-learning in a zero-sum game context
initialize Q-table
for episode in range(num_episodes):
    state = reset_environment()
    done = False
    while not done:
        action = select_action(state)  # Based on epsilon-greedy policy
        next_state, reward, done = environment.step(action)
        # Update Q-value using the Bellman equation
        Q[state][action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state][action])
        state = next_state
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Games in Game Theory - Overview}
    Game theory studies strategic interactions among rational decision-makers in various fields, such as economics and political science. The types of games can be categorized based on the nature of conflict and cooperation:
    
    \begin{enumerate}
        \item Zero-Sum Games
        \item Cooperative Games
        \item Non-Cooperative Games
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Games in Game Theory - Zero-Sum Games}
    \begin{block}{Definition}
        In a zero-sum game, one player's gain is balanced by the losses of other players, keeping the total payoff constant.
    \end{block}
    
    \begin{exampleblock}{Example}
        \textbf{Tennis Match:} If Player A wins \$10 from Player B, Player B loses \$10, maintaining a total change in wealth of zero.
    \end{exampleblock}
    
    \begin{itemize}
        \item Perfect competition is a hallmark (players directly oppose each other).
        \item Strategies often involve mixed strategies to keep opponents uncertain.
    \end{itemize}

    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            & Player B (C) & Player B (D) \\
            \hline
            Player A (A) & +10 (A wins) & -10 (A loses) \\
            \hline
            Player A (B) & -10 (A loses) & +10 (A wins) \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Games in Game Theory - Cooperative and Non-Cooperative Games}
    \begin{block}{Cooperative Games}
        Players can benefit by forming coalitions and collaborating for better outcomes.
    \end{block}
    
    \begin{exampleblock}{Example}
        \textbf{Project Collaboration:} Teams of developers can work together, share resources, and produce a superior product.
    \end{exampleblock}
    
    \begin{itemize}
        \item Focus on collective payoff.
        \item Players negotiate agreements on resource sharing or payoff distribution.
    \end{itemize}

    \begin{block}{Non-Cooperative Games}
        Players make decisions independently, without collaboration.
    \end{block}
    
    \begin{exampleblock}{Example}
        \textbf{Prisoner's Dilemma:} Two players are interrogated separately, and their optimal choice depends on the decision of the other.
    \end{exampleblock}
    
    \begin{itemize}
        \item Emphasis on individual strategies.
        \item Can lead to suboptimal results for all involved.
    \end{itemize}

    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            & Player B (Betray) & Player B (Silent) \\
            \hline
            Player A (Betray) & -1, -1 & -3, 0 \\
            \hline
            Player A (Silent) & 0, -3 & -2, -2 \\
            \hline
        \end{tabular}
    \end{table}   
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Algorithms Relevant to Games} 
    \begin{block}{Overview of Reinforcement Learning (RL)}
        Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment, receiving feedback in the form of rewards or penalties to maximize cumulative rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key RL Algorithms Applied in Games - Part 1}
    \begin{enumerate}
        \item \textbf{Q-Learning}
        \begin{itemize}
            \item A value-based off-policy RL approach that learns the value of taking a given action in a specific state through the Bellman equation.
            \item Update formula:
            \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
            \end{equation}
            \item Key components:
            \begin{itemize}
                \item $s$: Current state
                \item $a$: Action taken
                \item $r$: Reward received
                \item $s'$: Next state
                \item $\alpha$: Learning rate
                \item $\gamma$: Discount factor
            \end{itemize}
            \item \textbf{Example:} In a grid-based game, an agent learns to navigate towards a goal while avoiding obstacles.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key RL Algorithms Applied in Games - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Deep Q-Networks (DQN)}
        \begin{itemize}
            \item An extension of Q-Learning using deep learning techniques to approximate the Q-value function.
            \item Combines Q-Learning with a deep neural network to handle complex scenarios (e.g., playing Atari games).
            \item Stabilizes learning using experience replay and target networks.
            \item \textbf{Illustration:} The DQN architecture processes game screens and outputs Q-values for actions.
        \end{itemize}
        
        \item \textbf{Policy Gradient Methods}
        \begin{itemize}
            \item Optimize the policy directly instead of learning value functions, giving probabilities of actions given states.
            \item Key equation:
            \begin{equation}
                \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla \log \pi_\theta(a_t | s_t) R(\tau) \right]
            \end{equation}
            \item \textbf{Example:} In a real-time strategy game, agents adjust strategies over time to improve gameplay based on past experiences.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Exploration vs. Exploitation:} Balancing exploration of new actions and exploiting known rewarding actions is crucial for learning efficiency.
            \item \textbf{Real-World Applications:} RL algorithms are effectively applied in various games, demonstrating their capability in dynamic environments.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        Understanding these foundational RL algorithms empowers developers and researchers to create intelligent agents that adapt strategies in gaming environments, influencing gameplay design significantly.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        This overview of Q-Learning, Deep Q-Networks, and Policy Gradient Methods highlights diverse approaches to enhance AI in games, leading us to explore their practical applications in game development.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of RL in Game Development - Overview}
    \begin{block}{Overview}
        Reinforcement Learning (RL) has transformed game development by creating intelligent agents that provide dynamic experiences. It enhances gameplay through characters and bots that learn from their environment.
    \end{block}
    \begin{itemize}
        \item RL enables responsive and engaging gameplay.
        \item We will explore notable case studies of RL applications in commercial games and simulations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of RL in Game Development - Case Studies}
    \begin{enumerate}
        \item \textbf{AlphaGo by DeepMind}
            \begin{itemize}
                \item \textbf{Overview:} Designed to play Go, a complex board game.
                \item \textbf{Technique Used:} Deep Q-Network (DQN) with policy gradient methods.
                \item \textbf{Outcome:} Defeated world champion Lee Sedol in 2016.
                \item \textbf{Key Takeaway:} RL excels in complex environments.
            \end{itemize}

        \item \textbf{OpenAI's Dota 2 Bot}
            \begin{itemize}
                \item \textbf{Overview:} OpenAI Five played the real-time strategy game Dota 2.
                \item \textbf{Technique Used:} Proximal Policy Optimization (PPO).
                \item \textbf{Outcome:} Agents showed coordination and decision-making against professionals.
                \item \textbf{Key Takeaway:} RL is effective for teamwork in strategy games.
            \end{itemize}

        \item \textbf{Ubisoft's Ghost Recon AI}
            \begin{itemize}
                \item \textbf{Overview:} Integrated RL into NPC behavior in Ghost Recon.
                \item \textbf{Technique Used:} Multi-agent reinforcement learning.
                \item \textbf{Outcome:} NPCs displayed enhanced strategy and teamwork.
                \item \textbf{Key Takeaway:} RL improves NPC realism and player experience.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of RL in Game Development - Key Points}
    \begin{itemize}
        \item \textbf{Dynamic Learning:} RL systems learn and adapt from experiences unlike traditional AI.
        \item \textbf{Improved Gameplay Experience:} More challenging and unpredictable game behaviors enhance engagement.
        \item \textbf{Complex Environment Handling:} RL excels in environments with numerous variables and strategies.
    \end{itemize}
    \begin{block}{Conclusion}
        RL revolutionizes game AI, creating intelligent agents that provide enhanced player experiences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of RL in Game Development - Additional Resources}
    \begin{itemize}
        \item \textbf{YouTube Lectures:} Explore AI in games via DeepMind's channel.
        \item \textbf{Research Papers:} Understanding algorithms like DQN and PPO enhances knowledge of RL's foundational principles.
    \end{itemize}

    By understanding these applications, we appreciate RL’s profound impact on the future of game development and AI's role in interactive entertainment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Implementing RL in Games - Overview}
    Key challenges faced while applying reinforcement learning (RL) techniques in gaming environments include:
    \begin{enumerate}
        \item Sample Efficiency
        \item Exploration vs. Exploitation
        \item Non-stationary Environments
        \item High Dimensionality
        \item Reward Design
        \item Real-Time Constraints
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Implementing RL in Games - Sample Efficiency and Exploration}
    \begin{block}{1. Sample Efficiency}
        RL often requires a large number of interactions with the environment to learn effective policies.
        \begin{itemize}
            \item High computational costs
            \item Time constraints in game development cycles
        \end{itemize}
        \textbf{Example:} Training an RL agent for chess may require thousands of games for learning, whereas human players learn faster.
    \end{block}
    
    \begin{block}{2. Exploration vs. Exploitation}
        Balancing exploration of new strategies against the exploitation of known strategies is crucial.
        \begin{itemize}
            \item Too much exploration can hinder convergence.
            \item Over-exploitation may stop the discovery of better tactics.
        \end{itemize}
        \textbf{Example:} An RL agent may rely on a successful strategy in a shooting game, missing out on potentially superior tactics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Implementing RL in Games - Non-stationary Environments, Dimensionality, and Reward Design}
    \begin{block}{3. Non-stationary Environments}
        Dynamics in games can change over time, complicating learning for RL agents.
        \begin{itemize}
            \item Continuous adaptation and retraining is necessary.
        \end{itemize}
        \textbf{Example:} Opponent strategies in multiplayer games shift, making learned policies less effective.
    \end{block}
    
    \begin{block}{4. High Dimensionality}
        Games typically have vast state and action spaces which can hinder effective learning.
        \begin{itemize}
            \item Proper state representation is crucial.
        \end{itemize}
        \textbf{Example:} In an open-world game, agents face complex states, complicating learning without dimensionality reduction.
    \end{block}

    \begin{block}{5. Reward Design}
        Defining appropriate rewards is challenging and can lead to unintended agent behaviors.
        \begin{itemize}
            \item Poor reward design can sabotage learning goals.
        \end{itemize}
        \textbf{Example:} Rewarding speed alone in racing may lead to reckless driving behavior.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Implementing RL in Games - Real-Time Constraints and Conclusion}
    \begin{block}{6. Real-Time Constraints}
        Games often necessitate real-time decision-making.
        \begin{itemize}
            \item RL algorithms must produce quick, effective responses.
        \end{itemize}
        \textbf{Example:} An RL agent in a racing game must react within milliseconds to changes in the environment.
    \end{block}

    \begin{block}{Conclusion}
        Understanding these challenges is crucial for developing robust RL systems in gaming environments.
        \begin{itemize}
            \item Effective RL implementations require consideration of sample efficiency, exploration strategies, and real-time performance.
            \item Proper reward design is essential for guiding agents toward desired behaviors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-Agent Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Definition}: Multi-Agent Systems (MAS) involve multiple agents interacting within an environment to pursue individual or collective goals.
        \item \textbf{Relevance in Games}: Essential framework for understanding dynamics such as cooperation and competition among players.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning in MAS}
    \begin{itemize}
        \item \textbf{Concept}: Agents learn from their actions and the actions of others, leading to complex interactions shaped by cooperation, competition, and adversarial behavior.
        \item \textbf{Key Terms}:
            \begin{itemize}
                \item \textbf{Agent}: An entity that perceives its environment and takes actions.
                \item \textbf{State}: The current situation of the agent in the environment.
                \item \textbf{Action}: The set of choices available to the agent.
                \item \textbf{Reward}: Feedback signal received after taking an action.
                \item \textbf{Policy}: A strategy defining the actions an agent should take in various states.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of RL in MAS}
    \begin{enumerate}
        \item \textbf{Cooperative Learning}:
            \begin{itemize}
                \item \textbf{Example}: Team-based strategy games where agents optimize joint tactics based on team performance.
            \end{itemize}

        \item \textbf{Competitive Learning}:
            \begin{itemize}
                \item \textbf{Example}: Chess engines anticipating opponents' moves and reinforcing winning strategies.
            \end{itemize}

        \item \textbf{Adversarial Learning}:
            \begin{itemize}
                \item \textbf{Example}: Fighting games where agents counter players' moves by learning from past encounters.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Complex Interactions}: Non-stationary environments due to multiple agents lead to continually evolving strategies.
        \item \textbf{Scalability}: Multi-agent learning techniques must address increased complexity with more agents.
        \item \textbf{Communication}: Some systems use protocols to enable information sharing among agents for improved performance.
    \end{itemize}
    \begin{block}{Essential Formula}
        \[
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
        \]
    \end{block}
    \textbf{Conclusion}: Multi-Agent Reinforcement Learning provides insights into strategy development and dynamics in multi-agent environments. \\
    \textbf{Next Step}: Explore practical examples of RL in multiplayer games!
\end{frame}

\begin{frame}[fragile]{Practical Examples of RL in Multiplayer Games - Overview}
    \begin{itemize}
        \item Reinforcement Learning (RL) enhances gameplay experiences in multiplayer games.
        \item Key applications: 
        \begin{itemize}
            \item Player strategies
            \item Bot training
            \item Adaptive difficulty systems
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Practical Examples of RL in Multiplayer Games - Player Strategies}
    \begin{block}{1. Player Strategies}
        \begin{itemize}
            \item **Concept**: RL helps develop effective strategies by simulating choices and learning from rewards or penalties.
            \item **Example**: In *StarCraft II*, RL trains agents to optimize resource management and troop deployment through trial and error.
            \item **Illustration**: Players' choices (attack, defend, scout) lead to different outcomes, and RL algorithms evaluate these to reinforce successful strategies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Practical Examples of RL in Multiplayer Games - Bot Training and Adaptive Difficulty}
    \begin{block}{2. Bot Training}
        \begin{itemize}
            \item **Concept**: Bots trained with RL behave like human players, adapting based on the game state.
            \item **Example**: *Dota 2*'s OpenAI Five used RL to play at a high level, improving skills through gameplay simulations.
            \item **Key Point**: RL allows bots to adapt to human strategies, enhancing player engagement.
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Adaptive Difficulty Systems}
        \begin{itemize}
            \item **Concept**: Adaptive difficulty adjusts game challenges based on player performance using RL.
            \item **Example**: In *Left 4 Dead*, RL changes the number and aggressiveness of zombies in real-time based on player performance.
            \item **Formula**: 
                \[
                D' = D + k \cdot (P - T)
                \]
                Where \(D\) is the difficulty level, \(P\) is player performance, \(T\) is target performance, and \(k\) is a responsiveness constant.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in RL and Game Development - Introduction}
    \begin{block}{Emerging Trends}
        - Advancements in deep reinforcement learning are creating sophisticated agents that learn complex strategies.
        - Models like AlphaGo and OpenAI Five exemplify the power of combining deep learning with RL.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in RL and Game Development - Key Trends}
    \begin{enumerate}
        \item \textbf{Personalized Gaming Experiences}
            \begin{itemize}
                \item Adaptive Difficulty Adjustment: Games dynamically adjust challenge levels based on player performance.
                \item Tailored NPCs: Non-player characters learn from interactions, becoming more realistic.
            \end{itemize}
        \item \textbf{Procedural Content Generation}
            \begin{itemize}
                \item RL can create diverse game worlds, allowing unique experiences for each playthrough.
            \end{itemize}
        \item \textbf{Enhanced Game Testing}
            \begin{itemize}
                \item RL agents simulate player behavior to identify bugs faster than traditional methods.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in RL and Game Development - Challenges & Applications}
    \begin{block}{Challenges to Overcome}
        - Training Time and Cost: Extensive computational resources and time are needed.
        - Safety and Control: Ensuring RL agents do not exploit game mechanics is essential.
        - Data Efficiency: RL often requires vast amounts of data, posing a challenge in limited environments.
    \end{block}
    \begin{block}{Future Applications Beyond Gaming}
        - Game Design: Integrating RL into design workflows for better engagement metrics.
        - Simulation Training: Applications in realistic training environments, like military training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Illustration}
    \begin{block}{Key Takeaway Points}
        - The integration of RL is set to revolutionize game development, enhancing engagement and responsiveness.
        - Overcoming challenges is feasible with the evolution of computing power and algorithms.
    \end{block}
    \begin{block}{Illustration Suggestion}
        - A flowchart showing the interaction between RL agents, game environments, and player feedback is recommended.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-Learning Update Rule}
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    \end{equation}
    \begin{itemize}
        \item Where:
        \item \( Q(s, a) \) = estimated utility of taking action \( a \) in state \( s \)
        \item \( r \) = reward received after executing action \( a \)
        \item \( \gamma \) = discount factor
        \item \( \alpha \) = learning rate
    \end{itemize}
\end{frame}

\begin{frame}{Class Lab: Implementing a Simple RL Agent}
    Hands-on session where students implement a basic RL agent in a game environment.
\end{frame}

\begin{frame}{Objectives}
    \begin{enumerate}
        \item \textbf{Understanding Reinforcement Learning (RL)}: Grasp the fundamental concepts of RL, particularly in the context of developing agents for game environments.
        \item \textbf{Hands-On Experience}: Gain practical experience by implementing a basic RL agent using a game simulation framework.
        \item \textbf{Skill Development}: Build skills in coding, debugging, and optimizing RL algorithms.
    \end{enumerate}
\end{frame}

\begin{frame}{Key Concepts - Reinforcement Learning Basics}
    \begin{itemize}
        \item \textbf{Agent}: The learner or decision-maker (the RL agent).
        \item \textbf{Environment}: Everything the agent interacts with (the game).
        \item \textbf{State (s)}: The current situation of the agent.
        \item \textbf{Action (a)}: Choices the agent can make.
        \item \textbf{Reward (r)}: Feedback from the environment based on the agent's action.
        \item \textbf{Policy (π)}: A strategy employed by the agent that defines the action to take in each state.
    \end{itemize}
\end{frame}

\begin{frame}{Key Concepts - Q-learning Algorithm}
    A popular RL algorithm used to estimate the value of actions in states. The core update rule is:
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
    \end{equation}
    Where:
    \begin{itemize}
        \item $Q(s, a)$: Current action-value function.
        \item $\alpha$: Learning rate (how much to update).
        \item $r$: Reward received.
        \item $\gamma$: Discount factor (importance of future rewards).
        \item $s'$: New state after taking action $a$.
    \end{itemize}
\end{frame}

\begin{frame}{Implementation Steps}
    \begin{enumerate}
        \item \textbf{Environment Setup}:
        \begin{itemize}
            \item Choose a simple game environment (e.g., Grid World, Tic-Tac-Toe).
            \item Install necessary libraries (e.g., OpenAI Gym, Pygame).
        \end{itemize}

        \item \textbf{Initial Agent Creation}:
        \begin{itemize}
            \item Define the agent’s interaction with the environment (state space, action space).
            \item Initialize Q-values (use a zero matrix or random values).
        \end{itemize}

        \item \textbf{Implement Q-learning Logic}:
        \begin{itemize}
            \item Create a loop for episodes (iterations in the game).
            \item For each episode:
            \begin{itemize}
                \item Reset the environment.
                \item For each step in the episode:
                \begin{itemize}
                    \item Choose an action (e.g., ε-greedy strategy).
                    \item Take the action, observe the reward and next state.
                    \item Update the Q-values using the Q-learning formula.
                \end{itemize}
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Experiment and Optimize}:
        \begin{itemize}
            \item Run multiple episodes to allow the agent to learn.
            \item Adjust hyperparameters: learning rate ($\alpha$), discount factor ($\gamma$), and exploration rate ($\epsilon$).
            \item Visualize the learning curve (e.g., average reward per episode).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Example Code Snippet}
\begin{lstlisting}[language=Python]
import numpy as np
import random

# Initialize parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate
num_episodes = 1000

# Initialize Q-table
Q = np.zeros((num_states, num_actions))

for episode in range(num_episodes):
    state = reset_environment()
    done = False
    while not done:
        # Choose action based on epsilon-greedy strategy
        if random.uniform(0, 1) < epsilon:
            action = random.choice(range(num_actions))  # Explore
        else:
            action = np.argmax(Q[state])  # Exploit
        
        next_state, reward, done = take_action(state, action)  # Interact with environment
        # Update Q-value
        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
        state = next_state  # Move to next state
\end{lstlisting}
\end{frame}

\begin{frame}{Key Takeaways}
    \begin{itemize}
        \item Reinforcement Learning provides a mechanism for agents to learn optimal strategies by maximizing cumulative rewards.
        \item Implementing a basic RL agent offers insights into practical applications of these theories and their implementation challenges.
        \item By experimenting with various parameters, students can observe the effects of adjustments on agent performance.
    \end{itemize}
\end{frame}

\begin{frame}{Encouragement for Exploration}
    After this lab, reflect on:
    \begin{itemize}
        \item Real-world applications of RL in gaming and beyond.
        \item Challenges faced during implementation and how they were addressed.
    \end{itemize}
    This lab session sets a foundational context for exploring more complex RL concepts and techniques in game development moving forward.
\end{frame}

\begin{frame}[fragile]{Reflections and Learnings}
    % Slide introduction
    As we conclude our exploration of Reinforcement Learning (RL) in games, it’s critical to reflect on how these concepts apply to game development and the experiences we've had.
\end{frame}

\begin{frame}[fragile]{Understanding Reinforcement Learning in Games}
    \frametitle{Understanding Reinforcement Learning in Games}
    % Key Concepts
    \begin{itemize}
        \item \textbf{Agent and Environment}
        \begin{itemize}
            \item \textbf{Agent}: Learns to make decisions (e.g., player character).
            \item \textbf{Environment}: The game world for interactions and feedback.
        \end{itemize}
        \item \textbf{Reward Mechanism}
        \begin{itemize}
            \item Positive Reinforcement: Rewards for desirable behaviors.
            \item Negative Reinforcement: Penalties for undesirable actions.
        \end{itemize}
        \item \textbf{Exploration vs. Exploitation}
        \begin{itemize}
            \item Exploration: Trying new strategies.
            \item Exploitation: Leveraging known successful strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Practical Applications and Challenges}
    \frametitle{Practical Applications and Challenges}
    \begin{block}{Practical Applications}
        \begin{itemize}
            \item \textbf{Game Characters}: RL enhances NPC behaviors during gameplay.
            \item \textbf{Dynamic Difficulty Adjustment}: Adjusts difficulty in real-time based on performance.
            \item \textbf{Example: AlphaGo}: Demonstrates RL's power by defeating top players in Go.
        \end{itemize}
    \end{block}

    \begin{block}{Challenges Encountered}
        \begin{itemize}
            \item \textbf{Training Time}: Requires significant resources for RL agents.
            \item \textbf{Sample Efficiency}: High performance often demands extensive interactions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Reflective Questions and Key Points}
    \frametitle{Reflective Questions for Students}
    \begin{enumerate}
        \item What strategies did you find effective in your RL implementation?
        \item What challenges did you face? How did you address them?
        \item Where else can you identify applications of RL in gaming or other industries?
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Reflecting solidifies understanding of RL applications in games.
            \item Discussing challenges fosters critical thinking and problem-solving.
            \item RL principles have broader implications beyond gaming.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Mathematical Foundation}
    \frametitle{Mathematical Foundation: Bellman Equation}
    % Code snippet
    \begin{equation}
        Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q(s', a')
    \end{equation}
    This equation provides a conceptual foundation for understanding how actions impact future rewards in RL.
\end{frame}

\begin{frame}[fragile]{Conclusion}
    \frametitle{Conclusion}
    This reflection is an evaluation of past work and a way to build upon knowledge gained from implementing RL agents. Harness this knowledge to innovate in the field of gaming!
\end{frame}

\begin{frame}[fragile]{Assessment: Game and RL Integration}
    \begin{block}{Overview of Assessment Tasks}
        This section outlines the assessment components designed to evaluate your understanding of how Reinforcement Learning (RL) principles can be effectively integrated into game development. The assessment will gauge both theoretical knowledge and practical application.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Assessment Objectives}
    \begin{enumerate}
        \item \textbf{Understanding Core Concepts}: Demonstrate comprehension of key RL concepts such as agents, environments, states, actions, and rewards.
        \item \textbf{Application in Game Design}: Analyze how RL can enhance gameplay dynamics and decision-making processes in games.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Assessment Components}
    \begin{enumerate}
        \item \textbf{Research Paper (40\% of the grade)}:
            \begin{itemize}
                \item \textbf{Task}: Choose a game that utilizes RL techniques (e.g., AlphaGo, OpenAI's Gym).
                \item \textbf{Focus}: Discuss the RL algorithms used, their real-world applications, challenges faced during implementation, and the impact on game performance.
                \item \textbf{Outcome}: A 5-7 page paper demonstrating research and analysis.
            \end{itemize}
        
        \item \textbf{Practical Implementation (40\% of the grade)}:
            \begin{itemize}
                \item \textbf{Task}: Develop a simple game prototype that employs an RL algorithm.
                \item \textbf{Example}: Implement a grid world using Q-learning where an agent learns to navigate towards a goal while avoiding obstacles.
                \item \textbf{Technical Requirement}: Provide clear code documentation and explain your choice of RL algorithm.
            \end{itemize}
        
        \item \textbf{Presentation (20\% of the grade)}:
            \begin{itemize}
                \item \textbf{Task}: Present your findings and the game prototype to the class.
                \item \textbf{Guidelines}: Include the rationale behind your design choices, how RL influenced gameplay, and any results from user testing.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Key Points in Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Reinforcement Learning Basics}:
            \begin{itemize}
                \item \textbf{Agent}: The learner or decision-maker.
                \item \textbf{Environment}: The external system with which the agent interacts.
                \item \textbf{State (s)}: The current situation of the agent.
                \item \textbf{Action (a)}: Choices available to the agent.
                \item \textbf{Reward (r)}: Feedback from the environment based on the agent's action.
            \end{itemize}
            
        \item \textbf{Q-learning Formula}:
            \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
            \end{equation}
            where:
            \begin{itemize}
                \item \( \alpha \) = learning rate
                \item \( \gamma \) = discount factor
                \item \( r \) = reward received after taking action \( a \)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        The integration of RL into games significantly enhances player experience and engagement. By completing these assessments, you will deepen your understanding of RL concepts while developing practical skills in game development.
    \end{block}
    
    \begin{block}{Next Steps}
        Prepare to discuss your research and prototypes during the upcoming presentation session, and don't hesitate to reach out if you have questions or need further clarification on any aspects of the assessments!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Overview of Key Concepts}
    \begin{enumerate}
        \item \textbf{Reinforcement Learning Fundamentals}
            \begin{itemize}
                \item RL is an area of Machine Learning where agents learn decision-making from feedback.
                \item Key components include:
                    \begin{itemize}
                        \item \textbf{Agent}: The learner or decision maker.
                        \item \textbf{Environment}: The interactive system.
                        \item \textbf{Actions}: Choices made by the agent.
                        \item \textbf{Rewards}: Feedback based on actions.
                        \item \textbf{States}: Situations of the environment.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Temporal Difference Learning}
            \begin{itemize}
                \item Learns to predict future rewards by updating value functions from new experiences.
                \item Example: Winning in games reinforces positive past actions.
            \end{itemize}
        
        \item \textbf{Markov Decision Processes (MDPs)}
            \begin{itemize}
                \item Mathematical framework for decision-making under uncertainty.
                \item Defined by:
                    \begin{itemize}
                        \item States (S)
                        \item Actions (A)
                        \item Transition function (T)
                        \item Reward function (R)
                        \item Discount factor ($\gamma$)
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Integration of RL in Games}
    \begin{itemize}
        \item \textbf{Integration of RL in Games}
            \begin{itemize}
                \item Creates adaptive AI learning from player behaviors.
                \item Examples:
                    \begin{itemize}
                        \item NPCs adapting tactics to player strategies.
                        \item Dynamic difficulty adjustments based on performance.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Key Points to Emphasize}
            \begin{itemize}
                \item Versatility of RL across various fields: robotics, finance, etc.
                \item Importance of reward structures on learning efficiency.
                \item Hands-on projects enable practical application of RL concepts.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Code Snippet Example}
    \begin{block}{Code Snippet: Q-learning Approach}
    \begin{lstlisting}[language=Python]
import numpy as np

# Initialize parameters
Q = np.zeros((state_space_size, action_space_size))  # Q-table
alpha = 0.1  # Learning Rate
gamma = 0.9  # Discount Factor
epsilon = 0.1  # Exploration Rate

# Learning loop
for episode in range(num_episodes):
    state = env.reset()
    done = False
    while not done:
        if np.random.rand() < epsilon:  # Exploration
            action = np.random.choice(action_space)
        else:  # Exploitation
            action = np.argmax(Q[state, :])
        
        next_state, reward, done = env.step(action)
        
        # Update Q-value
        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])
        
        state = next_state
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Conclusion}
        This week has highlighted the potential of Reinforcement Learning in games, establishing a foundational knowledge for further applications.
    \end{block}
    
    \begin{block}{Q\&A Session}
        Please feel free to ask about any concepts requiring clarification or their application in your projects.
    \end{block}
\end{frame}


\end{document}