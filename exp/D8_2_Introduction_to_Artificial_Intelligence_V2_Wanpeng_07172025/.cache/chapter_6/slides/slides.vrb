\frametitle{Markov Decision Processes (MDPs) - Rewards and Policies}
\begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Rewards (R)}:
            \begin{itemize}
                \item \textbf{Definition:} A function that assigns a numerical value (reward) based on the state-action pair.
                \item \textbf{Notation:} \( R(s, a) \), expected reward after taking action \( a \) in state \( s \).
                \item \textbf{Example:} Capturing a piece may provide a positive reward, while losing a piece yields a negative reward.
            \end{itemize}

        \item \textbf{Policies (π)}:
            \begin{itemize}
                \item \textbf{Definition:} A mapping from states to actions defining the decision-maker's behavior.
                \item \textbf{Notation:} \( π(s) \) denotes the action chosen in state \( s \) according to policy \( π \).
                \item \textbf{Example:} A chess strategy that favors aggressive moves over defensive ones.
            \end{itemize}
    \end{enumerate}
