\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 7: AI Model Training \& Evaluation]{Week 7: AI Model Training \& Evaluation}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to AI Model Training \& Evaluation - Overview}
    
    \begin{block}{Overview}
        Artificial Intelligence (AI) models are integral to technology, powering applications from image recognition to natural language processing. 
        Understanding the training and evaluation processes is crucial for ensuring model effectiveness and reliability.
    \end{block}    

\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to AI Model Training \& Evaluation - Key Concepts}

    \begin{enumerate}
        \item \textbf{Model Training}
        \begin{itemize}
            \item \textbf{Definition}: The process of teaching an AI model to recognize patterns in data.
            \item \textbf{Process}:
            \begin{itemize}
                \item Data Collection
                \item Preprocessing
                \item Training Algorithms (e.g., Gradient Descent)
            \end{itemize}
        \end{itemize}

        \item \textbf{Model Evaluation}
        \begin{itemize}
            \item \textbf{Definition}: Assessing a model's performance on unseen data.
            \item \textbf{Methods}:
            \begin{itemize}
                \item Training vs. Validation Split
                \item Metrics such as accuracy, precision, recall, F1-score, ROC-AUC
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to AI Model Training \& Evaluation - Examples \& Key Points}

    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Training Example}: A neural network model trained for image classification learns patterns to distinguish classes.
            \item \textbf{Evaluation Example}: Testing the model using new images to compute accuracy in label prediction.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Quality of training data significantly affects performance.
            \item Overfitting occurs when a model learns noise instead of underlying patterns.
            \item Cross-validation helps mitigate overfitting and improves performance estimation.
        \end{itemize}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to AI Model Training \& Evaluation - Formulas and Code Snippets}

    \begin{block}{Loss Function Example}
        For regression tasks, the Mean Squared Error (MSE) can be used:
        \begin{equation}
        \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^2
        \end{equation}
        where \( y_{i} \) are actual values and \( \hat{y}_{i} \) are predicted values.
    \end{block}

    \begin{block}{Python Snippet for Training a Model}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

# Load data
data = pd.read_csv('data.csv')
X = data.drop('target', axis=1)
y = data['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Evaluate model
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy}')
        \end{lstlisting}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Introduction}
    In this section, we aim to establish a strong understanding of AI model training and evaluation practices. By the end of this lesson, you should be able to:
    \begin{enumerate}
        \item \textbf{Understand Different Training Methods:}
        \begin{itemize}
            \item Comprehend the various techniques used to train AI models, including supervised, unsupervised, and reinforcement learning.
            \item Recognize when to use each method based on the nature of the problem at hand.
        \end{itemize}
        \item \textbf{Familiarize Yourself with Key Performance Evaluation Metrics:}
        \begin{itemize}
            \item Learn the most common metrics for assessing model performance, such as accuracy, precision, recall, F1-score, and ROC-AUC.
            \item Understand the importance of these metrics in selecting the best model for deployment.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Key Concepts Explained}
    \begin{enumerate}
        \item \textbf{Training Methods:}
        \begin{itemize}
            \item \textbf{Supervised Learning:} Involves training a model on a labeled dataset; e.g., predicting house prices based on features.
            \item \textbf{Unsupervised Learning:} Used with unlabeled data; e.g., clustering customers based on purchasing behavior.
            \item \textbf{Reinforcement Learning:} Involves trial and error; e.g., teaching a robot to navigate obstacles via rewards.
        \end{itemize}
        \item \textbf{Performance Evaluation Metrics:}
        \begin{itemize}
            \item \textbf{Accuracy:} 
            \[
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
            \]
            \item \textbf{Precision:} The ratio of true positives to the sum of true positives and false positives.
            \item \textbf{Recall:} The ratio of true positives to the sum of true positives and false negatives.
            \item \textbf{F1-score:} The harmonic mean of precision and recall.
            \item \textbf{ROC-AUC:} The area under the Receiver Operating Characteristic curve summarizing performance across thresholds.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Examples and Conclusion}
    \begin{itemize}
        \item \textbf{Supervised Learning Example:} Predicting if an email is spam based on features like keywords. For instance, if the model predicts 80 out of 100 emails correctly, it has an accuracy of 80%.
        \item \textbf{Evaluation Metrics Example:} 
        \begin{itemize}
            \item If a spam filter identified 70 spam emails correctly (true positives), marked 10 legitimate emails as spam (false positives), and missed 20 actual spam emails (false negatives):
            \begin{itemize}
                \item \textbf{Precision} and \textbf{Recall} can be calculated to measure performance.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    By the conclusion of this section, you should have a solid understanding of AI model training methods and how performance is evaluated.
\end{frame}

\begin{frame}[fragile]
    \frametitle{AI Model Training Process - Overview}
    \begin{block}{Overview}
        The training of an AI model involves systematic steps that help the model recognize patterns in data and make predictions. This process is essential for understanding the fundamentals of AI and machine learning.
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item High-quality data is critical—garbage in leads to garbage out.
            \item Model performance assessment should include real-world applicability.
            \item Continuous learning and adaptation are vital for maintaining relevance in AI.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{AI Model Training Process - Steps}
    \begin{enumerate}
        \item \textbf{Data Collection}
            \begin{itemize}
                \item Gather relevant datasets (e.g., housing prices data).
            \end{itemize}

        \item \textbf{Data Preprocessing}
            \begin{itemize}
                \item Cleaning and normalization (e.g., handling outliers).
                \item Example Code Snippet:
                \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(raw_data)
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Feature Selection/Engineering}
            \begin{itemize}
                \item Determine and create relevant features for model training.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{AI Model Training Process - Continued Steps}
    \begin{enumerate}[resume]
        \item \textbf{Model Selection}
            \begin{itemize}
                \item Choose appropriate models (e.g., Decision Trees, Neural Networks).
            \end{itemize}

        \item \textbf{Training the Model}
            \begin{itemize}
                \item Adjust parameters using training data, usually via Gradient Descent:
                \begin{equation}
                \theta = \theta - \alpha \frac{\partial J(\theta)}{\partial \theta}
                \end{equation}
            \end{itemize}

        \item \textbf{Validation and Testing}
            \begin{itemize}
                \item Evaluate with validation datasets and assess accuracy with test datasets.
            \end{itemize}

        \item \textbf{Model Deployment}
            \begin{itemize}
                \item Integrate the model into production and monitor continuously.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Types of AI Models}
    \begin{block}{Overview of AI Models}
        Artificial Intelligence (AI) encompasses various approaches to enable machines to mimic human decision-making. The main types of AI models include:
    \end{block}
    \begin{enumerate}
        \item Supervised Learning
        \item Unsupervised Learning
        \item Reinforcement Learning
    \end{enumerate}
\end{frame}

\begin{frame}{1. Supervised Learning}
    \begin{block}{Definition}
        Supervised learning involves training a model on a labeled dataset, where each training example is paired with an output label.
    \end{block}
    
    \begin{block}{How it Works}
        \begin{itemize}
            \item The model learns the relationship between inputs and outputs using training data.
            \item A loss function evaluates the model's predictions against actual labels, guiding adjustments during training.
        \end{itemize}
    \end{block}

    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Classification:} Email spam detection (spam or not spam)
            \item \textbf{Regression:} Predicting house prices based on features (size, location)
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Point}
        Requires large amounts of labeled data, which can be time-consuming to gather.
    \end{block}
\end{frame}

\begin{frame}{2. Unsupervised Learning}
    \begin{block}{Definition}
        Unsupervised learning deals with unlabeled data. The model learns the structure or distribution of the data without explicit guidance.
    \end{block}
    
    \begin{block}{How it Works}
        \begin{itemize}
            \item The model groups similar data points, identifying patterns and relationships in the data.
        \end{itemize}
    \end{block}
    
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Clustering:} Customer segmentation in marketing
            \item \textbf{Dimensionality Reduction:} PCA to reduce data complexity
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Point}
        Can reveal insights in datasets without labels but may require human interpretation for practical applications.
    \end{block}
\end{frame}

\begin{frame}{3. Reinforcement Learning}
    \begin{block}{Definition}
        Reinforcement learning is inspired by behavioral psychology, where an agent interacts with an environment and learns to make decisions by receiving rewards or penalties.
    \end{block}

    \begin{block}{How it Works}
        \begin{itemize}
            \item The agent explores various actions and learns from feedback (rewards/penalties).
            \item The goal is to maximize cumulative rewards over time, adapting its strategy based on experience.
        \end{itemize}
    \end{block}

    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Game Playing:} AlphaGo learns to play Go by playing millions of games against itself.
            \item \textbf{Robotics:} Teaching a robot to navigate space by rewarding it for reaching goals.
        \end{itemize}
    \end{block}
  
    \begin{block}{Key Point}
        Powerful for sequential decision-making problems, but the process can be resource-intensive and time-consuming.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding these types of AI models is crucial for choosing the appropriate approach based on the problem's nature and available data. Each model type has unique strengths, weaknesses, and applications in the real world.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example}
    Here’s a simple example of a supervised learning algorithm using Python and Scikit-learn for classification:
    \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Load dataset
data = load_iris()
X, y = data.data, data.target

# Split the dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train a RandomForest model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Predict on test data
predictions = model.predict(X_test)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Overview}
    \begin{block}{Introduction}
        Data preparation is a critical step in the AI model training process. This stage ensures that the data used for training and testing AI models is clean, normalized, and split appropriately to enhance model performance and accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Importance of Data Cleaning}
    \begin{itemize}
        \item \textbf{Definition}: Identifying and correcting inaccuracies in the dataset, including handling missing values, removing duplicates, and fixing inconsistencies.
        \item \textbf{Why It's Important}:
        \begin{itemize}
            \item Improves accuracy by ensuring the model learns from high-quality data.
            \item Enhances training efficiency as a clean dataset reduces complexity during training.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Example}
        If a dataset contains incorrect entries like "n/a," “200,” or duplicates in user ages, these errors can skew the learning process. Cleaning these anomalies helps in better model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Normalization of Data}
    \begin{itemize}
        \item \textbf{Definition}: Scaling individual data points to have a similar range, usually between 0 and 1 or -1 and 1.
        \item \textbf{Why It’s Important}:
        \begin{itemize}
            \item Facilitates convergence for algorithms like gradient descent.
            \item Prevents feature dominance from variables with larger ranges.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Common Normalization Techniques}
        \begin{equation}
            x' = \frac{x - \text{min}(X)}{\text{max}(X) - \text{min}(X)}
        \end{equation}
        for Min-Max Scaling.
        
        \begin{equation}
            z = \frac{x - \mu}{\sigma}
        \end{equation}
        for Z-score Normalization (Standardization).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Splitting Data}
    \begin{itemize}
        \item \textbf{Definition}: Dividing the dataset into two parts:
        \begin{itemize}
            \item \textbf{Training Set}: Used to train the model (e.g., 80\% of the data).
            \item \textbf{Testing Set}: Used to evaluate the model's performance (e.g., 20\% of the data).
        \end{itemize}
        \item \textbf{Why It’s Important}:
        \begin{itemize}
            \item Prevents overfitting by ensuring the model generalizes well to new data.
            \item Provides a fair assessment reflecting true model performance.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Example}
        In a dataset of 1,000 images, 800 images might be used to train a model, with 200 reserved for evaluating its accuracy and generalization ability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation - Key Points and Conclusion}
    \begin{itemize}
        \item Data preparation is foundational to successful AI model performance.
        \item Quality data leads to better insights, predictions, and decisions.
        \item Use appropriate techniques for cleaning, normalizing, and splitting your dataset.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Effective data preparation is crucial for the success of AI models. Focusing on cleaning, normalization, and strategic splitting ensures robust training and accurate evaluations.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Training Algorithms - Introduction}
    \begin{block}{Introduction to Training Algorithms}
        In this section, we will explore \textbf{training algorithms}, which are essential for optimizing AI models to perform specific tasks. The choice of training algorithm can significantly impact the model's performance, speed, and convergence.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Training Algorithms - Key Concepts}
    \begin{itemize}
        \item \textbf{Training Algorithm}: The method used to adjust the model parameters based on the training data to minimize the difference between predicted and actual outcomes.
        \item \textbf{Gradient Descent}: The most widely-used training algorithm, which iteratively updates parameters in the direction opposite to the gradient of the loss function.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Gradient Descent Explained}
    \begin{block}{What it Does}
        Gradient descent aims to minimize the \textbf{loss function} (cost function), quantifying how well the model's predictions match the actual data.
    \end{block}
    \begin{block}{How it Works}
        \begin{itemize}
            \item Start with an initial set of parameters (weights).
            \item Calculate the gradient of the loss function concerning each parameter.
            \item Update the parameters: 
            \begin{equation}
                \theta = \theta - \alpha \nabla J(\theta)
            \end{equation}
            Where:
            \begin{itemize}
                \item $\theta$: model parameters
                \item $\alpha$: learning rate
                \item $\nabla J(\theta)$: gradient of the loss function
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Gradient Descent - Learning Rate}
    \begin{block}{Learning Rate ($\alpha$)}
        A hyperparameter that determines the step size for each update:
        \begin{itemize}
            \item If $\alpha$ is too large, it may overshoot.
            \item If $\alpha$ is too small, convergence may take too long.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Variants of Gradient Descent}
    \begin{enumerate}
        \item \textbf{Batch Gradient Descent}: Uses the entire dataset to compute the gradient. Can be slow for large datasets.
        \item \textbf{Stochastic Gradient Descent (SGD)}: Updates parameters using one data point at a time, speeding up training but introducing noise.
        \item \textbf{Mini-batch Gradient Descent}: Combines both, where gradients are computed over small batches, balancing efficiency and stability.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Gradient Descent - Example Illustration}
    Imagine you're trying to find the lowest point on a hilly landscape (the loss function). Gradient descent helps you take steps downhill towards the valley (optimal parameters). The steepness of the slope guides your next step, making it crucial to choose an appropriate learning rate.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Quick Code Snippet}
    Here’s a simplified example of gradient descent in Python:
    \begin{lstlisting}[language=Python]
def gradient_descent(X, y, theta, learning_rate, iterations):
    for i in range(iterations):
        predictions = X.dot(theta)
        errors = predictions - y
        gradient = X.T.dot(errors) / len(y)
        theta -= learning_rate * gradient
    return theta
    \end{lstlisting}
    This code updates the parameters $\theta$ iteratively based on the calculated gradient from the training data $X$ and target $y$.
\end{frame}

\begin{frame}
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Training algorithms, particularly gradient descent, are vital for model optimization.
        \item Understanding the different variants of gradient descent can help choose the best approach for your specific dataset and problem.
        \item Always monitor the learning rate, as it plays a critical role in the convergence of the training process.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Part 1}

    \begin{block}{Understanding Hyperparameters}
        \textbf{Definition:}
        Hyperparameters are settings or configurations that dictate the behavior of an AI model during training. Unlike model parameters, which are learned from the training data, hyperparameters are set before the training process begins and remain constant for the duration of the training.
    \end{block}

    \begin{block}{Role in Model Training}
        \begin{itemize}
            \item \textbf{Control Complexity:} Helps regulate the model's complexity, impacting generalization.
            \item \textbf{Efficiency:} Influences training time, optimization speed, and overall performance.
        \end{itemize}
    \end{block}

    \begin{block}{Common Hyperparameters}
        \begin{itemize}
            \item \textbf{Learning Rate:} Step size at each iteration. A smaller rate ensures precise convergence but may increase training time.
            \item \textbf{Batch Size:} Number of training examples in one iteration; larger batch sizes require more memory.
            \item \textbf{Epochs:} Number of complete passes through the dataset; more epochs can enhance performance but risk overfitting.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Part 2}

    \begin{block}{Examples of Hyperparameter Tuning}
        \begin{itemize}
            \item \textbf{Learning Rate Tuning:}
                \begin{itemize}
                    \item \textit{Underfitting:} Low learning rate may hinder meaningful learning.
                    \item \textit{Overfitting:} High learning rate can lead to quick convergence on suboptimal solutions.
                \end{itemize}
            \item \textbf{Batch Size:}
                \begin{itemize}
                    \item \textit{Example:} Batch size of 32 may lead to faster convergence than 256 but risks noisy updates.
                \end{itemize}
            \item \textbf{Grid Search:} Methodical tuning approach testing all combinations of hyperparameter values.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Part 3}

    \begin{block}{Example Code Snippet (Grid Search Implementation)}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Defining the model
model = RandomForestClassifier()

# Hyperparameter grid to search through
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Performing Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, 
                           cv=3, scoring='accuracy')
grid_search.fit(X_train, y_train)

print(f"Best hyperparameters: {grid_search.best_params_}")
        \end{lstlisting}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Importance: Proper tuning can drastically improve model performance.
            \item Trade-offs: Balance training time and model accuracy is essential.
            \item Tools: Use libraries like Scikit-learn or TensorFlow for built-in tuning functionalities.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics - Overview}
    \begin{block}{Overview}
        Evaluation metrics are crucial for assessing the performance of AI models. They provide quantifiable ways to determine how well your model is performing its intended tasks, allowing for informed decisions on model adjustments and improvements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics - Key Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item \textbf{Definition}: The ratio of correctly predicted instances to the total instances.
                \item \textbf{Formula}:
                \begin{equation}
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
                \end{equation}
                \item \textbf{Example}: If a model correctly predicts 90 out of 100 instances, accuracy is 90\%.
            \end{itemize}
        
        \item \textbf{Precision}
            \begin{itemize}
                \item \textbf{Definition}: Measures the accuracy of positive predictions.
                \item \textbf{Formula}:
                \begin{equation}
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
                \end{equation}
                \item \textbf{Example}: If 50 out of 70 predicted positives are true, precision is:
                \begin{equation}
                \text{Precision} \approx 0.71 \text{ or } 71\%
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics - Continuation}
    \begin{enumerate}[resume]
        \item \textbf{Recall (Sensitivity)}
            \begin{itemize}
                \item \textbf{Definition}: Measures the ability to find all relevant cases (actual positives).
                \item \textbf{Formula}:
                \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
                \end{equation}
                \item \textbf{Example}: If a model identifies 80 out of 100 actual positives, recall is:
                \begin{equation}
                \text{Recall} = 0.80 \text{ or } 80\%
                \end{equation}
            \end{itemize}

        \item \textbf{F1 Score}
            \begin{itemize}
                \item \textbf{Definition}: The harmonic mean of precision and recall.
                \item \textbf{Formula}:
                \begin{equation}
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                \end{equation}
                \item \textbf{Example}: For precision 0.71 and recall 0.80:
                \begin{equation}
                F1 \approx 0.75 \text{ or } 75\%
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Overview}
    \begin{block}{What is a Confusion Matrix?}
        A \textbf{confusion matrix} is a performance evaluation tool used in machine learning to assess the impact of a classification model's predictions. It provides a summary of the correct and incorrect predictions made by the model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Structure}
    \begin{block}{Structure of a Confusion Matrix}
        The confusion matrix is a table with four key components:
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline
                & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
                \hline
                \textbf{Actual Positive} & True Positive (TP) & False Negative (FN) \\
                \hline
                \textbf{Actual Negative} & False Positive (FP) & True Negative (TN) \\
                \hline
            \end{tabular}
        \end{center}
        \begin{itemize}
            \item \textbf{True Positive (TP)}: Correctly predicted positive cases.
            \item \textbf{True Negative (TN)}: Correctly predicted negative cases.
            \item \textbf{False Positive (FP)}: Incorrectly predicted positive cases (Type I error).
            \item \textbf{False Negative (FN)}: Incorrectly predicted negative cases (Type II error).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Key Metrics}
    \begin{block}{Key Metrics Derived from a Confusion Matrix}
        \begin{enumerate}
            \item \textbf{Accuracy}: Measures the overall correctness of the model.
            \[
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
            \]
            \item \textbf{Precision}: Indicates how many of the predicted positives were actually positive.
            \[
            \text{Precision} = \frac{TP}{TP + FP}
            \]
            \item \textbf{Recall (Sensitivity)}: Measures how well the model identifies positive cases.
            \[
            \text{Recall} = \frac{TP}{TP + FN}
            \]
            \item \textbf{F1 Score}: Harmonizes precision and recall into a single metric.
            \[
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \]
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Example}
    \begin{block}{Example: Email Classification}
        Consider a binary classification model that predicts whether an email is spam or not:
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline
                & \textbf{Predicted: Spam} & \textbf{Predicted: Not Spam} \\
                \hline
                \textbf{Actual: Spam} & 80 (TP) & 10 (FN) \\
                \hline
                \textbf{Actual: Not Spam} & 5 (FP) & 105 (TN) \\
                \hline
            \end{tabular}
        \end{center}
        \begin{itemize}
            \item \textbf{TP}: 80 emails correctly flagged as spam.
            \item \textbf{FP}: 5 emails incorrectly flagged as spam.
            \item \textbf{TN}: 105 emails correctly identified as not spam.
            \item \textbf{FN}: 10 emails incorrectly identified as not spam.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Summary of Metrics}
    \begin{block}{Calculate Key Metrics}
        Using these values:
        \begin{itemize}
            \item \textbf{Accuracy} = \( \frac{80 + 105}{80 + 10 + 5 + 105} = 0.925 \) or 92.5\%
            \item \textbf{Precision} = \( \frac{80}{80 + 5} = 0.941 \) or 94.1\%
            \item \textbf{Recall} = \( \frac{80}{80 + 10} = 0.889 \) or 88.9\%
            \item \textbf{F1 Score} = \( 2 \times \frac{0.941 \times 0.889}{0.941 + 0.889} = 0.914 \) or 91.4\%
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item A confusion matrix is essential for understanding the performance of classification models.
            \item It allows the identification of specific errors, enabling targeted model improvement.
            \item Metrics derived from the confusion matrix summarize model performance comprehensively.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Using a confusion matrix empowers data scientists to gain insights into their models beyond mere accuracy, guiding them in making informed decisions for model enhancements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation - Understanding Cross-Validation}
    
    \begin{block}{Definition}
        Cross-validation is a statistical method used to evaluate the performance of a machine learning model by partitioning the original training dataset into multiple subsets, training the model on some subsets, and validating it on others.
    \end{block}
    
    \begin{itemize}
        \item Ensures model generalization to unseen data.
        \item Detects overfitting and provides reliable performance estimates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation - Importance of Techniques}
    
    \begin{enumerate}
        \item \textbf{Mitigates Overfitting:}
            \begin{itemize}
                \item Assesses model performance across different data subsets.
            \end{itemize}
        
        \item \textbf{Provides Reliable Estimates:}
            \begin{itemize}
                \item Utilizes multiple iterations for a stable estimate.
            \end{itemize}
        
        \item \textbf{Utilizes Data Efficiently:}
            \begin{itemize}
                \item Maximizes data use, especially with limited datasets.
            \end{itemize}
        
        \item \textbf{Improves Model Selection:}
            \begin{itemize}
                \item Facilitates objective comparison of multiple models.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation - Techniques and Example}
    
    \begin{block}{Common Cross-Validation Techniques}
        \begin{enumerate}
            \item \textbf{K-Fold Cross-Validation:}
                \begin{itemize}
                    \item Divides dataset into $k$ subsets.
                    \item Trains $k$ times with $k-1$ folds for training and 1 for validation.
                \end{itemize}
            
            \item \textbf{Stratified K-Fold Cross-Validation:}
                \begin{itemize}
                    \item Maintains class proportions for imbalanced datasets.
                \end{itemize}
            
            \item \textbf{Leave-One-Out CV (LOOCV):}
                \begin{itemize}
                    \item Each sample is used for testing while all others for training.
                \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X, y = data.data, data.target

# Initialize model
model = RandomForestClassifier()

# K-Fold Cross-Validation
kf = KFold(n_splits=5)

# Evaluate model
scores = cross_val_score(model, X, y, cv=kf)
print(f"Cross-Validation Scores: {scores}")
print(f"Mean Accuracy: {scores.mean()}")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Concept Overview}
    \begin{block}{Overfitting}
        \begin{itemize}
            \item Overfitting occurs when an AI model learns the training data too well, capturing noise and outliers.
            \item Result: High accuracy on training data but poor performance on unseen data.
        \end{itemize}
    \end{block}

    \begin{block}{Underfitting}
        \begin{itemize}
            \item Underfitting refers to a model that is too simple to capture the underlying trends in the data.
            \item Result: Poor performance on both training and validation datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Implications and Examples}
    \begin{block}{Implications}
        \begin{itemize}
            \item Overfitting: Lacks generalization, leading to poor predictions on new data.
            \item Underfitting: Fails to learn relevant patterns, resulting in inaccuracies.
        \end{itemize}
    \end{block}

    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Overfitting Example}: A model predicting housing prices becomes too tailored to training data.
            \item \textbf{Underfitting Example}: A model using only house size fails to consider important factors like location.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Key Points and Techniques}
    \begin{block}{Key Points}
        \begin{enumerate}
            \item Trade-off: Strike a balance to avoid both overfitting and underfitting.
            \item Model Evaluation: Use metrics like Mean Squared Error (MSE) to assess performance.
        \end{enumerate}
        \begin{equation}
            \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \end{equation}
    \end{block}

    \begin{block}{Mitigation Techniques}
        \begin{itemize}
            \item For Overfitting: Regularization, pruning, boosting, or ensemble methods.
            \item For Underfitting: Increase model complexity, add features, or improve data quality.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Evaluation Metrics - Overview}
    \begin{block}{Understanding Evaluation Metrics}
        Evaluation metrics are essential tools in assessing the performance of AI models. They quantify how well a model makes predictions and assist in improving these models through iterative training processes. The choice of evaluation metrics can dramatically influence decision-making and product success across various industries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}: Measures the proportion of correct predictions.
        \item \textbf{Precision}: Ratio of true positive predictions to total predicted positives; useful where false positives are costly.
        \item \textbf{Recall}: Ratio of true positives to actual positives; critical for applications where false negatives are vital.
        \item \textbf{F1 Score}: Harmonic mean of precision and recall, providing a balance between the two.
        \item \textbf{ROC AUC}: Evaluates a model's ability to distinguish between classes across various thresholds.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Examples}
    \begin{itemize}
        \item \textbf{Healthcare: Diagnosing Diseases}
        \begin{itemize}
            \item \textbf{Scenario}: Model for detecting diabetic retinopathy in eye scans.
            \item \textbf{Metrics Used}: Precision and Recall.
            \item \textbf{Application}: High recall ensures identification of most patients with retinopathy for treatment.
        \end{itemize}
        
        \item \textbf{Finance: Credit Scoring}
        \begin{itemize}
            \item \textbf{Scenario}: Predicting creditworthiness of loan applicants.
            \item \textbf{Metrics Used}: F1 Score.
            \item \textbf{Application}: Balancing precision and recall to minimize risk and maximize approvals.
        \end{itemize}
        
        \item \textbf{E-commerce: Recommendation Systems}
        \begin{itemize}
            \item \textbf{Scenario}: Suggesting products based on past behavior.
            \item \textbf{Metrics Used}: Mean Average Precision (MAP).
            \item \textbf{Application}: High MAP scores suggest relevance to users, enhancing sales and satisfaction.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    \begin{block}{Understanding Ethical Implications in AI Model Training and Evaluation}
        \begin{itemize}
            \item \textbf{Definition of AI Ethics}: Focuses on the moral implications and societal impact of AI technologies.
            \item \textbf{Importance}: Ensures responsible development and deployment, fostering trust and safety.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Ethical Issues}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item AI models can perpetuate or amplify biases in training data.
                \item \textbf{Example}: Hiring algorithms trained on biased historical data may favor one demographic over others.
                \item \textbf{Key Point}: Evaluate datasets for representation and fairness to mitigate bias.
            \end{itemize}
        \item \textbf{Transparency and Accountability}
            \begin{itemize}
                \item AI systems should be explainable to users, allowing understanding of decisions.
                \item \textbf{Example}: In finance, a model rejecting a loan should clarify reasons (e.g., credit score, income).
                \item \textbf{Key Point}: Incorporate interpretability tools to enhance model transparency.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Mitigating Risks}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Privacy Concerns}
            \begin{itemize}
                \item Models often require personal data for training, raising issues regarding user consent and data protection.
                \item \textbf{Example}: Facial recognition systems can pose significant privacy disputes if deployed without regulations.
                \item \textbf{Key Point}: Follow data protection regulations (e.g., GDPR) and prioritize user consent in data collection.
            \end{itemize}
        
        \item \textbf{Strategies for Mitigating Ethical Risks}
            \begin{itemize}
                \item Conduct fairness audits to assess models for bias.
                \item Implement feedback loops to continuously adapt and improve models.
                \item Engage stakeholders to identify ethical challenges during model development.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Group Activity - Analyzing AI Model Evaluation Metrics}
    \begin{block}{Objective}
        In this group activity, students will analyze evaluation metrics for a given AI model and discuss their implications for model performance, robustness, and ethical considerations. This exercise aligns with our course’s objectives to understand AI model evaluation and its impact.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts to Understand}
    \begin{itemize}
        \item \textbf{Model Evaluation Metrics:}
        \begin{itemize}
            \item \textbf{Accuracy:} Proportion of correctly predicted instances. 
            \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
            \end{equation}
            \item \textbf{Precision:} Proportion of true positive predictions in predicted positives. 
            \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Recall (Sensitivity):} Proportion of true positive predictions in actual positives.
            \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{F1 Score:} Harmonic mean of precision and recall.
            \begin{equation}
            \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{ROC Curve and AUC:} Measures trade-off between true and false positive rates.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity Steps}
    \begin{enumerate}
        \item \textbf{Group Formation:} Divide into small groups of 4-5 students. Each group is assigned a dataset and model information.
        \item \textbf{Data Review:} Examine provided model evaluation metrics (accuracy, precision, recall, F1 score, etc.).
        \item \textbf{Analysis Questions:}
        \begin{itemize}
            \item What do the evaluation metrics indicate about model performance?
            \item Are there trade-offs between accuracy and other metrics?
            \item What ethical implications arise from these metrics?
        \end{itemize}
        \item \textbf{Discussion:} Present findings to the class and engage in discussion about interpretation of model outputs.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Understanding evaluation metrics is crucial for effectiveness measurement of AI models.
        \item Trade-offs exist between different metrics; choices should align with application requirements.
        \item Ethical considerations must accompany numerical evaluation to prevent negative real-world impacts.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Overview of AI Model Training \& Evaluation}
    This chapter provided an in-depth exploration of the essential processes involved in training and evaluating AI models. Below, we recap the key points to reinforce your understanding and their relevance to the course objectives.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Key Concepts}
    \begin{enumerate}
        \item \textbf{AI Model Training}
        \begin{itemize}
            \item \textbf{Definition}: Teaching an AI model to make predictions or decisions based on data inputs.
            \item \textbf{Data Preparation}: Importance of clean, well-labeled data (e.g., labeled datasets for image recognition).
            \item \textbf{Training Phases}:
            \begin{itemize}
                \item Supervised Learning: Learning from labeled data.
                \item Unsupervised Learning: Finding patterns in unlabeled data.
                \item Reinforcement Learning: Learning through trial and error.
            \end{itemize}
        \end{itemize}

        \item \textbf{Evaluation Metrics}
        \begin{itemize}
            \item \textbf{Purpose}: Assess how well the model performs on unseen data.
            \item \textbf{Common Metrics}:
            \begin{itemize}
                \item Accuracy
                \item Precision and Recall
                \item F1 Score
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Models and Best Practices}
    \begin{enumerate}
        \setcounter{enumi}{2} % Resume numbering from the previous frame
        \item \textbf{Overfitting and Underfitting}
        \begin{itemize}
            \item Overfitting: Learning noise in the training data.
            \item Underfitting: Too simplistic to capture trends.
        \end{itemize}

        \item \textbf{Model Selection}
        \begin{itemize}
            \item Importance of choosing the right model for the problem type and data.
            \item Mention of recent models like GPT-4.
        \end{itemize}

        \item \textbf{Best Practices}
        \begin{itemize}
            \item Regular cross-validation for robustness.
            \item Techniques like data augmentation and dropout to mitigate overfitting.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions \& Discussion - Overview}
    \begin{block}{Engagement Focus}
        This slide invites students to engage actively in the topic of AI Model Training and Evaluation, fostering an environment for inquiry and deeper understanding. Encouraging questions and discussions enhances engagement and facilitates a collaborative learning atmosphere.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions \& Discussion - Key Concepts}
    \begin{itemize}
        \item \textbf{Model Training}
            \begin{itemize}
                \item \textbf{Definition}: Teaching an AI model to make predictions using a dataset.
                \item \textbf{Key Techniques}:
                    \begin{enumerate}
                        \item Supervised Learning: Training with labeled data.
                        \item Unsupervised Learning: Training without labeled data.
                    \end{enumerate}
                \item \textbf{Example}: Discuss learning to identify cats from dogs using images.
            \end{itemize}
        
        \item \textbf{Model Evaluation}
            \begin{itemize}
                \item \textbf{Definition}: Assessing model performance for accuracy and generalizability.
                \item \textbf{Evaluation Metrics}:
                    \begin{enumerate}
                        \item Accuracy
                        \item Precision and Recall
                        \item F1 Score
                    \end{enumerate}
                \item \textbf{Example}: Use a confusion matrix to demonstrate metrics.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions \& Discussion - Challenges and Innovations}
    \begin{itemize}
        \item \textbf{Common Challenges}
            \begin{itemize}
                \item Overfitting: Learning noise instead of patterns.
                    \begin{itemize}
                        \item \textbf{Solution}: Techniques like cross-validation or regularization.
                    \end{itemize}
                \item Underfitting: Model too simplistic.
                    \begin{itemize}
                        \item \textbf{Solution}: Adjust model complexity or features.
                    \end{itemize}
            \end{itemize}

        \item \textbf{Latest Innovations}
            \begin{itemize}
                \item Briefly mention recent advancements like ChatGPT/GPT-4.
                \item Discuss their impact on training and evaluation paradigms.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions \& Discussion - Engaging Students}
    \begin{block}{Example Questions}
        \begin{itemize}
            \item What factors influence the choice of evaluation metric for a model?
            \item Can you think of a real-world application where precision is more critical than accuracy?
            \item How might ethical considerations impact model training and evaluation?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions \& Discussion - Conclusion and Call to Action}
    \begin{block}{Conclusion}
        Encouraging open dialogue will solidify understanding of AI model training and evaluation, connecting theory with practical applications.
    \end{block}
    
    \begin{block}{Call to Action}
        \begin{itemize}
            \item \textbf{Voice Your Questions}: What aspects of AI model training and evaluation are still unclear?
            \item \textbf{Share Experiences}: Has anyone tried building an AI model? What challenges did you face?
        \end{itemize}
    \end{block}
\end{frame}


\end{document}