\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 8: Model Evaluation Techniques}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation Techniques}
    Model evaluation is a crucial step in the data mining process, allowing data scientists and analysts to assess the performance of their predictive models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Model Evaluation in Data Mining}
    \begin{block}{Importance of Model Evaluation}
        \begin{itemize}
            \item \textbf{Performance Assessment}: Evaluate model accuracy and effectiveness.
            \item \textbf{Model Comparison}: Compare different models to select the best one.
            \item \textbf{Avoiding Overfitting}: Identify models that perform poorly on unseen data.
            \item \textbf{Refinement and Improvement}: Highlight areas for improvement through continuous evaluation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics}
    Some fundamental evaluation metrics include:

    \begin{itemize}
        \item \textbf{Accuracy}:
        \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
        \end{equation}

        \item \textbf{Precision}:
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \end{equation}

        \item \textbf{Recall (Sensitivity)}:
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}

        \item \textbf{F1 Score}:
        \begin{equation}
            F1 \text{ Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}

        \item \textbf{ROC Curve \& AUC}: The curve plots true positive rate vs. false positive rate; the AUC measures overall performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Concluding Thoughts}
    Model evaluation is essential in ensuring that models meet desired performance criteria and can be trusted in real-world applications. 

    Understanding various metrics is foundational to mastering model evaluation, setting the stage for deeper insights in future discussions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation - Definition}
    \begin{block}{Definition of Model Evaluation}
        Model evaluation refers to the process of assessing the performance and effectiveness of predictive models developed through data mining techniques. This evaluation helps determine how well a model generalizes to unseen data, ensuring its accuracy and reliability in various applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation - Significance}
    \begin{block}{Significance of Model Evaluation in Data Mining}
        \begin{enumerate}
            \item \textbf{Model Performance Validation}: Provides insight into how accurately the model makes predictions, thus validating the efforts made during model training and tuning.
            \item \textbf{Guidance for Optimization}: Highlights areas of strong and weak performance, allowing for informed adjustments in features or algorithm selection.
            \item \textbf{Comparison of Multiple Models}: Offers quantitative measurements to compare the performances of various models generated for the same problem clearly.
            \item \textbf{Avoiding Overfitting}: Prevents overfitting by ensuring the model is robust in real-world scenarios and not just trained on historical data.
            \item \textbf{Decision-Making}: Supports critical decisions in business and research, influencing strategy, target audiences, and resource allocation.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Evaluation Metrics}: Different metrics (e.g., accuracy, precision, recall) provide unique insights into model performance.
            \item \textbf{Cross-Validation}: Techniques like k-fold cross-validation are essential for reliable model performance assessment.
            \item \textbf{Visualizations}: Tools such as ROC curves and confusion matrices visually communicate model performance for better stakeholder understanding.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation - Example Scenario}
    \begin{block}{Example Scenario}
        Imagine you have developed a classification model to predict whether an email is spam or not. By employing model evaluation techniques:
        \begin{itemize}
            \item You can determine accuracy by comparing predictions against a labeled dataset.
            \item For instance, if the model predicts 80 out of 100 emails correctly, the accuracy is:
            \begin{equation}
                \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}} = \frac{80}{100} = 0.80 \text{ or } 80\%
            \end{equation}
            \item Metrics like precision and recall will help assess how many predicted spam emails were actually spam and how many spam emails were correctly identified.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation - Conclusion}
    \begin{block}{Conclusion}
        Engaging in model evaluation ensures that the spam detection model is not only effective but also applicable in real-world settings. This foundational understanding will pave the way for exploring specific evaluation metrics, such as precision, in the next slide.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - What is Precision?}
    Precision is a key evaluation metric used in classification models to assess the quality of results. 
    Specifically, it measures how many of the predicted positive instances are actually positive.

    \begin{block}{Formula}
        \[
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \]
    \end{block}

    \begin{itemize}
        \item \textbf{True Positives (TP)}: Count of correctly predicted positive cases.
        \item \textbf{False Positives (FP)}: Count of incorrectly predicted positive cases.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Importance of Precision}
    Precision is crucial when the cost of false positives is high. 
    For instance, in medical diagnoses for a serious illness, a false positive might lead to unnecessary anxiety and additional invasive tests for a patient.

    \begin{itemize}
        \item Ensuring positive predictions are reliable is vital in high-stakes situations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Example}
    Consider a spam email classification system.
    \begin{itemize}
        \item \textbf{True Positives (TP)}: Legitimate spam emails accurately identified as spam (e.g., 80).
        \item \textbf{False Positives (FP)}: Legitimately important emails incorrectly classified as spam (e.g., 20).
    \end{itemize}
    
    Using the formula:
    \begin{equation}
        \text{Precision} = \frac{80}{80 + 20} = \frac{80}{100} = 0.80
    \end{equation}
    
    This indicates that 80\% of the emails flagged as spam are indeed spam, indicating a high precision in this classification model.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Key Points & Conclusion}
    \begin{itemize}
        \item High precision means fewer false positives; however, it might come at the cost of lower recall (which measures the ability to capture true positives).
        \item Particularly significant in domains where false positives may incur significant costs.
    \end{itemize}

    \textbf{Conclusion:}
    Precision is crucial in model evaluation, providing insights into the quality of predictions. Balancing precision with other metrics, such as recall, enables a comprehensive view of model performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - Definition}
    \begin{block}{Definition of Recall}
        Recall, also known as sensitivity or true positive rate, is a metric used to evaluate the performance of a classification model. It measures the proportion of actual positive cases that were correctly identified by the model. In other words, recall assesses how well the model captures all the relevant instances in a dataset.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - Calculation}
    \begin{block}{Calculation of Recall}
        Recall can be calculated using the following formula:
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
        \end{equation}
        
        Where:
        \begin{itemize}
            \item \textbf{True Positives (TP)}: The number of positive instances correctly identified by the model.
            \item \textbf{False Negatives (FN)}: The number of actual positive instances that the model incorrectly labeled as negative.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - Importance and Example}
    \begin{block}{Importance of Recall}
        Recall becomes especially crucial in scenarios where missing a positive instance (i.e., a false negative) carries significant consequences:
        \begin{itemize}
            \item \textbf{Medical Diagnosis:} Failing to identify a patient who is actually sick can lead to untreated health issues.
            \item \textbf{Spam Detection:} A false negative means a spam email is incorrectly classified as legitimate, exposing users to harmful content.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Calculation}
        Consider a dataset where:
        \begin{itemize}
            \item TP = 80 (correctly identified positive cases)
            \item FN = 20 (actual positive cases missed)
        \end{itemize}
        Using the recall formula:
        \begin{equation}
            \text{Recall} = \frac{80}{80 + 20} = \frac{80}{100} = 0.8 \, (\text{or} \, 80\%)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - Summary}
    \begin{block}{Summary}
        Recall is a vital metric for understanding the effectiveness of a classification model, especially when dealing with potentially harmful false negatives. 
        \begin{itemize}
            \item **High Recall:** Indicates effective identification of positive instances in critical applications (e.g., disease detection).
            \item **Trade-off with Precision:** Increasing recall may reduce precision, highlighting the need for balance.
            \item **Use Cases:** Particularly relevant in domains with high costs of false negatives, such as medical diagnostics, fraud detection, and security screening.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Introduction}
    \textbf{Introduction to the F1 Score}  
    The F1 score is a vital metric used to evaluate the performance of classification models, particularly in scenarios where class imbalance exists. It balances precision and recall, offering a single score to summarize performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Understanding Key Concepts}
    \begin{block}{Precision}
        \begin{itemize}
            \item The ratio of true positive predictions to the total positive predictions made by the model.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
            \end{equation}
            \item \textbf{Example}: If your model predicts 70 instances as positive, but only 50 of them are correct:
            \begin{equation}
                \text{Precision} = \frac{50}{70} = 0.71
            \end{equation}
        \end{itemize}
    \end{block}

    \begin{block}{Recall}
        \begin{itemize}
            \item The ratio of true positive predictions to the actual total positives in the dataset.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
            \end{equation}
            \item \textbf{Example}: If there are 100 actual positive instances, and the model correctly identifies 50:
            \begin{equation}
                \text{Recall} = \frac{50}{100} = 0.50
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Formula and Usage}
    \begin{block}{F1 Score Formula}
        The F1 score is calculated as the harmonic mean of precision and recall, providing a balance between the two metrics:
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
        \begin{itemize}
            \item \textbf{Intuition}: The F1 score is particularly useful when you care equally about precision and recall; when one is low, the overall score reflects that inadequacy.
        \end{itemize}
    \end{block}

    \begin{block}{When to Use the F1 Score}
        \begin{itemize}
            \item \textbf{Imbalanced Datasets}: Helps prevent misleading accuracy metrics.
            \item \textbf{Cost of False Negatives}: Emphasizes recall without neglecting precision.
            \item \textbf{Performance Measurement}: Combines both sensitivity (recall) and specificity (precision) into one convenient metric.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Key Points and Summary}
    \begin{itemize}
        \item The F1 score provides a balanced evaluation when precision and recall are equally important.
        \item Valuable in domains such as text classification, medical diagnostics, and fraud detection.
        \item Ranges from 0 to 1, where 1 indicates perfect precision and recall.
    \end{itemize}

    \begin{block}{Summary}
        The F1 score is crucial for evaluating models in the presence of imbalanced data where both precision and recall play pivotal roles. 
        Understanding and correctly applying the F1 score can significantly enhance model performance assessment, guiding improvements in predictive modeling tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix}
    \begin{block}{Understanding the Confusion Matrix}
        A **Confusion Matrix** is a fundamental tool used to evaluate the performance of a classification model. It visualizes actual vs. predicted classifications, helping to identify how well the model is performing and which errors it is making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of a Confusion Matrix}
    Consider a binary classification problem with outcomes:
    \begin{itemize}
        \item Positive Class (1)
        \item Negative Class (0)
    \end{itemize}
    
    The confusion matrix is structured as follows:

    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & \textbf{Actual Positive (1)} & \textbf{Actual Negative (0)} \\
        \hline
        \textbf{Predicted Positive (1)} & True Positive (TP) & False Positive (FP) \\
        \hline
        \textbf{Predicted Negative (0)} & False Negative (FN) & True Negative (TN) \\
        \hline
    \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Definitions and Metrics}
    \begin{block}{Key Definitions}
        \begin{itemize}
            \item \textbf{True Positive (TP)}: Correctly predicted positive observations.
            \item \textbf{True Negative (TN)}: Correctly predicted negative observations.
            \item \textbf{False Positive (FP)}: Incorrectly predicted positive observations (Type I Error).
            \item \textbf{False Negative (FN)}: Incorrectly predicted negative observations (Type II Error).
        \end{itemize}
    \end{block}

    \begin{block}{Precision and Recall}
        \begin{itemize}
            \item \textbf{Precision}:
            \[
            \text{Precision} = \frac{TP}{TP + FP}
            \]
            \item \textbf{Recall}:
            \[
            \text{Recall} = \frac{TP}{TP + FN}
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Email Classification}
    \begin{block}{Scenario}
        Consider an email spam detection model with the following outcomes:
        \begin{itemize}
            \item Actual positive (Spam): 80
            \item Actual negative (Not Spam): 20
        \end{itemize}
        
        After model prediction:
        \begin{itemize}
            \item True Positives (TP): 70
            \item True Negatives (TN): 15
            \item False Positives (FP): 5
            \item False Negatives (FN): 10
        \end{itemize}
    \end{block}

    \begin{block}{Confusion Matrix}
        \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            & \textbf{Spam (1)} & \textbf{Not Spam (0)} \\
            \hline
            \textbf{Predicted Spam (1)} & 70 & 5 \\
            \hline
            \textbf{Predicted Not Spam (0)} & 10 & 15 \\
            \hline
        \end{tabular}
        \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Calculating Metrics}
    \begin{block}{Calculations}
        \begin{itemize}
            \item \textbf{Precision}:
            \[
            \text{Precision} = \frac{70}{70 + 5} \approx 0.933
            \]
            \item \textbf{Recall}:
            \[
            \text{Recall} = \frac{70}{70 + 10} = 0.875
            \]
        \end{itemize}
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item The confusion matrix provides insight into model performance beyond accuracy.
            \item Precision and recall help understand the types of errors being made.
            \item Monitoring these metrics is essential for improving model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC and AUC - Overview}
    \begin{itemize}
        \item The Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC) are crucial for evaluating the performance of classification models, especially in binary tasks.
        \item They provide insights into the trade-offs between true positive rates (TPR) and false positive rates (FPR) across various thresholds.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC Curve - Understanding}
    \begin{itemize}
        \item \textbf{Definition}: Graphical representation of a binary classifier's performance as the discrimination threshold varies.
        \item \textbf{Axes}:
        \begin{itemize}
            \item \textbf{Y-Axis}: True Positive Rate (TPR), calculated as:
            \begin{equation}
                \text{TPR} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{X-Axis}: False Positive Rate (FPR), calculated as:
            \begin{equation}
                \text{FPR} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}
            \end{equation}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC Curve - Interpretation}
    \begin{itemize}
        \item \textbf{Ideal Point}: Top-left corner (TPR = 1, FPR = 0) indicates perfect classification.
        \item \textbf{Random Classifier}: A diagonal line from (0, 0) to (1, 1) represents random guessing; models should outperform this line.
        \item \textbf{Comparison of Models}: The farther a ROC curve is from the diagonal, the better the model's performance.
        
        \medskip
        \textbf{Example:}
        \begin{itemize}
            \item Threshold: 0.3 $\rightarrow$ TPR: 0.8, FPR: 0.1
            \item Threshold: 0.5 $\rightarrow$ TPR: 0.7, FPR: 0.2
            \item Threshold: 0.7 $\rightarrow$ TPR: 0.6, FPR: 0.3
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Area Under the Curve (AUC)}
    \begin{itemize}
        \item \textbf{Definition}: AUC quantifies overall classifier performance by calculating the area under the ROC curve.
        \item \textbf{Interpretation}:
        \begin{itemize}
            \item AUC = 1.0: Perfect model
            \item AUC = 0.5: No discriminative ability (random choice)
            \item AUC < 0.5: Model worse than random guessing
        \end{itemize}
        
        \medskip
        \textbf{Key Formula}: Calculate AUC using the trapezoidal rule by summing the areas of trapezoids formed between ROC curve points.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points on ROC and AUC}
    \begin{itemize}
        \item \textbf{Robustness}: ROC and AUC provide a robust evaluation metric, less impacted by class imbalance.
        \item \textbf{Threshold Selection}: Helps in selecting optimal thresholds for predictions based on the trade-off between sensitivity and specificity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Application - Example Code}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Assuming y_true is the true labels and y_scores is the predicted probabilities
fpr, tpr, thresholds = roc_curve(y_true, y_scores)
auc_value = roc_auc_score(y_true, y_scores)

plt.plot(fpr, tpr, label=f'AUC = {auc_value:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item ROC and AUC are essential for evaluating binary classification models.
        \item Understanding these metrics aids informed decisions on model performance and threshold settings, leading to better deployments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-Class Classification Metrics - Introduction}
    \begin{itemize}
        \item In multi-class classification, a model predicts one label from several classes.
        \item Evaluation metrics need to extend binary metrics like precision, recall, and F1 score to accommodate multiple classes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-Class Classification Metrics - Key Metrics}
    \begin{enumerate}
        \item \textbf{Precision}:
        \begin{itemize}
            \item Definition: \textit{Of all positive predictions, how many are actually positive?}
            \item Formula:
            \begin{equation} 
                \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} 
            \end{equation}
            \item Multi-Class Extensions:
            \begin{itemize}
                \item Micro-averaging:
                \begin{equation} 
                    \text{Micro-Precision} = \frac{\sum \text{TP}_i}{\sum (\text{TP}_i + \text{FP}_i)} 
                \end{equation}
                \item Macro-averaging:
                \begin{equation} 
                    \text{Macro-Precision} = \frac{1}{N} \sum \text{Precision}_i 
                \end{equation}
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Recall}:
        \begin{itemize}
            \item Definition: \textit{Of all actual positives, how many were correctly identified?}
            \item Formula:
            \begin{equation} 
                \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} 
            \end{equation}
            \item Multi-Class Extensions:
            \begin{itemize}
                \item Micro-Recall:
                \begin{equation} 
                    \text{Micro-Recall} = \frac{\sum \text{TP}_i}{\sum (\text{TP}_i + \text{FN}_i)} 
                \end{equation}
                \item Macro-Recall:
                \begin{equation} 
                    \text{Macro-Recall} = \frac{1}{N} \sum \text{Recall}_i 
                \end{equation}
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-Class Classification Metrics - F1 Score}
    \begin{itemize}
        \item \textbf{F1 Score}:
        \begin{itemize}
            \item Definition: Harmonic mean of precision and recall.
            \item Formula:
            \begin{equation} 
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} 
            \end{equation}
            \item Multi-Class Extensions:
            \begin{itemize}
                \item Micro-F1 Score:
                \begin{equation} 
                    \text{Micro-F1} = 2 \times \frac{\text{Micro-Precision} \times \text{Micro-Recall}}{\text{Micro-Precision} + \text{Micro-Recall}} 
                \end{equation}
                \item Macro-F1 Score:
                \begin{equation} 
                    \text{Macro-F1} = \frac{1}{N} \sum F1_i 
                \end{equation}
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item Use macro-averaging for balanced class importance.
            \item Use micro-averaging for class imbalance scenarios.
            \item Metrics provide a comprehensive view of model performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Best Practices - Overview}
    \begin{block}{Objective}
        Understand the essential practices for evaluating machine learning models, focusing on cross-validation techniques and effective performance reporting.
    \end{block}
    
    \begin{itemize}
        \item Importance of Model Evaluation
        \item Cross-Validation Techniques
        \item Performance Reporting Guidelines
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Importance of Model Evaluation}
    \begin{block}{Key Points}
        Model evaluation is critical to validate how well our model generalizes to unseen data. 
        \begin{itemize}
            \item Prevents overfitting
            \item Enhances prediction accuracy
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Cross-Validation Techniques}
    \begin{block}{Definition}
        Cross-validation is a robust method for assessing model performance by partitioning the training dataset into subsets.
    \end{block}
    
    \begin{itemize}
        \item k-Fold Cross-Validation
        \item Stratified Cross-Validation
        \item Leave-One-Out Cross-Validation (LOOCV)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Cross-Validation Techniques - k-Fold}
    \begin{block}{k-Fold Cross-Validation}
        \begin{itemize}
            \item Divides dataset into **k** subsets (or folds).
            \item Trains on **k-1** folds, validates on the remaining fold.
            \item Repeats **k** times, each fold is a validation set once.
            \item \textbf{Advantages}: Reliable estimate of performance, reduces reliance on single train/test split.
        \end{itemize}
        
        \textbf{Example}: If **k = 5**, split into 5 parts, train on 4 and validate on 1.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Cross-Validation Techniques - Stratified & LOOCV}
    \begin{block}{Stratified Cross-Validation}
        \begin{itemize}
            \item Useful for imbalanced datasets.
            \item Maintains class proportions in each fold.
            \item \textbf{Example}: 70\% Class A, 30\% Class B ratio in each fold.
        \end{itemize}
    \end{block}

    \begin{block}{Leave-One-Out Cross-Validation (LOOCV)}
        \begin{itemize}
            \item **k** equals the number of samples.
            \item Each sample acts as a validation set once.
            \item \textbf{Advantages}: Effective for small datasets.
            \item \textbf{Disadvantages}: Computationally expensive for large datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Performance Reporting Guidelines}
    \begin{block}{Key Metrics}
        Consistently reporting model performance using standardized metrics is key.
        \begin{itemize}
            \item \textbf{Accuracy}:
            \[
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Samples}}
            \]

            \item \textbf{Precision}:
            \[
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \]

            \item \textbf{Recall}:
            \[
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \]

            \item \textbf{F1 Score}:
            \[
            F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Performance Reporting Guidelines (cont.)}
    \begin{block}{Reporting Best Practices}
        \begin{itemize}
            \item Use confusion matrices to visualize predictions.
            \item Provide context (e.g., baseline models).
            \item Utilize multiple metrics for comprehensive evaluation.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Cross-validation reduces bias and variance.
            \item Accurate reporting aids stakeholder understanding.
            \item Align metrics with business goals and context.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Final Thoughts}
        Following best practices in model evaluation enables more reliable insights and supports the selection of the best-performing model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Overview}
    \begin{itemize}
        \item Summary of key metrics discussed.
        \item Importance of model evaluation in selecting the best-performing model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Metrics in Model Evaluation}
    \begin{itemize}
        \item Model evaluation ensures performance on unseen data.
        \item Key metrics include:
            \begin{itemize}
                \item Accuracy
                \item Precision
                \item Recall
                \item F1 Score
                \item ROC-AUC
                \item Mean Absolute Error
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics - Details}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item Definition: Ratio of correct predictions to total instances.
                \item Formula: 
                \[
                \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
                \]
            \end{itemize}
        \item \textbf{Precision}
            \begin{itemize}
                \item Definition: Correct positive predictions to total predicted positives.
                \item Formula: 
                \[
                \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                \]
            \end{itemize}
        \item \textbf{Recall}
            \begin{itemize}
                \item Definition: Correct positive predictions to all actual positives.
                \item Formula: 
                \[
                \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                \]
            \end{itemize}
        \item \textbf{F1 Score}
            \begin{itemize}
                \item Definition: Harmonic mean of precision and recall.
                \item Formula: 
                \[
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Key Metrics - Details}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{ROC-AUC}
            \begin{itemize}
                \item Definition: Performance measurement for classification problems, showing trade-off between true positive and false positive rates.
                \item Higher AUC indicates a better model (ideal = 1).
            \end{itemize}
        \item \textbf{Mean Absolute Error (MAE)}
            \begin{itemize}
                \item Definition: Average of absolute differences between predictions and actual values.
                \item Formula: 
                \[
                MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
                \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relevance in Model Selection}
    \begin{itemize}
        \item Balancing Metrics: Different metrics serve varying purposes; understand their context for informed model selection.
        \item Cross-Validation: Techniques like k-fold cross-validation helps evaluate performance robustly, reducing overfitting risks.
        \item Industry Application: Emphasis on specific metrics can depend on the application domain (e.g., recall in healthcare).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    \begin{itemize}
        \item Selecting evaluation metrics significantly impacts model performance assessment.
        \item Always consider the context, cost of errors, and the overall goal of your model.
    \end{itemize}
    \textbf{Thank you!}
\end{frame}


\end{document}