\frametitle{Advantage Estimation, Policy & Value Function Update}

    \begin{block}{3. Advantage Estimation}
        Compute the advantage function ($A$) for each action taken:
        \begin{equation}
            A_t = \delta_t + (\gamma \lambda) \delta_{t+1} + (\gamma \lambda^2) \delta_{t+2} + \ldots
        \end{equation}
        where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$.
    \end{block}

    \begin{block}{4. Policy Update}
        \begin{equation}
            L(\theta) = \hat{\mathbb{E}}_t \left[\min\left(\frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} A_t, \text{clip}\left(\frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}, 1 - \epsilon, 1 + \epsilon\right) A_t\right)\right]
        \end{equation}
        This ensures the new policy does not deviate too far from the old policy, maintaining stability during training.
    \end{block}

    \begin{block}{5. Value Function Update}
        Update the value function using the mean squared error loss:
        \begin{equation}
            L(\phi) = \hat{\mathbb{E}} \left[(V_\phi(s_t) - R_t)^2\right]
        \end{equation}
    \end{block}

    \begin{block}{6. Repeat}
        Return to step 2 and repeat the process for a predetermined number of iterations or until convergence.
    \end{block}
