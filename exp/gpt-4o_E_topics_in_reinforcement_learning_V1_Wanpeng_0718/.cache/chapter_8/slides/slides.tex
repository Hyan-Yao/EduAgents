\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 8: Mid-term Review and Examination}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Mid-term Review}
    \begin{block}{Description}
        Overview of the mid-term review process, including goals and expectations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of the Mid-term Review Process}
    The mid-term review is a critical stage in your learning journey, providing a structured opportunity to reflect on the first half of the course content and assess your understanding of key concepts in Reinforcement Learning (RL).
\end{frame}

\begin{frame}[fragile]
    \frametitle{Goals of the Mid-term Review}
    \begin{enumerate}
        \item \textbf{Consolidate Understanding}:
            Reinforce your comprehension of topics covered from Weeks 1 to 7, such as:
            \begin{itemize}
                \item Introduction to Reinforcement Learning
                \item Foundations of RL
                \item Markov Decision Processes (MDPs)
                \item Value Functions
                \item Basic Algorithms in RL
                \item SARSA
                \item Policy Gradient Methods
            \end{itemize}
        \item \textbf{Identify Knowledge Gaps}:
            Recognize areas where further study or clarification is required, enabling focused revision for the examination ahead.
        \item \textbf{Prepare for Assessment}:
            Equip yourself with the necessary knowledge and skills needed to perform well in the mid-term examination.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Expectations}
    \begin{itemize}
        \item \textbf{Be Engaged}: Actively participate in review sessions, ask questions, and collaborate with peers to enhance understanding.
        \item \textbf{Self-assessment}: Utilize practice questions and quizzes to evaluate your readiness for the exam.
        \item \textbf{Resource Utilization}: Review lecture notes, readings, and supplementary materials to ensure a comprehensive grasp of the content.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item This review is not just about memorization; it's about understanding and applying concepts.
        \item Collaboration with classmates can enhance your learning experience.
        \item Time management is crucial; allocate sufficient time for each topic.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Illustrations}
    \begin{block}{Example of MDP Application}
        Consider a simple game as an MDP where states represent different game positions, actions are the possible moves, and rewards are the outcomes of those moves. This understanding will deepen during the review.
    \end{block}
    
    \begin{block}{Illustration of Value Function}
        A diagram illustrating how value functions estimate the expected return (future rewards) from each state can solidify the concept. In equations, it might look like this:
        \begin{equation}
            V(s) = \sum_{s', r} P(s', r | s, a) [r + \gamma V(s')]
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Approaching the mid-term review with a positive mindset and organized strategy can significantly impact your performance in exams. Use this time to ask questions, clarify doubts, and engage deeply with the material learned over the past weeks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Topics Covered in Weeks 1-7 - Overview}
    \begin{itemize}
        \item Introduction to Reinforcement Learning (RL)
        \item Foundations of Reinforcement Learning
        \item Markov Decision Processes (MDPs)
        \item Value Functions
        \item Basic Algorithms in Reinforcement Learning
        \item SARSA (State-Action-Reward-State-Action)
        \item Policy Gradients
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Introduction to Reinforcement Learning}
    \begin{block}{Concept}
        Reinforcement Learning is a machine learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.
    \end{block}
    \begin{itemize}
        \item Agent-Environment Interaction: The agent acts within the environment and receives feedback in the form of rewards or penalties.
        \item Trial and Error: The agent improves its decision-making through experience over time.
    \end{itemize}
    \begin{example}
        A robot learning to navigate a maze by testing different paths and receiving positive reinforcement when it moves closer to the exit.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Foundations of Reinforcement Learning}
    \begin{block}{Concept}
        Core principles that form the basis of RL, including exploration vs. exploitation and the concept of reward.
    \end{block}
    \begin{itemize}
        \item Exploration vs. Exploitation: Balance between trying new strategies and using known rewarding strategies.
        \item Rewards: Instantaneous feedback from the environment that guides the agent's learning process.
    \end{itemize}
    \begin{figure}
        \centering
        % Here you would place a graph showing a trade-off between exploration and exploitation over time.
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Markov Decision Processes (MDPs)}
    \begin{block}{Concept}
        Mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of a decision-maker.
    \end{block}
    \begin{itemize}
        \item Components of MDP: State (S), Action (A), Transition Model (P), Reward Function (R), and Discount Factor ($\gamma$).
        \item Markov Property: Future state depends only on the current state.
    \end{itemize}
    \begin{example}
        A board game where a player's position (state) changes based on their dice roll (action).
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Value Functions}
    \begin{block}{Concept}
        Functions that estimate how good it is for an agent to be in a given state or to take a particular action in a given state.
    \end{block}
    \begin{itemize}
        \item State Value Function ($V(s)$): Represents the expected return from state $s$.
        \item Action Value Function ($Q(s,a)$): Represents the expected return from state $s$ after taking action $a$.
    \end{itemize}
    \begin{equation}
        V(s) = \sum_{a} \pi(a|s) \sum_{s',r} P(s', r | s, a) [r + \gamma V(s')]
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Basic Algorithms in Reinforcement Learning}
    \begin{block}{Concept}
        Fundamental algorithms that lay the groundwork for more advanced techniques.
    \end{block}
    \begin{itemize}
        \item Dynamic Programming: Using a model of the environment to compute value functions and optimal policies.
        \item Monte Carlo Methods: Learning directly from episodes of experience without requiring a model.
    \end{itemize}
    \begin{example}
        Using the Monte Carlo method to estimate the value of a state in a simple game.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{6. SARSA (State-Action-Reward-State-Action)}
    \begin{block}{Concept}
        An on-policy algorithm to estimate action values and improve policy based on the actions taken by the agent.
    \end{block}
    \begin{itemize}
        \item Update Rule: Updates the action value estimate based on the current state, action, reward received, next state, and next action.
        \item Formally:
        \begin{equation}
            Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma Q(s', a') - Q(s,a) \right]
        \end{equation}
    \end{itemize}
    \begin{example}
        A gamer adjusting strategies in real time based on immediate feedback from the game.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{7. Policy Gradients}
    \begin{block}{Concept}
        Methods that adjust the policy directly rather than the action value functions.
    \end{block}
    \begin{itemize}
        \item Stochastic Policy: Determines the probabilities of taking actions in a state.
        \item Advantages: Often used in complex environments where action spaces are large.
    \end{itemize}
    \begin{example}
        A robot learning to walk by adjusting its gait based on feedback from its movements.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item This mid-term review encapsulates the foundational elements of Reinforcement Learning covered in the first seven weeks.
        \item Each topic builds upon the last, creating a cohesive understanding necessary for upcoming complex concepts.
    \end{itemize}
    \begin{block}{Note}
        Ensure you review these concepts actively as they will be critical for understanding upcoming topics and for your mid-term examination!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives Review - Introduction}
    \begin{block}{Purpose}
        This slide serves as a recap of the primary learning objectives established at the beginning of the course, particularly as they relate to the content covered during Weeks 1-7. 
    \end{block}
    By synthesizing these objectives with the key concepts we've explored, we can better prepare for both the mid-term examination and future topics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 1}
    \begin{enumerate}
        \item \textbf{Understanding the Fundamentals of Reinforcement Learning:}
            \begin{itemize}
                \item \textbf{Explanation:} Grasp the core principles and terminology of reinforcement learning, including agents, actions, states, and rewards.
                \item \textbf{Example:} An agent (e.g., a robot) learns to navigate a maze by receiving positive rewards for reaching the exit and negative rewards for hitting walls.
            \end{itemize}

        \item \textbf{Modeling with Markov Decision Processes (MDPs):}
            \begin{itemize}
                \item \textbf{Explanation:} Learn to use MDPs to mathematically model decision-making situations where outcomes are partly random and partly under the control of a decision-maker.
                \item \textbf{Key Points:}
                    \begin{itemize}
                        \item States (S)
                        \item Actions (A)
                        \item Transition probabilities (P)
                        \item Rewards (R)
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continuation of the enumerate list
        \item \textbf{Calculating Value Functions:}
            \begin{itemize}
                \item \textbf{Explanation:} Understand how to calculate state-value and action-value functions to evaluate the desirability of states and actions within an MDP.
                \item \textbf{Formula:}
                \begin{equation}
                    V(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a)[R(s,a,s') + \gamma V(s')]
                \end{equation}
            \end{itemize}

        \item \textbf{Implementing Basic Algorithms:}
            \begin{itemize}
                \item \textbf{Explanation:} Implement core RL algorithms such as Dynamic Programming, SARSA, and Q-learning.
                \item \textbf{Example:} Using Q-learning to update the action-value function iteratively based on agent experiences in the environment.
                \item \textbf{Q-learning Update Rule:}
                \begin{equation}
                    Q(s, a) \leftarrow Q(s, a) + \alpha \Big(R + \gamma \max_{a'} Q(s', a') - Q(s, a)\Big)
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{4} % Continuation of the enumerate list
        \item \textbf{Exploring Policy Gradients:}
            \begin{itemize}
                \item \textbf{Explanation:} Learn to use policy gradient methods to directly parameterize and optimize the policy.
                \item \textbf{Key Points:} Focus on maximizing the expected cumulative reward using gradient ascent.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Summary}
        By connecting these learning objectives to the material covered in Weeks 1-7, we establish a comprehensive understanding of reinforcement learning principles that will not only aid in exam preparation but will also serve as foundational knowledge for advancing in this field.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tips for Mid-Term Success}
    \begin{itemize}
        \item \textbf{Review Key Terms:} Ensure you can explain the meaning and importance of terms we’ve discussed.
        \item \textbf{Practice Problems:} Work on example problems that require the application of algorithms and calculations, particularly for MDPs and value functions.
        \item \textbf{Understand the Concepts Deeply:} Rather than memorizing, aim to grasp how concepts interlink to form the larger reinforcement learning framework.
    \end{itemize}

    \begin{block}{Final Reminder}
        Prepare for the exam by revisiting these objectives and considering how they apply to your practice problems and theoretical questions!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Reinforcement Learning Concepts - Overview}
    \begin{itemize}
        \item Core concepts:
        \begin{itemize}
            \item Agents
            \item Environments
            \item Rewards
            \item Policies
            \item Exploration vs. Exploitation Dilemma
        \end{itemize}
        \item Understanding these concepts is essential for navigating the field of reinforcement learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Reinforcement Learning Concepts - Agents and Environments}
    \begin{block}{1. Agents}
        \textbf{Definition}: An agent is anything that can act in an environment. It makes decisions based on observations and interactions within the environment.
        \begin{itemize}
            \item \textbf{Example}: In a gaming context, the player (computer or human) controlling the game can be considered the agent.
        \end{itemize}
    \end{block}

    \begin{block}{2. Environments}
        \textbf{Definition}: The environment is everything that the agent interacts with. It provides feedback based on the agent’s actions.
        \begin{itemize}
            \item \textbf{Example}: In a self-driving car scenario, the road, other vehicles, and traffic signals make up the environment.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Reinforcement Learning Concepts - Rewards, Policies, and Exploration}
    \begin{block}{3. Rewards}
        \textbf{Definition}: Rewards are feedback signals indicating the success of an action taken by an agent. The agent aims to maximize its cumulative reward over time.
        \begin{itemize}
            \item \textbf{Illustration}: In a maze, reaching the exit might yield a positive reward (+10), while hitting a wall could result in a negative reward (-5).
        \end{itemize}
    \end{block}

    \begin{block}{4. Policies}
        \textbf{Definition}: A policy is a strategy used by the agent to determine the next action based on the current state of the environment. It can be deterministic or stochastic.
        \begin{itemize}
            \item \textbf{Example}: A deterministic policy might always choose to go right at a decision point, while a stochastic policy may have a 70\% chance to go right and a 30\% chance to go left.
        \end{itemize}
    \end{block}
    
    \begin{block}{5. Exploration vs. Exploitation Dilemma}
        \textbf{Definition}: This is the challenge of choosing between exploring new actions to discover their rewards (exploration) and using known actions that yield high rewards (exploitation).
        \begin{itemize}
            \item \textbf{Key Point}: Balancing exploration and exploitation is crucial for effective learning—too much exploration can lead to suboptimal performance, while too much exploitation can prevent finding better strategies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation of Exploration vs. Exploitation}
    The relationship between exploration and exploitation can be expressed mathematically using the epsilon-greedy strategy:
    
    \begin{equation}
        \text{Policy } (\pi): 
        \begin{cases}
            \text{Choose a random action (exploration) with probability } \epsilon \\
            \text{Choose the action that maximizes expected reward (exploitation) with probability } 1 - \epsilon
        \end{cases}
    \end{equation}
    
    \begin{itemize}
        \item This foundational structure supports various algorithms, which will be discussed in the next slide.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Important Algorithms - Overview}
    \begin{block}{Overview of Key Algorithms in Reinforcement Learning}
        In the realm of reinforcement learning (RL), foundational algorithms play pivotal roles in how agents learn to make decisions. 
        This section covers:
        \begin{itemize}
            \item Q-learning
            \item SARSA
            \item Policy Gradients
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Important Algorithms - Q-Learning}
    \begin{block}{Q-Learning}
        \textbf{Description:}  
        Q-learning is a model-free, off-policy algorithm that seeks to learn the value of actions in states. The core idea is to iteratively update a **Q-value** that estimates the maximum expected future rewards achievable from each state-action pair.

        \textbf{Key Formula:}
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
        \end{equation}
        
        \textbf{Example:}  
        Consider an agent navigating a maze. If it moves from position \( A \) to position \( B \) and receives a reward of 10, Q-learning will update the Q-value based on this new experience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Important Algorithms - SARSA}
    \begin{block}{SARSA (State-Action-Reward-State-Action)}
        \textbf{Description:}  
        SARSA is a model-free, on-policy algorithm which learns the value of the current policy. The agent learns Q-values based on the action it actually takes.

        \textbf{Key Formula:}
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
        \end{equation}
        
        \textbf{Example:}  
        In a tic-tac-toe game, if the agent moves to square 5 (receiving a reward of +1), SARSA updates Q-values based on that specific move rather than the best possible reward.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Important Algorithms - Policy Gradients}
    \begin{block}{Policy Gradients}
        \textbf{Description:}  
        Policy Gradient methods optimize the policy directly rather than the value function. These approaches are effective in high-dimensional action spaces and can handle stochastic policies.

        \textbf{Key Formula:}
        \begin{equation}
        \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
        \end{equation}
        
        \textbf{Example:}  
        In robotic control tasks, a robot can use policy gradients to adjust movements based on feedback, refining actions based on their outcomes over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item **Q-learning** is effective for learning optimal actions without a model of the environment.
            \item **SARSA** offers stable learning but may converge slower.
            \item **Policy Gradients** excel in complex action spaces, suitable for nuanced actions.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        These algorithms form the foundation of reinforcement learning, each suited for diverse environments and challenges. Understanding their mechanisms prepares us for complex systems in RL and real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    As we proceed, we'll connect these algorithms to \textbf{Markov Decision Processes (MDPs)}, which are fundamental to many reinforcement learning strategies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Decision Processes (MDPs)}
    \begin{block}{Understanding MDPs in Reinforcement Learning}
        An MDP is a mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of a decision maker. 
        It provides a formal way to define the environment in reinforcement learning problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs}
    \begin{enumerate}
        \item \textbf{States (S)}: Represent all possible situations in which an agent can find itself. 
              Example: In a grid world, each cell can be considered a state.
              
        \item \textbf{Actions (A)}: The set of actions the agent can take while in a given state. 
              Example: In a grid world, possible actions might be $\{up, down, left, right\}$.
              
        \item \textbf{Transition Model (P)}: Defines the probability of reaching a new state given the current state and the action taken. 
              Notation: $P(s' | s, a)$ indicates the probability of transitioning to state $s'$ from state $s$ after action $a$.
              
        \item \textbf{Reward Function (R)}: Provides immediate feedback to the agent after taking an action in a state. 
              It assigns a numerical value to the outcome, e.g., obtaining +10 for reaching a goal state and -1 for hitting a wall.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Policy ($\pi$)}: A strategy that defines the agent’s behavior at a given time. 
              It maps states to probabilities of selecting each possible action.
              Example: A deterministic policy could state "always move right when in state X."
              
        \item \textbf{Value Function (V)}: Estimates how good it is for the agent to be in a given state, considering future rewards. 
              It helps to decide on taking actions based on expected future performance.
              
        \item \textbf{Discount Factor ($\gamma$)}: A value between 0 and 1 reflecting the importance of future rewards. 
              Example: With $\gamma = 0.9$, the agent will favor rewards that come sooner over those that come later.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Diagrammatic Representation of MDPs}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{mdp_diagram.png}
    \end{center}
    \begin{block}{Key Points}
        \begin{itemize}
            \item MDPs provide a complete framework for modeling typical decision-making scenarios in reinforcement learning.
            \item Understanding MDP components (states, actions, rewards) is crucial before implementing algorithms like Q-learning or SARSA.
            \item The interplay between exploration and exploitation is vital for optimizing MDP performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Robot Navigation}
    \begin{block}{Consider a Robot Navigating a Maze}
        \begin{itemize}
            \item Each location is a \textbf{state}.
            \item Moving from one location to another is an \textbf{action}.
            \item The obstacles or rewards encountered are the \textbf{outcomes}.
            \item Strategies the robot uses to reach the end goal represent \textbf{policies}.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Markov Decision Processes enable structured thinking about environments and are foundational in developing effective reinforcement learning algorithms. 
    Understanding MDPs is fundamental for efficient learning and decision-making in uncertain environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions and Bellman Equations}
    \begin{block}{Introduction}
        Overview of value functions, Bellman equations, and their significance in dynamic programming.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Value Functions}
    \begin{itemize}
        \item \textbf{Definition:} Value functions encapsulate the expected return that an agent can achieve from a given state following a particular policy.
    \end{itemize}
    \begin{block}{Types of Value Functions}
        \begin{enumerate}
            \item \textbf{State Value Function (V):} 
                \begin{itemize}
                    \item Represents the expected return when starting from state $s$ and following policy $\pi$.
                    \item Formula: 
                    \[
                    V^\pi(s) = \mathbb{E}[R_t \mid S_t = s, \pi]
                    \]
                \end{itemize}
            \item \textbf{Action Value Function (Q):} 
                \begin{itemize}
                    \item Represents the expected return of taking action $a$ in state $s$ and then following policy $\pi$.
                    \item Formula:
                    \[
                    Q^\pi(s, a) = \mathbb{E}[R_t \mid S_t = s, A_t = a, \pi]
                    \]
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Bellman Equation}
    \begin{itemize}
        \item \textbf{Definition:} The Bellman Equation expresses the relationship between the value of a state and the values of its successor states.
    \end{itemize}
    \begin{block}{Formulations}
        \begin{enumerate}
            \item \textbf{For State Value Function:}
                \[
                V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} P(s', r \mid s, a) [r + \gamma V^\pi(s')]
                \]
                \begin{itemize}
                    \item It states that the value of state $s$ is the expected value over all actions $a$, considering immediate reward $r$ and discounted future value.
                \end{itemize}
                
            \item \textbf{For Action Value Function:}
                \[
                Q^\pi(s, a) = \mathbb{E}_{s', r}[r + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a')]
                \]
                \begin{itemize}
                    \item Highlights the relationship between taking action $a$ and the resulting values of subsequent states as guided by the policy.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Dynamic Programming}
    \begin{itemize}
        \item Bellman equations provide a recursive decomposition of the value functions.
        \item Enable the application of dynamic programming techniques to solve for optimal policies and value functions efficiently, leading to:
        \begin{itemize}
            \item Value Iteration
            \item Policy Iteration
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Review of Ethical Considerations - Part 1}
    \begin{block}{Understanding Ethical Considerations}
        Ethical considerations refer to the moral implications and responsibilities that arise from the application of technologies, including reinforcement learning (RL). As RL is increasingly integrated into various sectors—from healthcare to finance—it is vital to address the potential consequences of its use.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Review of Ethical Considerations - Part 2}
    \begin{block}{Key Ethical Challenges in Reinforcement Learning}
        \begin{itemize}
            \item \textbf{Bias and Fairness}
                \begin{itemize}
                    \item RL systems can inherit biases present in training data.
                    \item Example: An RL agent trained on biased data may reinforce unfair outcomes for specific groups.
                \end{itemize}
            
            \item \textbf{Transparency and Explainability}
                \begin{itemize}
                    \item RL models can be complex, making decision-making processes hard to understand.
                    \item Example: Patients deserve to know how a healthcare model suggests a treatment plan.
                \end{itemize}
            
            \item \textbf{Safety and Robustness}
                \begin{itemize}
                    \item RL agents may behave unpredictably, which can lead to dangerous situations.
                    \item Example: An autonomous vehicle using RL may misinterpret an obstacle.
                \end{itemize}
            
            \item \textbf{Privacy Concerns}
                \begin{itemize}
                    \item Collecting personal data in RL applications raises privacy risks.
                    \item Example: User data in social recommendation systems must be handled with care.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Review of Ethical Considerations - Part 3}
    \begin{block}{Regulatory and Societal Implications}
        \begin{itemize}
            \item \textbf{Regulation}
                \begin{itemize}
                    \item Governments must establish regulations ensuring accountability in RL applications.
                \end{itemize}
                
            \item \textbf{Public Trust}
                \begin{itemize}
                    \item Ethical RL can foster public trust, while unethical applications may lead to backlash.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Ethical considerations are critical to ensuring that RL applications are fair, transparent, secure, and respect user privacy. This understanding is vital for practitioners and researchers in developing responsible AI technologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mid-term Examination Details - Format}
    \begin{block}{Format of the Examination}
        The mid-term examination will consist of \textbf{two main sections}:
        \begin{itemize}
            \item \textbf{Multiple Choice Questions (MCQs)}: 30 questions, each worth 1 point. These questions will assess your understanding of key concepts discussed in class and in the readings.
            \item \textbf{Short Answer Questions}: 3 questions, each worth 10 points. These will require you to explain concepts in your own words, demonstrate application, and analyze scenarios related to reinforcement learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mid-term Examination Details - Structure}
    \begin{block}{Structure of the Examination}
        \begin{itemize}
            \item \textbf{Duration}: 120 minutes total
            \item \textbf{Total Points}: 60 points
            \item \textbf{Sections}:
            \begin{itemize}
                \item MCQs: 30 points (1 point each)
                \item Short Answers: 30 points (10 points each)
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mid-term Examination Details - Preparation Tips}
    \begin{block}{Preparation Tips}
        \begin{enumerate}
            \item \textbf{Review Course Materials}:
                \begin{itemize}
                    \item Revisit lecture notes and assigned readings, focusing on key ethical considerations in reinforcement learning.
                \end{itemize}
            \item \textbf{Practice Questions}:
                \begin{itemize}
                    \item Work on practice MCQs and sample short answer questions to familiarize yourself with the exam format. Consider forming study groups.
                \end{itemize}
            \item \textbf{Key Topics to Focus On}:
                \begin{itemize}
                    \item Core principles of reinforcement learning (e.g., exploration vs. exploitation).
                    \item Major ethical implications (e.g., bias, fairness).
                    \item Algorithms discussed (e.g., Q-learning, policy gradients).
                \end{itemize}
            \item \textbf{Time Management}:
                \begin{itemize}
                    \item Allocate your time wisely (60 minutes for MCQs, 60 minutes for short answers).
                \end{itemize}
            \item \textbf{Formulas to Remember}:
                \begin{equation}
                    Q(s, a) \gets Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
                \end{equation}
                where:
                \begin{itemize}
                    \item $s, a$ are the current state and action
                    \item $r$ is the reward received
                    \item $\alpha$ is the learning rate
                    \item $\gamma$ is the discount factor
                    \item $s'$ is the new state after taking action $a$
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session – Engaging with Review Content and Exam Preparation}
    \begin{block}{Overview}
        This session is dedicated to addressing questions, clarifying doubts, and fostering discussions concerning:
        \begin{itemize}
            \item Material covered over the last eight weeks
            \item Strategies for the upcoming mid-term examination
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Concept Clarifications}
    \begin{itemize}
        \item \textbf{Mid-term Examination Format:}
            \begin{itemize}
                \item Types of Questions: Multiple-choice, short answer, and essay questions.
                \item Weighting of Sections: Understanding each section's contribution to your overall score.
            \end{itemize}
        \item \textbf{Study Strategies:}
            \begin{itemize}
                \item Review Sessions: Importance of group study and peer discussions.
                \item Using Past Papers: Familiarize with question styles by practicing old exams.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Encouraging Student Engagement}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Active Participation: Encourage questions on challenging topics.
            \item Critical Thinking: Invite discussions on complex concepts.
            \item Resource Utilization: Highlight textbooks, lecture notes, and online resources.
        \end{itemize}
    \end{block}
    \begin{block}{Preparation Reminders}
        \begin{itemize}
            \item Time Management: Allocate study time based on topic difficulty.
            \item Mock Exams: Take full-length practice exams under timed conditions.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        This Q\&A session is your opportunity to clarify and engage. What questions do you have?
    \end{block}
\end{frame}


\end{document}