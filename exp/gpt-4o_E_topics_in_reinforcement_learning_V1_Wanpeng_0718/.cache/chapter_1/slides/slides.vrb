\frametitle{1. Q-Learning}
    \begin{block}{Concept}
        Q-learning is a model-free RL algorithm focused on action-value learning, aiming to maximize cumulative rewards.
    \end{block}

    \begin{block}{How it Works}
        \begin{itemize}
            \item Uses a Q-table to store values of state-action pairs.
            \item Updates Q-values using the Bellman equation:
            \begin{equation}
            Q(s, a) \gets Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
            \end{equation}
            Where:
            \begin{itemize}
                \item $Q(s, a)$ = current estimate of the Q-value for state $s$ and action $a$
                \item $r$ = reward after taking action $a$
                \item $\gamma$ = discount factor (0 ≤ $\gamma$ < 1)
                \item $\alpha$ = learning rate (0 < $\alpha$ ≤ 1)
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        In a grid world, an agent learns to navigate to a goal, updating the Q-values based on rewards received after each move.
    \end{block}
