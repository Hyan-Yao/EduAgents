\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 4: Value Functions and Bellman Equations}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Week 4: Value Functions and Bellman Equations}
    
    \begin{block}{Concept Overview}
        This week, we delve into two foundational concepts in reinforcement learning and dynamic programming: 
        \textbf{Value Functions} and \textbf{Bellman Equations}. 
        These concepts are vital for understanding how agents make decisions 
        based on their environment and the anticipated rewards of their actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Value Functions}
    
    \begin{itemize}
        \item \textbf{Value Functions:}
        \begin{itemize}
            \item A Value Function measures the expected return or future rewards 
            from a given state or action.
            \item Two primary types of value functions:
            \begin{itemize}
                \item \textbf{State Value Function (V(s))}: 
                Represents the expected return when starting from state \(s\) and following a particular policy \(\pi\).
                
                \item \textbf{Action Value Function (Q(s, a))}: 
                Represents the expected return when starting from state \(s\), taking action \(a\), 
                and then following policy \(\pi\).
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Example:}
        Consider a simple grid world where an agent must navigate to a goal. 
        The value function assigns higher values to states closer to the goal, 
        reflecting the likelihood of achieving rewards sooner.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Bellman Equations}
    
    \begin{itemize}
        \item \textbf{Bellman Equations:}
        The Bellman Equation provides a recursive definition of the value functions, 
        linking the value of a state or state-action pair to the value of subsequent states.
        
        \begin{block}{Mathematical Definitions}
            \begin{equation}
                V(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) 
                [R(s, a, s') + \gamma V(s')]
            \end{equation}
            
            \begin{equation}
                Q(s, a) = \sum_{s'} P(s'|s, a) 
                [R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q(s', a')]
            \end{equation}
        \end{block}
        
        \item \textbf{Variables:}
        \begin{itemize}
            \item \(R(s, a, s')\) is the immediate reward received 
            after transitioning to state \(s'\) due to action \(a\).
            \item \(\gamma\) (0 ≤ \(\gamma\) < 1) is the discount factor 
            that weighs the importance of future rewards.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}

    \begin{itemize}
        \item \textbf{Key Points to Emphasize:}
        \begin{itemize}
            \item \textbf{Importance of Recursion:} 
            The Bellman Equation utilizes recursion to break down the value of a single state into the values of subsequent states, establishing a pathway for calculating optimal policies.
            
            \item \textbf{Agent Decision-Making:} 
            The concept of value functions is central to how agents estimate the best actions to take in various states, driving the learning process in environments with uncertainty.
        \end{itemize}
        
        \item \textbf{Illustration Suggestion:} 
        Consider a diagram illustrating the relationship between states, actions, and values in a simple environment, showcasing how one state leads to others, with the corresponding values determined by the Bellman Equation.
        
        \item \textbf{Conclusion:} 
        By understanding value functions and Bellman equations, we can explore more complex algorithms in reinforcement learning, ultimately enhancing our ability to develop intelligent agents capable of making optimal decisions in dynamic environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview - Key Concepts}
    In this section, we will delve into two foundational concepts in reinforcement learning and dynamic programming: 
    \textbf{Value Functions} and \textbf{Bellman Equations}. These concepts are essential for understanding how agents make decisions and learn from their environment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview - Value Functions}
    \begin{block}{1. Value Functions}
        \textbf{Definition}: A value function estimates the expected return (cumulative reward) an agent can achieve, starting from a given state and following a specific policy. It quantifies the "goodness" of a state.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Types:}
        \begin{itemize}
            \item \textbf{State Value Function} ($V(s)$): 
            \[
            V^{\pi}(s) = \mathbb{E}_{\pi} \left[ R_t | S_t = s \right]
            \]
            \item \textbf{Action Value Function} ($Q(s, a)$): 
            \[
            Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_t | S_t = s, A_t = a \right]
            \]
        \end{itemize}
    \end{itemize}
    
    \textbf{Example:} In a game with states A (Safe) and B (Danger), if $V(A) > V(B)$, then state A is preferable.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview - Bellman Equations}
    \begin{block}{2. Bellman Equations}
        \textbf{Definition}: The Bellman equation recursively defines the value of a state based on the values of its successor states and serves as the foundation for reinforcement learning algorithms.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Mathematical Formulation:}
        \begin{itemize}
            \item \textbf{For the State Value Function:}
            \[
            V^{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} P(s', r | s, a) \left[ r + \gamma V^{\pi}(s') \right]
            \]
            \item \textbf{For the Action Value Function:}
            \[
            Q^{\pi}(s, a) = \sum_{s', r} P(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s', a') \right]
            \]
        \end{itemize}
    \end{itemize}
    
    \begin{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Value functions evaluate states and actions, while Bellman equations connect current states to future states.
            \item They assist in finding optimal policies through iterative methods or dynamic programming.
            \item Integral to algorithms such as Q-learning and Policy Iteration.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary of Key Concepts}
    \begin{enumerate}
        \item \textbf{Value Functions:}
        \begin{itemize}
            \item Quantify the expected return of a state or action in decision-making.
            \item Two main types:
                \begin{itemize}
                    \item \textbf{State Value Function ($V(s)$)}: Expected return from state $s$ under policy.
                    \item \textbf{Action Value Function ($Q(s, a)$)}: Expected return from state $s$, action $a$, under policy.
                \end{itemize}
            \item \textit{Example:} In a board game, value functions can reflect winning potential based on strategies.
        \end{itemize}

        \item \textbf{Bellman Equation:}
        \begin{itemize}
            \item Recursive equation to compute value functions, following the principle of optimality.
            \item \textit{Formulation:}
            \[
            V(s) = R(s) + \gamma \sum_{s'} P(s'|s, a)V(s')
            \]
            \begin{itemize}
                \item $R(s)$ = immediate reward from state $s$
                \item $\gamma$ = discount factor ($0 < \gamma < 1$)
                \item $P(s'|s, a)$ = transition probability to future state $s'$ given action $a$
            \end{itemize}
            \item \textit{Example:} The Bellman equation updates estimated state values in reinforcement learning.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Points to Emphasize}
    \begin{itemize}
        \item The relationship between value functions and optimal policies is crucial—optimal actions correspond to states that maximize value.
        \item Understanding the Bellman equation is vital for solving Markov Decision Processes (MDPs) and developing algorithms like Dynamic Programming and Q-Learning.
        \item The concept of discounting ($\gamma$) reflects the diminishing value of future rewards, which is pivotal in long-term decision-making scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Final Thoughts}
    \begin{block}{Conclusion}
        This week emphasized the importance of value functions and the Bellman equation in understanding decision processes. These concepts form the basis for various algorithms in fields like artificial intelligence and economics, where evaluating states and strategic planning are critical. Mastering these principles allows students to deepen their insights into reinforcement learning and related domains.
    \end{block}
\end{frame}


\end{document}