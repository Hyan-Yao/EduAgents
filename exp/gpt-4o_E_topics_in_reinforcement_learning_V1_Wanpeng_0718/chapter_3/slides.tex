\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 3: Markov Decision Processes}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Overview}
    \begin{block}{Definition}
        Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making situations where outcomes are partly random and partly under the control of a decision maker. MDPs are fundamental in reinforcement learning, enabling agents to make a sequence of decisions in a stochastic environment to maximize some notion of cumulative reward.
    \end{block}

    \begin{itemize}
        \item \textbf{Discrete State Space:} A finite (or countably infinite) set of states representing all possible situations an agent can encounter.
        \item \textbf{Action Choices:} Finite set of actions that influence subsequent states at each state.
        \item \textbf{Transition Probabilities:} Probabilities that determine how the process transitions from one state to another based on the current state and action taken.
        \item \textbf{Rewards:} Feedback received by the agent that indicates the quality of its actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Significance}
    \begin{itemize}
        \item \textbf{Decision Making Under Uncertainty:} MDPs effectively capture uncertain scenarios, making them ideal for applications in reinforcement learning like robotics, gaming, and autonomous systems.
        \item \textbf{Optimal Policy Determination:} MDPs facilitate finding an optimal policy—a strategy that defines the best action to take in each state to maximize cumulative reward.
        \item \textbf{Foundation for Algorithms:} MDPs serve as the backbone for algorithms in reinforcement learning, such as Value Iteration, Policy Iteration, and Q-Learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Example}
    \begin{block}{Simple Grid World Example}
        Imagine an agent navigating a 3x3 grid:
        \begin{itemize}
            \item \textbf{States:} Each cell in the grid is a state (e.g., S1 = (0,0), S2 = (0,1), etc.).
            \item \textbf{Actions:} The agent can move UP, DOWN, LEFT, or RIGHT from each state.
            \item \textbf{Rewards:} 
                \begin{itemize}
                    \item +1 for reaching the goal state (e.g., S9 = (2,2)).
                    \item -1 for hitting a wall or going out of bounds.
                \end{itemize}
            \item \textbf{Transitions:} 
                \begin{itemize}
                    \item Action outcomes may not be deterministic; e.g., moving UP from (1,1) can lead to (0,1) or may cause a slip back to (1,1) based on probabilities.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Key Points}
    \begin{itemize}
        \item MDPs are foundational for reinforcement learning frameworks.
        \item Understanding MDPs equips one with the necessary tools to design and analyze intelligent agents that operate in complex environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Objective}
    The expected cumulative reward for an agent in MDP can be expressed using the following formulation:

    \begin{block}{Objective}
        Maximize the expected return from state $ S $:
        \begin{equation}
            R = \sum_{t=0}^{\infty} \gamma^t r_t
        \end{equation}
        where $ r_t $ is the reward at time $ t $, and $ \gamma $ (0 ≤ $ \gamma $ < 1) is the discount factor that balances immediate and future rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs - Overview}
    Markov Decision Processes (MDPs) are mathematical frameworks used to model decision-making in situations where outcomes are partly random and partly under the control of a decision maker. 
    \begin{itemize}
        \item Four key components define an MDP:
        \begin{itemize}
            \item States (S)
            \item Actions (A)
            \item Rewards (R)
            \item Transitions (T)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs - States and Actions}
    \begin{enumerate}
        \item \textbf{States (S)}
        \begin{itemize}
            \item **Definition**: Situations or configurations in which an agent can find itself.
            \item **Example**: In a grid world, a state might represent "the agent is at cell (2,3)."
        \end{itemize}
        
        \item \textbf{Actions (A)}
        \begin{itemize}
            \item **Definition**: Possible moves or decisions made by the agent when in a state.
            \item **Example**: "Move up," "move down," "move left," or "move right" in the grid world.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs - Rewards and Transitions}
    \begin{enumerate}[resume]
        \item \textbf{Rewards (R)}
        \begin{itemize}
            \item **Definition**: Immediate returns received after transitioning states due to an action.
            \item **Example**: Moving to a state with treasure yields +10; entering a trap yields -5.
        \end{itemize}
        
        \item \textbf{Transitions (T)}
        \begin{itemize}
            \item **Definition**: Probability of moving from one state to another given an action.
            \item **Example**: "Move right" from cell (2,3) may lead to (2,4) with probability 0.8 and back to (2,3) with 0.2.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Closing Thoughts}
    \begin{itemize}
        \item **Interconnectedness**: The four components influence decision-making in complex environments.
        \item **Sequential Decision Making**: MDPs help in choosing actions over time to maximize cumulative rewards.
        \item **Policy Development**: An agent develops a policy ($\pi$) mapping states to actions based on expected long-term rewards.
    \end{itemize}
    
    \begin{block}{Next Steps}
        In the next slide, we will dive deeper into the first component: \textbf{States}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States in Markov Decision Processes (MDPs)}
    \begin{block}{Definition of States}
        In the context of MDPs, a **state** represents a specific situation or configuration of the environment at any given time.
    \end{block}
    \begin{itemize}
        \item Denoted as \( S \), states capture all relevant aspects required for decision-making.
        \item States can vary based on the context of the problem being addressed.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of States}
    \begin{itemize}
        \item \textbf{Situational Awareness}: States provide a snapshot of the current context, aiding agents in assessing their environment effectively.
        \item \textbf{Decision Making}: The choice of actions is directly influenced by the current state, guiding the agent towards optimal behavior.
        \item \textbf{Completeness}: Every state must uniquely capture the necessary information to predict future outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of States}
    \begin{enumerate}
        \item \textbf{Chess Game}: 
        \begin{itemize}
            \item State includes positions of all pieces on the board, whose turn it is, and additional game information.
        \end{itemize}
        \item \textbf{Robot Navigation}: 
        \begin{itemize}
            \item State represents the robot's current location (x, y coordinates), orientation, and nearby obstacles.
        \end{itemize}
        \item \textbf{Weather Modeling}: 
        \begin{itemize}
            \item State might consist of temperature, humidity, wind speed, and forecast conditions (e.g., rainy or sunny).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points about States}
    \begin{itemize}
        \item \textbf{Discrete vs. Continuous States}: 
        \begin{itemize}
            \item States can be discrete (finite set) or continuous (range of values).
        \end{itemize}
        \item \textbf{Observation}: 
        \begin{itemize}
            \item In partially observable environments, an agent may not have access to the full state, leading to uncertainty, referred to as "hidden states."
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualization and Summary}
    \begin{block}{State Representation}
        For example, in a chess game, a state can be visualized as a chessboard with pieces in specific locations, aiding in understanding how actions relate to states.
    \end{block}

    \begin{block}{Summary}
        States in MDPs are essential for defining the environment in which an agent operates, guiding decision-making through a clear representation of various situations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    Explore how \textbf{actions} taken in each state affect the transitions to new states and inform the agent's learning process in the next slide.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions - Overview}
    \begin{block}{Understanding Actions in Markov Decision Processes (MDPs)}
        \begin{itemize}
            \item Actions are choices made by an agent that affect the state of the environment.
            \item Represent decision points based on current states.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions - Role and Influence}
    \begin{block}{Role of Actions}
        \begin{enumerate}
            \item Actions define how the system transitions between states.
            \item Transition from state \( s_t \) to state \( s_{t+1} \) when action \( a_t \) is taken:
            \begin{equation}
                s_{t+1} \sim P(\cdot | s_t, a_t)
            \end{equation}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Influence on State Transitions}
        \begin{itemize}
            \item Detailing deterministic and stochastic actions:
            \begin{itemize}
                \item \textbf{Deterministic Actions:} Always lead to a specific next state.
                \item \textbf{Stochastic Actions:} Result in a probability distribution over possible next states.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions - Key Points and Example Scenario}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Critical for decision-making and maximizing expected rewards.
            \item \textbf{Exploration vs. Exploitation:} The dilemma faced by agents.
            \item \textbf{Policy Definition:}
            \begin{itemize}
                \item Deterministic: \( \pi: S \rightarrow A \)
                \item Stochastic: \( \pi: S \rightarrow P(A) \)
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Scenario}
        \begin{itemize}
            \item \textbf{Robot Navigation Task:}
            \begin{itemize}
                \item States: Room A, Room B, Room C
                \item Actions: Move to Room A, Move to Room B, Move to Room C
            \end{itemize}
            \item Choosing to "Move to Room B" results in transitioning to Room B with certain probability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in Markov Decision Processes (MDPs)}
    
    \begin{block}{What Are Rewards?}
        In MDPs, \textbf{rewards} are numerical values representing the immediate benefit received for taking specific actions in a certain state.
        Rewards serve as feedback that guides the decision-making process.
    \end{block}
    
    \begin{itemize}
        \item Rewards help evaluate the desirability of states and actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Structure}
    
    \begin{itemize}
        \item \textbf{Assignment of Rewards:}
        \begin{itemize}
            \item Based on:
            \begin{enumerate}
                \item Current State \( s \)
                \item Action \( a \)
            \end{enumerate}
        \end{itemize}
        
        \item \textbf{Notation:}
        \begin{itemize}
            \item Reward function \( R(s, a) \) or \( R(s') \) for resulting state.
            \item For any state \( s \) and action \( a \):
            \[
                R(s, a) = \text{Reward received after taking action } a \text{ in state } s
            \]
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Rewards Guide Decisions}
    
    \begin{itemize}
        \item \textbf{Positive and Negative Rewards:}
        \begin{itemize}
            \item Positive rewards encourage actions.
            \item Negative rewards penalize undesired actions, e.g.:
            \begin{itemize}
                \item +10 points for defeating an enemy.
                \item -5 points for self-inflicted damage.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Long-Term vs. Short-Term Rewards:}
        \begin{itemize}
            \item Balancing immediate and future rewards is crucial.
            \item \textbf{Discount Factor} \( \gamma \):
            \[
                R_{total} = R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots
            \]
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Rewards}
    
    \begin{enumerate}
        \item \textbf{Robotics Navigation:}
        \begin{itemize}
            \item \textbf{State}: Position of the robot.
            \item \textbf{Action}: Move forward, turn left, or turn right.
            \item \textbf{Reward}: 
            \begin{itemize}
                \item +10 for reaching a target,
                \item -5 for bumping into an obstacle.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Game Playing:}
        \begin{itemize}
            \item \textbf{State}: Player's position on the game board.
            \item \textbf{Action}: Move to an adjacent square.
            \item \textbf{Reward}: 
            \begin{itemize}
                \item +3 for landing on a treasure square,
                \item -1 for stepping on a trap.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    
    \begin{itemize}
        \item Rewards are essential to \textbf{incentivize} desired behaviors.
        \item The design of the reward function impacts the agent's learning and behavior.
        \item Balancing between immediate and future rewards is crucial for robust decision-making strategies.
    \end{itemize}

    \begin{block}{Conclusion}
        Understanding the reward structure in MDPs is fundamental as it significantly influences the agent's learning process.
        The structuring of rewards can lead to vastly different behaviors and outcomes, making it a critical component in decision-making systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transitions - Understanding Transition Probabilities in MDPs}

    \begin{block}{Key Concept: Transition Probabilities}
        - Define likelihood to move from one state to another after a specific action. \\
        - Fundamental in MDPs for regulating decision-making under uncertainty.
    \end{block}

    \begin{block}{Mathematical Representation}
        Transition probabilities can be denoted as:
        \begin{equation}
            P(s' | s, a)
        \end{equation}
        where:
        \begin{itemize}
            \item **s**: Current state
            \item **a**: Action taken
            \item **s'**: Next state
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transitions - Importance of Transition Probabilities}

    \begin{itemize}
        \item \textbf{Modeling Uncertainty:} Quantifies uncertainty in the environment. \\
        In real-world scenarios, transitions may not be deterministic.
        
        \item \textbf{Informed Decision-Making:} Guides agents on likely outcomes, aiding in strategy selection.
        
        \item \textbf{Dynamic Environments:} Adjust policies as states change based on previous actions.
    \end{itemize}

    \begin{block}{Example Scenario}
        In a driving scenario:
        - Driving towards an intersection (state A):
          \begin{itemize}
              \item $P(\text{Stopped} | A, \text{Stop}) = 0.8$
              \item $P(\text{Ahead} | A, \text{Stop}) = 0.2$ (accident occurs)
          \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transitions - Recap and Visualization}

    \begin{block}{Recap of Key Points}
        - Transition probabilities describe dynamics of state changes in MDPs.\\
        - Defined by the current state and action taken.\\
        - Understanding them is essential for informed, strategic decisions.
    \end{block}

    \begin{block}{Visualizing Transition Probabilities}
        - Represent states and actions in a directed graph:
        \begin{itemize}
            \item Nodes represent states
            \item Directed edges represent possible transitions, labeled with probabilities.
        \end{itemize}
        This visual reinforces how transitions are influenced by chosen actions.
    \end{block}

    \begin{block}{Preparing for Next Concept}
        Next, we will explore \textbf{Policies} - how they govern agent behavior in MDPs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies - Definition}
    \begin{block}{Definition of a Policy in MDPs}
        In the context of Markov Decision Processes (MDPs), a \textbf{policy} is a strategy used by an agent to determine its actions based on the current state of the environment. Essentially, a policy defines how the agent behaves, mapping states to actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies - Mathematical Representation}
    \begin{block}{Mathematical Representation}
        A policy, often denoted as \( \pi \), can be formally defined as:
        \begin{itemize}
            \item \textbf{Deterministic Policy:} 
                \[
                \pi: S \rightarrow A
                \]
                where \( S \) is the set of states, and \( A \) is the set of actions. This type of policy yields a specific action for each state.
                
            \item \textbf{Stochastic Policy:} 
                \[
                \pi(a|s) = P(A_t = a | S_t = s)
                \]
                where \( \pi(a|s) \) provides the probability of taking action \( a \) when in state \( s \). This policy assigns probabilities to multiple actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies - Deterministic vs. Stochastic}
    \begin{block}{Deterministic Policies}
        \begin{itemize}
            \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Predictable; for a given state, the same action is always chosen.
                    \item Simpler to implement and understand.
                \end{itemize}
            \item \textbf{Example:} In a chess game, if the policy dictates to always move the knight to a specific position when in a certain configuration, that is a deterministic policy.
        \end{itemize}
        
        \textbf{Use Case:} Suitable for environments with predictable outcomes where decisions can be consistently defined.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies - Stochastic Policies}
    \begin{block}{Stochastic Policies}
        \begin{itemize}
            \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Introduces variability; allows for different actions to be chosen at the same state based on a probability distribution.
                    \item More flexible for complex environments where randomness and exploration are beneficial.
                \end{itemize}
            \item \textbf{Example:} In a robot navigation scenario, if conditions are unpredictable, a policy might dictate: "Move right 70\% of the time and left 30\% of the time when at a crossroads."
        \end{itemize}
        
        \textbf{Use Case:} Useful in complex environments or during exploration phases, where the agent may need to try different actions to discover their effects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item A policy defines how an agent behaves in an MDP.
            \item Deterministic policies map each state to a single action, while stochastic policies provide a distribution over possible actions.
            \item The choice of policy impacts the agent’s performance and its ability to optimize decision-making in various environments.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding the concept of policies is crucial in the study of MDPs as they play a vital role in guiding agents toward making optimal decisions. Whether you opt for a deterministic approach or a stochastic one depends on the specific problem you are solving and the nature of the environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Introduction}
    \begin{block}{Introduction to Value Functions}
        In Markov Decision Processes (MDPs), value functions are critical in evaluating how good it is to be in a specific state or to take a specific action in a state. 
        They provide a measure of the long-term expected utility, guiding decision-making processes in uncertain environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Types}
    \begin{block}{Types of Value Functions}
        \begin{enumerate}
            \item \textbf{State-Value Function (V(s))}:
                \begin{itemize}
                    \item \textbf{Definition:} The state-value function, denoted as \( V(s) \), measures the expected return from state \( s \) when following a specific policy \( \pi \).
                    \item \textbf{Formula:} 
                    \[
                    V_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_t | S_t = s \right] = \sum_{a \in A} \pi(a|s) \sum_{s', r} P(s', r | s, a) [r + \gamma V_{\pi}(s')]
                    \]
                    \item \textbf{Importance:} Evaluates how advantageous it is to be in state \( s \).
                \end{itemize}
            
            \item \textbf{Action-Value Function (Q(s, a))}:
                \begin{itemize}
                    \item \textbf{Definition:} The action-value function, denoted as \( Q(s, a) \), evaluates the expected return from taking action \( a \) in state \( s \) under a specific policy \( \pi \).
                    \item \textbf{Formula:} 
                    \[
                    Q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_t | S_t = s, A_t = a \right] = \sum_{s', r} P(s', r | s, a) [r + \gamma V_{\pi}(s')]
                    \]
                    \item \textbf{Importance:} Provides insight into the value of specific actions within states.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Key Points and Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Fundamental components in reinforcement learning guiding policy selection.
            \item \( V(s) \) focuses on the value of states; \( Q(s, a) \) weighs the value of actions.
            \item Assist in refining policies and transitioning to optimal solutions.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Consider a simple MDP where an agent can either 'Move Right' or 'Move Left' from state \( s \):
        \begin{itemize}
            \item If \( Q(s, \text{Move Right}) = 10 \) and \( Q(s, \text{Move Left}) = 5 \), the agent should prefer 'Move Right' to maximize expected return.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding value functions enables agents in MDPs to effectively evaluate states and actions, leading to optimal decision-making strategies that maximize long-term rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition to Next Slide}
    Next, we will explore the Bellman equations, which mathematically define the relationships between these value functions and reinforce their significance in computing optimal policies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Overview}
    \begin{block}{Definition}
        The Bellman equations are fundamental in the study of Markov Decision Processes (MDPs).
        They create a recursive relationship to compute value functions.
    \end{block}
    \begin{itemize}
        \item Break down complex decisions into smaller, manageable problems.
        \item Define relationships between the expected utility of a state and subsequent states.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Key Concepts}
    \begin{enumerate}
        \item \textbf{Value Functions}
        \begin{itemize}
            \item \textbf{State-Value Function (V)}:
            \[
            V^\pi(s) = \mathbb{E}[R_t | S_t = s, \pi]
            \]
            \item \textbf{Action-Value Function (Q)}:
            \[
            Q^\pi(s, a) = \mathbb{E}[R_t | S_t = s, A_t = a, \pi]
            \]
        \end{itemize}
        
        \item \textbf{Bellman Equation for State-Value Function}:
        \[
        V^\pi(s) = \sum_{a} \pi(a | s) \sum_{s', r} P(s', r | s, a) [r + \gamma V^\pi(s')]
        \]
        
        \item \textbf{Bellman Equation for Action-Value Function}:
        \[
        Q^\pi(s, a) = \sum_{s', r} P(s', r | s, a) [r + \gamma \sum_{a'} \pi(a' | s') Q^\pi(s', a')]
        \]
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Bellman Equations in MDPs}
    \begin{itemize}
        \item \textbf{Recursive Relationships}:
        \begin{itemize}
            \item Value of a state/action can be determined recursively.
            \item Reliance on expected rewards of future states.
        \end{itemize}
        \item \textbf{Dynamic Programming}:
        \begin{itemize}
            \item Backbone of algorithms like Value Iteration and Policy Iteration.
            \item Iteratively update value functions until convergence.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    Consider an MDP with:
    \begin{itemize}
        \item States: \( S1, S2 \)
        \item Actions: \( A1, A2 \)
    \end{itemize}
    If:
    \begin{itemize}
        \item From state \( S1 \), performing action \( A1 \) leads to state \( S2 \) with a reward of 10.
        \item Current estimate: \( V^\pi(S2) = 5 \).
    \end{itemize}
    You can plug in these values into the Bellman equation to update \( V^\pi(S1) \).

\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Points}
    \begin{itemize}
        \item Bellman equations are essential in solving MDPs (Value and Policy Iteration).
        \item Highlight the recursive nature of decision-making in uncertain environments.
        \item Mastery of Bellman equations lays groundwork for reinforcement learning applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Overview}
    \begin{block}{Overview of Markov Decision Processes (MDPs)}
        Markov Decision Processes provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs consist of:
        \begin{itemize}
            \item States
            \item Actions
            \item Rewards
            \item Transition probabilities
        \end{itemize}
        This framework enables the evaluation of policies dictating the course of action in each state.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Key Applications}
    \begin{enumerate}
        \item \textbf{Robotics}
        \begin{itemize}
            \item \textbf{Description}: MDPs are pivotal in robot path planning and navigation, helping robots to decide on actions while considering uncertainties.
            \item \textbf{Example}: A mobile robot navigating through a cluttered room.
            \item \textbf{Illustration}:
            \begin{itemize}
                \item States: {Start, Obstacle, Goal}
                \item Actions: {Move Forward, Turn Left, Turn Right}
                \item Transition: Moving forward may hit an obstacle.
            \end{itemize}
        \end{itemize}

        \item \textbf{Economics}
        \begin{itemize}
            \item \textbf{Description}: MDPs model decision-making over time under uncertainty (e.g., consumer behavior, investment).
            \item \textbf{Example}: An investor's optimal investment strategy over time.
            \item \textbf{Key Formula}:
            \begin{equation}
                V(s) = \max_a \sum_{s'} P(s' | s, a) \times [R(s, a) + \gamma V(s')]
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - AI Applications}
    \begin{itemize}
        \item \textbf{Artificial Intelligence}
        \begin{itemize}
            \item \textbf{Description}: MDPs are foundational for AI applications, particularly in reinforcement learning.
            \item \textbf{Example}: NPCs in video games use MDPs for strategic decision-making (e.g., attack or retreat).
            \item \textbf{Key Point}: Reinforcement Learning algorithms like Q-learning derive from MDP principles.
        \end{itemize}
    \end{itemize}

    \begin{block}{Conclusion}
        MDPs provide a robust framework for tackling complex decision-making problems across various domains, such as robotics, economics, and artificial intelligence.
    \end{block}

    \begin{block}{Takeaway Points}
        \begin{itemize}
            \item \textbf{Flexibility}: Adaptable to various fields, quantifying uncertainties.
            \item \textbf{Optimization}: Identify optimal policies for decision-making.
            \item \textbf{Learning}: Facilitate the development of intelligent agents learning from their surroundings.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}