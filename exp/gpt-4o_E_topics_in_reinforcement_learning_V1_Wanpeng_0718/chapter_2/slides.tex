\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 2: Foundations of RL}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Foundations of Reinforcement Learning}
    \begin{block}{Overview}
        Reinforcement Learning (RL) is a branch of machine learning focused on how agents take actions in an environment to maximize cumulative rewards.
    \end{block}
    \begin{itemize}
        \item Importance of understanding key concepts: 
        \begin{itemize}
            \item Agents
            \item Environments
            \item Rewards
            \item Policies
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Reinforcement Learning}

    \begin{enumerate}
        \item \textbf{Agents}
            \begin{itemize}
                \item \textbf{Definition:} An entity that interacts with the environment.
                \item \textbf{Example:} A self-driving car navigating roads.
            \end{itemize}
        \item \textbf{Environments}
            \begin{itemize}
                \item \textbf{Definition:} The context in which the agent operates.
                \item \textbf{Example:} The traffic system with vehicles and pedestrians.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts Continuation}

    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Rewards}
            \begin{itemize}
                \item \textbf{Definition:} Feedback signals that guide the agent's learning.
                \item \textbf{Example:} Positive reward for successfully navigating a turn, negative penalty for running a red light.
            \end{itemize}
        \item \textbf{Policies}
            \begin{itemize}
                \item \textbf{Definition:} A strategy used by an agent to determine actions based on the environment's state.
                \item \textbf{Example:} Policies governing stopping for red lights and yielding to pedestrians.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The \textbf{agent-environment framework} is fundamental in RL.
            \item Understanding \textbf{rewards and policies} is crucial for effective learning.
            \item The interplay of these elements enables agents to adapt and optimize their behavior.
        \end{itemize}
    \end{block}

    \begin{block}{Illustrative Example: Chess}
        \begin{itemize}
            \item \textbf{Agent:} The player or chess algorithm.
            \item \textbf{Environment:} The chessboard and pieces.
            \item \textbf{Rewards:} Points for winning, losing, or capturing pieces.
            \item \textbf{Policy:} Strategy for selecting moves based on the board configuration.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Agents in RL - Definition of Agents}

    In Reinforcement Learning (RL), an **Agent** is an entity that makes decisions by interacting with an environment to achieve a specific goal. The agent observes the state of the environment, takes actions based on these observations, and receives feedback in the form of rewards or penalties.

    \begin{itemize}
        \item **Autonomy:** Operates without direct human oversight.
        \item **Adaptability:** Learns from experiences to improve decision-making.
        \item **Goal-oriented:** Aims to maximize cumulative rewards over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Agents in RL - Roles of Agents}

    The roles of agents in the RL process can be summarized as follows:

    \begin{enumerate}
        \item **Observation:** The agent observes the current state of the environment.
        \item **Action Selection:** Selecting an action based on the observation, guided by a policy.
        \item **Interacting with Environment:** Performing the chosen action affects the environment.
        \item **Receiving Feedback:** Gaining rewards or penalties post-action.
        \item **Updating Knowledge:** Using feedback to adjust the policy and improve future decisions.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Agents in RL - Example and Key Points}

    \textbf{Example of an Agent in a Grid World:}
    \begin{itemize}
        \item **State (S):** Position on the grid.
        \item **Action (A):** Possible moves (up, down, left, right).
        \item **Reward (R):** +10 points for reaching a destination; -5 points for falling into a trap.
    \end{itemize}

    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Learning and adaptability of agents are central to RL.
        \item Balance between exploration (trying new actions) and exploitation (maximizing known rewards).
        \item Design of an agent's policy influences its performance significantly.
    \end{itemize}

    \textbf{Conclusion:} Agents are crucial in RL as they learn from their environment through actions and rewards.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Environments}
    \begin{block}{What is an Environment in RL?}
        In reinforcement learning, the \textbf{environment} is the context in which an agent operates. It includes everything the agent interacts with to learn and make decisions and provides feedback based on the agent's actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of the Environment}
    \begin{enumerate}
        \item \textbf{State (s)}: Represents the current situation of the environment.
        \begin{itemize}
            \item Example: In a chess game, the layout of pieces.
        \end{itemize}
        
        \item \textbf{Action (a)}: Set of possible actions the agent can take.
        \begin{itemize}
            \item Example: In driving simulation, turning, accelerating, braking.
        \end{itemize}
        
        \item \textbf{Transition Probability (P)}: Likelihood of moving from one state to another given an action.
        \begin{itemize}
            \item Example: In a grid world, moving east has an 80\% chance of success.
        \end{itemize}
        
        \item \textbf{Reward (r)}: Feedback value received after action execution.
        \begin{itemize}
            \item Example: Winning a round gives a +1 reward.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agent-Environment Interaction Cycle}
    \begin{itemize}
        \item \textbf{Observation}: The agent observes the current state of the environment.
        \item \textbf{Action Selection}: The agent selects an action based on the observed state.
        \item \textbf{Feedback}: The agent receives a reward and transitions to a new state.
        \item \textbf{Learning}: The agent updates its strategy based on the received feedback.
    \end{itemize}
    \begin{block}{Example: An RL Agent in a Maze}
        \begin{itemize}
            \item \textbf{Environment}: A maze with walls and paths.
            \item \textbf{State}: Agent's current position.
            \item \textbf{Actions}: Move up, down, left, right.
            \item \textbf{Rewards}: Reaching the exit gives a reward, hitting a wall incurs a penalty.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in Reinforcement Learning - Overview}
    \begin{block}{What Are Rewards?}
        In Reinforcement Learning (RL), rewards are signals that provide feedback to an agent based on its actions within an environment.
        \begin{itemize}
            \item Positive feedback for desirable behavior.
            \item Negative feedback for undesirable behavior.
        \end{itemize}
        Mathematically, a reward at time \( t \) is represented as \( r_t \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Rewards}
    \begin{enumerate}
        \item \textbf{Guiding Learning:}
        \begin{itemize}
            \item Rewards direct the agent towards achieving specific goals.
            \item They help agents to learn which actions yield the best outcomes, optimizing behavior to maximize cumulative rewards (the return).
        \end{itemize}

        \item \textbf{Establishing Preferences:}
        \begin{itemize}
            \item Differentiates helpful actions from harmful ones.
            \item In environments with delayed rewards, understanding the sequence of actions leading to a reward is crucial.
        \end{itemize}

        \item \textbf{Influencing Decisions:}
        \begin{itemize}
            \item Rewards impact the strategy adopted by the agent, informing whether actions were beneficial or detrimental.
        \end{itemize}

        \item \textbf{Example:}
        \begin{itemize}
            \item In a game (e.g., chess):
            \begin{itemize}
                \item Positive reward (+10) for winning.
                \item Negative reward (-10) for losing.
                \item Smaller rewards for advantageous positions.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Rewards Influence Agent Behavior}
    \begin{itemize}
        \item \textbf{Exploration vs. Exploitation:}
        \begin{itemize}
            \item Agents balance exploring new actions and utilizing known rewarding actions. Poor reward design can lead to early favoring of exploitation.
        \end{itemize}

        \item \textbf{Temporal Difference Learning:}
        \begin{equation}
            Q(s, a) = r + \gamma \max_a Q(s', a)
        \end{equation}
        \begin{itemize}
            \item Here, \( Q(s, a) \) is the quality of action \( a \) in state \( s \), \( r \) is the immediate reward, and \( \gamma \) is the discount factor.
        \end{itemize}

        \item \textbf{Sparse vs. Dense Rewards:}
        \begin{itemize}
            \item \textbf{Sparse Rewards:} Long sequences without rewards hinder learning (e.g., a maze with a reward only at the exit).
            \item \textbf{Dense Rewards:} Frequent guidance can speed up learning but may confuse the agent if over-rewarded.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Conclusion}
    \begin{itemize}
        \item Rewards drive the learning process in RL.
        \item Effective reward design influences the agent's strategy and efficiency.
        \item Exploration strategies must align with the reward structure to optimize learning outcomes.
    \end{itemize}
    \begin{block}{Conclusion}
        Rewards are fundamental in reinforcement learning, guiding agents toward desirable behaviors and impacting decision-making. Understanding and designing reward systems are vital for the success of RL applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies: Directives for Action}
    
    \begin{block}{Understanding Policies in Reinforcement Learning (RL)}
        A policy is a strategy employed by an RL agent to determine its actions based on the current state of the environment. It is a mapping from states to actions and can be either deterministic or stochastic.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definition of a Policy}

    \begin{itemize}
        \item \textbf{Deterministic Policy:}
        \[
        \pi(s) = a
        \]
        Where \( \pi \) is the policy function, \( s \) is a state, and \( a \) is the action taken in that state.
        
        \item \textbf{Stochastic Policy:}
        \[
        \pi(a|s) = P(A=a | S=s)
        \]
        The probability of taking action \( a \) in state \( s \) defines the variability in the agent's action choice.
    \end{itemize}
    
    \begin{block}{Role of Policies}
        Policies guide the agent's decisions throughout the learning process, influencing how it perceives and interacts with its environment to maximize cumulative rewards over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples to Illustrate Policies}

    \begin{enumerate}
        \item \textbf{Example 1: Grid World}
            \begin{itemize}
                \item Deterministic Policy: The agent follows a specific path every time it starts in the same state.
                \item Stochastic Policy: The agent might choose to move in various directions based on a probability distribution (e.g., 70\% chance to move up, 30\% chance to move right).
            \end{itemize}
        \item \textbf{Example 2: Self-Driving Cars}
            \begin{itemize}
                \item A car's policy determines how to respond in various traffic situations (e.g., stop at traffic lights, yield to pedestrians).
                \item The policy evolves as the car learns from experiences and real-time data.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Conclusion}
        The policy is essential in RL, defining how an agent interacts with its environment. Optimizing policies leads to better decision-making and improved performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation Dilemma}
    \begin{block}{Understanding the Dilemma}
        In reinforcement learning (RL), the agent faces the **exploration vs. exploitation** dilemma when making decisions about how to act in an environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation Dilemma - Exploration}
    \begin{itemize}
        \item \textbf{Exploration}: Trying new actions to discover potential rewards.
        \item \textbf{Goal}: Gather information to improve future decision-making.
        \item \textbf{Example}: In chess, trying a new opening for potential long-term benefits.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation Dilemma - Exploitation}
    \begin{itemize}
        \item \textbf{Exploitation}: Choosing known actions believed to yield the highest reward.
        \item \textbf{Goal}: Leverage known information to maximize short-term benefits.
        \item \textbf{Example}: Continuing with a successful chess opening rather than risking a new strategy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implications of the Dilemma}
    \begin{itemize}
        \item Balancing exploration and exploitation is critical in RL.
        \item \textbf{Key Considerations}:
        \begin{enumerate}
            \item Long-Term vs. Short-Term Rewards
            \item Learning Rate
            \item Environment Complexity
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Strategies to Address the Dilemma}
    \begin{itemize}
        \item \textbf{Epsilon-Greedy Strategy}:
        \begin{itemize}
            \item Best-known action most of the time, random action with probability $\epsilon$.
            \item \textbf{Example}: $\epsilon = 0.1 \implies 10\% \text{ chance to explore.}$
        \end{itemize}
        
        \item \textbf{Upper Confidence Bound (UCB)}:
        \begin{equation}
        UCB(a) = \overline{X}_a + c \sqrt{\frac{\ln(n)}{n_a}}
        \end{equation}
        \begin{itemize}
            \item $\overline{X}_a$: average reward of action $a$
            \item $n$: total actions taken
            \item $n_a$: number of times action $a$ has been taken
            \item $c$: exploration parameter
        \end{itemize}
        
        \item \textbf{Thompson Sampling}:
        \begin{itemize}
            \item Selecting actions based on probability distributions over expected rewards.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        The exploration vs. exploitation dilemma is critical for RL success. Finding the right balance enhances learning efficiency and improves decision-making capabilities, leading to better performance in complex environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions Overview}
    \begin{block}{Understanding Value Functions}
        Value functions are fundamental components of reinforcement learning (RL) that help estimate the expected future rewards an agent can receive from being in a particular state or taking a specific action. They are essential for guiding the agent's decision-making process.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Value Functions}
    \begin{enumerate}
        \item \textbf{State Value Function (V(s))}:
            \begin{itemize}
                \item Represents the expected return from a given state following a certain policy.
                \item \textbf{Formula}:
                \begin{equation}
                V^{\pi}(s) = \mathbb{E}_{\pi} [ R_t | S_t = s ]
                \end{equation}
                \item \textbf{Explanation}: Where \( V^{\pi}(s) \) is the value of state \( s \) and \( R_t \) is the return received after time \( t \).
            \end{itemize}
        
        \item \textbf{Action Value Function (Q(s, a))}:
            \begin{itemize}
                \item Represents the expected return from taking a specific action \( a \) in state \( s \) and then following a particular policy.
                \item \textbf{Formula}:
                \begin{equation}
                Q^{\pi}(s, a) = \mathbb{E}_{\pi} [ R_t | S_t = s, A_t = a ]
                \end{equation}
                \item \textbf{Explanation}: Here, \( Q^{\pi}(s, a) \) gives the value of taking action \( a \) in state \( s \).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Value Functions}
    \begin{itemize}
        \item \textbf{Decision-Making}: They guide the agent's actions by estimating long-term benefits.
        \item \textbf{Efficient Learning}: Aid in updating knowledge of actions and states for faster convergence on optimal policies.
        \item \textbf{Handling Uncertainty}: Allow the agent to express uncertainty in its environment and adapt accordingly.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example in Context}
    \begin{block}{Chess Game Scenario}
        \begin{itemize}
            \item \textbf{State}: The current arrangement of pieces on the board.
            \item \textbf{Action}: Moving a piece, for example, moving a knight.
            \item \textbf{Value Function}: Assists the AI in evaluating immediate gains as well as potential future scenarios resulting from that move.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Value functions simplify decision-making by quantifying potential rewards.
        \item They are critical for exploring new strategies and exploiting known successful actions.
        \item The relationship between state value and action value functions enhances comprehension of the environment.
    \end{itemize}

    \begin{block}{Conclusion}
        By understanding value functions, students will gain insights into how reinforcement learning algorithms enhance decision-making processes. This concept is closely tied to the upcoming topic on Markov Decision Processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Decision Processes (MDPs)}
    \begin{block}{Overview of MDPs}
        A Markov Decision Process (MDP) is a mathematical framework used for modeling decision making in environments. MDPs formalize making a sequence of decisions under uncertainty, applicable in various fields including robotics, finance, and game playing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDP}
    An MDP is defined by the following components:

    \begin{enumerate}
        \item \textbf{States (S)}: A finite set of all possible situations.
        \item \textbf{Actions (A)}: A finite set of actions available in each state.
        \item \textbf{Transition Function (P)}: Defines the probability of transitioning from one state to another given an action (\( P(s'|s, a) \)).
        \item \textbf{Reward Function (R)}: Defines the immediate reward after transitioning from state \( s \) to \( s' \) upon taking action \( a \) (\( R(s, a, s') \)).
        \item \textbf{Discount Factor ($\gamma$)}: Between 0 and 1, it governs the importance of future rewards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How MDPs Work}
    Using MDPs, an agent can analyze its environment and choose actions to maximize cumulative rewards over time. The Markov property indicates that future states depend only on the current state and action, not on prior histories.

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item MDPs model complex decision-making environments effectively.
            \item The interrelationship among states, actions, transitions, rewards, and discount factors is essential for solving MDPs.
            \item The Markov property simplifies decision-making, allowing efficient algorithm development for optimal policies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of an MDP}
    \textbf{Scenario: A robot navigating through a maze.}
    \begin{itemize}
        \item \textbf{States (S)}: Each position in the maze.
        \item \textbf{Actions (A)}: Move up, down, left, right.
        \item \textbf{Transition Function (P)}: Likelihood of moving to intended cell vs. hitting a wall.
        \item \textbf{Reward Function (R)}: +10 for reaching the exit, -1 for each move, -5 for hitting an obstacle.
        \item \textbf{Discount Factor ($\gamma$)}: 0.95 to prioritize earlier rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    MDPs are foundational in understanding reinforcement learning. They establish the structure for problem formulation that enables the development of algorithms to discover optimal decision-making strategies under uncertainty. Moving forward, we will explore concepts like the Bellman equations critical for solving MDPs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations Fundamentals}

    \begin{block}{Overview}
        Bellman Equations are foundational principles in Reinforcement Learning (RL) that describe the relationship between the value of a state and the values of its successor states.
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}

    \begin{enumerate}
        \item \textbf{Value Function (V)}  
        \[
        V(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s\right]
        \]
        
        \item \textbf{Action Value Function (Q)}  
        \[
        Q(s, a) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a\right]
        \]
        
        \item \textbf{Bellman Equation for Value Functions}  
        \[
        V(s) = \sum_{a \in A} \pi(a|s) \sum_{s'} P(s'|s, a) \left[R(s, a, s') + \gamma V(s')\right]
        \]
        
        \item \textbf{Bellman Optimality Equation}  
        \[
        V^*(s) = \max_{a} \sum_{s'} P(s'|s, a) \left[R(s, a, s') + \gamma V^*(s')\right]
        \]
    \end{enumerate}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Simplified MDP}

    Consider a grid world where an agent moves right or down to reach a goal. The grid cells represent states, and the allowed actions determine transitions and rewards. Using the Bellman equation, we calculate:
    
    \[
    V(0,0) = \frac{1}{2} \left[R(0,1) + \gamma V(0,1)\right] + \frac{1}{2} \left[R(1,0) + \gamma V(1,0)\right]
    \]
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}

    \begin{itemize}
        \item \textbf{Importance:} Bellman equations are crucial for deriving optimal policies and value functions in RL.
        \item \textbf{Recursive Structure:} They utilize recursion to define the value at a state concerning future values.
        \item \textbf{Connection to Policy:} They underpin critical RL algorithms (e.g., Value Iteration, Policy Iteration).
    \end{itemize}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}

    Understanding Bellman Equations is essential for mastering RL. They form the theoretical backbone for various algorithms used to find optimal strategies in MDPs, enabling agents to learn effectively in complex environments.

\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Overview}
    \begin{itemize}
        \item Summary of foundational concepts discussed in Reinforcement Learning (RL).
        \item Key takeaways encapsulating our discussions through the week.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Fundamentals of RL}
            \begin{itemize}
                \item Definition: An agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
                \item Key components: Agent, Environment, Action, Reward.
            \end{itemize}
        \item \textbf{Trial and Error}
            \begin{itemize}
                \item Exploration vs. Exploitation: The balance between trying new actions and leveraging known actions is crucial.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts Continues}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Bellman Equations}
            \begin{itemize}
                \item Define the value function, critical for determining future rewards.
                \item Formula: 
                \begin{equation}
                    V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right)
                \end{equation}
            \end{itemize}
        \item \textbf{Applications of RL}
            \begin{itemize}
                \item Fields: Robotics, Game Playing, Recommendation Systems, Finance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Importance of the environment for effective learning.
        \item Focus on long-term goals over immediate rewards.
        \item Robustness of RL techniques: Policy Gradient methods, Q-learning, and DQNs.
        \item Mastering these principles is essential for advanced RL techniques.
    \end{itemize}
\end{frame}


\end{document}