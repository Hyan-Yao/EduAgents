\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 5: Regression Techniques]{Week 5: Regression Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Techniques - Overview}
    \begin{block}{Overview of Regression in Data Mining}
        Regression techniques are a fundamental aspect of data mining and statistical analysis that allow us to understand the relationships between variables. When we analyze data, we often want to predict a target variable based on the values of other variables. This is where regression comes into play.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Techniques - Importance}
    \begin{block}{Importance of Regression}
        \begin{enumerate}
            \item \textbf{Understanding Relationships:} Identify how dependent and independent variables interact (e.g., sales vs. advertising spend).
            \item \textbf{Predictive Analytics:} Predict future outcomes (e.g., house prices based on features).
            \item \textbf{Decision Making:} Supports data-driven decisions in various fields like business, healthcare, and finance.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Techniques - Applications}
    \begin{block}{Applications of Regression}
        \begin{itemize}
            \item \textbf{Economics:} Analyzing trends in economic indicators (e.g., GDP vs. unemployment rates).
            \item \textbf{Healthcare:} Predicting patient outcomes based on treatment methods or demographics.
            \item \textbf{Marketing:} Evaluating the effectiveness of marketing campaigns on product sales.
            \item \textbf{Environmental Studies:} Estimating pollutant levels based on various emissions sources.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Techniques - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Types of Regression:}
                \begin{itemize}
                    \item \textbf{Linear Regression:} Predicts a continuous outcome variable as a linear combination of predictors.
                    \item \textbf{Multiple Regression:} Uses multiple independent variables to predict a continuous outcome.
                    \item \textbf{Logistic Regression:} Applicable for binary outcome variables (e.g., yes/no).
                \end{itemize}
            \item \textbf{Mathematical Representation:}
                \begin{equation}
                    Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
                \end{equation}
                Where:
                \begin{itemize}
                    \item \( Y \): Dependent variable
                    \item \( \beta_0 \): Intercept
                    \item \( \beta_1, \beta_2, ..., \beta_n \): Coefficients for predictors \( X_1, X_2, ..., X_n \)
                    \item \( \epsilon \): Error term
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Techniques - Conclusion}
    \begin{block}{Conclusion}
        Understanding regression techniques is crucial for effective data analysis. By learning about these methods, we equip ourselves with tools to uncover patterns and make informed predictions in various fields, enhancing our ability to solve real-world problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Techniques - Summary}
    \begin{block}{Summary}
        Regression techniques are essential in data mining for understanding relationships, predicting outcomes, and supporting decision-making. Whether in business, healthcare, or environmental studies, regression models provide critical insights into the data-driven world.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Overview}
    Understanding regression techniques is vital for analyzing and predicting trends within data sets. In this section, we will outline the essential learning objectives for mastering regression techniques:
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Key Concepts}
    \begin{enumerate}
        \item \textbf{Define Regression Techniques}
            \begin{itemize}
                \item Understand what regression is and its role in data analysis.
                \item Example: Predicting housing prices based on features like size, location, and number of bedrooms.
            \end{itemize}
        
        \item \textbf{Differentiate Between Types of Regression}
            \begin{itemize}
                \item \textbf{Linear Regression:} Predicting a continuous target variable (e.g., predicting sales based on advertising spend).
                \item \textbf{Logistic Regression:} Used for binary outcomes (e.g., predicting whether an email is spam).
                \item \textbf{Polynomial Regression:} Captures non-linear relationships (e.g., modeling a quadratic equation to fit data).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Key Concepts (cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue from the previous frame
        \item \textbf{Understand the Regression Equation}
            \begin{equation}
            Y = b_0 + b_1X_1 + b_2X_2 + \ldots + b_nX_n + \epsilon
            \end{equation}
            Where:
            \begin{itemize}
                \item $Y$ = dependent variable,
                \item $b_0$ = y-intercept,
                \item $b_1, b_2, \ldots, b_n$ = coefficients,
                \item $X_1, X_2, \ldots, X_n$ = independent variables,
                \item $\epsilon$ = error term.
            \end{itemize}
        
        \item \textbf{Interpretation of Regression Outputs}
            \begin{itemize}
                \item Learn how to interpret regression coefficients and the significance of p-values.
                \item Example: A coefficient of 2.5 indicates that for each unit increase in that variable, the dependent variable increases by 2.5 units.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Key Concepts (cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{5} % Continue from the previous frame
        \item \textbf{Assessing Model Performance}
            \begin{itemize}
                \item Familiarize with metrics such as R-squared, Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) to evaluate model accuracy.
                \item Example: An R-squared value of 0.85 indicates that 85\% of the variability in the dependent variable is explained by the model.
            \end{itemize}
        
        \item \textbf{Exploring Assumptions of Regression Models}
            \begin{itemize}
                \item Understand key assumptions underlying regression models, such as linearity, independence, homoscedasticity, and normality of residuals.
            \end{itemize}
        
        \item \textbf{Applying Regression Techniques to Real World Scenarios}
            \begin{itemize}
                \item Gain practical experience by applying regression to case studies and datasets, enhancing your skills in data-driven decision making.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Definition}
    
    \begin{block}{Definition}
        Linear Regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The primary goal is to predict the value of the dependent variable based on the values of the independent variables.
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Formula}
    
    \begin{block}{Formula}
        The formula for a simple linear regression with one independent variable is:
        \begin{equation}
            Y = \beta_0 + \beta_1 X + \epsilon 
        \end{equation}
        
        Where:
        \begin{itemize}
            \item \( Y \) = dependent variable (what we are trying to predict)
            \item \( X \) = independent variable (the predictor)
            \item \( \beta_0 \) = y-intercept (predicted value of \( Y \) when \( X \) = 0)
            \item \( \beta_1 \) = slope of the line (indicates how much \( Y \) changes for a one-unit change in \( X \))
            \item \( \epsilon \) = error term (difference between actual and predicted values)
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Key Points and Applications}
    
    \begin{block}{Key Points to Emphasize}
        \begin{enumerate}
            \item Nature of Relationship: Linear regression assumes a linear relationship between the variables.
            \item Multiple Linear Regression: Extends formula to:
            \begin{equation}
                Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n + \epsilon
            \end{equation}
            \item Model Evaluation: Effectiveness can be evaluated using metrics like R-squared.
        \end{enumerate}
    \end{block}

    \begin{block}{Application Examples}
        \begin{itemize}
            \item Business Context: Analyzing advertising spend to understand its impact on sales revenue.
            \item Healthcare Scenario: Evaluating factors like age and weight to predict blood pressure.
            \item Real Estate Analysis: Estimating house prices based on features like size and location.
        \end{itemize}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Assumptions}
    \begin{itemize}
        \item Linear regression is used to model relationships between a dependent variable and independent variables.
        \item Key assumptions must be met to ensure the validity of the model's results.
        \item Understanding these assumptions is crucial for accurate analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Assumptions of Linear Regression - Part 1}
    \begin{enumerate}
        \item \textbf{Linearity}
            \begin{itemize}
                \item Relationship between independent and dependent variables must be linear.
                \item \textit{Example}: Predicting house prices based on square footage cannot be curved.
                \item \textit{Check}: Use scatter plots to visualize the relationship.
            \end{itemize}

        \item \textbf{Independence}
            \begin{itemize}
                \item Observations should be independent; residuals must not be correlated.
                \item \textit{Example}: One student’s score should not influence another's.
                \item \textit{Check}: Use the Durbin-Watson statistic to test residual correlation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Assumptions of Linear Regression - Part 2}
    \begin{enumerate}[resume]
        \item \textbf{Homoscedasticity}
            \begin{itemize}
                \item Variance of residuals must remain constant across levels of independent variable(s).
                \item \textit{Example}: In salary vs. experience regression, residual spread should be consistent.
                \item \textit{Check}: Analyze residual plot; a funnel shape indicates issues.
            \end{itemize}

        \item \textbf{Normality of Residuals}
            \begin{itemize}
                \item Residuals should be approximately normally distributed.
                \item \textit{Example}: Differences between observed and predicted values should form a bell curve.
                \item \textit{Check}: Use Q-Q plots or histograms for assessment.
            \end{itemize}

        \item \textbf{No Multicollinearity}
            \begin{itemize}
                \item Predictors should not be highly correlated with each other.
                \item \textit{Example}: Correlation between advertising spend and social media engagement can distort significance.
                \item \textit{Check}: Calculate Variance Inflation Factor (VIF); values above 5 or 10 indicate problems.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Meeting these assumptions enhances the reliability of results.
            \item Violations can lead to incorrect conclusions and predictions.
            \item Always perform diagnostic checks post-model fitting to confirm assumptions hold.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding and validating these assumptions is essential for interpreting results correctly.
        \begin{itemize}
            \item Addressing violations early on yields more accurate and meaningful analyses.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Overview}
    \begin{equation}
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
    \end{equation}
    where \(Y\) is the dependent variable, \(X_i\) are independent variables, \(\beta_i\) are coefficients, and \(\epsilon\) is the error term.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet}
    \begin{lstlisting}[language=Python]
import statsmodels.api as sm
import pandas as pd

# Fit a linear regression model
X = pd.DataFrame({'Experience': [1, 2, 3, 4, 5], 'Engagement': [5, 7, 8, 9, 10]})
Y = [15000, 18000, 22000, 25000, 30000]
X = sm.add_constant(X)  # Adds a constant term to the predictor
model = sm.OLS(Y, X).fit()
print(model.summary())
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Evaluating Linear Regression Models}
    \begin{block}{Introduction}
        Evaluating the performance of linear regression models is essential to determine how well your model predicts outcomes based on input variables. 
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Evaluation Metrics}
    \begin{enumerate}
        \item \textbf{R-squared (R²)}
        \begin{itemize}
            \item \textbf{Definition}: Represents the proportion of variance in the dependent variable predicted from the independent variables. R² ranges from 0 to 1.
            \item \textbf{Interpretation}:
            \begin{itemize}
                \item R² = 0 means no variability is explained.
                \item R² = 1 means all variability is explained.
                \item Example: R² = 0.85 indicates 85\% of variability in the target variable is explained.
            \end{itemize}
            \item \textbf{Formula}:
            \begin{equation}
                R^2 = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Evaluation Metrics (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{1} % Continue numbering from previous frame
        \item \textbf{Root Mean Squared Error (RMSE)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the standard deviation of the residuals (prediction errors).
            \item \textbf{Interpretation}:
            \begin{itemize}
                \item A lower RMSE value indicates a better fit of the model.
                \item RMSE is in the same units as the dependent variable for easy interpretation.
            \end{itemize}
            \item \textbf{Formula}:
            \begin{equation}
                RMSE = \sqrt{\frac{1}{n} \sum (y_i - \hat{y}_i)^2}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Example for Clarity}
    Consider predicting house prices based on features like size and location. 
    \begin{itemize}
        \item \textbf{Results}:
        \begin{itemize}
            \item R² = 0.92: 92\% of price variability explained—very good.
            \item RMSE = \$15,000: on average, predictions deviate from actual prices by this amount.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item R² is useful for understanding explanatory power but can be misleading in overly complex models (always verify with additional metrics).
        \item RMSE provides a direct measure of prediction error and is crucial for assessing model accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Both R-squared and RMSE are vital for validating model performance. Always consider the context and model assumptions when interpreting these metrics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Note on Coding}
    In practical scenarios, libraries like \texttt{scikit-learn} in Python can be used:
    \begin{lstlisting}[language=Python]
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Sample y_true and y_pred
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]

r2 = r2_score(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))

print(f'R-squared: {r2}, RMSE: {rmse}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Logistic Regression - Part 1}
    \textbf{Definition of Logistic Regression}
    
    Logistic regression is a statistical method used for binary classification problems where the dependent variable (outcome) is categorical. It estimates the probability that a given input belongs to a particular category.

    \begin{block}{Formula}
        The logistic regression model predicts the probability \( P \) of the outcome as follows:
        \[
        P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \]
    \end{block}

    \begin{itemize}
        \item \( P(Y=1 | X) \) is the probability of the outcome being 1 given predictors \( X \).
        \item \( \beta_0, \beta_1, ... , \beta_n \) are the coefficients of the model.
        \item \( e \) is the base of the natural logarithm.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Logistic Regression - Part 2}
    \textbf{Importance of Logistic Regression}

    \begin{itemize}
        \item \textbf{Interpretable Results}: Coefficients can be translated into odds ratios for easier interpretation.
        \item \textbf{Probability Estimation}: Provides probabilities associated with predictions, crucial for decision-making.
        \item \textbf{Widely Applicable}: Used in fields such as medicine, finance, and marketing.
    \end{itemize}
    
    \textbf{When and Why to Use Logistic Regression}
    
    \begin{itemize}
        \item \textbf{When to Use}:
            \begin{itemize}
                \item Response variable is binary (yes/no, 0/1).
                \item Estimating probabilities and classifying observations.
            \end{itemize}
        \item \textbf{Why Use It}:
            \begin{itemize}
                \item Non-linearity: Effective for modeling non-linear relationships.
                \item Sensitivity to Classification: Useful for imbalanced datasets.
                \item Easy to Implement: Less computationally intensive.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Logistic Regression - Part 3}
    \textbf{Example}

    Consider a medical study predicting whether a patient has a disease based on various health metrics:

    \begin{itemize}
        \item \textbf{Input Variables (X)}: Age, Blood Pressure, Cholesterol Level
        \item \textbf{Output Variable (Y)}: Disease status (1 if has, 0 if not)
    \end{itemize}

    The logistic regression model assesses how likely patients with certain characteristics are to have the disease, aiding in diagnosis and treatment plans.

    \textbf{Key Points to Emphasize}
    
    \begin{itemize}
        \item Logistic regression is fundamental for binary classification tasks.
        \item It estimates probabilities using the logistic function.
        \item Understanding coefficients helps in inferring the importance of predictors.
    \end{itemize}

    \textbf{Conclusion}

    In summary, logistic regression offers robust methodology for classifying binary outcomes while providing interpretable insights.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Model Details}
    \begin{block}{Understanding Logistic Regression}
        Logistic regression is a model used to predict binary outcomes (0 or 1). 
        It estimates the probability that an input belongs to a category.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation}
    \begin{enumerate}
        \item \textbf{Logistic Function}:
        \begin{equation}
            f(z) = \frac{1}{1 + e^{-z}}
        \end{equation}
        where 
        \[
        z = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n.
        \]
        \begin{itemize}
            \item \( z \): Linear combination of input features (log-odds).
            \item \( \beta_i \): Model coefficients/weights.
            \item \( x_i \): Predictor variables.
        \end{itemize}
        
        \item \textbf{Interpretation}:
        \begin{itemize}
            \item Outputs range from 0 to 1, indicating probabilities.
            \item S-shaped curve provides clear probability transitions between classes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Estimating Parameters and Example}
    \begin{block}{Estimating Parameters}
        Coefficients \( \beta \) are estimated using \textbf{Maximum Likelihood Estimation (MLE)}, maximizing the likelihood of observed data.
    \end{block}

    \begin{block}{Example}
        Predicting whether a student passes (1) or fails (0) based on study hours:
        \begin{equation}
            z = -3 + 0.5 \times (\text{Hours Studied})
        \end{equation}
        For 6 hours of study:
        \[
        z = -3 + 0.5 \times 6 = 0
        \]
        \[
        P(\text{Pass}) = \frac{1}{1 + e^{-0}} = 0.5
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences: Linear vs Logistic Regression - Overview}
    \begin{itemize}
        \item \textbf{Linear Regression}: Models the relationship between a continuous dependent variable and one or more independent variables using a linear equation.
        \item \textbf{Logistic Regression}: Used for binary classification, modeling the probability that an input point falls into one of two categories using the logistic function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences: Linear vs Logistic Regression - Comparison}
    \begin{block}{Key Differences}
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Feature} & \textbf{Linear Regression} & \textbf{Logistic Regression} \\ \hline
            Nature of Output & Continuous value & Categorical (binary) \\ \hline
            Equation Form & \( Y = \beta_0 + \beta_1X_1 + \cdots + \beta_nX_n + \epsilon \) & \( P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \cdots + \beta_nX_n)}} \) \\ \hline
            Loss Function & Mean Squared Error (MSE) & Log loss (Binary Cross-Entropy) \\ \hline
            Interpretability & Coefficients indicate direct output change & Coefficients in terms of log-odds (odds ratios) \\ \hline
            Assumptions & Linear relationship, homoscedasticity & Linear relationship for log-odds, no multicollinearity \\ \hline
            Outcome Distribution & Normal distribution & Logistic (S-shaped) distribution \\ \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences: Linear vs Logistic Regression - Examples and Conclusion}
    \begin{itemize}
        \item \textbf{Linear Regression Example}: 
        Predicting house prices based on size (e.g., price increases by \$200 for each additional square foot).
        \item \textbf{Logistic Regression Example}: 
        Predicting student exam outcomes (pass/fail) based on study hours and attendance.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Understanding the differences is crucial for selecting the appropriate model based on data characteristics and research questions. Each method has its distinct purposes and requires careful interpretation.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementation in Data Mining Tools}
    \begin{block}{Overview of Regression Techniques}
        Regression analysis is crucial in data mining, helping model relationships between variables. Using programming tools like Python and R enables practical applications in data analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Regression in Python}
    \begin{itemize}
        \item \textbf{Libraries to Use:}
        \begin{itemize}
            \item \textbf{NumPy:} For numerical operations
            \item \textbf{Pandas:} For data manipulation and analysis
            \item \textbf{Scikit-learn:} For machine learning algorithms, including regression
            \item \textbf{StatsModels:} For statistical modeling and hypothesis testing
        \end{itemize}
        
        \item \textbf{Basic Implementation Steps:}
        \begin{enumerate}
            \item \textbf{Import Libraries}
            \begin{lstlisting}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
            \end{lstlisting}
            
            \item \textbf{Load Data}
            \begin{lstlisting}
data = pd.read_csv('data.csv')
            \end{lstlisting}
            
            \item \textbf{Prepare Data}
            \begin{lstlisting}
X = data[['feature1', 'feature2']]
y = data['target']
            \end{lstlisting}
        
            \item \textbf{Split Data}
            \begin{lstlisting}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            \end{lstlisting}
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Regression in Python (cont'd)}
    \begin{itemize}
        \item \textbf{Continue Basic Implementation Steps:}
        \begin{enumerate}
            \setcounter{enumi}{4} % Continue numbering from previous frame
            \item \textbf{Create a Model}
            \begin{lstlisting}
model = LinearRegression()
model.fit(X_train, y_train)
            \end{lstlisting}
            
            \item \textbf{Make Predictions}
            \begin{lstlisting}
predictions = model.predict(X_test)
            \end{lstlisting}
            
            \item \textbf{Evaluate Model}
            Use metrics like Mean Squared Error (MSE):
            \begin{lstlisting}
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, predictions)
print(f'Mean Squared Error: {mse}')
            \end{lstlisting}
        \end{enumerate}
        
        \item \textbf{Key Points to Emphasize:}
        \begin{itemize}
            \item Model Fit: Importance of avoiding overfitting and underfitting
            \item Data Preprocessing: Clean and prepare data for accurate outcomes
            \item Choosing the Right Model: Match regression technique to data type
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementing Regression in R}
    \begin{itemize}
        \item \textbf{Libraries to Use:}
        \begin{itemize}
            \item \textbf{Base R:} Essential statistical functions
            \item \textbf{dplyr:} For data manipulation
            \item \textbf{ggplot2:} For visualization
            \item \textbf{caret:} For machine learning
        \end{itemize}
        
        \item \textbf{Basic Implementation Steps:}
        \begin{enumerate}
            \item Install and Load Libraries:
            \begin{lstlisting}
install.packages("dplyr")
library(dplyr)
            \end{lstlisting}
            
            \item Load Data
            \begin{lstlisting}
data <- read.csv('data.csv')
            \end{lstlisting}
            
            \item Prepare Data
            Specify the formula for the regression model:
            \begin{lstlisting}
model <- lm(target ~ feature1 + feature2, data=data)
            \end{lstlisting}
            
            \item Summarize Model
            \begin{lstlisting}
summary(model)
            \end{lstlisting}
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Case Studies}
    \begin{block}{Introduction to Regression Techniques}
        Regression analysis plays a pivotal role in understanding relationships between variables in various fields. 
        It provides a framework for predicting outcomes based on historical data, allowing for data-driven decision-making.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Real-World Applications of Regression Techniques}

    \begin{enumerate}
        \item \textbf{Linear Regression Case Study: Housing Market Analysis}
        \begin{itemize}
            \item \textbf{Context:} Real estate agencies use linear regression to predict housing prices based on factors such as square footage, number of bedrooms, and location.
            \item \textbf{Example:}
                \begin{equation}
                    \text{Price} = \beta_0 + \beta_1 (\text{SquareFootage}) + \beta_2 (\text{Bedrooms}) + \beta_3 (\text{Location}) + \epsilon
                \end{equation}
                - A 1,000 sq. ft. house may sell for \$250,000 based on a derived model.
            \item \textbf{Outcome:} Accurate pricing strategies assist buyers and sellers.
        \end{itemize}
        
        \vfill
        
        \item \textbf{Logistic Regression Case Study: Medical Diagnosis}
        \begin{itemize}
            \item \textbf{Context:} Used to predict the likelihood of a disease based on symptoms and medical history.
            \item \textbf{Example:}
                \begin{equation}
                    P(\text{Disease} = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 (\text{Age}) + \beta_2 (\text{Cholesterol}) + \beta_3 (\text{BloodPressure})}} 
                \end{equation}
                - Calculates probabilities like heart disease based on input values.
            \item \textbf{Outcome:} Enables early diagnosis and better treatment outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}

    \begin{itemize}
        \item \textbf{Flexibility of Regression Methods:} Applicable from marketing to medicine.
        \item \textbf{Decision-Making Tool:} Supports strategic decisions with actionable insights based on empirical data.
        \item \textbf{Scope of Application:} 
        \begin{itemize}
            \item Linear regression is ideal for continuous outcomes.
            \item Logistic regression excels in binary outcome scenarios.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Example Code}
    
    Regression techniques, whether linear or logistic, provide essential frameworks with a positive impact across industries by enabling organizations to predict trends and understand relationships between key variables effectively.
    
    \begin{block}{Code Snippet Example (Python)}
    \begin{lstlisting}[language=Python]
import pandas as pd
import statsmodels.api as sm

# Load your dataset
data = pd.read_csv('housing_data.csv')

# Define independent variables
X = data[['SquareFootage', 'Bedrooms', 'Location']]
X = sm.add_constant(X)  # Adds an intercept term

# Define dependent variable
y = data['Price']

# Fit the linear regression model
model = sm.OLS(y, X).fit()

# Print model summary
print(model.summary())
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Regression Analysis - Overview}
    \begin{block}{Overview}
        Regression analysis is a powerful statistical tool for modeling relationships between a dependent variable and independent variables. 
        Challenges such as \textbf{Overfitting} and \textbf{Multicollinearity} can impact model accuracy and reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Regression Analysis - Overfitting}
    \begin{block}{1. Overfitting}
        \textbf{Definition:} Overfitting occurs when a regression model learns both the underlying pattern and the noise from the training data. 

        \begin{itemize}
            \item Causes excellent performance on training data but poor on unseen data.
            \item Signs include high accuracy on training data and significantly lower accuracy on validation/test data.
        \end{itemize}
        
        \textbf{Example:} 
        Fitting a 5th-degree polynomial to the training data points (1, 2), (2, 2), (3, 3) yields perfect fit but poor prediction on new data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Addressing Overfitting}
    \begin{block}{How to Address Overfitting}
        \begin{itemize}
            \item \textbf{Simplifying the Model:} Use fewer predictors or a simpler structure.
            \item \textbf{Regularization Techniques:} 
                \begin{itemize}
                    \item Lasso: Minimize \( ||y - X\beta||_2^2 + \lambda ||\beta||_1 \)
                    \item Ridge: Minimize \( ||y - X\beta||_2^2 + \lambda ||\beta||_2^2 \)
                \end{itemize}
            \item \textbf{Cross-Validation:} Use k-fold cross-validation for performance assessment.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Regression Analysis - Multicollinearity}
    \begin{block}{2. Multicollinearity}
        \textbf{Definition:} Multicollinearity arises when two or more independent variables are highly correlated, leading to redundant information.

        \begin{itemize}
            \item High Variance Inflation Factor (VIF) values (e.g., VIF > 10) indicate multicollinearity.
            \item Creates sensitivity in coefficient estimates, complicating interpretation.
        \end{itemize}

        \textbf{Example:}
        In predicting house prices, including both square footage and total number of rooms may lead to multicollinearity.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Addressing Multicollinearity}
    \begin{block}{How to Address Multicollinearity}
        \begin{itemize}
            \item \textbf{Remove Highly Correlated Predictors:} Use correlation matrices for analysis.
            \item \textbf{Principal Component Analysis (PCA):} Transform correlated variables into uncorrelated components.
            \item \textbf{Combine Variables:} Create new variables that encapsulate the information of highly correlated predictors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Understanding and addressing overfitting and multicollinearity are crucial for developing robust regression models.
        By simplifying models, utilizing regularization, and carefully selecting predictors, we can enhance model performance and reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Ethical Considerations in Regression Analysis}
    Regression techniques are powerful tools used in data analysis, but their application necessitates careful attention to ethical factors. Here, we discuss key ethical considerations to ensure responsible usage of regression models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Data Integrity and Quality}
    \begin{itemize}
        \item \textbf{Explanation:} The quality of data used in regression modeling profoundly impacts its outcomes. Using biased, incomplete, or incorrect data can lead to misleading results.
        \item \textbf{Key Point:} Always source data responsibly and validate its accuracy.
        \item \textbf{Example:} In predicting housing prices, using data that omits key variables (like location or economic conditions) may skew results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Transparency and Interpretability}
    \begin{itemize}
        \item \textbf{Explanation:} The models should be transparent in how they operate, and results should be easy to understand by stakeholders.
        \item \textbf{Key Point:} It’s crucial to communicate the assumptions and limitations of the regression model clearly.
        \item \textbf{Example:} When presenting a model that predicts loan defaults, explain how the model was developed and which predictors were used.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Fairness and Avoiding Discrimination}
    \begin{itemize}
        \item \textbf{Explanation:} Regression models can inadvertently perpetuate biases present in datasets. This can lead to unfair treatment of certain groups based on race, gender, or socioeconomic status.
        \item \textbf{Key Point:} Conduct fairness assessments to ensure that the model's predictions are equitable.
        \item \textbf{Example:} A credit scoring model that negatively impacts certain demographics might reinforce systemic biases.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Informed Consent and Privacy}
    \begin{itemize}
        \item \textbf{Explanation:} Data used in regression analysis typically comes from individuals or organizations. Always seek informed consent when using personal data and ensure compliance with privacy regulations.
        \item \textbf{Key Point:} Protect individuals' privacy by anonymizing data and detailing how their data will be used.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Accountability}
    \begin{itemize}
        \item \textbf{Explanation:} Researchers and analysts must take responsibility for their models and the consequences of their outputs.
        \item \textbf{Key Point:} Establish a protocol for evaluating and monitoring the impact of regression outcomes on stakeholders.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{6. Use of Results}
    \begin{itemize}
        \item \textbf{Explanation:} Consider the implications of the analysis and how the results will be used in decision-making.
        \item \textbf{Key Point:} Ensure that the findings are not used maliciously or irresponsibly.
        \item \textbf{Example:} A model that predicts employee performance should not be solely used for punitive measures without a holistic approach to employee evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Ethical considerations in regression are crucial for ensuring that analysis is conducted responsibly and that the results benefit society. By being mindful of these factors, analysts can contribute to more fair, transparent, and accurate data practices.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visual Aid Suggestion}
    \begin{itemize}
        \item A flowchart showing ethical decision-making in regression analysis, highlighting steps such as data sourcing, validation, model development, and interpretation. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Overview}
    \begin{block}{Understanding Regression Techniques}
        Regression techniques are statistical methods used to model and analyze the relationships between variables, aiding in prediction based on one or more predictor variables. These techniques are crucial in fields like economics, biology, and engineering for data-driven decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Types of Regression}
    \begin{enumerate}
        \item \textbf{Types of Regression}:
            \begin{itemize}
                \item \textbf{Simple Linear Regression}:
                    \begin{itemize}
                        \item Predicts the relationship between two variables using a linear equation.
                        \item Example: Predicting exam scores based on study hours.
                        \item Formula: \( Y = b_0 + b_1X \)
                    \end{itemize}
                    
                \item \textbf{Multiple Linear Regression}:
                    \begin{itemize}
                        \item Extends to multiple predictors.
                        \item Example: Forecasting house prices based on size, location, and age.
                        \item Formula: \( Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n \)
                    \end{itemize}
                    
                \item \textbf{Logistic Regression}:
                    \begin{itemize}
                        \item Used for binary outcomes.
                        \item Example: Classifying emails as spam or not.
                        \item Formula: \( P(Y=1) = \frac{1}{1 + e^{-(b_0 + b_1X_1 + ... + b_nX_n)}} \)
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Important Concepts}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Assumptions of Regression}:
            \begin{itemize}
                \item Linearity: The relationship is linear.
                \item Independence: Observations are independent.
                \item Homoscedasticity: Constant variance of errors.
                \item Normality: Residuals are normally distributed.
            \end{itemize}        

        \item \textbf{Importance of Mastering Regression Techniques}:
            \begin{itemize}
                \item Predictive Power: Enables accurate predictions.
                \item Insight into Relationships: Understand how variables interact.
                \item Foundation for Machine Learning: Many algorithms are based on regression.
            \end{itemize}
    \end{enumerate}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Key Formulas and Conclusion}
    \begin{block}{R-squared (Goodness of Fit)}
        Measures how well the regression predictions approximate real data points:
        \[
        R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}
        \]
        where:
        \begin{itemize}
            \item \( \text{SS}_{\text{res}} \): Sum of squares of residuals
            \item \( \text{SS}_{\text{tot}} \): Total sum of squares
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Mastering regression techniques is vital for any data analyst to effectively interpret and manage complex data, enhancing contributions to their field. Continuous practice is key to proficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session on Regression Techniques}
    \begin{block}{Overview}
        This Q\&A session is designed to clarify your understanding and application of the regression techniques covered in previous discussions.
        Let's dive deeper into your thoughts, questions, and case studies to ensure we can effectively utilize regression analysis in various contexts.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts to Review}
    \begin{itemize}
        \item \textbf{Regression Analysis}: A statistical method used to examine the relationship between a dependent variable and one or more independent variables.
        
        \item \textbf{Types of Regression}:
            \begin{itemize}
                \item \textbf{Simple Linear Regression}: Models the relationship between two variables using a straight line.
                    \begin{equation}
                        Y = b_0 + b_1X + \epsilon
                    \end{equation}
                    Where:
                    \begin{itemize}
                        \item \( Y \): Dependent variable
                        \item \( X \): Independent variable
                        \item \( b_0 \): Y-intercept
                        \item \( b_1 \): Slope of the line
                        \item \( \epsilon \): Error term
                    \end{itemize}
                \item \textbf{Multiple Linear Regression}: Extends simple linear regression to multiple predictors.
                \item \textbf{Logistic Regression}: Used when the dependent variable is categorical (e.g., binary outcomes).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Points and Examples}
    \begin{enumerate}
        \item \textbf{Understanding Assumptions}:
            \begin{itemize}
                \item Linearity, independence, homoscedasticity, normal distribution of errors.
                \item Importance of checking these assumptions for valid model performance.
            \end{itemize}
        \item \textbf{Common Issues}:
            \begin{itemize}
                \item Multicollinearity: Occurs when two or more predictor variables are highly correlated.
                \item Overfitting: A model that is too complex may fit the training data well but performs poorly on new data.
            \end{itemize}
        \item \textbf{Model Evaluation Techniques}:
            \begin{itemize}
                \item R-squared value: Indicates the proportion of variance for the dependent variable that's explained by the independent variables.
                \item Adjusted R-squared: Adjusts for the number of predictors in the model.
            \end{itemize}
    \end{enumerate}
    
    \textbf{Example Scenario:} Predicting house prices based on size (square footage).
    \begin{equation}
        \text{Price} = 50,000 + 150 \times \text{Size}
    \end{equation}
    This indicates that for each additional square foot, the price is expected to increase by $150.
\end{frame}


\end{document}