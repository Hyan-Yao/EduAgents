\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 6: Clustering Techniques]{Week 6: Clustering Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Clustering in Data Mining}
    \begin{block}{What is Clustering?}
        Clustering is a technique used in data mining and machine learning to group a set of objects so that:
        \begin{itemize}
            \item Objects in the same group (or cluster) are more similar to each other 
            \item Objects in different groups are more dissimilar
        \end{itemize}
        It is a form of unsupervised learning, as there is no labeled data for training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Clustering}
    \begin{enumerate}
        \item \textbf{Data Simplification:} Reduces complexity by organizing large datasets into meaningful subgroups.
        \item \textbf{Pattern Recognition:} Aids in visually identifying patterns in data.
        \item \textbf{Anomaly Detection:} Helps in detecting outliers or anomalies crucial for areas such as fraud detection.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Clustering}
    \begin{enumerate}
        \item \textbf{Customer Segmentation:} 
            Businesses segment their customers to tailor marketing strategies.
            \begin{itemize}
                \item \textit{Example:} Retail companies segment customers based on shopping history.
            \end{itemize}
        \item \textbf{Image Segmentation:} 
            Used in computer vision to partition images for easier analysis.
        \item \textbf{Document Clustering:} 
            Groups similar documents in NLP for topic detection.
        \item \textbf{Genomic Data Analysis:} 
            Groups genes/relation in bioinformatics based on expression data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item Clustering is a powerful unsupervised learning technique for data organization.
        \item It plays an essential role in domains like marketing, computer vision, and genomics.
        \item The ability to identify groups and anomalies makes clustering invaluable in data analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Essential Visualization Example}
    \begin{block}{Visualization Concept}
        Imagine a 2D scatter plot with colored dots representing data points. 
        \begin{itemize}
            \item Clustering algorithms form circles (clusters) around areas with high dot density.
            \item This visualizes how similar data points are grouped together.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Information}
    \begin{itemize}
        \item \textbf{Popular Clustering Algorithms:} 
            k-means, hierarchical clustering, DBSCAN, Gaussian mixture models (GMM).
        \item \textbf{Evaluation Metrics:} 
            Quality of clustering can be evaluated using metrics like silhouette score and Davies-Bouldin index.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering?}
    \begin{block}{Definition of Clustering}
        Clustering is a data analysis technique that involves grouping a set of objects in such a way that objects within the same group (or cluster) are more similar to each other than those in other groups. It is an unsupervised learning method where the algorithm identifies patterns without predefined labels.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Clustering in Data Analysis}
    Clustering plays a critical role in various fields of data analysis, such as:
    \begin{itemize}
        \item \textbf{Data Exploration:} Understanding the underlying structure of the data by revealing natural groupings.
        \item \textbf{Market Segmentation:} Identifying distinct customer groups based on purchasing behavior, allowing tailored marketing strategies.
        \item \textbf{Image and Document Retrieval:} Classifying and organizing large datasets to facilitate easier retrieval and analysis of relevant information.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics and Examples of Clustering}
    \begin{block}{Key Characteristics of Clustering}
        \begin{itemize}
            \item \textbf{Unsupervised Learning:} No prior labels are required; the algorithm discovers patterns on its own.
            \item \textbf{Similarity Measure:} Clusters are formed based on a similarity measure (e.g., Euclidean distance).
            \item \textbf{Scalability:} Effective for large datasets, enabling quick processing and visualization.
        \end{itemize}
    \end{block}

    \begin{block}{Examples of Clustering Applications}
        \begin{enumerate}
            \item \textbf{Social Network Analysis:} Grouping users based on their connections and interactions to identify communities.
            \item \textbf{Medical Diagnosis:} Classifying patients based on symptom similarities for targeted treatment.
            \item \textbf{Anomaly Detection:} Identifying abnormal patterns for applications such as fraud detection.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques}
    \begin{block}{Introduction to Clustering Techniques}
        Clustering is a vital unsupervised machine learning technique used to group a set of objects. 
        Objects in the same group (or cluster) are more similar to each other than to those in other groups. 
        Various techniques efficiently achieve clustering, each with its own strengths and weaknesses.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Clustering Techniques - Overview}
    In this session, we will explore four primary types of clustering techniques:
    \begin{itemize}
        \item Partitioning Techniques
        \item Hierarchical Techniques
        \item Density-Based Techniques
        \item Model-Based Techniques
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Partitioning Techniques}
    \begin{itemize}
        \item \textbf{Description:} Partitioning methods divide data into k clusters, each represented by a centroid. The aim is to minimize the distance between data points and their assigned centroid.
        \item \textbf{Example:} K-Means Clustering
        \begin{enumerate}
            \item Choose the number of clusters (k).
            \item Randomly initialize k centroids.
            \item Assign each data point to the nearest centroid.
            \item Update centroids by calculating the mean of all points in each cluster.
            \item Repeat steps 3-4 until centroids stabilize.
        \end{enumerate}
        \item \textbf{Key Point:} K-Means is sensitive to centroid initialization and may converge to a local minima.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Hierarchical Techniques}
    \begin{itemize}
        \item \textbf{Description:} Hierarchical clustering creates a tree-like structure (dendrogram). It can be agglomerative (bottom-up) or divisive (top-down).
        \item \textbf{Example:} Agglomerative Clustering
        \begin{enumerate}
            \item Treat each data point as a singleton cluster.
            \item Merge the two closest clusters.
            \item Repeat until only one cluster remains or a desired number of clusters is reached.
        \end{enumerate}
        \item \textbf{Key Point:} Hierarchical methods do not require the number of clusters to be specified in advance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Density-Based Techniques}
    \begin{itemize}
        \item \textbf{Description:} Density-based methods group points that are closely packed together while marking outliers that lie alone in low-density regions.
        \item \textbf{Example:} DBSCAN
        \begin{enumerate}
            \item Define parameters: eps (neighborhood radius) and minPts (minimum number of points in the neighborhood).
            \item Classify core points, border points, and noise.
            \item Form clusters based on core points that are reachable from each other.
        \end{enumerate}
        \item \textbf{Key Point:} DBSCAN can discover arbitrary-shaped clusters and is robust against noise.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Model-Based Techniques}
    \begin{itemize}
        \item \textbf{Description:} These techniques assume a statistical model for the data and find the best fit. They examine relationships and distributions within the data.
        \item \textbf{Example:} Gaussian Mixture Models (GMM)
        \begin{enumerate}
            \item Assume the data is generated from a mixture of several Gaussian distributions.
            \item Use algorithms like Expectation-Maximization to identify clusters.
        \end{enumerate}
        \item \textbf{Key Point:} GMMs allow for different shapes and sizes of clusters and can handle overlaps between different clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Clustering Techniques}
    Clustering techniques can broadly be categorized as follows:
    \begin{itemize}
        \item \textbf{Partitioning Techniques}: e.g., K-Means
        \item \textbf{Hierarchical Techniques}: e.g., Agglomerative Clustering
        \item \textbf{Density-Based Techniques}: e.g., DBSCAN
        \item \textbf{Model-Based Techniques}: e.g., Gaussian Mixture Models
    \end{itemize}
    Understanding these techniques is crucial for revealing the underlying structure of data and will prepare you for a deep dive into specific methods in the upcoming sections.
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering}
    K-Means Clustering is a popular unsupervised machine learning algorithm used to partition datasets into K distinct, non-overlapping clusters. 
    The aim is to group similar data points together while ensuring that clusters are as distinct as possible.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps of the K-Means Algorithm}
    \begin{enumerate}
        \item \textbf{Initialization:}
        \begin{itemize}
            \item Choose the number of clusters, K.
            \item Randomly select K initial centroids from the dataset.
        \end{itemize}
        
        \item \textbf{Assignment Step:} Assign each data point to the nearest centroid based on a distance metric: 
        \begin{equation}
            \text{Distance}(x_i, \text{centroid}_j) = \sqrt{\sum_{d=1}^{D} (x_{id} - c_{jd})^2}
        \end{equation}

        \item \textbf{Update Step:} Calculate the new centroids:
        \begin{equation}
            c_{j}^{new} = \frac{1}{|C_j|} \sum_{x \in C_j} x
        \end{equation}

        \item \textbf{Convergence Check:} Repeat steps until centroids stabilize or assignments do not change.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Centroid Initialization and Convergence}
    \textbf{Centroid Initialization:}
    \begin{itemize}
        \item \textbf{Random Initialization:} Select random data points as initial centroids.
        \item \textbf{K-Means++:} A method that spreads out the initial centroids for better clustering quality.
    \end{itemize}

    \textbf{Convergence Criteria:} 
    \begin{itemize}
        \item K-Means can converge to a local minimum depending on the initial centroid positions.
        \item Multiple runs with different initializations can yield different results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of K-Means Clustering}
    Assume we have the following dataset representing customer purchase behavior:

    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            Customer & Spend (X) & Frequency (Y) \\
            \hline
            A & 200 & 5 \\
            B & 180 & 4 \\
            C & 250 & 10 \\
            D & 300 & 20 \\
            E & 150 & 2 \\
            \hline
        \end{tabular}
    \end{table}

    In applying K-Means with K=2:
    \begin{itemize}
        \item Initialize centroids (e.g., customers A and C).
        \item Assign each customer to the nearest centroid.
        \item Update centroids based on customer assignments.
        \item Repeat until centroids stabilize.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering Examples - Introduction}
    \begin{block}{Overview}
        K-Means is a powerful unsupervised learning algorithm used for clustering:
        \begin{itemize}
            \item Groups data points into distinct clusters.
            \item Minimizes variance within clusters and maximizes variance between clusters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering Examples - Visual Example 1}
    \begin{block}{Customer Segmentation}
        \textbf{Dataset:} Retail Customer Data\\
        \textbf{Description:} Annual spending and age of customers.
        
        \begin{itemize}
            \item \textbf{Clusters Identified:}
            \begin{itemize}
                \item Cluster 1: Young, low spenders
                \item Cluster 2: Middle-aged, moderate spenders
                \item Cluster 3: Older, high spenders
            \end{itemize}
            \item \textbf{Interpretation:} Enables targeted marketing strategies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Diagram Explanation}
        \begin{itemize}
            \item Each point represents a customer.
            \item Centroids (K) show the center of each cluster.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering Examples - Visual Example 2}
    \begin{block}{Wine Quality Analysis}
        \textbf{Dataset:} Wine Quality Ratings\\
        \textbf{Description:} Features include acidity, sweetness, and alcohol content.
        
        \begin{itemize}
            \item \textbf{Clusters Identified:}
            \begin{itemize}
                \item Cluster 1: Low-quality wines (high acidity, low sweetness)
                \item Cluster 2: Medium-quality wines (balanced attributes)
                \item Cluster 3: High-quality wines (low acidity, high sweetness)
            \end{itemize}
            \item \textbf{Interpretation:} Reveals relationships guiding wine producers.
        \end{itemize}
    \end{block}
    
    \begin{block}{Diagram Explanation}
        \begin{itemize}
            \item Colors distinguish quality clusters.
            \item Dimensionality reduction visualizes higher-dimensional attributes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering - Key Points and Additional Insights}
    \begin{block}{Key Points to Emphasize}
        \begin{enumerate}
            \item Each cluster indicates a unique profile for decision-making.
            \item Understanding centroid movement helps grasp algorithm convergence.
            \item Choosing the value of k significantly influences clustering quality.
        \end{enumerate}
    \end{block}

    \begin{block}{Algorithm Steps Overview}
        \begin{enumerate}
            \item Initialization: Select k initial centroids.
            \item Assignment: Assign each point to the nearest centroid.
            \item Update: Recalculate centroids based on memberships.
            \item Convergence Check: Repeat until centroids stabilize.
        \end{enumerate}
    \end{block}

    \begin{block}{Mathematical Representation}
        \begin{equation}
            J = \sum_{i=1}^{k} \sum_{x_j \in C_i} ||x_j - \mu_i||^2
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Overview}
    \begin{block}{What is Hierarchical Clustering?}
        Hierarchical clustering is a technique for grouping similar objects into clusters.
        It builds a hierarchy of clusters represented by a dendrogram.
    \end{block}
    
    \begin{block}{Types of Hierarchical Clustering}
        \begin{itemize}
            \item Agglomerative Approach (Bottom-Up)
            \item Divisive Approach (Top-Down)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Agglomerative Approach}
    \begin{block}{Agglomerative Approach}
        \begin{enumerate}
            \item Starts with each data point as an individual cluster.
            \item Iteratively merges the closest pair of clusters.
            \item Common linkage criteria:
            \begin{itemize}
                \item \textbf{Single Linkage}: Minimum distance between elements in different clusters.
                \item \textbf{Complete Linkage}: Maximum distance between elements in different clusters.
                \item \textbf{Average Linkage}: Average distance between pairs of elements in different clusters.
            \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Example}
        Assume distances: 
        \( \text{Dist}(A, B) = 1, \text{Dist}(A, C) = 2, \text{Dist}(B, C) = 1.5, \text{Dist}(D, A) = 3 \).
        Start merging A and B, followed by the next closest clusters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Divisive Approach and Dendrograms}
    \begin{block}{Divisive Approach}
        \begin{itemize}
            \item Begins with one cluster containing all data points.
            \item Iteratively splits the most dissimilar cluster until each data point is its own cluster.
            \item Less commonly used due to complexity.
        \end{itemize}
    \end{block}

    \begin{block}{Dendrogram Interpretation}
        - A dendrogram visually represents the clustering process.
        - Vertical axis indicates distance or dissimilarity for clusters merging.
        \begin{itemize}
            \item **Height** signifies cluster similarity; lower heights indicate more similar clusters.
            \item **Cutting the Dendrogram** allows for choosing the number of clusters by cutting at a specified height.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Dendrogram}
    Consider clusters emerging from points A, B, C, and D:

    \begin{lstlisting}
       |
       |        --------
       |       |
       |       |           -----
       |-------|          |     |
       |       |----------     |----- 
       |       |            | B |    |
    ___|_____|______       |___|___|___|___
          A      D     C
    \end{lstlisting}
    
    \begin{block}{Key Takeaways}
        - Hierarchical clustering allows for flexible data representation through dendrograms.
        - Applicable in various fields: biology, marketing, document clustering.
        - Agglomerative clustering is popular due to its simplicity and efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering Examples - Introduction}
    Hierarchical clustering is a method of cluster analysis that builds a hierarchy of clusters. It is particularly valuable for exploratory data analysis. 
    \begin{itemize}
        \item This slide presents visual examples of hierarchical clustering results.
        \item Dendrograms are used to demonstrate these clustering outcomes.
        \item We will discuss various applications of hierarchical clustering on different datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrograms}
    A dendrogram is a tree-like diagram that records the sequences of merges or splits during the clustering process, visually representing the arrangement of clusters and the distance at which they merge.
    \begin{block}{Key Components of Dendrograms}
        \begin{itemize}
            \item **X-Axis:** Represents individual data points or clusters.
            \item **Y-Axis:** Represents the distance at which clusters are merged.
            \item **Branches:** Indicate merging of clusters; longer branches signify greater distances.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Hierarchical Clustering}
    \textbf{Example 1: Animal Species Clustering}\\
    - Dataset: Animal characteristics (weight, height, habitat)\\
    - \textit{Dendrogram Interpretation:}
        \begin{itemize}
            \item Each species starts as its own cluster, merging as we move up.
            \item Example: A cluster including dogs merges with cats at a certain similarity level.
        \end{itemize}
    \textbf{Application:} Aids in understanding species similarities, classification, and evolutionary studies.

    \vspace{0.5cm}
    
    \textbf{Example 2: Customer Segmentation}\\
    - Dataset: Customer purchase history\\
    - \textit{Dendrogram Interpretation:}
        \begin{itemize}
            \item Customers with similar habits cluster together.
            \item Different branches indicate distinct buying segments (e.g., 'Frequent Shoppers' vs. 'Occasional Buyers').
        \end{itemize}
    \textbf{Application:} Supports targeted marketing and personalization strategies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continued Examples}
    \textbf{Example 3: Gene Expression Analysis}\\
    - Application in bioinformatics to group genes with similar expression patterns across conditions.
    - \textit{Dendrogram Interpretation:}
        \begin{itemize}
            \item Genes with similar expression profiles during stress may cluster together.
            \item Dendrograms show similarity in gene expression under various conditions.
        \end{itemize}
    \textbf{Application:} Helps researchers identify gene functions and pathways in biological processes.
    
    \vspace{0.5cm}

    \textbf{Key Points Emphasized:}
    \begin{itemize}
        \item **Flexibility:** No need to specify the number of clusters in advance.
        \item **Visualization:** Dendrograms provide insights into data structure and relationships.
        \item **Sensitivity:** Can be affected by noise and outliers in the data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    Hierarchical clustering, represented intuitively through dendrograms, is a powerful tool across various fields (biology, marketing, social sciences).
    \begin{itemize}
        \item Understanding outputs can drive insightful decisions based on data structure.
    \end{itemize}
    \textbf{Next Steps:} Upcoming slides will compare k-means and hierarchical clustering focusing on their pros, cons, and suitable applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of K-Means and Hierarchical Clustering}
    \begin{itemize}
        \item Overview of clustering techniques
        \item K-Means Clustering
        \item Hierarchical Clustering
        \item Summary Table
        \item Conclusion
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Overview of Clustering Techniques}
    Clustering is a key technique in unsupervised learning used to group similar data points based on features.
    \begin{itemize}
        \item Two popular methods are
        \begin{itemize}
            \item K-Means Clustering
            \item Hierarchical Clustering
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. K-Means Clustering}
    \textbf{Explanation:}
    \begin{itemize}
        \item Partitions data into K clusters based on proximity to centroids.
        \item Involves iterative refinement through centroid adjustment.
    \end{itemize}

    \textbf{Key Points:}
    \begin{itemize}
        \item \textbf{Pros:}
        \begin{itemize}
            \item Efficiency: O(n * k * i)
            \item Simplicity: Easy to implement
            \item Speed: Quick convergence
        \end{itemize}
        \item \textbf{Cons:}
        \begin{itemize}
            \item K Selection: Requires predefined K
            \item Sensitive to Initialization: Variability in outcomes
            \item Shape and Size Limitations: Struggles with irregular clusters
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of K-Means Clustering}
    Consider a dataset of customers' purchasing behavior. 
    \begin{itemize}
        \item Using K-Means, we can find clusters such as high, medium, and low spenders by setting K=3.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Hierarchical Clustering}
    \textbf{Explanation:}
    \begin{itemize}
        \item Builds a hierarchy of clusters via bottom-up (agglomerative) or top-down (divisive) approaches.
        \item Produces a dendrogram representing nested clusters.
    \end{itemize}

    \textbf{Key Points:}
    \begin{itemize}
        \item \textbf{Pros:}
        \begin{itemize}
            \item No predefined clusters: Flexible grouping
            \item Dendrogram visualization: Provides clear structure
        \end{itemize}
        \item \textbf{Cons:}
        \begin{itemize}
            \item Computational Complexity: Slower; O(n^3)
            \item Scalability Issues: Less effective with large datasets
            \item Noise Sensitivity: Vulnerable to outliers
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Hierarchical Clustering}
    A hierarchical approach could analyze genetic similarity among species. 
    \begin{itemize}
        \item The resulting dendrogram shows genetic relationships.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Summary Table}
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Feature} & \textbf{K-Means} & \textbf{Hierarchical Clustering} \\
            \hline
            Initialization & Requires K to be set & No initial K required \\
            \hline
            Output Structure & Flat clusters & Nested clusters (dendrogram) \\
            \hline
            Complexity & O(n * k * i) & O(n\textsuperscript{3}) for certain methods \\
            \hline
            Scalability & Strong with large data & Poor with very large data \\
            \hline
            Sensitivity to Outliers & High & High \\
            \hline
            Ease of Interpretation & Simple & Visual (dendrogram) \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Conclusion}
    \begin{itemize}
        \item Both K-Means and Hierarchical Clustering serve unique purposes.
        \item Selection depends on:
        \begin{itemize}
            \item Data characteristics
            \item Scalability needs
            \item Interpretive preferences
        \end{itemize}
        \item Understanding these differences helps to design effective clustering strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics for Clustering}
    \begin{block}{Introduction}
        When performing clustering analysis, it's essential to assess the quality of the clusters formed. Evaluation metrics help to understand how well the clusters represent the underlying data structure.
    \end{block}
    \begin{itemize}
        \item Silhouette Coefficient
        \item Davies-Bouldin Index
        \item Inertia
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Silhouette Coefficient}
    \begin{block}{Definition}
        Measures how similar an object is to its own cluster compared to other clusters.
    \end{block}
    \begin{itemize}
        \item \textbf{Range}: -1 to +1
        \begin{itemize}
            \item +1: Well clustered
            \item 0: Overlapping clusters
            \item -1: Misassigned
        \end{itemize}
        \item \textbf{Formula}:
        \[
        S(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
        \]
        \begin{itemize}
            \item \( a(i) \): Average distance to points in the same cluster
            \item \( b(i) \): Average distance to points in the nearest cluster
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Silhouette Coefficient}
    \begin{block}{Customer Purchase Dataset}
        If a customer in Cluster A is much closer to other customers in Cluster A than to those in Cluster B, the Silhouette Score will be high.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Davies-Bouldin Index}
    \begin{block}{Definition}
        Assesses the average similarity ratio of each cluster with its most similar cluster.
    \end{block}
    \begin{itemize}
        \item \textbf{Range}: Lower values indicate better clustering.
        \item \textbf{Formula}:
        \[
        DB = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \left(\frac{s_i + s_j}{d(i, j)}\right)
        \]
        \begin{itemize}
            \item \( k \): Number of clusters
            \item \( s_i \): Average distance within cluster \( i \)
            \item \( d(i,j) \): Distance between clusters \( i \) and \( j \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Davies-Bouldin Index}
    \begin{block}{Cluster Comparison}
        If two clusters are very close to each other and have a large spread of points, the Davies-Bouldin Index will increase, indicating poorer clusters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inertia}
    \begin{block}{Definition}
        Measures how tightly the clusters are packed. It is the sum of squared distances from points to their respective cluster centers.
    \end{block}
    \begin{itemize}
        \item \textbf{Range}: Non-negative score; lower values indicate more compact clusters.
        \item \textbf{Formula}:
        \[
        I = \sum_{i=1}^{n} \sum_{j=1}^{k} \| x_i - c_j \|^2
        \]
        \begin{itemize}
            \item \( n \): Total number of data points
            \item \( k \): Total number of clusters
            \item \( x_i \): Data point \( i \)
            \item \( c_j \): Centroid of cluster \( j \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Inertia}
    \begin{block}{K-Means Clustering Scenario}
        If points are close to their centroids, inertia will be minimized, indicating effective clustering.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Silhouette Coefficient: Ideal for understanding cluster cohesiveness vs. separation.
        \item Davies-Bouldin Index: Useful for comparing different clustering solutions.
        \item Inertia: A practical metric in K-Means to evaluate how close the data points are to their cluster centroids.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Understanding and applying these evaluation metrics provide valuable insights into the effectiveness of clustering algorithms. These metrics help to refine clustering techniques and improve model accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Overview}
    \begin{block}{Overview of Clustering}
        Clustering is a fundamental technique in data analysis that groups similar objects into clusters. This allows us to discover patterns and relationships within data, which helps in understanding the data better and informs decision-making in various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Marketing}
    \begin{block}{1. Marketing}
        Clustering is widely used in marketing to segment customers based on their buying behavior, demographics, and preferences. This enables businesses to tailor their marketing strategies and product offerings.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Example: Customer Segmentation}
        \begin{itemize}
            \item Retail companies use clustering to identify groups of customers with similar buying patterns.
            \item For instance, a clothing retailer might use k-means clustering to segment customers into groups like ``budget-conscious,'' ``trendy shoppers,'' and ``luxury buyers.''
            \item This segmentation allows targeted advertising campaigns, personalized recommendations, and improved customer retention.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Biology and Social Sciences}
    \begin{block}{2. Biology}
        In biology, clustering techniques are crucial for classifying species, analyzing genetic data, and studying ecological patterns.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Example: Gene Expression Analysis}
        \begin{itemize}
            \item Biologists utilize hierarchical clustering to group genes based on their expression levels across various conditions or time points.
            \item Clusters of co-expressed genes during a developmental phase can provide insights into biological processes and disease mechanisms.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{3. Social Sciences}
        Clustering methods help analyze survey data, understand societal trends, and enhance community planning.
    \end{block}

    \begin{itemize}
        \item \textbf{Example: Survey Data Analysis}
        \begin{itemize}
            \item Researchers can categorize responses from large surveys into distinct groups.
            \item In public health studies, clustering can identify different attitudes towards healthcare access among various demographics, guiding policy-making and improving community health initiatives.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Key Points and Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Clustering helps recognize patterns and relationships in complex datasets.
            \item Applications span multiple fields, including marketing, biology, and social sciences.
            \item Real-world examples demonstrate the practicality and effectiveness of clustering techniques.
        \end{itemize}
    \end{block}
    
    \begin{block}{Summary}
        Clustering is a powerful tool that provides insightful interpretations of data across various domains. By grouping similar items, such as customers, genes, or survey responses, we can derive meaningful conclusions that drive strategy, research, and innovation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Clustering}
    \begin{block}{Introduction to Ethical Implications}
        Clustering can significantly affect decision-making in areas such as healthcare, marketing, and law enforcement. However, ethical responsibilities and challenges must be addressed to prevent unintended consequences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations}
    \begin{enumerate}
        \item \textbf{Data Bias}
        \begin{itemize}
            \item \textbf{Definition}: Bias due to unrepresentative data can arise from:
            \begin{itemize}
                \item Sampling error
                \item Historical discrimination
                \item Incomplete data
            \end{itemize}
            \item \textbf{Impact}: Can lead to stereotypical groupings, reinforcing inequalities.
            \item \textbf{Example}: Marketing algorithms based on biased purchasing data may misrepresent demographics.
        \end{itemize}
        
        \item \textbf{Privacy Concerns}
        \begin{itemize}
            \item \textbf{Definition}: Clustering may inadvertently identify individuals.
            \item \textbf{Impact}: Risks of harassment or discrimination due to unauthorized data exposure.
            \item \textbf{Example}: Clustering in social media could reveal sensitive user information.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Ethical Considerations}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Transparency and Accountability}
        \begin{itemize}
            \item \textbf{Definition}: Clustering methods are often obscure.
            \item \textbf{Impact}: Lack of clarity can erode trust and accountability.
            \item \textbf{Example}: Resource allocation in law enforcement without method explanation can lead to biases.
        \end{itemize}

        \item \textbf{Interpretation and Misuse}
        \begin{itemize}
            \item \textbf{Definition}: Clusters may be misinterpreted, leading to harmful outcomes.
            \item \textbf{Impact}: Incorrect interpretations can influence policy and strategy negatively.
            \item \textbf{Example}: Misclassification in healthcare could result in inadequate treatments for patients.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Awareness of Bias}: Assess data for biases before clustering.
        \item \textbf{Ensure Privacy}: Use anonymization and aggregation techniques.
        \item \textbf{Promote Transparency}: Document methods to strengthen trust.
        \item \textbf{Critical Interpretation}: Encourage careful analysis to avoid misleading conclusions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Further Reading}
    \begin{block}{Conclusion}
        Ethical considerations in clustering are vital. Data practitioners must ensure methods promote fairness, protect privacy, and maintain transparency for responsible outcomes.
    \end{block}
    \begin{block}{References}
        \begin{itemize}
            \item "Weapons of Math Destruction" by Cathy O'Neil
            \item "Data Science for Business" by Foster Provost \& Tom Fawcett
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Summary of Clustering Techniques}
    \begin{block}{Overview of Clustering Techniques}
        Clustering is a fundamental technique in data mining and machine learning used to group similar data points together, helping uncover patterns and structures within large datasets without requiring labeled outcomes. In this summary, we will revisit the key points discussed regarding clustering techniques, their purpose, and their importance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Definition and Algorithms}
    \begin{enumerate}
        \item \textbf{Definition of Clustering:}
        \begin{itemize}
            \item Partitioning a set of data points into distinct groups (clusters), where points within a cluster are more similar to each other than to those in other clusters.
        \end{itemize}

        \item \textbf{Common Clustering Algorithms:}
        \begin{itemize}
            \item \textbf{K-Means Clustering:} A centroid-based algorithm that forms 'K' clusters by minimizing variance within clusters.
              \begin{itemize}
                  \item \textit{Example:} Segmenting customers based on purchasing behaviors.
              \end{itemize}

            \item \textbf{Hierarchical Clustering:} Builds a multi-level hierarchy of clusters through agglomerative or divisive strategies.

            \item \textbf{DBSCAN:} Identifies clusters based on density of data points, handling varying shapes.
              \begin{itemize}
                  \item \textit{Example:} Geographic clustering of user locations.
              \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Evaluation, Applications, and Ethics}
    \begin{enumerate}
        \item \textbf{Evaluation Metrics for Clustering:}
        \begin{itemize}
            \item \textbf{Silhouette Score:} Measures how similar an object is to its own cluster. Score ranges between -1 and 1, with higher values indicating better-defined clusters.
            \item \textbf{Inertia:} Sum of squared distances of samples to their closest cluster center in K-Means; lower inertia indicates better clustering.
        \end{itemize}

        \item \textbf{Applications of Clustering:}
        \begin{itemize}
            \item \textbf{Market Segmentation:} Identifying distinct customer segments.
            \item \textbf{Anomaly Detection:} Recognizing outliers for fraud detection.
            \item \textbf{Image Segmentation:} Grouping similar pixels for object detection.
        \end{itemize}
        
        \item \textbf{Ethical Considerations:}
        \begin{itemize}
            \item Clustering decisions must be made with awareness of biases; ethical practices are crucial to avoid unfair outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Clustering in Data Mining}
    \begin{block}{Significance}
        Clustering techniques are vital for understanding data distribution, enhancing decision-making, and serving as a foundation for exploratory data analysis. By leveraging these techniques, data scientists and analysts can derive insights and take strategic actions based on uncovered patterns.
    \end{block}
    
    \begin{block}{Conclusion}
        Clustering is an essential tool for data exploration and analysis, with diverse applications across industries. Understanding different techniques empowers effective and ethical data utilization.
    \end{block}
\end{frame}


\end{document}