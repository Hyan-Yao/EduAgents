\frametitle{Training Neural Networks - Backpropagation and Optimization}
    \begin{itemize}
        \item \textbf{Backpropagation:}
            \begin{itemize}
                \item \textbf{Forward Pass:} Compute predictions through network layers.
                \item \textbf{Loss Calculation:} Measure prediction error using a loss function (e.g., Mean Squared Error).
                \item \textbf{Backward Pass:} Compute gradients using the chain rule and update weights.
            \end{itemize}
        \item \textbf{Loss Function Example:}
        \begin{equation}
        \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \end{equation}
        \item \textbf{Optimization Techniques:}
            \begin{itemize}
                \item \textbf{Stochastic Gradient Descent (SGD):} Updates weights using small batches.
                \item \textbf{Adaptive Learning Rates:}
                    \begin{itemize}
                        \item \textbf{Adam Optimizer:} Adjusts learning rates for each parameter.
                        \item \textbf{Formula:}
                        \begin{equation}
                        m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
                        v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
                        \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t
                        \end{equation}
                    \end{itemize}
            \end{itemize}
    \end{itemize}
