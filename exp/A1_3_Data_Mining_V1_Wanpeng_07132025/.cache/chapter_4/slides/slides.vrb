\frametitle{Major Activation Functions}
    \begin{enumerate}
        \item **Sigmoid Function**
        \begin{itemize}
            \item Formula: \( f(x) = \frac{1}{1 + e^{-x}} \)
            \item Output Range: (0, 1)
            \item Use: Binary classification tasks.
            \item Pros: Smooth gradient; interpretable as probabilities.
            \item Cons: Vanishing gradient issue.
        \end{itemize}

        \item **ReLU (Rectified Linear Unit)**
        \begin{itemize}
            \item Formula: \( f(x) = \max(0, x) \)
            \item Output Range: [0, âˆž)
            \item Use: Hidden layers of deep networks.
            \item Pros: Reduces vanishing gradient issues.
            \item Cons: "Dying ReLU" problem.
        \end{itemize}

        \item **Tanh (Hyperbolic Tangent)**
        \begin{itemize}
            \item Formula: \( f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)
            \item Output Range: (-1, 1)
            \item Use: Hidden/output layers of certain architectures.
            \item Pros: Zero-centered for faster convergence.
            \item Cons: Still suffers from vanishing gradients.
        \end{itemize}
    \end{enumerate}
