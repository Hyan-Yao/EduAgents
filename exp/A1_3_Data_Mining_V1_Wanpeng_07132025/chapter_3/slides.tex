\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Classification Algorithms}
    \begin{block}{Definition}
        Classification algorithms are machine learning techniques used in data mining to categorize data into predefined classes or groups.
    \end{block}
    
    \begin{block}{Importance}
        They transform raw data into structured information, facilitating decision-making processes across various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Do We Need Classification in Data Mining?}
    \begin{itemize}
        \item \textbf{Decision Making:} Helps make informed decisions by predicting outcomes based on historical data.
        \begin{itemize}
            \item Example: Predicting loan applicant risk.
        \end{itemize}
        
        \item \textbf{Automation:} Enables automated systems to classify data, increasing efficiency and reducing errors.
        \begin{itemize}
            \item Example: Email categorization (Spam, Promotions, Primary).
        \end{itemize}
        
        \item \textbf{Insight Extraction:} Helps glean insights into patterns and relationships.
        \begin{itemize}
            \item Example: Classifying customer reviews into categories.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}
    Classification algorithms are integral to many contemporary AI applications, demonstrating importance in various fields:
    \begin{itemize}
        \item \textbf{Healthcare:} Diagnosing diseases from patient data.
        \begin{itemize}
            \item Example: Classifying medical images to detect tumors.
        \end{itemize}
        
        \item \textbf{Finance:} Fraud detection systems.
        \begin{itemize}
            \item Example: Identifying fraudulent transactions.
        \end{itemize}
        
        \item \textbf{Retail:} Customer segmentation and targeted marketing.
        \begin{itemize}
            \item Example: Classifying shoppers based on purchase history.
        \end{itemize}
        
        \item \textbf{Natural Language Processing (NLP):} Applications like ChatGPT utilize classification algorithms to enhance user interaction.
        \begin{itemize}
            \item Example: Classifying user queries for intent understanding.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Classification algorithms are essential for transforming datasets into actionable insights across a range of industries.
        \item They enhance decision-making, automate tasks, and contribute to AI applications' development.
        \item Real-world use cases span various fields, highlighting their versatility and importance.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Understanding and applying classification algorithms unlocks the potential of data, contributing to advancements in technology and service delivery.
    \end{block}
    
    \begin{block}{Next}
        We will delve deeper into what classification algorithms are, including definitions, purposes, and key tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Classification Algorithms? - Introduction}
    \begin{block}{Definition}
        Classification algorithms are a subset of supervised machine learning techniques that categorize data points into predefined classes based on their features. 
    \end{block}
    
    \begin{block}{Purpose}
        \begin{itemize}
            \item \textbf{Making Predictions:} Predict categorical labels based on input features (e.g., classifying emails as 'spam' or 'not spam').
            \item \textbf{Data Organization:} Facilitate better data understanding and decision-making through meaningful class organization.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Classification Algorithms? - Key Tasks}
    \begin{block}{Key Tasks in Classification}
        \begin{enumerate}
            \item \textbf{Categorizing Data Points:} Identify categories of new observations using learned patterns.
            \item \textbf{Model Training:} Train the model using labeled data to map input features to class labels.
            \item \textbf{Model Evaluation:} Assess performance using metrics such as accuracy, precision, recall, and F1-score.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Examples of Classification Algorithms}
        \begin{itemize}
            \item Logistic Regression
            \item Naive Bayes
            \item Support Vector Machines (SVM)
            \item Decision Trees
            \item Random Forest
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Classification Algorithms? - Applications}
    \begin{block}{Real-World Applications}
        \begin{itemize}
            \item \textbf{Healthcare:} Classifying patients based on symptoms.
            \item \textbf{Finance:} Detecting fraudulent transactions.
            \item \textbf{Social Media:} Categorizing posts for sentiment analysis.
            \item \textbf{Natural Language Processing:} Applications in intent detection and sentiment classification.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula Overview}
        A simplified representation of a classification model can be expressed as:
        \begin{equation}
            P(Y|X) = \frac{P(X|Y) \cdot P(Y)}{P(X)}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( P(Y|X) \) is the posterior probability.
            \item \( P(X|Y) \) is the likelihood of features given class.
            \item \( P(Y) \) is the prior probability.
            \item \( P(X) \) is the marginal likelihood of features.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Introduction}
    \begin{block}{What are Decision Trees?}
        Decision Trees are a popular and intuitive method used in classification tasks, where the goal is to categorize data points based on their features.
    \end{block}
    \begin{itemize}
        \item Similar to a flowchart, leading to conclusions at each branch based on decisions.
        \item Applications span various fields such as healthcare and finance.
    \end{itemize}
    \pause
    \begin{block}{Motivation}
        Understanding decision trees is crucial for mastering more complex classification algorithms in data mining and AI.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Structure}
    \begin{itemize}
        \item \textbf{Nodes:} Points of decision.
            \begin{itemize}
                \item \textit{Root Node:} Represents the entire dataset.
                \item \textit{Internal Nodes:} Further splits the dataset based on feature values.
            \end{itemize}
        \item \textbf{Branches:} Connections between nodes representing decisions based on feature outcomes.
        \item \textbf{Leaves:} Terminal nodes giving the final classification.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Prediction Process}
    \begin{enumerate}
        \item \textbf{Starting at the Root Node:} Evaluate the feature at the root and make a branching decision.
        \item \textbf{Following Branches:} Sequentially evaluate subsequent nodes by following relevant branches.
        \item \textbf{Reaching a Leaf Node:} Assign the predicted class based on the majority class in that leaf.
    \end{enumerate}
    \pause
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Advantages:} Easy interpretation, handles numeric/categorical data, minimal preparation.
            \item \textbf{Disadvantages:} Prone to overfitting, sensitive to small data changes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Example}
    Consider the simple dataset with features "Weather" (Sunny, Overcast, Rain) and output "Play Tennis" (Yes, No):
    \begin{center}
    \begin{tikzpicture}[node distance=1.5cm, every node/.style={rectangle, draw, align=center}]
        \node (root) {Weather};
        \node (sunny) [below left=of root] {Sunny};
        \node (overcast) [below=of root] {Overcast};
        \node (rain) [below right=of root] {Rain};
        \node (humidity) [below left=of sunny] {Humidity};
        \node (yes) [below=of overcast] {Yes};
        \node (windy) [below right=of rain] {Windy};
        \node (high) [below left=of humidity] {High};
        \node (low) [below right=of humidity] {Low};
        \node (no) [below=of high] {No};
        \node (yes2) [below=of low] {Yes};
        \node (weak) [below left=of windy] {Weak};
        \node (strong) [below right=of windy] {Strong};
        \node (yes3) [below=of weak] {Yes};
        \node (no2) [below=of strong] {No};
        
        \draw[->] (root) -- (sunny);
        \draw[->] (root) -- (overcast);
        \draw[->] (root) -- (rain);
        \draw[->] (sunny) -- (humidity);
        \draw[->] (humidity) -- (high);
        \draw[->] (humidity) -- (low);
        \draw[->] (rain) -- (windy);
        \draw[->] (windy) -- (weak);
        \draw[->] (windy) -- (strong);
    \end{tikzpicture}
    \end{center}
    \pause
    \begin{block}{Structure Explained}
        - The root node corresponds to "Weather".
        - Splits based on weather conditions and further evaluates features leading to classifications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Decision Trees}
    \begin{itemize}
        \item Widely used in various fields:
            \begin{itemize}
                \item Healthcare: Diagnosing diseases.
                \item Finance: Credit approval.
            \end{itemize}
        \item Foundations of ensemble methods like Random Forests improving prediction accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Introduction}
    \begin{block}{Introduction to Decision Trees}
        Decision trees are powerful tools for classification and regression tasks in machine learning. 
        They mimic human decision-making by employing a tree-like model of decisions and their possible consequences.
    \end{block}
    \begin{itemize}
        \item Each node represents a feature (or attribute).
        \item Each branch represents a decision rule.
        \item Each leaf node represents an outcome (or class label).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Key Concepts}
    \begin{block}{1. Entropy}
        Entropy measures disorder or impurity in a dataset. It quantifies uncertainty in predicting class labels.
        \begin{equation}
            \text{Entropy}(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)
        \end{equation}
        where \( S \) is a dataset and \( p_i \) is the proportion of instances belonging to class \( i \).
    \end{block}
    \begin{block}{Example}
        For a dataset with two classes (A and B):
        \[
        \text{Entropy} = -[0.7 \log_2(0.7) + 0.3 \log_2(0.3)] \approx 0.88
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Key Concepts (Cont.)}
    \begin{block}{2. Information Gain (IG)}
        Information gain is the reduction in entropy from partitioning the data based on a feature:
        \begin{equation}
            IG(S, A) = \text{Entropy}(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Entropy}(S_v)
        \end{equation}
    \end{block}
    \begin{block}{Example}
        If the entropy before a split is 0.88 and it reduces to 0.5 after splitting on attribute \( A \):
        \[
        IG(S, A) = 0.88 - 0.5 = 0.38
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Building a Decision Tree}
    \begin{enumerate}
        \item Start with the entire dataset as the root node.
        \item Calculate entropy and information gain for each feature.
        \item Select the feature with the highest information gain to split the data.
        \item Repeat recursively for each child node until:
        \begin{itemize}
            \item All instances in a node have the same class.
            \item No features are left to split.
            \item A predefined stopping criterion is met (e.g., maximum depth).
        \end{itemize}
    \end{enumerate}  
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Key Points & Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Interpretability:} Easy to interpret and visualize.
            \item \textbf{Non-Parametric Nature:} No assumption of underlying data distribution.
            \item \textbf{Tendency to Overfit:} Sensitive to noise; requires management (e.g., pruning).
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Decision trees are fundamental in machine learning, providing clear decision-making through intuitive structures. Understanding entropy and information gain is essential for building effective decision trees.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Limitations of Decision Trees - Overview}
    Decision trees are a popular method for classification and regression tasks in data mining and machine learning.
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Intuitive structure
            \item Strengths and weaknesses in model evaluation
            \item Applicability in real-world scenarios
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees}
    \begin{enumerate}
        \item \textbf{Interpretability}:
            \begin{itemize}
                \item Easily visualizable as flowcharts.
                \item Example: Loan approval decision based on income and credit score.
            \end{itemize}
        
        \item \textbf{Non-parametric}:
            \begin{itemize}
                \item No assumption of data distribution.
                \item Can handle both categorical and numerical data.
            \end{itemize}
        
        \item \textbf{Feature Selection}:
            \begin{itemize}
                \item Automatically selects features based on information gain.
                \item Illustration: Only relevant variables are used for splits.
            \end{itemize}

        \item \textbf{Robust to Outliers}:
            \begin{itemize}
                \item Effective even with outliers as splits depend on thresholds.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees}
    \begin{enumerate}
        \item \textbf{Overfitting}:
            \begin{itemize}
                \item Complex trees may fit noise instead of actual patterns.
                \item Example: Splitting on every unique value can lead to poor performance on unseen data.
                \item \textbf{Solution}: Pruning or setting a maximum depth.
            \end{itemize}
        
        \item \textbf{Instability}:
            \begin{itemize}
                \item Small changes in dataset can lead to different tree structures.
                \item Illustration: Different trees can lead to varying predictions from similar samples.
            \end{itemize}
        
        \item \textbf{Bias towards Dominant Classes}:
            \begin{itemize}
                \item Favoring prevalent classes may lead to biased predictions.
                \item Example: Majority class (e.g., 80\% of the data points) could skew results.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    Decision trees offer **interpretability**, **non-parametric fitting**, and **automatic feature selection**. However, they are also prone to **overfitting**, **instability**, and can show **bias** against less common classes.

    \begin{block}{Conclusion}
        Understanding both advantages and limitations is crucial for effective application in data mining and machine learning, enhancing model selection and evaluation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Naive Bayes Classifier - Introduction}
    \begin{block}{Introduction to Naive Bayes}
        The Naive Bayes classifier is a powerful and popular supervised machine learning algorithm based on the principles of probability. It is widely used for applications like text classification (e.g., spam detection).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Naive Bayes Classifier - Motivations}
    \begin{itemize}
        \item \textbf{Efficiency}: Requires a small amount of training data for parameter estimation.
        \item \textbf{Interpretability}: Provides straightforward explanations of classification results.
        \item \textbf{Scalability}: Works well with large datasets for real-time applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Naive Bayes Classifier - Basic Principles}
    \begin{block}{Conditional Probability}
        Naive Bayes relies on Bayes' theorem:
        \begin{equation}
            P(C | F) = \frac{P(F | C) \cdot P(C)}{P(F)}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( P(C | F) \) = Probability of class \( C \) given features \( F \).
            \item \( P(F | C) \) = Probability of features \( F \) given class \( C \).
            \item \( P(C) \) = Prior probability of class \( C \).
            \item \( P(F) \) = Total probability of features \( F \).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Naive Bayes Classifier - Assumptions}
    \begin{block}{Feature Independence}
        Naive Bayes assumes that features are independent given the class label:
        \begin{equation}
            P(F | C) = P(f_1 | C) \cdot P(f_2 | C) \cdot \ldots \cdot P(f_n | C)
        \end{equation}
        \textbf{Implications}: This assumption allows for simplified calculations, and surprisingly, it performs well in practice despite often being violated.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Naive Bayes Classifier - Example: Email Classification}
    \begin{block}{Process}
        \begin{enumerate}
            \item Calculate prior probabilities: \( P(spam) \) and \( P(not \ spam) \).
            \item Calculate likelihoods for features: \( P(word_1 | spam) \), \( P(word_1 | not \ spam) \).
            \item Use Bayes' theorem to calculate the posterior probability for each class.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Naive Bayes Classifier - Key Points}
    \begin{itemize}
        \item Effective for large datasets and real-time applications.
        \item Despite strong independence assumptions, it often performs well.
        \item Commonly applied in fields like finance, medical diagnosis, and NLP tasks such as sentiment analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Naive Bayes Works - Overview}
    \begin{block}{Introduction}
        Naive Bayes is a classification technique based on Bayes' Theorem, which provides a probabilistic framework for predicting outcomes based on input features.
    \end{block}
    \begin{itemize}
        \item Importance of Bayes' Theorem
        \item Application in classification
        \item Independence assumption and implications
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem and Classification}
    \begin{block}{Bayes' Theorem}
        The theorem updates probabilities based on new evidence:
        \begin{equation}
            P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
        \end{equation}
        Where:
        \begin{itemize}
            \item $P(A|B)$: Posterior probability
            \item $P(B|A)$: Likelihood
            \item $P(A)$: Prior probability
            \item $P(B)$: Normalizing constant
        \end{itemize}
    \end{block}
    \begin{block}{Classification with Bayes' Theorem}
        For a given instance \( X \):
        \begin{equation}
            P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
        \end{equation}
        Determine class using:
        \begin{equation}
            \hat{C} = \arg\max_{C} P(C|X)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Independence Assumption and Applications}
    \begin{block}{Independence Assumption}
        Naive Bayes assumes features \( X_1, X_2, \ldots, X_n \) are conditionally independent given class \( C \):
        \begin{equation}
            P(X|C) = P(X_1|C) \cdot P(X_2|C) \cdots P(X_n|C)
        \end{equation}
        \begin{itemize}
            \item Simplifies computation, especially with many features.
            \item Often yields effective results despite its simplicity.
        \end{itemize}
    \end{block}
    \begin{block}{Real-World Applications}
        Naive Bayes is used in:
        \begin{itemize}
            \item Spam Detection: Classifying emails as spam or not.
            \item Sentiment Analysis: Classifying reviews as positive or negative.
            \item Medical Diagnosis: Predicting diseases based on symptoms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Limitations of Naive Bayes - Introduction}
    \begin{block}{Introduction}
        Naive Bayes is a family of probabilistic classifiers based on Bayes' theorem, which assumes independence between predictors. This slide outlines the strengths and weaknesses of the Naive Bayes classification algorithm.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Naive Bayes}
    \begin{itemize}
        \item \textbf{Simplicity}:
        \begin{itemize}
            \item Easy to implement and understand.
            \item Requires little training data to estimate parameters.
            \item \textit{Example:} Quickly classifies emails in spam detection based on keywords.
        \end{itemize}

        \item \textbf{Speed}:
        \begin{itemize}
            \item Highly efficient in training and prediction phases.
            \item Suitable for real-time applications.
            \item \textit{Example:} Handles large datasets (e.g., document classification) faster than complex algorithms.
        \end{itemize}

        \item \textbf{Performance}:
        \begin{itemize}
            \item Often performs well on text classification tasks (e.g., sentiment analysis).
            \item Effective with high-dimensional data.
        \end{itemize}

        \item \textbf{Robustness}:
        \begin{itemize}
            \item Handles irrelevant features well, beneficial in noisy datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Naive Bayes}
    \begin{itemize}
        \item \textbf{Independence Assumption}:
        \begin{itemize}
            \item Assumes all features are independent given the class label.
            \item \textit{Illustration:} Correlated features, like "free" and "offer" in an email, can mislead probabilities.
        \end{itemize}

        \item \textbf{Zero Frequency Problem}:
        \begin{itemize}
            \item Probability of zero to absent features can lead to erroneous classifications.
            \item \textit{Solution:} Techniques like Laplace smoothing can alleviate this issue.
        \end{itemize}

        \item \textbf{Limited Expressiveness}:
        \begin{itemize}
            \item Cannot capture relationships between features, leading to drawbacks in feature-interaction influenced scenarios.
        \end{itemize}

        \item \textbf{Poor Performance with Small Datasets}:
        \begin{itemize}
            \item Might not generalize well with small datasets, resulting in overfitting.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Easy to use:} Naive Bayes is simple, fast, and effective for large datasets, especially text classification.
        \item \textbf{Independence Assumption:} Can limit accuracy when features are interdependent.
        \item \textbf{Model Selection:} Understanding strengths and limitations is crucial for selecting the right model for specific tasks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{References}
    \begin{itemize}
        \item "Pattern Recognition and Machine Learning" by Christopher M. Bishop
        \item Online tutorials on text classification using Naive Bayes (e.g., sklearn documentation)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Overview}
    \begin{block}{Introduction}
        When building classification models, it is crucial to assess their performance using quantitative measures. Evaluation metrics provide insights into model predictions and guide decisions for improvements.
    \end{block}
    \begin{itemize}
        \item Importance of assessing model performance
        \item Overview of key metrics: Accuracy, Precision, Recall, F1 Score
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Key Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item \textbf{Definition:} Ratio of correctly predicted instances to total instances.
                \item \textbf{Formula:} 
                \[
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
                \]
                \item \textbf{Key Point:} Can be misleading on imbalanced datasets.
            \end{itemize}

        \item \textbf{Precision}
            \begin{itemize}
                \item \textbf{Definition:} Measures correctness of positive predictions.
                \item \textbf{Formula:} 
                \[
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
                \]
                \item \textbf{Key Point:} High precision is critical in high false positive cost scenarios.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Recall (Sensitivity)}
            \begin{itemize}
                \item \textbf{Definition:} Ability to identify all relevant instances (true positives).
                \item \textbf{Formula:}
                \[
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
                \]
                \item \textbf{Key Point:} High recall is important where missing positives is costly.
            \end{itemize}

        \item \textbf{F1 Score}
            \begin{itemize}
                \item \textbf{Definition:} Harmonic mean of precision and recall.
                \item \textbf{Formula:}
                \[
                \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
                \]
                \item \textbf{Key Point:} Useful in imbalanced class distributions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Definition}
    \begin{block}{Definition of Accuracy}
        Accuracy is one of the most straightforward metrics used in model evaluation, particularly for classification tasks. It is defined as the ratio of correctly predicted observations to the total number of observations.
    \end{block}
    \begin{equation}
        \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    Where:
    \begin{itemize}
        \item **TP** = True Positives (correctly predicted positive cases)
        \item **TN** = True Negatives (correctly predicted negative cases)
        \item **FP** = False Positives (incorrectly predicted positive cases)
        \item **FN** = False Negatives (incorrectly predicted negative cases)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Significance and Limitations}
    \begin{block}{Significance of Accuracy}
        \begin{itemize}
            \item **Simple Interpretation**: Provides a quick measure of performance. High accuracy indicates good model performance.
            \item **Model Comparison**: Acts as a common denominator for assessing model performance under similar conditions.
            \item **Baseline Metric**: Often serves as a baseline metric that any model must exceed to demonstrate real value.
        \end{itemize}
    \end{block}
    
    \begin{block}{Limitations of Accuracy}
        \begin{itemize}
            \item **Class Imbalance**: Can be misleading in class-imbalanced situations where accuracy doesn't reflect true model performance.
            \item **No Insight on Errors**: Does not provide details on the types of errors (false positives vs. false negatives) made by the model.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Real-World Example}
    \begin{block}{Real-World Application Example}
        Consider a spam email classification model:
        \begin{itemize}
            \item True Positives (TP): 80 emails correctly classified as spam
            \item True Negatives (TN): 100 emails correctly classified as not spam
            \item False Positives (FP): 10 emails incorrectly classified as spam
            \item False Negatives (FN): 10 emails incorrectly classified as not spam
        \end{itemize}
        Using the accuracy formula, we calculate:
        \begin{equation}
            \text{Accuracy} = \frac{80 + 100}{80 + 100 + 10 + 10} = \frac{180}{200} = 0.90 \text{ (or } 90\%\text{)}
        \end{equation}
        This demonstrates the importance of understanding other metrics for a complete performance evaluation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Definitions}
    \begin{block}{Understanding Precision and Recall}
        \begin{itemize}
            \item \textbf{Precision}: 
            \[
            \text{Precision} = \frac{TP}{TP + FP}
            \]
            Where TP = True Positives, FP = False Positives
            
            \item \textbf{Recall}: 
            \[
            \text{Recall} = \frac{TP}{TP + FN}
            \]
            Where FN = False Negatives
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Importance}
    \begin{block}{Importance of Precision and Recall}
        \begin{itemize}
            \item \textbf{Imbalanced Datasets}: 
            \begin{itemize}
                - Accuracy can be misleading (e.g., predicting all healthy in a 95\% healthy dataset).
            \end{itemize}
            \item \textbf{Contextual Relevance}: 
            \begin{itemize}
                - Precision is crucial when false positives are costly (e.g., spam detection).
                - Recall is vital when missing true positives is critical (e.g., cancer detection).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Scenarios}
    \begin{block}{Scenarios More Informative Than Accuracy}
        \begin{enumerate}
            \item \textbf{Spam Detection}:
                \begin{itemize}
                    - High Precision: Most classified emails as spam are indeed spam.
                    - High Recall: Most spam emails are detected.
                \end{itemize}
            \item \textbf{Medical Diagnostics}:
                \begin{itemize}
                    - High Recall is essential for critical diseases, like cancer.
                \end{itemize}
            \item \textbf{Fraud Detection}:
                \begin{itemize}
                    - High Precision ensures legitimate transactions are not misclassified.
                \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Use context to decide on crucial metrics (Precision, Recall, or both).
            \item Trade-off between Precision and Recall can be managed by adjusting thresholds.
            \item Best used together or with F1 Score for balanced evaluation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Definition}
    \begin{block}{Definition of the F1 Score}
        The \textbf{F1 Score} is a metric that provides a balance between \textbf{precision} and \textbf{recall}, defined as the harmonic mean of the two. 
        It is particularly useful in situations where class distributions are imbalanced.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Formula}
    \begin{block}{Formula}
        The F1 Score is calculated as follows:
        \begin{equation}
            F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{block}
    Where:
    \begin{itemize}
        \item \textbf{Precision} = \(\frac{True Positives}{True Positives + False Positives}\)
        \item \textbf{Recall} = \(\frac{True Positives}{True Positives + False Negatives}\)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Importance and Application}
    \begin{block}{Importance of F1 Score}
        \begin{itemize}
            \item \textbf{Balanced Metric}: Ideal for imbalanced datasets where accuracy might be misleading.
            \item \textbf{Focus on Positive Class}: Addresses important positive class in cases like spam detection and medical diagnosis.
        \end{itemize}
    \end{block}

    \begin{block}{Application in Imbalanced Class Scenarios}
        Consider a medical test for a rare disease:
        \begin{itemize}
            \item A model predicting 'healthy' for all might achieve high accuracy (99%), but fail to identify any positives.
            \item \textbf{Precision and Recall} could be low, highlighting the importance of using the F1 Score for evaluation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Example Calculation}
    \begin{block}{Example Calculation}
        Let's assume:
        \begin{itemize}
            \item True Positives (TP) = 20
            \item False Positives (FP) = 5
            \item False Negatives (FN) = 10
        \end{itemize}
        
        \textbf{Calculating Precision and Recall}:
        \begin{itemize}
            \item Precision = \( \frac{TP}{TP + FP} = \frac{20}{20 + 5} = 0.8 \)
            \item Recall = \( \frac{TP}{TP + FN} = \frac{20}{20 + 10} = \frac{2}{3} \approx 0.67 \)
        \end{itemize}
        
        \textbf{Calculating F1 Score}:
        \begin{equation}
            F1 = 2 \times \frac{0.8 \times \frac{2}{3}}{0.8 + \frac{2}{3}} \approx 0.727
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Summary and Visualization}
    \begin{block}{Summary}
        The F1 Score is crucial for evaluating models with imbalanced datasets.
        \begin{itemize}
            \item A higher F1 Score indicates a better balance between capturing true positives and minimizing false positives.
            \item It is essential in scenarios where false negatives are costly.
        \end{itemize}
    \end{block}

    \begin{block}{Visualization Tip}
        Consider using diagrams or graphs to visualize the trade-offs between precision and recall, reinforcing the concept.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Importance of Choosing the Right Metric}
    \begin{block}{Overview}
        Choosing the appropriate evaluation metric is crucial in classification tasks as it impacts the insights derived from model performance.
    \end{block}
    \begin{itemize}
        \item Different metrics highlight diverse aspects of model performance.
        \item Each metric is suitable for specific scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Common Classification Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textit{Definition:} Ratio of correct predictions to total instances.
            \item \textit{Formula:}  
            \[
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
            \]
            \item \textit{Use Case:} When class distribution is balanced.
        \end{itemize}
        
        \item \textbf{Precision}
        \begin{itemize}
            \item \textit{Definition:} True positive predictions to total predicted positives.
            \item \textit{Formula:}  
            \[
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \]
            \item \textit{Use Case:} High cost of false positives (e.g., spam detection).
        \end{itemize}
        
        \item \textbf{Recall}
        \begin{itemize}
            \item \textit{Definition:} True positive predictions to total actual positives.
            \item \textit{Formula:}  
            \[
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \]
            \item \textit{Use Case:} High cost of false negatives (e.g., fraud detection).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Choosing the Right Metric}
    \begin{itemize}
        \item Considerations for Metric Selection:
        \begin{itemize}
            \item \textbf{Imbalanced Classes}: Prefer F1 Score, Precision, or Recall over Accuracy.
            \item \textbf{Cost of Errors}: Analyze implications of false positives versus false negatives.
            \item \textbf{General Purpose}: Use Accuracy when classes are balanced or if metrics yield similar insights.
        \end{itemize}
        \item \textbf{Key Takeaways}:
        \begin{itemize}
            \item Metric choice significantly affects model performance evaluation.
            \item Understanding the context and problem domain is essential for selecting the right metrics.
            \item Evaluating models on multiple metrics provides a comprehensive view of their performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Application of Decision Trees and Naive Bayes}
    \begin{block}{Introduction to Classification Algorithms}
        Classification algorithms are crucial in data mining and machine learning as they help predict the category of data points. Two widely used algorithms are Decision Trees and Naive Bayes. Understanding their real-world applications can illustrate their significance and effectiveness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Decision Trees}
    \begin{block}{Definition}
        A Decision Tree is a flowchart-like structure that splits data into branches based on feature values, leading to decisions or classifications at the leaf nodes.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Use Cases:}
        \begin{itemize}
            \item \textbf{Healthcare Diagnosis:}
            \begin{itemize}
                \item Example: Helps diagnose diseases, classifying patients as "likely to have diabetes" or "not likely" based on factors like age, BMI, and blood sugar levels.
                \item Illustration: A tree might start with "Is BMI > 30?" leading to further questions.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Intuitive and easy to interpret.
            \item Handles both categorical and continuous data.
            \item Prone to overfitting if not properly tuned.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Naive Bayes}
    \begin{block}{Definition}
        Naive Bayes is a probabilistic classifier based on Bayes' Theorem, assuming independence among features.
    \end{block}

    \begin{itemize}
        \item \textbf{Use Cases:}
        \begin{itemize}
            \item \textbf{Spam Detection:}
            \begin{itemize}
                \item Example: Email services filter spam by analyzing word frequency to classify emails as "spam" or "not spam."
                \item Formula: 
                \begin{equation}
                    P(Class|Data) = \frac{P(Data|Class) \cdot P(Class)}{P(Data)}
                \end{equation}
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Highly efficient and fast, especially with large datasets.
            \item Works well with text classification problems.
            \item Relies on feature independence, which might not always hold true.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison and Insights}
    \begin{itemize}
        \item \textbf{Interpretability:} Decision Trees provide clear paths to decisions, whereas Naive Bayes offers probabilistic outputs.
        \item \textbf{Training Time:} Naive Bayes typically trains faster than Decision Trees due to its simplicity.
        \item \textbf{Performance:} 
        \begin{itemize}
            \item Decision Trees can capture complex relationships, but might overfit.
            \item Naive Bayes is robust and can perform well even with limited data.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Conclusion}
        Both Decision Trees and Naive Bayes are powerful classification tools in various industries. Understanding their applications helps us select the best model for a given problem, maximizing accuracy and efficiency in predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Summary and Key Takeaways - Overview of Classification Algorithms}
    \begin{block}{Definition}
        Classification algorithms are a type of supervised learning technique used to categorize data into predefined classes or labels based on input features.
    \end{block}
    \begin{itemize}
        \item \textbf{Common Algorithms:}
        \begin{itemize}
            \item \textbf{Decision Trees}: A flowchart-like structure where internal nodes represent decisions based on feature values, and leaf nodes represent outcomes. Useful for interpretability.
            \item \textbf{Naive Bayes}: A probabilistic classifier based on Bayes' theorem that assumes independence among predictors. Great for text classification tasks like spam detection.
        \end{itemize}
        \item \textbf{Example}: In email filtering, a Decision Tree might classify emails as "spam" or "not spam" based on features such as the presence of certain keywords.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Summary and Key Takeaways - Model Evaluation Techniques}
    \begin{block}{Importance of Evaluation}
        Ensures that the model performs well on training data and generalizes effectively to unseen data.
    \end{block}
    \begin{itemize}
        \item \textbf{Common Metrics:}
        \begin{itemize}
            \item \textbf{Accuracy}: The ratio of correctly predicted instances to total instances.
            \item \textbf{Precision}: The ratio of true positives to the sum of true and predicted positives, reflecting the model's correctness in predicting the positive class.
            \item \textbf{Recall (Sensitivity)}: The ratio of true positives to the actual positives, measuring the model's ability to identify all relevant cases.
            \item \textbf{F1 Score}: The harmonic mean of precision and recall, providing a balance between the two.
        \end{itemize}
    \end{itemize}
    \begin{block}{Formulae}
        \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
        \end{equation}
        \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
        \end{equation}
        \begin{equation}
        \text{F1 Score} = 2 \times \frac{Precision \times Recall}{Precision + Recall}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Summary and Key Takeaways - Relevance in Real-World Applications}
    \begin{itemize}
        \item \textbf{Healthcare}: Predictive models for diagnosing diseases based on patient data.
        \item \textbf{Finance}: Fraud detection models using transaction data to identify fraudulent activities.
        \item \textbf{Customer Service}: Churn prediction models that help businesses retain customers.
        \item \textbf{Example}: The utilization of Naive Bayes for categorizing customer reviews into positive or negative helps companies improve their products and services.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Summary and Key Takeaways - Key Conclusions}
    \begin{itemize}
        \item Classification algorithms are crucial in decision-making processes across various sectors.
        \item Evaluating model effectiveness is essential to ensure reliability in real-world applications.
        \item Understanding accuracy, precision, recall, and F1 score provides insights into the strengths and weaknesses of different models.
        \item Real-world applications of these algorithms drive significant value in businesses, impacting decision-making, customer satisfaction, and operational efficiency.
    \end{itemize}
    \begin{block}{Conclusion}
        Mastering classification algorithms and evaluation techniques is fundamental in the realm of data science and artificial intelligence, enabling the extraction of substantial insights from data while ensuring that models remain accurate and reliable in dynamic environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Further Discussion - Introduction}
    \begin{block}{Overview}
        We will open the floor for questions and discussions related to classification algorithms and model evaluation.
    \end{block}
    \begin{itemize}
        \item Importance of discussions: 
        \begin{itemize}
            \item Clarify complex topics
            \item Share insights
            \item Understand diverse applications
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Discussion Points - Classification Algorithms}
    \begin{block}{Understanding Classification Algorithms}
        \begin{itemize}
            \item Why are classification algorithms central to data mining?
            \begin{itemize}
                \item **Motivation**: Predict categories or labels for data points.
            \end{itemize}
            \item Common algorithms:
            \begin{enumerate}
                \item \textbf{Logistic Regression}
                \item \textbf{Decision Trees}
                \item \textbf{Support Vector Machines (SVM)}
            \end{enumerate}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Discussion Points - Model Evaluation Metrics}
    \begin{block}{Model Evaluation}
        \begin{itemize}
            \item How do we evaluate classification models?
            \begin{itemize}
                \item **Accuracy**:
                \[
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Observations}}
                \]
                \item **Precision and Recall**:
                    \begin{itemize}
                        \item Precision: Correctness among predicted positives
                        \item Recall: Ability to find all relevant instances (true positives)
                    \end{itemize}
                \item **F1 Score**:
                \[
                F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
                \]
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Discussion Points - Applications and Encouragement}
    \begin{block}{Applications and Recent Trends}
        \begin{itemize}
            \item Discuss recent advancements in AI:
            \begin{itemize}
                \item **ChatGPT and Language Models**: Utilizing classification in intent understanding and sentiment analysis.
                \item **Healthcare**: Use of classification models in predictive analytics.
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Encouraging Active Participation}
        \begin{itemize}
            \item Invite students to share experiences:
            \begin{itemize}
                \item Real-world applications of classification algorithms
                \item Challenges in implementation
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Wrap Up}
        Engaging in questions and discussions is vital for understanding classification algorithms and their evaluations.
        \begin{itemize}
            \item Your insights will enhance collective comprehension.
            \item Feel free to share your curiosity and experiences!
        \end{itemize}
    \end{block}
\end{frame}


\end{document}