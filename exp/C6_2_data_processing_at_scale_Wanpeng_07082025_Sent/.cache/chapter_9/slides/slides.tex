\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Title Page Information
\title[Spark Streaming]{Week 9: Spark Streaming and Real-Time Analytics}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Spark Streaming - Overview}
    \begin{block}{Overview of Spark Streaming's Role in Real-Time Data Processing}
        Spark Streaming is an extension of Apache Spark that enables processing and analyzing real-time data streams. It facilitates continuous data processing using the same core concepts as batch processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Spark Streaming - Key Concepts}
    \begin{enumerate}
        \item \textbf{Micro-batching:} 
        \begin{itemize}
            \item Inputs are divided into smaller batches (usually milliseconds) for balanced latency and throughput.
            \item \textit{Illustration:} 
            \[
            \text{Data Stream Input} \rightarrow [\text{Micro-batch 1}] \rightarrow \text{Processing} \rightarrow [\text{Micro-batch 2}] \rightarrow \text{Processing} \rightarrow \cdots
            \]
        \end{itemize}
        
        \item \textbf{DStreams (Discretized Streams):} 
        \begin{itemize}
            \item DStreams represent a continuous stream of data and are processed in a series of RDDs (Resilient Distributed Datasets).
        \end{itemize}
        
        \item \textbf{Integration with Spark Ecosystem:}
        \begin{itemize}
            \item Seamless integration with other Spark components (e.g., Spark SQL, Spark MLlib).
            \item Supports data sources such as Kafka, Flume, and HDFS.
        \end{itemize}
        
        \item \textbf{Real-Time Analytics:}
        \begin{itemize}
            \item Used for monitoring logs, tracking social media, and processing transactions for immediate insights.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Spark Streaming - Use Cases and Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Real-Time Processing:} Enables timely decision-making.
            \item \textbf{Scalability:} Supports distributed and fault-tolerant processing over large data streams.
            \item \textbf{Ease of Use:} Familiar Spark APIs make transitioning from batch to streaming workflows easier.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Use Case}
        Consider an e-commerce platform analyzing user activity in real time to recommend products. 
        \begin{itemize}
            \item Using Spark Streaming, the platform can process user actions (clicks, searches, purchases) live, updating the recommendation engine on-the-fly.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Spark Streaming - Code Snippet}
    \begin{lstlisting}[language=python]
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Initialize SparkContext and StreamingContext
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1) # 1 second batch interval

# Create DStream that will connect to hostname:port
lines = ssc.socketTextStream("localhost", 9999)

# Process and count words in the DStream
words = lines.flatMap(lambda line: line.split(" "))
wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

# Print the results
wordCounts.pprint()

# Start the computation
ssc.start()
ssc.awaitTermination()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Objectives of Spark Streaming - Overview}
    \begin{block}{Core Learning Objectives}
        Identify key objectives related to Spark Streaming and real-time analytics.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Objectives of Spark Streaming - Part 1}
    \begin{enumerate}
        \item \textbf{Grasp the Fundamentals of Spark Streaming}
        \begin{itemize}
            \item \textbf{Definition}: An extension of Apache Spark for scalable, fault-tolerant stream processing.
            \item \textbf{Importance}: Enables real-time analytics, data transformation, and machine learning applications.
        \end{itemize}
        
        \item \textbf{Differentiate Between Batch and Stream Processing}
        \begin{itemize}
            \item \textbf{Batch Processing}: Analyzes historical data in fixed-size chunks.
            \item \textbf{Streaming Processing}: Delivers insights in real-time, ideal for live applications (e.g., fraud detection).
            \item \textbf{Key Point}: Input data ingestion methods vary: continuous vs. periodic.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Objectives of Spark Streaming - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Understanding the Spark Streaming Architecture}
        \begin{itemize}
            \item \textbf{DStream}: Represents continuous data stream in batches.
            \item \textbf{Receiver}: Collects data from sources like Kafka, Flume, and TCP.
            \item \textbf{Processing Engines}: Utilize Spark core functionalities and RDDs for data processing.
        \end{itemize}
        
        \item \textbf{Learn the API and Programming Model}
        \begin{itemize}
            \item \textbf{Spark Streaming API}: Provides high-level abstractions for stream processing.
            \item \textbf{Common Operations}: Include \texttt{map()}, \texttt{reduce()}, \texttt{filter()}, and window functions.
        \end{itemize}
        
        \begin{lstlisting}[language=Python, title=Python Code Snippet]
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)  # 1 second window
lines = ssc.socketTextStream("localhost", 9999)
words = lines.flatMap(lambda line: line.split(" "))
wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
wordCounts.pprint()
ssc.start()
ssc.awaitTermination()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Objectives of Spark Streaming - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Explore Real-World Applications}
        \begin{itemize}
            \item Use cases include social media monitoring, fraud detection, and operational dashboards.
            \item \textbf{Key Point}: Recognizing use cases heightens understanding of technology's applicability.
        \end{itemize}
        
        \item \textbf{Analyze Performance and Fault Tolerance}
        \begin{itemize}
            \item \textbf{Checkpointing}: Saves the state for recovery from failures.
            \item \textbf{Backpressure}: Controls data flow to prevent overload on the system.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Objectives of Spark Streaming - Summary}
    By the end of this presentation, attendees will be able to:
    \begin{itemize}
        \item Recognize the significance of Spark Streaming in data-centric applications.
        \item Differentiate between various processing models.
        \item Leverage APIs for real-time data analytics.
        \item Implement fault-tolerant solutions for scalable streaming services.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles of Streaming Data - Part 1}
    \begin{block}{Introduction to Streaming Data Architecture}
        Streaming data architecture is designed to handle continuously flowing data in real-time, enabling immediate insights and actions. This contrasts with batch processing architectures that deal with data in large, discrete chunks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles of Streaming Data - Part 2}
    \begin{block}{Key Components of Streaming Data Architecture}
        \begin{itemize}
            \item \textbf{Data Sources:} Sensors, user activity logs, social media feeds, etc. High velocity and volume.
            \item \textbf{Stream Processing Engine:} Core of streaming architecture (e.g., Apache Spark Streaming, Flink). Processes data on-the-fly.
            \item \textbf{Storage Options:} Temporary (Redis, Kafka) for intermediate processing and permanent (HDFS, NoSQL) for long-term storage.
            \item \textbf{Data Consumers:} Reporting tools, dashboards, and applications that utilize processed streaming data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles of Streaming Data - Part 3}
    \begin{block}{Batch vs. Stream Processing}
        \begin{tabular}{|l|l|l|}
            \hline
            Feature & Batch Processing & Stream Processing \\
            \hline
            Data Handling & Operates on fixed datasets & Processes data continuously \\
            Latency & High latency (minutes to hours) & Low latency (seconds to milliseconds) \\
            Use Case Examples & Monthly reports, periodic ETL processes & Fraud detection, real-time monitoring \\
            Processing Model & Requires complete datasets & Works on windows of data \\
            Resource Utilization & Optimized for large batch jobs & Needs constant resource availability \\
            \hline
        \end{tabular}
    \end{block}
    \begin{block}{Key Point}
        Streaming data processing is favored for real-time applications, while batch processing suits scenarios without immediate needs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles of Streaming Data - Part 4}
    \begin{block}{Example Scenario}
        \textbf{Context:} A social media platform wants to analyze user interactions in real-time.
        \begin{itemize}
            \item \textbf{Batch Processing Approach:} Hourly aggregation of user interactions and report generation.
            \item \textbf{Stream Processing Approach:} Immediate updates of engagement metrics on the dashboard as soon as a user interacts.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles of Streaming Data - Part 5}
    \begin{block}{Conclusion}
        Understanding the architectural principles of streaming data systems is essential for effective real-time analytics. By recognizing the differences between batch and stream processing, you can choose the right approach for better insights and faster decision-making.
    \end{block}
    \begin{block}{Ready for Further Study?}
        In the next slide, we will explore how to integrate Apache Kafka with Spark Streaming for an effective real-time data ingestion strategy!
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Integrating Spark with Kafka - Introduction}
    \begin{itemize}
        \item Integrating Apache Kafka with Apache Spark Streaming enables real-time data processing and analytics.
        \item This integration supports:
        \begin{itemize}
            \item Efficient data ingestion
            \item Transformation and storage
        \end{itemize}
        \item Ideal for responsive applications in various domains (finance, social media, IoT).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Integrating Spark with Kafka - Key Concepts}
    \begin{enumerate}
        \item \textbf{Apache Kafka}
        \begin{itemize}
            \item Distributed messaging system for high-throughput, fault-tolerant data streaming.
            \item Facilitates publish-subscribe messaging between producers and consumers.
        \end{itemize}

        \item \textbf{Spark Streaming}
        \begin{itemize}
            \item A Spark component for processing real-time data streams.
            \item Processes data in micro-batches for near real-time analytics.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integrating Spark with Kafka - Integration Steps}
    \textbf{Key Steps to Integration:}
    \begin{enumerate}
        \item \textbf{Setup Kafka Environment}
        \begin{itemize}
            \item Install and configure a Kafka broker.
            \item Create a Kafka topic (e.g., \texttt{spark-topic}):
            \begin{lstlisting}[language=bash]
kafka-topics.sh --create --topic spark-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Setup Spark Streaming}
        \begin{itemize}
            \item Ensure Spark is properly installed.
            \item Include Kafka dependencies in your Spark application:
            \begin{lstlisting}[language=scala]
libraryDependencies += "org.apache.spark" %% "spark-streaming-kafka-0-10" % "3.2.1"
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integrating Spark with Kafka - Integration Steps (cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Create a Streaming Context}
        \begin{itemize}
            \item Set up Spark Streaming context:
            \begin{lstlisting}[language=scala]
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka010._

val spark = SparkSession.builder.appName("KafkaSparkIntegration").getOrCreate()
val ssc = new StreamingContext(spark.sparkContext, Seconds(5))
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Connect to Kafka}
        \begin{itemize}
            \item Use direct stream approach:
            \begin{lstlisting}[language=scala]
val kafkaParams = Map("bootstrap.servers" -> "localhost:9092", "key.deserializer" -> "org.apache.kafka.common.serialization.StringDeserializer", "value.deserializer" -> "org.apache.kafka.common.serialization.StringDeserializer", "group.id" -> "use_a_separate_group_id", "auto.offset.reset" -> "latest", "enable.auto.commit" -> (false: java.lang.Boolean))
val topics = Array("spark-topic")
val stream = KafkaUtils.createDirectStream[String, String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](topics, kafkaParams))
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Kafka Architecture Overview}
    \begin{block}{Summary}
        Apache Kafka is a distributed streaming platform designed for high-throughput and fault-tolerant processing of real-time data feeds.
        Key components include topics, producers, and consumers which are essential for effective integration with frameworks like Apache Spark.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Components of Kafka Architecture - Topics}
    \begin{itemize}
        \item \textbf{Topics:}
        \begin{itemize}
            \item \textbf{Definition:} Categories or feed names for published records.
            \item \textbf{Characteristics:}
            \begin{itemize}
                \item \textbf{Partitioned:} Topics can be divided into partitions for parallel processing.
                \item \textbf{Ordered:} Messages within a partition are strictly ordered.
            \end{itemize}
        \end{itemize}
        \item \textbf{Example:} A \emph{tweets} topic can have multiple partitions to allow for simultaneous writes and reads.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Kafka Architecture - Producers and Consumers}
    \begin{itemize}
        \item \textbf{Producers:}
        \begin{itemize}
            \item \textbf{Definition:} Client applications that publish data to Kafka topics.
            \item \textbf{Responsibilities:}
            \begin{itemize}
                \item Decide which partition to send data to.
                \item Handle retries on failures.
            \end{itemize}
            \item \textbf{Java Code Example:}
            \begin{lstlisting}[language=Java]
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaProducer<String, String> producer = new KafkaProducer<>(props);
producer.send(new ProducerRecord<String, String>("tweets", "key", "This is a tweet!"));
producer.close();
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Consumers:}
        \begin{itemize}
            \item \textbf{Definition:} Applications that read data from Kafka topics.
            \item \textbf{Characteristics:}
            \begin{itemize}
                \item Consume from one or more topics.
                \item Belong to consumer groups for load balancing.
            \end{itemize}
            \item {\textbf{Key Concepts:}}
            \begin{itemize}
                \item \textbf{Offsets:} Unique identifiers for messages.
                \item \textbf{Group Management:} Dynamic joining/leaving of consumer groups.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Working with Streams in Spark}
    \begin{block}{Introduction to DStreams}
        DStreams, or Discretized Streams, are the fundamental abstraction used in Apache Spark Streaming. They allow for the processing of live data streams in a distributed and fault-tolerant manner.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts of DStreams}
    \begin{itemize}
        \item \textbf{DStream:} A sequence of RDDs representing a continuous stream of data.
        \item \textbf{Input DStreams:} Continuously received streams from sources like Kafka or Flume.
        \item \textbf{Output Operations:} Publish results to external systems, databases, or files.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Operations}
    \begin{enumerate}
        \item \textbf{map():} Applies a function to each element.
        \begin{lstlisting}
        words = lines.flatMap(lambda line: line.split(" "))
        \end{lstlisting}

        \item \textbf{filter():} Filters elements based on a condition.
        \begin{lstlisting}
        filteredWords = words.filter(lambda word: "error" in word)
        \end{lstlisting}

        \item \textbf{reduceByKey():} Combines values for each key.
        \begin{lstlisting}
        wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
        \end{lstlisting}

        \item \textbf{window():} Groups data over specified time intervals.
        \begin{lstlisting}
        windowedStream = filteredWords.window(windowDuration=60, slideDuration=30)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Fault Tolerance:} DStreams provide fault tolerance through RDD lineage.
        \item \textbf{Batch Interval:} Critical for performance; impacts real-time insights.
        \item \textbf{Integration with Batch Processing:} Unify batch and stream processing.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Visualization}
    \begin{block}{Summary}
        DStreams provide an efficient model for working with continuous data streams in Spark, enabling real-time analytics and applications.
    \end{block}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{conceptual_diagram.png}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing a Spark Streaming Application - Overview}
    \begin{itemize}
        \item Spark Streaming allows scalable and fault-tolerant stream processing of live data streams.
        \item In this session, we will develop a basic Spark Streaming application that processes data in real-time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing a Spark Streaming Application - Step-by-Step}
    \begin{enumerate}
        \item \textbf{Set Up Spark Streaming Context}
        \begin{lstlisting}
        import org.apache.spark.streaming._

        val conf = new SparkConf()
            .setAppName("SimpleStreamingApp")
            .setMaster("local[*]")
        val ssc = new StreamingContext(conf, Seconds(5))  // Batch interval of 5 seconds
        \end{lstlisting}

        \item \textbf{Create a DStream}
        \begin{lstlisting}
        val lines = ssc.socketTextStream("localhost", 9999)  // Listening on socket port 9999
        \end{lstlisting}

        \item \textbf{Transform DStream}
        \begin{lstlisting}
        val words = lines.flatMap(line => line.split(" "))  // Split lines into words
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing a Spark Streaming Application - Continuing Steps}
    \begin{enumerate}[resume]
        \item \textbf{Perform Actions}
        \begin{lstlisting}
        val wordCounts = words.map(word => (word, 1))
                              .reduceByKey(_ + _)  // Count occurrences of each word
        \end{lstlisting}

        \item \textbf{Output the Results}
        \begin{lstlisting}
        wordCounts.print()  // Print the counts to the console
        \end{lstlisting}

        \item \textbf{Start the Streaming Context}
        \begin{lstlisting}
        ssc.start()  // Start processing
        ssc.awaitTermination()  // Wait for termination signal
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Windowed Operations in Streaming - Overview}
    \begin{block}{Understanding Windowed Operations}
        Windowed operations in Spark Streaming enable processing of streams over specified time intervals, referred to as windows. Instead of treating streams as endless flows, we segment them into manageable chunks for batch-like processing. 
    \end{block}
    
    \begin{itemize}
        \item **Window Length:** Duration of aggregation (e.g., 1 minute, 5 minutes)
        \item **Sliding Interval:** Frequency of alignment (e.g., every minute, every 30 seconds)
        \item **Window Types:**
        \begin{itemize}
            \item Tumbling Windows: Fixed-size, non-overlapping
            \item Sliding Windows: Overlapping with a specified step
            \item Session Windows: Dynamically sized based on idle time
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Windowed Operations in Streaming - Example}
    \begin{block}{Example: Website Live Traffic Monitoring}
        Consider monitoring website traffic with:
        \begin{itemize}
            \item **Window Length:** 5 minutes (capturing data every 5 minutes)
            \item **Sliding Interval:** 1 minute (results generated every minute)
        \end{itemize}
        
        This allows for calculating:
        \begin{itemize}
            \item Average number of visitors
            \item Unique sessions within each window
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Windowed Operations in Streaming - Spark Code Example}
    \begin{block}{Spark Code Snippet}
        \begin{lstlisting}[language=python]
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Initialize Spark Streaming Context
sc = SparkContext("local[2]", "Windowed Operations")
ssc = StreamingContext(sc, 1)  # batch interval of 1 second

# Create a DStream from a source (e.g., socket)
lines = ssc.socketTextStream("localhost", 9999)

# Perform windowed operations
windowed_counts = lines.window(60, 30).count()  # 1-minute window, slide every 30 seconds

# Output results to console
windowed_counts.pprint()

# Start the streaming context
ssc.start()
ssc.awaitTermination()
        \end{lstlisting}
    \end{block}
    
    \begin{itemize}
        \item **Key Points:**
        \begin{itemize}
            \item Efficiency through batch-like processing
            \item Use cases in real-time analytics
            \item Flexibility in selecting window sizes and types
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-Time Analytics Techniques}
    \begin{block}{Introduction to Real-Time Analytics}
        Real-time analytics refers to the process of continuously analyzing streaming data as it is generated. Unlike traditional analytics, which often rely on batch processing, real-time analytics provides immediate insights, enabling organizations to act swiftly based on current data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques in Real-Time Data Processing - Part 1}
    \begin{enumerate}
        \item \textbf{Event Detection and Triggering}
        \begin{itemize}
            \item \textbf{Concept}: Automatically identifying specific events or changes in data that require immediate attention or action.
            \item \textbf{Example}: A fraud detection system that flags unusual transaction patterns in real-time.
            \item \textbf{Key Point}: Enhances security and operational efficiency.
        \end{itemize}

        \item \textbf{Streaming Aggregation}
        \begin{itemize}
            \item \textbf{Concept}: Summarizing and aggregating data points over defined time windows to produce insights without storing all individual records.
            \item \textbf{Example}: Calculating the streaming average of user purchases over the last 5 minutes.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Average}(t) = \frac{\sum_{i=1}^{N} x_i}{N}
            \end{equation}
            where \(N\) is the number of items in the window, and \(x_i\) represents individual data points.
            \item \textbf{Key Point}: Highlight trends and patterns that would be difficult to see in raw data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques in Real-Time Data Processing - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Real-Time Predictive Analytics}
        \begin{itemize}
            \item \textbf{Concept}: Utilizing machine learning algorithms to make predictions based on incoming data streams.
            \item \textbf{Example}: Predicting customer behavior, such as churn, based on real-time customer interaction data.
            \item \textbf{Key Point}: Dynamic guidance of business strategies as conditions change.
        \end{itemize}

        \item \textbf{Windowed Computation}
        \begin{itemize}
            \item \textbf{Concept}: Processing streams of data in fixed-sized or sliding windows.
            \item \textbf{Example}: Assessing the average temperature readings over the last hour from IoT sensors.
            \item \textbf{Key Point}: Timely analysis while keeping computational resources manageable.
        \end{itemize}

        \item \textbf{Anomaly Detection}
        \begin{itemize}
            \item \textbf{Concept}: Identifying data points that deviate significantly from normal patterns.
            \item \textbf{Example}: Flagging network traffic spikes that may indicate a cybersecurity threat.
            \item \textbf{Key Point}: Quick identification can prevent system failures or security breaches.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    Real-time analytics techniques empower organizations to leverage data-driven decision-making effectively. 
    \begin{itemize}
        \item By implementing these techniques, companies can respond to events as they unfold.
        \item Enhancing competitive edge and operational resilience.
    \end{itemize}

    \textbf{Next Steps}: In the following slide, we will discuss the challenges associated with implementing real-time analytics and how to mitigate these issues.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Real-Time Data Processing}
    \begin{block}{Overview}
        Real-time data processing involves continuously inputting, processing, and outputting data as it becomes available. While it offers significant advantages, the journey of handling real-time streams is fraught with several challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges - Part 1}
    \begin{enumerate}
        \item \textbf{Data Volume and Velocity}
        \begin{itemize}
            \item \textit{Explanation:} Real-time systems often encounter high-speed data influx, requiring scalable architectures.
            \item \textit{Example:} A social media platform that processes millions of user actions per second.
        \end{itemize}

        \item \textbf{Data Quality}
        \begin{itemize}
            \item \textit{Explanation:} Ensuring accurate and clean data is critical for analytics; noisy or corrupted data can lead to poor decision-making.
            \item \textit{Example:} In healthcare, sensor data from wearable devices may have inconsistencies, affecting patient monitoring.
        \end{itemize}

        \item \textbf{System Latency}
        \begin{itemize}
            \item \textit{Explanation:} Introducing delays in processing can undermine the purpose of real-time analytics.
            \item \textit{Example:} In stock trading, a delay of a few seconds may result in significant financial loss.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue from previous frame
        \item \textbf{Scalability}
        \begin{itemize}
            \item \textit{Explanation:} Systems must adapt to fluctuating data loads without compromising processing speed or reliability.
            \item \textit{Example:} E-commerce sites facing traffic spikes during sales events require robust scalable solutions.
        \end{itemize}

        \item \textbf{Fault Tolerance}
        \begin{itemize}
            \item \textit{Explanation:} Systems must be resilient to failures to ensure continuous operation and prevent data loss.
            \item \textit{Example:} A cloud-based messaging service that efficiently handles server failures without downtime.
        \end{itemize}

        \item \textbf{Complex Event Processing (CEP)}
        \begin{itemize}
            \item \textit{Explanation:} Identifying meaningful patterns among high-velocity data streams can be complex and requires sophisticated algorithms.
            \item \textit{Example:} Fraud detection in financial transactions involves recognizing patterns across multiple data sources in real time.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{6} % Continue from previous frame 
        \item \textbf{Integration with Legacy Systems}
        \begin{itemize}
            \item \textit{Explanation:} Seamless integration with existing systems can pose challenges, particularly when those systems are not designed for real-time data handling.
            \item \textit{Example:} A traditional retail company wants to incorporate real-time inventory management but struggles with older databases.
        \end{itemize}

        \item \textbf{Security and Privacy Concerns}
        \begin{itemize}
            \item \textit{Explanation:} Processing sensitive information in real-time raises risks related to data breaches and regulatory compliance.
            \item \textit{Example:} Financial services dealing with personal data must enforce strict security measures during real-time transactions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Conclusion}
        Understanding these challenges is essential for designing effective real-time data processing systems. Solutions may include leveraging frameworks like Apache Spark for scalability, employing data validation techniques for quality, and integrating machine learning models to enhance insights from live data streams.
    \end{block}

    \begin{itemize}
        \item Real-time processing is complex and demands advanced architecture and design strategies.
        \item The balance between speed, accuracy, and system resilience is critical to success in real-time analytics.
    \end{itemize}

    \begin{block}{Further Reading}
        Explore the challenges in more depth during our upcoming lecture on Machine Learning with Streaming Data where we will outline techniques to overcome these hurdles.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Machine Learning with Streaming Data - Overview}
    \begin{block}{Overview}
        Machine learning models can be applied to real-time data streams using Apache Spark Streaming. 
        This enables organizations to analyze incoming data on-the-fly, allowing for timely insights and actions. 
        The integration of machine learning with streaming data transforms static models into dynamic ones that adapt to changing data patterns.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Machine Learning with Streaming Data - Key Concepts}
    \begin{itemize}
        \item \textbf{Streaming Data:} Continuously generated data from different sources in small batches (e.g., sensor readings, social media updates).
        \item \textbf{Machine Learning Model:} A statistical model trained on historical data to recognize patterns and make predictions.
        \item \textbf{Real-Time Analytics:} The ability to analyze data as it becomes available, predicting outcomes or classifying data in real-time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Machine Learning with Spark Streaming}
    \begin{enumerate}
        \item \textbf{Data Ingestion:} Collect streaming data from sources (e.g., Kafka, Flume) into micro-batches. \\
              \textit{Example:} Monitoring credit card transactions for fraud detection.
        
        \item \textbf{Model Training:} Typically, models are trained on historical data first, then applied to incoming streams. \\
              \textit{Example:} A recommendation system pre-trained on user interactions.
        
        \item \textbf{Scoring:} Apply the trained model to streaming data with the `transform` method for DStreams.
        \begin{lstlisting}
        # Assuming model is a pre-trained instance of a machine learning model
        def model_predict(data_frame):
            return model.transform(data_frame)

        # Applying the model to the streaming data
        predictions = stream_data.foreachRDD(lambda rdd: model_predict(rdd))
        \end{lstlisting}
        
        \item \textbf{Output Handling:} Direct results from the model to various sinks (e.g., databases, dashboards) for visualization and further processing.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Key Points}
    \begin{itemize}
        \item \textbf{Examples of Applications:}
        \begin{itemize}
            \item Fraud Detection: Detect anomalous transaction patterns in real-time.
            \item Predictive Maintenance: Monitor machinery to predict failures.
            \item Content Personalization: Update user recommendations based on latest interactions.
        \end{itemize}
        
        \item \textbf{Key Points to Emphasize:}
        \begin{itemize}
            \item \textbf{Latency:} Require low-latency processing for real-time responses.
            \item \textbf{Model Update:} Continuous learning enhances model performance with evolving data distributions.
            \item \textbf{Scalability:} Spark can handle massive data streams efficiently across distributed systems.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{Conclusion}
        Applying machine learning in a streaming context opens new avenues for actionable insights and timely decision-making. 
        By integrating Spark Streaming with machine learning techniques, organizations can make real-time predictions that drive operational efficiencies and improve customer experiences.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Performance Tuning for Spark Streaming}
    \begin{block}{Overview}
        Optimizing Spark Streaming applications is crucial for ensuring efficient processing of live data streams. Performance tuning involves adjusting various parameters and configurations to improve throughput, latency, and resource utilization of your streaming jobs.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Batch Interval and Resource Allocation}
    \begin{enumerate}
        \item \textbf{Use of Batch Interval:}
        \begin{itemize}
            \item Smaller batch intervals increase responsiveness but can lead to higher overhead.
            \item Larger batch intervals reduce overhead but can increase latency.
            \item Monitor data stream characteristics to adjust batch size effectively.
        \end{itemize}
        
        \item \textbf{Resource Allocation:}
        \begin{itemize}
            \item Allocate the right amount of resources to improve performance.
            \item Avoid overloading workers to prevent straggler tasks.
            \item Use \texttt{spark.executor.memory} and \texttt{spark.executor.cores} to fine-tune resource allocations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Data Serialization and Fault Tolerance}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Data Serialization:}
        \begin{itemize}
            \item Efficient serialization reduces network data transfer and improves processing times.
            \item Use Kryo serialization for better performance.
        \end{itemize}
        
        \item \textbf{Fault Tolerance and Checkpointing:}
        \begin{itemize}
            \item Checkpointing allows for state recovery in case of failures.
            \item Use judiciously, as it introduces performance penalties.
            \item Example: \texttt{streamingContext.checkpoint("hdfs://path/to/checkpoint")}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Optimization Strategies for Transformations}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Optimize Transformations:}
        \begin{itemize}
            \item Use narrow transformations (map, filter) for efficiency.
            \item Prefer \texttt{reduceByKey} over \texttt{groupByKey} to reduce data shuffling.
        \end{itemize}
        
        \item \textbf{Caching Intermediate Data:}
        \begin{itemize}
            \item Use \texttt{persist()} or \texttt{cache()} to reduce recomputation costs.
            \item Example: \texttt{DStream.cache()} for frequently accessed DStreams.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Monitoring and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{6}
        \item \textbf{Monitoring and Metrics:}
        \begin{itemize}
            \item Regular monitoring reveals performance bottlenecks.
            \item Utilize Spark’s web UI to track metrics such as task duration and execution time.
        \end{itemize}
        
        \item \textbf{Conclusion:}
        \begin{itemize}
            \item Effective performance tuning can improve responsiveness and efficiency.
            \item Consider all parameters for optimal real-time application performance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example for Batch Interval}
    \begin{lstlisting}[language=Python]
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 5)  # 5 seconds batch interval
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies and Real-World Applications of Spark Streaming}

    \begin{block}{Introduction to Spark Streaming}
        Spark Streaming is a component of the Apache Spark ecosystem that processes real-time data streams, enabling real-time analytics and actionable insights for businesses.
    \end{block}

    \begin{itemize}
        \item Real-time data processing applications
        \item Enhances decision-making
        \item Improves operational efficiency
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Fraud Detection in Financial Services}

    \begin{block}{Overview}
        Financial institutions utilize Spark Streaming for real-time fraud detection by analyzing transaction data continuously.
    \end{block}

    \begin{itemize}
        \item \textbf{Data Source: } Credit card transaction streams
        \item \textbf{Processing: } Window operations for events in the last minute
    \end{itemize}

    \begin{block}{Example Logic}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, window

spark = SparkSession.builder.appName("FraudDetection").getOrCreate()

transactions = spark.readStream.schema(schema).json("path/to/transactions")

fraudulent_transactions = transactions.withWatermark("timestamp", "1 minute") \
                                        .groupBy(window(col("timestamp"), "1 minute"), col("user_id")) \
                                        .count() \
                                        .filter(col("count") > threshold)

fraudulent_transactions.writeStream.format("console").start()
        \end{lstlisting}
    \end{block}

    \begin{itemize}
        \item Immediate alerts on suspicious activity
        \item Quick identification minimizes losses
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: IoT Sensor Data Processing}

    \begin{block}{Overview}
        Manufacturers analyze real-time data from IoT sensors for predictive maintenance and production optimization.
    \end{block}

    \begin{itemize}
        \item \textbf{Data Source: } IoT sensor streams (temperature, pressure, vibration)
        \item \textbf{Integration: } Streaming data feeds predictive models for maintenance
    \end{itemize}

    \begin{block}{Application Logic}
        \begin{lstlisting}[language=Python]
sensor_data = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092") \
                                  .option("subscribe", "sensors").load()

processed_data = sensor_data.groupBy("machine_id") \
                            .agg(avg("temperature").alias("avg_temp"),
                                 max("pressure").alias("max_pressure"))

processed_data.writeStream.format("parquet").option("path", "output/sensor_data").start()
        \end{lstlisting}
    \end{block}

    \begin{itemize}
        \item Proactive maintenance reduces equipment failures
        \item Increased efficiency and cost savings in operations
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways - Overview}
    In this chapter, we explored Spark Streaming and its significance in enabling real-time data processing and analytics. 
    \begin{itemize}
        \item Businesses increasingly rely on real-time insights.
        \item Spark Streaming becomes a powerful tool for processing data streams efficiently.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways - Key Concepts}
    \begin{enumerate}
        \item \textbf{What is Spark Streaming?}
        \begin{itemize}
            \item An extension of Apache Spark for real-time data processing.
            \item Processes data in mini-batches, combining benefits of batch and stream processing.
        \end{itemize}

        \item \textbf{Core Components:}
        \begin{itemize}
            \item \textbf{DStream (Discretized Stream):} A continuous stream of data represented as a sequence of RDDs. 
            \item \textbf{Transformations and Actions:} 
            \begin{itemize}
                \item Transformations like \texttt{map}, \texttt{filter}, and \texttt{reduceByKey}.
                \item Actions such as \texttt{foreachRDD} to trigger processing.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Data Sources for Streaming:} 
        \begin{itemize}
            \item Apache Kafka
            \item Flume
            \item Socket Streams
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways - Windowing Operations}
    \begin{itemize}
        \item Enables aggregation over specific time windows (e.g., 1 minute, 5 minutes).
        \item Useful for calculating metrics over recent data.
        \item \textbf{Example:} Sliding window to compute average user activity over a 10-minute rolling window.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways - Stateful vs Stateless Processing}
    \begin{itemize}
        \item \textbf{Stateless Processing:} Each record is processed independently.
        \item \textbf{Stateful Processing:} Maintains state information (e.g., counts, averages) across multiple records.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways - Practical Applications}
    \begin{itemize}
        \item \textbf{Real-Time Analytics:} Monitoring sales, user engagement, and operational metrics instantly.
        \item \textbf{Fraud Detection:} Quick identification and response to fraudulent activities.
        \item \textbf{IoT Data Processing:} Analyzing real-time data from devices for insights and alerts.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Examples}
    \textbf{Examples of Spark Streaming in Action:}
    \begin{itemize}
        \item A social media platform analyzing hashtag trends in real-time.
        \item E-commerce sites providing personalized product recommendations based on user behavior.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways - Key Points to Emphasize}
    \begin{itemize}
        \item Spark Streaming is ideal for real-time data processing and analytics.
        \item Understanding DStreams and windowing enhances handling of streaming data.
        \item Flexibility in leveraging stateful and stateless operations for application design.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways - Conclusion}
    Spark Streaming equips organizations to harness the power of real-time data analytics, enabling faster decision-making and improving customer experience.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Example Code Snippet}
    \begin{lstlisting}[language=python]
# Example Code Snippet
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)  # 1 second batch interval
lines = ssc.socketTextStream("localhost", 9999)

# Transformations
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# Output result
wordCounts.pprint()
ssc.start()
ssc.awaitTermination()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions and Trends}
    \begin{block}{Overview}
        As we delve into the future of real-time analytics and Spark Streaming, we must consider the evolving landscape shaped by technological advancements, increased data velocity, and changing user requirements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Increased Adoption of AI and Machine Learning}
    \begin{itemize}
        \item \textbf{Concept:} Integration of AI/ML algorithms with real-time data processing enables instant predictions and automated decisions.
        \item \textbf{Example:} Retail companies using Spark Streaming to analyze customer purchasing patterns and adjust inventory in real-time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Emerging Trends in Real-Time Analytics}
    \begin{enumerate}
        \item \textbf{Edge Computing}
            \begin{itemize}
                \item \textbf{Concept:} Moving processing closer to IoT devices reduces latency and bandwidth.
                \item \textbf{Impact:} Spark Streaming can process data at the edge, making it suitable for immediate insights in applications like autonomous vehicles.
                \item \textbf{Example:} Smart sensors in manufacturing analyze performance metrics on-site.
            \end{itemize}
        
        \item \textbf{Enhanced Data Integration}
            \begin{itemize}
                \item \textbf{Concept:} Seamless integration from various data sources is vital for real-time analytics.
                \item \textbf{Trend:} Spark's capability to connect with sources like Kafka and HDFS simplifies data integration.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Spark Streaming}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Serverless Architecture}
            \begin{itemize}
                \item \textbf{Concept:} Reducing infrastructure management overhead allows more focus on coding.
                \item \textbf{Future Directions:} Spark Streaming can be deployed serverlessly, scaling based on data load.
                \item \textbf{Example:} Triggering jobs in response to events enhances flexibility.
            \end{itemize}

        \item \textbf{Data Privacy and Compliance}
            \begin{itemize}
                \item \textbf{Concept:} Tightening data regulations necessitate rigorous data privacy measures.
                \item \textbf{Trend:} Stream processing frameworks will focus on secure handling of sensitive data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Collaborative Decision-Making with Real-Time Analytics}
    \begin{itemize}
        \item \textbf{Concept:} As remote work increases, tools will evolve to support real-time collaboration on data analytics.
        \item \textbf{Future Directions:} Dashboards and alerts from Spark Streaming enable teams to analyze data collaboratively.
        \item \textbf{Example:} Distributed teams analyzing streaming customer feedback during product launches.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item The future of real-time analytics and Spark Streaming is driven by technological advances and the need for speed in decision-making.
        \item Understanding these trends allows businesses to leverage real-time data effectively.
    \end{itemize}

    \begin{block}{Interactive Element}
        Ask the audience: How do you envision integrating these trends into your current projects?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Objective}
    \begin{block}{Objective of the Session}
        The purpose of this Q\&A session is to clarify any concepts and applications related to Spark Streaming and real-time analytics that were discussed earlier. This is your chance to:
        \begin{itemize}
            \item Engage with the material
            \item Express your thoughts
            \item Ask questions to deepen your understanding
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Key Concepts to Review}
    \begin{enumerate}
        \item \textbf{Spark Streaming Overview:}
        \begin{itemize}
            \item Processing live data streams
            \item Extends Apache Spark with APIs in Scala, Python
        \end{itemize}
        
        \item \textbf{Real-Time Analytics:}
        \begin{itemize}
            \item Analysis of data as it becomes available
            \item Quick decision-making is crucial for many industries
        \end{itemize}

        \item \textbf{Data Sources:}
        \begin{itemize}
            \item Common sources include social media feeds, logs, IoT devices
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Additional Concepts}
    \begin{enumerate}
        \setcounter{enumii}{3}
        \item \textbf{Windowed Operations:}
        \begin{itemize}
            \item Operations on a defined time frame
            \item Example: Calculating averages over the last 10 minutes
        \end{itemize}
        \begin{block}{Example Code Snippet}
            \begin{lstlisting}[language=python]
from pyspark.streaming import StreamingContext
ssc = StreamingContext(sc, 10)  # 10 second window
lines = ssc.socketTextStream("localhost", 9999)
windowedCounts = lines.map(lambda x: (x, 1)).reduceByKeyAndWindow(sum, None, 60, 10)
            \end{lstlisting}
        \end{block}

        \item \textbf{Fault Tolerance:}
        \begin{itemize}
            \item Provides data replication and checkpointing
            \item Ensures data recovery in case of failure
        \end{itemize}

        \item \textbf{Integration with Other Spark Components:}
        \begin{itemize}
            \item Works with Spark SQL, MLlib, GraphX
            \item Enhances analytics capabilities
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Practical Applications}
    \begin{block}{Examples and Applications}
        \begin{itemize}
            \item \textbf{Financial Sector:} Real-time fraud detection systems
            \item \textbf{Social Media Analysis:} Sentiment measurement and trends
        \end{itemize}
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Spark Streaming enables real-time processing
            \item Integration with other Spark components enhances capabilities
            \item Engage actively—clarifying uncertainties boosts understanding
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Prepare Your Questions}
    \begin{block}{Prepare Your Questions}
        Think about queries related to:
        \begin{itemize}
            \item Specific use cases you found interesting
            \item Portions of the code or concepts that seem complex
            \item Potential applications in industries you are familiar with
        \end{itemize}
    \end{block}
    \begin{block}{Engage, Learn, and Explore!}
        Your questions will help deepen the discussion and knowledge!
    \end{block}
\end{frame}


\end{document}