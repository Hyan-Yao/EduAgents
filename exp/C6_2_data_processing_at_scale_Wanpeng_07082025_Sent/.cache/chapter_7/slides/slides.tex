\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}

% Title Page Information
\title[Streaming Data]{Week 7: Introduction to Streaming Data}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Streaming Data}
    Streaming data refers to a continuous flow of data generated from various sources in real-time. It is characterized by:
    \begin{itemize}
        \item Rapid generation, often requiring immediate processing and analysis
        \item The need to extract actionable insights from the data
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics of Streaming Data}
    \begin{itemize}
        \item \textbf{Real-time Processing:} Enables instantaneous analysis for immediate results.
        \item \textbf{High Velocity:} Data arrives rapidly, in large volumes, requiring robust systems.
        \item \textbf{Time-sensitive:} Relevance decreases with time, making timely processing critical.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Streaming Data Sources}
    \begin{itemize}
        \item \textbf{Social Media Feeds:} Continuous streams from platforms like Twitter and Facebook.
        \item \textbf{IoT Devices:} Ongoing data from smart appliances and environmental sensors.
        \item \textbf{Financial Markets:} Continuous updates on stock prices and market trends.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Streaming Data}
    \begin{itemize}
        \item \textbf{Enhanced Decision-Making:} Immediate responses to events improve experiences and efficiency.
        \item \textbf{Competitive Edge:} Faster responses to market changes can lead to outperforming competitors.
        \item \textbf{Integration with Big Data:} Enriches business intelligence systems alongside data warehouses.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Transformational Impact:} Alters processing and action on information.
        \item \textbf{Scalable Solutions:} Use technologies like Apache Kafka or Apache Flink.
        \item \textbf{Complex Event Processing:} Specialized systems analyze data streams for patterns and events.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    Consider an e-commerce website that tracks user behavior. By analyzing streaming data, it can:
    \begin{itemize}
        \item Update product recommendations based on user interactions.
        \item Send targeted promotions during a shopping session to increase conversions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding streaming data is essential for modern data practitioners. It drives:
    \begin{itemize}
        \item Real-time analytical capabilities for informed decision-making across industries.
        \item A shift towards faster operations and adaptability in business environments.
    \end{itemize}
    In the upcoming content, we will explore definitions and contrast streaming data with traditional batch processing.
\end{frame}

\begin{frame}[fragile]{What is Streaming Data?}
\begin{block}{Definition of Streaming Data}
Streaming data refers to a continuous flow of data generated from multiple sources—such as sensors, user activity, social media feeds, and financial transactions—that are sent and processed in real-time. Unlike traditional batch data processing, which analyzes a set of collected data at scheduled intervals, streaming data allows for immediate data ingestion, processing, and analytics.
\end{block}
\end{frame}

\begin{frame}[fragile]{Key Characteristics of Streaming Data}
\begin{itemize}
    \item \textbf{Real-time Processing:} Data is processed as it arrives, enabling immediate insights and actions.
    \item \textbf{Continuous Flow:} Data arrives in an ongoing stream rather than in fixed-size batches.
    \item \textbf{Time-Sensitive:} Often involves data that is relevant based on its time of arrival.
    \item \textbf{High Volume and Speed:} Typically involves large volumes of data that need to be handled swiftly.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Comparison of Streaming Data and Batch Processing}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{Streaming Data} & \textbf{Batch Processing} \\
\hline
Data Ingestion & Continuous flow & Periodic collection \\
Latency & Low latency (near real-time) & Higher latency (depends on batch interval) \\
Data Processing & Event-driven; processes data on-the-fly & Processes all data at once after collection \\
Use Cases & Fraud detection, live analytics, IoT applications & Monthly sales reports, data warehousing \\
Complexity & Often requires complex event processing & Generally simpler and easier to manage \\
\hline
\end{tabular}
\end{frame}

\begin{frame}[fragile]{Examples of Streaming Data}
\begin{itemize}
    \item \textbf{Social Media Feeds:} Continuous updates from platforms like Twitter or Facebook can be analyzed in real-time for sentiment analysis or trend detection.
    \item \textbf{Financial Transactions:} Monitoring transactions in real-time can help detect fraudulent activities immediately.
    \item \textbf{IoT Sensors:} Devices collecting temperature, pressure, or speed data can send continuous streams of information for immediate analysis and action.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Importance of Understanding Streaming vs. Batch}
\begin{block}{Key Points to Remember}
\begin{itemize}
    \item Streaming data supports quick, real-time insights which are essential in dynamic environments.
    \item Identify scenarios where batch processing might be insufficient for timely decision-making.
    \item Technologies for streaming data include Apache Kafka, Apache Flink, and Spark Streaming.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Stream Processing}
    Why stream processing is critical for real-time data applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Stream Processing}
    \begin{block}{Definition}
        Stream processing refers to the continuous input, processing, and output of data streams in real-time. This contrasts with traditional batch processing, where data is collected over a period and processed in large chunks.
    \end{block}
    \begin{itemize}
        \item Enables timely decision-making
        \item Enhances responsiveness to changing conditions
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Stream Processing Critical?}
    \begin{enumerate}
        \item \textbf{Real-Time Insights}
            \begin{itemize}
                \item Immediate data processing allows for on-the-fly decisions.
                \item Example: Stock trading platforms analyze market data in real-time.
            \end{itemize}
        
        \item \textbf{Event-Driven Architecture}
            \begin{itemize}
                \item Actions triggered by specific events in data streams.
                \item Example: Fraud detection systems can flag transactions instantly.
            \end{itemize}

        \item \textbf{Scalability}
            \begin{itemize}
                \item Efficiently handles large volumes of high-velocity data.
                \item Systems can scale horizontally across multiple nodes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Enhanced User Experience and Efficiency}
    \begin{enumerate}[resume]
        \item \textbf{Enhanced User Experience}
            \begin{itemize}
                \item Applications use stream processing for personalization.
                \item Example: Netflix and Spotify provide real-time recommendations based on user behavior.
            \end{itemize}

        \item \textbf{Improved Operational Efficiency}
            \begin{itemize}
                \item Real-time monitoring of operations identifies inefficiencies.
                \item Example: Predictive maintenance in manufacturing plants.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario: Real-Time Analytics in E-Commerce}
    An e-commerce platform processes user interactions:
    \begin{itemize}
        \item \textbf{User Actions:} Viewing products, adding items to cart, completing purchases.
        \item \textbf{Stream Processing Enhancements:}
            \begin{itemize}
                \item Adjust inventory in real-time.
                \item Offer personalized discounts instantly.
                \item Detect fraudulent activities as they occur.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Timeliness:} Immediate processing to exploit data opportunities.
        \item \textbf{Flexibility:} Adapts to various data sources and formats.
        \item \textbf{Proactivity:} Anticipate needs and prevent issues through instant action.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Stream processing is essential for any real-time data applications, enabling businesses to act quickly, effectively, and efficiently in a data-driven world. 
    \newline
    In the next slide, we will explore key concepts that further elucidate the intricate mechanics of stream processing.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Stream Processing - Part 1}
    \begin{block}{Understanding Data Streams}
        \begin{itemize}
            \item \textbf{Definition}: A data stream is a continuous flow of data generated from various sources such as servers, user interactions, or IoT devices.
            \item \textbf{Characteristics}:
            \begin{itemize}
                \item \textbf{Unbounded}: Data streams can continuously generate data without an end.
                \item \textbf{Time-sensitive}: Events are associated with their occurrence time, making timely processing essential.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Consider a sensor in a smart home that reports temperature data every minute. This series of temperature readings forms a data stream that can be analyzed in real-time to adjust HVAC systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Stream Processing - Part 2}
    \begin{block}{Event Time vs. Processing Time}
        Understanding the distinction between event time and processing time is crucial in stream processing:
        \begin{enumerate}
            \item \textbf{Event Time}
            \begin{itemize}
                \item \textbf{Definition}: The time at which an event actually occurs.
                \item \textbf{Importance}: This provides context to the event and is vital for accurate temporal analysis.
                \item \textbf{Example}: If a stock price changes at 1:00 PM, it should be processed in that time frame.
            \end{itemize}
            
            \item \textbf{Processing Time}
            \begin{itemize}
                \item \textbf{Definition}: The time at which an event is processed by the system.
                \item \textbf{Challenges}: Delays in processing may lead to discrepancies.
                \item \textbf{Example}: A stock price logged at 1:10 PM due to latency but occurred at 1:00 PM may skew analytics.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Stream Processing - Part 3}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Stream processing allows for immediate analysis of incoming data, facilitating real-time decision-making.
            \item The difference between event time and processing time impacts data accuracy and system design.
            \item Understanding data stream behavior and associated timings is foundational for efficient streaming applications.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        Mastering data streams and the distinction between event time and processing time is vital for developing sophisticated and responsive real-time data applications.
    \end{block}

    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
import time
from datetime import datetime

# Simulating event generation with event time
def generate_event(data):
    event_time = datetime.now()  # Event time
    time.sleep(0.1)  # Simulating processing delay
    processing_time = datetime.now()  # Processing time
    return event_time, processing_time, data

# Example usage
event_data = generate_event("Stock price update: $100")
print("Event Time:", event_data[0], "| Processing Time:", event_data[1])
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Stream Processing}
    \begin{block}{Overview}
        This presentation discusses three main challenges in stream processing:
        \begin{itemize}
            \item Latency
            \item Fault Tolerance
            \item Scalability
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Latency in Stream Processing}
    \begin{block}{Definition}
        Latency refers to the time it takes for a data point to be produced, processed, and consumed. Low latency is critical in real-time applications.
    \end{block}
    \begin{itemize}
        \item \textbf{Types of Latency}:
            \begin{itemize}
                \item *End-to-End Latency*: Time from data creation to final use.
                \item *Processing Latency*: Time spent queuing and processing.
            \end{itemize}
        \item \textbf{Example}:
            \begin{itemize}
                \item A stock trading application must process thousands of trades in milliseconds to ensure timely execution.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fault Tolerance}
    \begin{block}{Definition}
        Fault tolerance ensures a system can continue operating correctly despite failures, particularly relevant in stream processing.
    \end{block}
    \begin{itemize}
        \item \textbf{Stateful vs Stateless Processing}:
            \begin{itemize}
                \item *Stateful Operations*: Maintain context, more vulnerable to data loss.
                \item *Stateless Operations*: Do not maintain context, less prone to data loss.
            \end{itemize}
        \item \textbf{Techniques for Fault Tolerance}:
            \begin{itemize}
                \item *Checkpoints*: Regularly save the application's state.
                \item *Reprocessing*: Reprocess data from stored state after a failure.
            \end{itemize}
        \item \textbf{Example}:
            \begin{itemize}
                \item In a streaming application tracking user activity, procedures must ensure no events are missed during server restarts.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Scalability}
    \begin{block}{Definition}
        Scalability refers to the system's ability to handle increasing data volumes without performance degradation.
    \end{block}
    \begin{itemize}
        \item \textbf{Horizontal vs Vertical Scaling}:
            \begin{itemize}
                \item *Horizontal Scaling*: Adding more machines (e.g., distributed computing).
                \item *Vertical Scaling*: Upgrading existing machines with more resources.
            \end{itemize}
        \item \textbf{Load Balancing}:
            \begin{itemize}
                \item Distribute data streams evenly across system nodes to optimize performance.
            \end{itemize}
        \item \textbf{Example}:
            \begin{itemize}
                \item A video streaming service must scale to accommodate millions of concurrent users during peak times.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Key Takeaway}
        Addressing latency, ensuring fault tolerance, and maintaining scalability are critical for robust stream processing systems. 
    \end{block}
    \begin{itemize}
        \item Developers must carefully consider these challenges during the design and implementation phases.
        \item Building a resilient system capable of processing real-time data effectively is essential for success.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles - Overview}
    Stream processing systems are designed to handle continuous data flows effectively. Understanding the architectural principles behind these systems is crucial for designing robust, efficient, and scalable applications. Below are key principles to consider:
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles - Key Concepts}
    \begin{enumerate}
        \item \textbf{Event-Driven Architecture}
        \begin{itemize}
            \item \textit{Definition}: Real-time response to events instead of batch processing.
            \item \textit{Example}: Stock trading systems updating prices as trades occur.
        \end{itemize}
        
        \item \textbf{Decoupling Data Producers and Consumers}
        \begin{itemize}
            \item \textit{Concept}: Independent operation of producers (e.g., IoT devices) and consumers (e.g., analytics applications).
            \item \textit{Benefit}: Flexibility in scaling components and reduces the impact of changes.
        \end{itemize}
        
        \item \textbf{Scalability}
        \begin{itemize}
            \item \textit{Horizontal Scaling}: Adding more instances to handle increased load.
            \item \textit{Example}: Adding more brokers in Apache Kafka to distribute workload.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles - Advanced Concepts}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Fault Tolerance}
        \begin{itemize}
            \item \textit{Mechanism}: Continuation of operation during failures with data reliability.
            \item \textit{Implementation}: Techniques such as replication and checkpointing.
            \item \textit{Illustration}: Node failures and seamless recovery.
        \end{itemize}
        
        \item \textbf{Order Preservation}
        \begin{itemize}
            \item \textit{Need}: Ordering of data is crucial, especially for transactions.
            \item \textit{Strategy}: Using partitioning strategies in systems like Kafka.
        \end{itemize}
        
        \item \textbf{Low Latency}
        \begin{itemize}
            \item \textit{Objective}: Reducing delay between data arrival and its processing.
            \item \textit{Example}: Real-time fraud detection to flag suspicious activities promptly.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architectural Principles - Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Real-time data processing is essential in today’s data-driven environments.
            \item Understanding these architectural principles is critical for developing efficient and resilient stream processing systems.
        \end{itemize}
    \end{block}
    
    \begin{block}{Final Note}
        Architectural principles guide the development of well-structured applications capable of handling live data streams effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet - Kafka Producer}
    \begin{lstlisting}[language=Python]
# Example of creating a simple Kafka producer in Python
from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers='localhost:9092')
producer.send('my_topic', b'Hello, Stream!')
producer.flush()
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Tools and Frameworks - Introduction}
    \begin{block}{Introduction to Streaming Data Tools}
        Stream processing is essential for handling continuous data flows in real-time. Two popular tools/frameworks for processing streaming data are:
        \begin{itemize}
            \item \textbf{Apache Kafka}
            \item \textbf{Spark Streaming}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools and Frameworks - Apache Kafka}
    \begin{block}{1. Apache Kafka}
        \begin{itemize}
            \item \textbf{Overview:}
            \begin{itemize}
                \item Distributed event streaming platform with high throughput, fault tolerance, and scalability.
                \item Allows organizations to publish and subscribe to streams of records in a fault-tolerant way.
            \end{itemize}

            \item \textbf{Key Components:}
            \begin{itemize}
                \item \textbf{Producers:} Applications that send data to Kafka topics.
                \item \textbf{Consumers:} Applications that receive data from topics.
                \item \textbf{Topics:} Categories to which records are published.
            \end{itemize}

            \item \textbf{Usage Example:} 
            Real-Time Analytics: Processing user activity streams in an online application to gain insights.

            \item \textbf{Strengths:} 
            \begin{itemize}
                \item Decoupled architecture and durability (data stored on disk).
                \item High message retention capabilities.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools and Frameworks - Spark Streaming}
    \begin{block}{2. Spark Streaming}
        \begin{itemize}
            \item \textbf{Overview:}
            \begin{itemize}
                \item Built on top of Apache Spark, enabling scalable and fault-tolerant stream processing.
                \item Treats streams as an infinite series of small micro-batches.
            \end{itemize}

            \item \textbf{Key Features:}
            \begin{itemize}
                \item Integrates with various sources (Kafka, Flume, HDFS).
                \item Supports machine learning and graph processing on streaming data.
            \end{itemize}

            \item \textbf{Usage Example:} 
            Monitoring Systems: Analyzing logs in real time from a server to detect anomalies.

            \item \textbf{Strengths:}
            \begin{itemize}
                \item Unified processing model for batch and streaming data.
                \item Powerful API for complex processing.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Building a Streaming Data Application}
    \begin{block}{Introduction to Streaming Applications}
        Streaming data applications process and analyze data continuously as it is generated. Apache Kafka is a popular tool for building these applications due to its high throughput, scalability, and resilience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Create a Basic Streaming Application - Part 1}
    \begin{enumerate}
        \item \textbf{Set Up Your Environment}
        \begin{itemize}
            \item \textbf{Install Apache Kafka}: Download and install Kafka from the official website. Ensure you have Java installed, as Kafka runs on the Java Virtual Machine (JVM).
            \item \textbf{Start Zookeeper}: Kafka relies on Zookeeper to manage distributed brokers. Run Zookeeper using:
            \begin{lstlisting}[language=bash]
bin/zookeeper-server-start.sh config/zookeeper.properties
            \end{lstlisting}
            \item \textbf{Start Kafka Broker}: Next, start the Kafka server with:
            \begin{lstlisting}[language=bash]
bin/kafka-server-start.sh config/server.properties
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Create a Basic Streaming Application - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Create a Kafka Topic}
        Define a Kafka topic where your streaming data will be sent and received:
        \begin{lstlisting}[language=bash]
bin/kafka-topics.sh --create --topic my_stream_topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
        \end{lstlisting}

        \item \textbf{Produce Data}
        Write a producer application that sends data to the topic. Example in Python:
        \begin{lstlisting}[language=python]
from kafka import KafkaProducer
import json

producer = KafkaProducer(bootstrap_servers='localhost:9092')
data = {'message': 'Hello, Kafka!'}
producer.send('my_stream_topic', json.dumps(data).encode('utf-8'))
producer.flush()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Create a Basic Streaming Application - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Consume Data}
        Create a consumer that listens to the topic for new messages:
        \begin{lstlisting}[language=python]
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer('my_stream_topic', bootstrap_servers='localhost:9092', auto_offset_reset='earliest', enable_auto_commit=True)
for message in consumer:
    print(json.loads(message.value))
        \end{lstlisting}

        \item \textbf{Process Data}
        Based on your application needs, implement processing logic:
        \begin{lstlisting}[language=python]
for message in consumer:
    data = json.loads(message.value)
    if data['message'] == 'specific_condition':
        print(data)
        \end{lstlisting}
        
        \item \textbf{Scale and Monitor}
        As data volume grows, consider adding additional Kafka brokers and partitions. Use monitoring tools like Kafka Manager or Prometheus for tracking performance metrics.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Kafka's Architecture}: Understand the roles of producers, consumers, and brokers.
        \item \textbf{Reliability}: Kafka ensures message durability via replication.
        \item \textbf{Performance}: Make use of batching and compression to optimize data production and consumption.
    \end{itemize}
    By following these steps, you’ll be able to create a basic streaming data application using Apache Kafka. Experiment with different configurations and processing logic to fully leverage the power of streaming data!
\end{frame}

\begin{frame}
    \frametitle{Data Ingestion Techniques}
    % Introduction to the topic
    Data ingestion is the process of collecting and importing data from various sources to a storage system, often in real-time for fast access and processing. For streaming data, efficiency and speed are crucial due to the continuous flow of data from sources like sensors, user activities, or social media.
\end{frame}

\begin{frame}
    \frametitle{Ingestion Techniques Overview}
    % Overview of different ingestion methods
    \begin{enumerate}
        \item Pull-Based Ingestion
        \item Push-Based Ingestion
        \item Batch Ingestion
        \item Hybrid Ingestion
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Ingestion Technique: Pull-Based}
    \begin{block}{Description}
        The consumer application actively requests data from the source at regular intervals.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} A web application polls a logging server every minute for new entries.
        \item \textbf{Pros:}
            \begin{itemize}
                \item Better control over data flow.
                \item Easy to implement and manage.
            \end{itemize}
        \item \textbf{Cons:}
            \begin{itemize}
                \item May introduce latency.
                \item Not optimal for high-frequency data updates.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Ingestion Technique: Push-Based}
    \begin{block}{Description}
        The data source pushes data to the consumer application as events occur.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} IoT devices send temperature readings to a cloud service when changes occur.
        \item \textbf{Pros:}
            \begin{itemize}
                \item Reduces latency.
                \item Efficient for event-driven architectures.
            \end{itemize}
        \item \textbf{Cons:}
            \begin{itemize}
                \item Can overwhelm consumers during data bursts.
                \item Requires robust error handling and flow control.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Ingestion Technique: Batch}
    \begin{block}{Description}
        Data is collected over a period and ingested in batches.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} End-of-day sales data is aggregated and uploaded to a database.
        \item \textbf{Pros:}
            \begin{itemize}
                \item Efficient for processing large volumes at once.
                \item Eases error correction and data cleansing.
            \end{itemize}
        \item \textbf{Cons:}
            \begin{itemize}
                \item Not suitable for real-time applications.
                \item Introduces delays between data generation and processing.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Ingestion Technique: Hybrid}
    \begin{block}{Description}
        Combines both pull and push methods to leverage benefits from both.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} A pipeline that pulls data from databases while listening for real-time updates via webhooks.
        \item \textbf{Pros:}
            \begin{itemize}
                \item Flexibility in handling various data types.
                \item Balances real-time needs with batch processing.
            \end{itemize}
        \item \textbf{Cons:}
            \begin{itemize}
                \item More complex architecture.
                \item Requires thorough management.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Considerations}
    \begin{itemize}
        \item \textbf{Latency:} Opt for low-latency methods for real-time applications (e.g., push-based).
        \item \textbf{Scalability:} Select techniques that can scale with data while considering cost and complexity.
        \item \textbf{Data Volume \& Velocity:} Understand your incoming data's rate and volume to choose the efficient method.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example}
    % Example code for Push-Based Ingestion using Kafka
    \begin{lstlisting}[language=Java]
KafkaProducer<String, String> producer = new KafkaProducer<>(props);
producer.send(new ProducerRecord<>("topicName", "key", "value"));
    \end{lstlisting}
    This Java code demonstrates a simple way to send messages to a Kafka topic, enabling real-time data ingestion.
\end{frame}

\begin{frame}
    \frametitle{Summary and Next Steps}
    Selecting the appropriate data ingestion technique is crucial for the efficiency of a streaming data application. 
    \begin{itemize}
        \item Consider requirements such as latency, data volume, and system complexity.
        \item Next, explore how to implement \textbf{Real-Time Data Analytics} with the processed streaming data for actionable insights.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Real-Time Data Analytics}
    \begin{block}{Introduction to Real-Time Data Analytics}
        Real-Time Data Analytics involves analyzing and interpreting data as it flows into a system, enabling immediate insights and prompt decision-making.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Streaming Data}: Continuously generated data from sources like sensors or social media (e.g., IoT sensor data, financial transactions).
        
        \item \textbf{Latency}: The delay between data generation and analytics output. Real-time analytics seeks low-latency responses.

        \item \textbf{Event Processing}: Detecting and responding to events in data streams as they happen, involving real-time computation.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Important Components}
    \begin{itemize}
        \item \textbf{Data Streams}: Continuous flows of data for real-time processing.
        
        \item \textbf{Stream Processing Engines}: Tools like Apache Kafka Streams, Apache Flink, and Apache Spark Streaming facilitate streaming data processing.

        \item \textbf{Windowing}: Techniques to limit the data analyzed at one time (e.g., tumbling windows).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Use Cases of Real-Time Data Analytics}
    \begin{itemize}
        \item \textbf{Financial Services}: Monitoring stock prices for instant trade execution based on market changes.

        \item \textbf{Healthcare}: Real-time patient monitoring to alert professionals of abnormal patterns.

        \item \textbf{Social Media Monitoring}: Analyzing live data to track brand sentiment and inform marketing strategies.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Speed and Efficiency}: Quickly transforms data into actionable insights for competitive advantages.

        \item \textbf{Scalability}: Systems must manage fluctuating data volumes as organizations grow.

        \item \textbf{Integration with Other Technologies}: Integrates machine learning models for dynamic predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet (Apache Kafka - Python)}
    \begin{lstlisting}[language=Python]
from kafka import KafkaConsumer

# Create a consumer that subscribes to a topic
consumer = KafkaConsumer('my_topic', 
                         bootstrap_servers='localhost:9092',
                         auto_offset_reset='earliest',
                         enable_auto_commit=True)

# Process messages in real time
for message in consumer:
    print(f"Received message: {message.value.decode('utf-8')}")
    # Perform analysis or trigger actions based on the message
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Implementing real-time data analytics is essential for leveraging timely insights from streaming data. Focusing on speed, efficiency, and integration capabilities can effectively enhance decision-making processes.
\end{frame}

\begin{frame}
    \frametitle{Integrating Machine Learning with Streaming Data}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Streaming Data}: Continuously generated data from sources like sensors, social media, and IoT devices. Requires real-time processing.
            \item \textbf{Machine Learning (ML)}: A branch of artificial intelligence that focuses on learning from data, identifying patterns, and predicting outcomes.
            \item \textbf{Integration of ML with Streaming Data}: Combining ML with real-time data processing for timely insights and actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Applying Machine Learning to Streaming Data}
    \begin{enumerate}
        \item \textbf{Stream Processing Frameworks}:
        \begin{itemize}
            \item Utilize frameworks like Apache Kafka, Apache Flink, or Apache Spark Streaming.
        \end{itemize}
        
        \item \textbf{Model Training on Streaming Data}:
        \begin{itemize}
            \item \textbf{Incremental Learning}: Update model parameters continuously with new data.
            \item \textbf{Example}:
            \end{itemize}
            \begin{block}{Code Snippet}
                \begin{lstlisting}[language=Python]
from sklearn.linear_model import SGDClassifier

model = SGDClassifier()
for batch in streaming_data_batches:
    X, y = preprocess(batch)
    model.partial_fit(X, y)
                \end{lstlisting}
            \end{block}

        \item \textbf{Anomaly Detection}:
        \begin{itemize}
            \item Identify unusual patterns in real-time data (e.g., flagging suspicious financial transactions).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Real-Time Decision Making}: Integration allows timely, informed decisions that impact efficiency and satisfaction.
        \item \textbf{Scalability}: Tools enable horizontal scaling for increased data loads.
        \item \textbf{Feedback Loop}: Continuous learning from new data to maintain model accuracy.
        \item \textbf{Challenges}: Issues like high data velocity, model decay, and data quality impact predictions.
    \end{itemize}

    \begin{block}{Conclusion}
        By integrating ML with streaming data, organizations enhance their analytics capabilities, yielding actionable insights and optimizations.
    \end{block}
    
    \textbf{Next Steps:} Performance Evaluation in the context of streaming data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation - Introduction}
    \begin{block}{Introduction to Performance Evaluation}
        In the world of streaming data, performance evaluation is crucial for real-time responsiveness and efficient data processing. Traditional metrics from batch processing are often inadequate for the dynamic nature of streaming applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation - Key Metrics}
    \begin{block}{Key Metrics for Evaluating Performance}
        \begin{enumerate}
            \item \textbf{Latency}
                \begin{itemize}
                    \item \textbf{Definition}: Time from data ingestion to processing and output.
                    \item \textbf{Importance}: Lower latency is vital for real-time analytics, e.g., fraud detection.
                    \item \textbf{Measurement}:
                    \[
                    \text{Average Latency} = \frac{\sum_{i=1}^{n} (\text{End Time}_i - \text{Start Time}_i)}{n}
                    \]
                    \item \textbf{Example}: 100 data points in 5 seconds imply average latency = 0.05 seconds.
                \end{itemize}
            \item \textbf{Throughput}
                \begin{itemize}
                    \item \textbf{Definition}: Number of data units processed per second.
                    \item \textbf{Importance}: High throughput indicates efficient data handling.
                    \item \textbf{Measurement}:
                    \[
                    \text{Throughput} = \frac{\text{Total Number of Records Processed}}{\text{Total Processing Time}}
                    \]
                    \item \textbf{Example}: 10,000 records in 10 seconds results in throughput = 1,000 records/second.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation - More Key Metrics}
    \begin{block}{Key Metrics for Evaluating Performance (Cont.)}
        \begin{enumerate}
            \setcounter{enumi}{2} % To continue numbering from previous frame
            \item \textbf{Backpressure}
                \begin{itemize}
                    \item \textbf{Definition}: Manage data flow based on system capacity.
                    \item \textbf{Importance}: Prevents overload and ensures data integrity.
                    \item \textbf{Example}: When processing rate is slower than ingestion rate, backpressure signals to slow down the source.
                \end{itemize}
            \item \textbf{Fault Tolerance}
                \begin{itemize}
                    \item \textbf{Definition}: System’s ability to operate correctly despite failures.
                    \item \textbf{Importance}: Critical for reliable processing in unpredictable environments.
                    \item \textbf{Measurement}: Evaluated through recovery time and system availability.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation - Methods}
    \begin{block}{Methods to Evaluate Performance}
        \begin{enumerate}
            \item \textbf{Benchmarking}
                \begin{itemize}
                    \item \textbf{Description}: Compare performance against industry standards or historical data.
                    \item \textbf{Process}: Conduct controlled tests with sample datasets, measuring consistently.
                \end{itemize}
            \item \textbf{Load Testing}
                \begin{itemize}
                    \item \textbf{Description}: Assess application behavior under extreme conditions.
                    \item \textbf{Tool Examples}: Apache JMeter, Gatling for high data load simulation.
                \end{itemize}
            \item \textbf{Monitoring Tools}
                \begin{itemize}
                    \item \textbf{Description}: Continuous assessment of performance metrics in real-time.
                    \item \textbf{Tool Examples}: Apache Kafka, Prometheus, and Grafana for visualizing metrics.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Continuous monitoring and evaluation are crucial for responsiveness and reliability.
            \item Metrics like latency and throughput directly impact user experience.
            \item Using proper evaluation methods helps identify bottlenecks, enabling better resource allocation and fault tolerance.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Efficient performance evaluation is vital for streaming applications. By understanding and measuring key metrics and implementing robust evaluation methods, developers can meet real-time processing requirements and ensure high-quality data insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies}
    \begin{block}{Introduction to Streaming Data Processing}
        Streaming data processing is a paradigm that enables organizations to handle vast amounts of real-time data and derive insights swiftly. This section explores several case studies showcasing practical applications across various industries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-time Fraud Detection in Financial Services}
    \begin{itemize}
        \item \textbf{Concept:} 
        Financial institutions utilize streaming data to monitor transactions in real-time, identifying potentially fraudulent activities.
        
        \item \textbf{How it works:} 
        \begin{itemize}
            \item Data streams include transaction details, customer behavior, and historical fraud patterns.
            \item Algorithms process this data on-the-fly using machine learning models to assess transaction risk.
        \end{itemize}
        
        \item \textbf{Example:} 
        If a credit card is used in two different countries within minutes, the system flags it for manual review, reducing financial losses and enhancing security.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Smart Traffic Management}
    \begin{itemize}
        \item \textbf{Concept:} 
        Cities utilize streaming data from traffic cameras and sensors to optimize traffic flow and reduce congestion.
        
        \item \textbf{How it works:} 
        \begin{itemize}
            \item Real-time data is analyzed to adjust traffic signals based on current conditions.
        \end{itemize}
        
        \item \textbf{Example:} 
        A system reroutes traffic when it detects delays due to accidents, improving travel times and reducing emissions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Online Retail and Customer Experience}
    \begin{itemize}
        \item \textbf{Concept:} 
        E-commerce platforms leverage streaming data for personalized recommendations and inventory management.
        
        \item \textbf{How it works:} 
        \begin{itemize}
            \item Data streams include user click patterns, purchase history, and social media activity.
            \item Algorithms analyze this data to serve tailored recommendations and restocking alerts.
        \end{itemize}
        
        \item \textbf{Example:} 
        If a user frequently views sports gear, the platform highlights new arrivals or promotions in that category.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monitoring Internet of Things (IoT) Devices}
    \begin{itemize}
        \item \textbf{Concept:} 
        Streaming data is crucial for managing IoT devices, enabling timely decision-making.
        
        \item \textbf{How it works:} 
        \begin{itemize}
            \item Sensors in smart home devices send continuous usage data or malfunction alerts.
            \item Data is processed to trigger alerts or automate responses, like adjusting thermostat settings.
        \end{itemize}
        
        \item \textbf{Example:} 
        A smart thermostat learns from user behavior to optimize heating/cooling, leading to energy savings.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Real-Time Insight:} 
        Streaming data enables immediate actions instead of reacting to historical data.
        
        \item \textbf{Automation and Efficiency:} 
        Many processes can be automated, leading to improved efficiency and response times.
        
        \item \textbf{Scalability:} 
        Streaming frameworks can scale with growing data volumes without performance loss.
    \end{itemize}
    
    \begin{block}{Conclusion}
        These case studies highlight the significant impact of streaming data processing in various sectors, enhancing efficiency, safety, and customer experiences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Streaming Data - Introduction}
    \begin{block}{Introduction to Future Trends}
        As the world becomes increasingly data-centric, streaming data technologies are evolving rapidly. Understanding these trends is crucial for anyone involved in data science, machine learning, or software development. This slide explores emerging technologies and predictions that will shape the future of streaming data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Streaming Data - Key Trends}
    \begin{enumerate}
        \item \textbf{Increased Adoption of Real-Time Analytics:}
            \begin{itemize}
                \item Organizations are moving towards real-time data analytics to make immediate, informed decisions.
                \item This shift enables businesses to respond to customer behavior, market changes, and operational efficiencies instantaneously.
                \item \textit{Example}: Retailers using real-time transaction data to tailor marketing campaigns on-the-fly.
            \end{itemize}
        
        \item \textbf{Integration of AI and Machine Learning:}
            \begin{itemize}
                \item Advancements in AI will enhance streaming data systems, allowing for more intelligent data processing and predictive analytics.
                \item Machine learning models can be deployed to analyze trends and anomalies in real-time streams.
                \item \textit{Example}: Fraud detection systems leveraging streaming data to identify unusual transaction patterns instantly.
            \end{itemize}
            
        \item \textbf{Serverless Architecture for Stream Processing:}
            \begin{itemize}
                \item The rise of serverless computing will simplify the deployment and scalability of streaming applications.
                \item Developers can focus on code without investing time in managing server infrastructure.
                \item \textit{Example}: Using AWS Lambda functions to process streaming data from IoT devices.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Streaming Data - Technologies and Security}
    \begin{itemize}
        \item \textbf{Technologies Driving the Future:}
            \begin{itemize}
                \item \textbf{Apache Kafka and Stream Processing Frameworks:}
                    \begin{itemize}
                        \item Apache Kafka remains a leading choice for handling large data streams, integrated with frameworks like Apache Flink and Apache Pulsar for real-time processing capabilities.
                        \item This combination enables complex event processing and analytics.
                    \end{itemize}

                \item \textbf{Edge Computing:}
                    \begin{itemize}
                        \item With the Internet of Things (IoT) expanding, processing data at the edge reduces latency and enhances performance.
                        \item Especially important for applications like autonomous vehicles and smart cities.
                    \end{itemize}
            \end{itemize}
    
        \item \textbf{Emphasis on Security and Privacy:}
            \begin{itemize}
                \item As streaming data usage grows, so does the need for robust security measures.
                \item Ensuring data integrity, encryption, and compliant processing methods will be fundamental to maintaining user trust.
                \item Organizations must invest in technologies that ensure secure data handling throughout the streaming process.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Streaming Data - Conclusion}
    \begin{block}{Conclusion and Call to Action}
        Staying informed about these future trends is essential for leveraging streaming data effectively. As you progress in your data career, consider exploring how these trends can be applied to your projects and the technologies that support them. Engage in hands-on learning with tools like Apache Kafka, cloud platforms, and machine learning libraries to prepare for the evolving landscape of data processing.
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Real-time analytics and AI integration will be at the forefront.
            \item Serverless architectures and edge computing will revolutionize streaming applications.
            \item Security must remain a top priority as streaming data continues to grow.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Concepts in Streaming Data}
    
    \begin{block}{Definition of Stream Processing}
        Stream processing refers to the continuous input, processing, and output of data streams rather than batch processing, enabling real-time analytics and decision-making.
    \end{block}

    \begin{block}{Real-Time Data Handling}
        Unlike traditional batch processing, stream processing manages data on-the-fly, essential for applications such as:
        \begin{itemize}
            \item Fraud detection
            \item User activity monitoring
            \item Stock price analysis
        \end{itemize}
    \end{block}

    \begin{block}{Event-Driven Architectures}
        Stream processing is key to event-driven architectures, allowing systems to react to events as they occur—crucial for modern applications like IoT and social media.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Scalability and Applications}
    
    \begin{block}{Scalability and Flexibility}
        Stream processing frameworks (e.g., Apache Kafka, Apache Flink, Apache Spark Streaming) provide scalable solutions capable of handling large volumes of data efficiently.
    \end{block}

    \begin{block}{Examples of Stream Processing Applications}
        \begin{itemize}
            \item \textbf{E-Commerce:} Dynamic recommendation engines that update product suggestions based on user behavior in real-time.
            \item \textbf{Finance:} Instantaneous fraud detection systems analyzing transactions as they occur.
            \item \textbf{Healthcare:} Monitoring patient vital signs in real-time to trigger alerts for critical conditions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways and Final Thoughts}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item \textbf{Timeliness:} Quicker insights for rapid decision-making.
            \item \textbf{Enhanced User Experience:} Real-time processing improves user engagement.
            \item \textbf{Data Monetization:} Businesses leverage real-time data for competitive advantages.
        \end{itemize}
    \end{block}

    \begin{block}{Final Thoughts}
        In a world driven by speed and agility, stream processing emerges as a transformative approach. Continuous advancements in streaming technologies will give industries a competitive edge.
    \end{block}
    
    \textbf{Note:} Prepare any questions for our upcoming Q\&A session to discuss your thoughts on these concepts!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A - Opening Remarks}
    \begin{block}{Welcome}
        Welcome to the Q\&A session on Streaming Data! This is an opportunity to clarify your understanding, discuss concepts we covered during the week, and explore any questions or scenarios you might have encountered.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A - Key Concepts}
    \begin{itemize}
        \item \textbf{Definition of Streaming Data:} Continuous data flows generated in real-time for immediate processing and analysis.
        \item \textbf{Stream Processing:} Real-time processing allowing data to be ingested, processed, and acted upon without significant delay.
        \item \textbf{Tools and Technologies:}
        \begin{itemize}
            \item \textbf{Apache Kafka:} Open-source platform for handling real-time data feeds.
            \item \textbf{Apache Flink:} Stream processing framework for high-performance distributed computations.
            \item \textbf{Apache Spark Streaming:} Extension of Apache Spark for scalable, fault-tolerant stream processing.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A - Illustrative Examples}
    \begin{enumerate}
        \item \textbf{Use Case - E-Commerce:}
        \begin{itemize}
            \item \textbf{Scenario:} User clicks on a product link on an e-commerce website.
            \item \textbf{Streaming Data Component:} Immediate data point sent to analytics for tracking user behavior.
        \end{itemize}
        
        \item \textbf{Use Case - Social Media Monitoring:}
        \begin{itemize}
            \item \textbf{Scenario:} A new hashtag trends on Twitter.
            \item \textbf{Streaming Data Component:} Continuous streams of tweets analyzed in real-time to gauge public sentiment.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A - Discussion Prompts}
    \begin{itemize}
        \item What challenges do organizations face while using streaming data?
        \item Can you think of situations where real-time processing might not be necessary?
        \item How does streaming data integrate with traditional data storage systems?
    \end{itemize}
\end{frame}


\end{document}