\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Chapter 6: Model Evaluation]{Chapter 6: Model Evaluation}
\author[Your Name]{Your Name}
\institute[Your Institution]{
  Your Department\\
  Your Institution\\
  \vspace{0.3cm}
  Email: your.email@domain.com\\
  Website: www.yourwebsite.com
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Model Evaluation}
    \begin{block}{Significance}
        Model evaluation is a critical component in the development of machine learning systems. It assesses how well a model performs, ensuring effectiveness and reliability before real-world deployment.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Performance Analysis}: Measures model accuracy, essential for reliability.
        \item \textbf{Generalization}: Assesses the model's ability to perform well on unseen data.
        \item \textbf{Informed Decision-making}: Provides insights for stakeholders on model deployment.
        \item \textbf{Comparative Assessment}: Facilitates model comparisons to identify the best approach.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Process}
    \begin{enumerate}
        \item \textbf{Train/Test Split}:
            \begin{itemize}
                \item Split the dataset into parts (training, validation, testing).
                \item Example: 800 for training, 200 for testing from a 1000 sample dataset.
            \end{itemize}
        \item \textbf{Evaluation Metrics}:
            \begin{itemize}
                \item \textbf{Accuracy}:
                \begin{equation}
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Samples}}
                \end{equation}
                
                \item \textbf{Precision}:
                \begin{equation}
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
                \end{equation}

                \item \textbf{Recall}:
                \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
                \end{equation}

                \item \textbf{F1 Score}:
                \begin{equation}
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Iterative Process}: Evaluation is an ongoing process that should be revisited after tuning or retraining.
        \item \textbf{Holistic Approach}: Utilize multiple metrics for comprehensive insights into model performance.
        \item \textbf{Real-World Impact}: A well-evaluated model can significantly influence business and healthcare decisions.
    \end{itemize}

    \begin{block}{Conclusion}
        Evaluating machine learning models is essential for understanding their potential and limitations, especially as they become integral to decision-making processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Model Evaluation}
    \begin{block}{Understanding the Necessity}
        Evaluating machine learning models is crucial in the development process. It helps assess performance, reliability, and the impact on real-life decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Aspects of Model Evaluation}
    \begin{enumerate}
        \item \textbf{Assessing Performance}
            \begin{itemize}
                \item Performance evaluation measures how well a model performs on tasks.
                \item A model with high training accuracy may not generalize well.
                \item \textit{Example:} A spam detection model may fail on new spam techniques.
            \end{itemize}
        
        \item \textbf{Ensuring Reliability}
            \begin{itemize}
                \item Reliability refers to the model's consistent predictions across data samples.
                \item Inconsistent results in critical applications (e.g., healthcare) can have severe consequences.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact and Continuous Improvement}
    \begin{enumerate}
        \item \textbf{Impact on Decision-Making}
            \begin{itemize}
                \item Decisions based on model outputs have real-world consequences.
                \item Understanding capabilities helps stakeholders make informed decisions.
                \item \textit{Example:} Businesses using predictive models must evaluate rigorously to avoid financial pitfalls.
            \end{itemize}

        \item \textbf{Identifying Bias and Continuous Improvement}
            \begin{itemize}
                \item Model bias can lead to unfair outcomes.
                \item Continuous evaluation supports refinement based on changing data.
                \item \textit{Example:} A recommendation system adapts to user preferences over time.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Evaluation Metrics}
    \begin{block}{Introduction}
        In machine learning, evaluating the effectiveness of a model is essential for determining how well it performs its intended task. Various metrics allow us to measure performance in distinct ways, depending on our goals. This slide explores key evaluation metrics:
        \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
            \item F1-score
            \item ROC-AUC
            \item Confusion Matrix
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Accuracy}
    \begin{equation}
        \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
    \end{equation}
    \begin{itemize}
        \item \textbf{Definition}: The proportion of correct predictions (both true positives and true negatives) among the total number of cases.
        \item \textbf{Example}: If a model correctly predicts 90 out of 100 samples, its accuracy is 90\%.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Precision}
    \begin{equation}
        \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
    \end{equation}
    \begin{itemize}
        \item \textbf{Definition}: The proportion of true positive predictions relative to the total positive predictions (true positives + false positives). It measures the accuracy of positive predictions.
        \item \textbf{Example}: If a model predicted 50 positives but only 30 were true positives, its precision is \( \frac{30}{50} = 0.6 \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Recall}
    \begin{equation}
        \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
    \end{equation}
    \begin{itemize}
        \item \textbf{Definition}: The proportion of true positive predictions relative to all actual positives (true positives + false negatives). It indicates the model's ability to identify relevant instances.
        \item \textbf{Example}: If a model identifies 30 true positives out of 50 actual positives, recall is \( \frac{30}{50} = 0.6 \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. F1-score}
    \begin{equation}
        \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
    \begin{itemize}
        \item \textbf{Definition}: The harmonic mean of precision and recall. Useful for datasets with imbalanced classes.
        \item \textbf{Example}: If precision is 0.6 and recall is 0.6, the F1-score is \( 2 \times \frac{0.6 \times 0.6}{0.6 + 0.6} = 0.6 \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. ROC-AUC}
    \begin{itemize}
        \item \textbf{Definition}: The Area Under the Receiver Operating Characteristic Curve (ROC-AUC) measures the ability of a model to distinguish between classes.
        \item \textbf{Key Point}: AUC value ranges from 0 to 1; closer to 1 indicates better model performance.
        \item \textbf{Illustration}: In a ROC curve plot, the True Positive Rate (Recall) is on the y-axis, and the False Positive Rate is on the x-axis. A curve that hugs the top-left corner indicates high performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{6. Confusion Matrix}
    \begin{block}{Definition}
        A table summarizing the performance of a classification model. It shows counts of true positives, true negatives, false positives, and false negatives.
    \end{block}
    \begin{block}{Example Visualization}
        \begin{center}
        \begin{tabular}{|c|c|c|}
        \hline
                 & Predicted Positive & Predicted Negative \\ \hline
        Actual Positive &        TP          &         FN         \\ \hline
        Actual Negative &        FP          &         TN         \\ \hline
        \end{tabular}
        \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Different metrics serve different purposes; choose based on the context of the problem.
        \item Accuracy can be misleading in imbalanced datasets; consider using precision, recall, and F1-score for a more nuanced evaluation.
        \item Always visualize the results with a confusion matrix or ROC curve to understand model performance visually.
    \end{itemize}
    By comprehensively understanding these evaluation metrics, you can better assess your model's effectiveness and refine it for improved performance!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Different Metrics}
    \begin{block}{Overview of Evaluation Metrics}
        Model evaluation is crucial in the machine learning workflow, helping us understand how well our models perform.
        Several metrics offer different insights into model performance, each suitable for specific scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy}
    \begin{itemize}
        \item \textbf{Definition}: The ratio of correctly predicted instances to the total instances.
        \item \textbf{Formula}:
        \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        Where:
        \begin{itemize}
            \item \textbf{TP} = True Positives
            \item \textbf{TN} = True Negatives
            \item \textbf{FP} = False Positives
            \item \textbf{FN} = False Negatives
        \end{itemize}
        \item \textbf{Significance}: Easy to understand; best used when classes are balanced.
        \item \textbf{Example}: For a model predicting spam, if 90 out of 100 emails are classified correctly, Accuracy is 90\%.
        \item \textbf{Best Utilized}: In balanced datasets; not effective with imbalanced datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall}
    \begin{itemize}
        \item \textbf{Precision}:
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted positive observations to total predicted positives.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Precision} = \frac{TP}{TP + FP}
            \end{equation}
            \item \textbf{Significance}: Crucial where cost of false positives is high (e.g., disease detection).
            \item \textbf{Best Utilized}: When false positives must be minimized (e.g., spam detection).
        \end{itemize}
        \item \textbf{Recall (Sensitivity)}:
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted positive observations to all actual positives.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Recall} = \frac{TP}{TP + FN}
            \end{equation}
            \item \textbf{Significance}: Emphasizes the model's ability to capture all relevant instances.
            \item \textbf{Best Utilized}: In situations where false negatives are critical (e.g., medical diagnoses).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score and ROC-AUC}
    \begin{itemize}
        \item \textbf{F1-Score}:
        \begin{itemize}
            \item \textbf{Definition}: Harmonic mean of precision and recall, providing balance.
            \item \textbf{Formula}:
            \begin{equation}
            \text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Significance}: Useful when balancing precision and recall is essential.
            \item \textbf{Best Utilized}: In class imbalances where both false positives and false negatives are important.
        \end{itemize}
        \item \textbf{ROC-AUC}:
        \begin{itemize}
            \item \textbf{Definition}: Area under ROC curve; a metric representing trade-offs between true positive and false positive rates.
            \item \textbf{Significance}: Evaluates model performance independent of classification threshold.
            \item \textbf{Best Utilized}: For binary classification problems, especially with imbalanced classes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Model Selection}: Different metrics guide different aspects of model performance; choose based on specific demands.
        \item \textbf{Imbalance Awareness}: Assess dataset balance, as it affects metric relevance.
        \item \textbf{Model Interpretation}: Utilize multiple metrics for a comprehensive understanding of model efficacy.
    \end{itemize}
    Engaging with these metrics leads to iterative improvement and robust real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation Techniques}
    \begin{block}{Overview}
        Cross-validation is a technique used in machine learning to assess model performance and generalizability. It helps prevent overfitting by evaluating how well a model will perform on independent datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Cross-Validation?}
    \begin{itemize}
        \item Statistical method for assessing model performance.
        \item Helps to evaluate the model on unseen datasets.
        \item Reduces the risk of overfitting by validating on multiple subsets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Types of Cross-Validation}
    \begin{itemize}
        \item \textbf{K-Fold Cross-Validation}
        \begin{itemize}
            \item \textbf{Definition}: Dataset is divided into 'k' folds; trained on 'k-1' and validated on 1.
            \item \textbf{Steps}:
            \begin{enumerate}
                \item Split dataset into k equal parts.
                \item Train on k-1 parts and validate on the remaining part.
                \item Average results for overall performance.
            \end{enumerate}
            \item \textbf{Key Point}: Choice of 'k' is crucial; common values are 5 and 10.
        \end{itemize}

        \item \textbf{Stratified Cross-Validation}
        \begin{itemize}
            \item \textbf{Definition}: Variation ensuring each fold has the same class proportion.
            \item \textbf{Steps}:
            \begin{enumerate}
                \item Identify classes in the dataset.
                \item Distribute samples proportionally into each fold.
                \item Train and validate as in k-fold.
            \end{enumerate}
            \item \textbf{Key Point}: Maintains class balance, crucial for imbalanced datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of Cross-Validation}
    \begin{itemize}
        \item \textbf{Robustness}: Provides a reliable estimate of model performance.
        \item \textbf{Reduced Overfitting}: Validates across different subsets, preventing the model from learning noise.
        \item \textbf{Better Hyperparameter Tuning}: Insights for optimal parameters through multiple validations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    Here is an example using Python's scikit-learn for k-fold cross-validation:
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score

# Loading dataset
data = load_iris()
X, y = data.data, data.target

# Initializing KFold
kf = KFold(n_splits=5)
model = RandomForestClassifier()

accuracies = []

# Running k-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    accuracies.append(accuracy_score(y_test, predictions))

# Average accuracy
print("Average Accuracy: ", sum(accuracies) / len(accuracies))
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Cross-validation techniques, notably k-fold and stratified methods, are essential for developing robust models. They ensure that evaluation metrics reflect true model performance when dealing with unseen data, making them vital steps in any data science workflow.
\end{frame}

\begin{frame}[fragile]{Interpreting Model Performance}
    \begin{block}{Introduction}
        Guidance on how to interpret evaluation metrics and the common pitfalls in assessing model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding Model Evaluation Metrics - Part 1}
    When evaluating the performance of a machine learning model, it’s crucial to understand the metrics used, as they provide insights into how well the model performs.

    \begin{enumerate}
        \item \textbf{Accuracy}:
            \begin{itemize}
                \item \textbf{Definition}: The ratio of correctly predicted instances to the total instances.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
                \end{equation}
                \item \textbf{Example}: If a model predicts 90 true positives (TP) and 10 false positives (FP) out of 100 total, the accuracy is 90\%.
            \end{itemize}
        
        \item \textbf{Precision}:
            \begin{itemize}
                \item \textbf{Definition}: The ratio of true positives to the sum of true and false positives.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Precision} = \frac{TP}{TP + FP}
                \end{equation}
                \item \textbf{Example}: If a model finds 80 TP but also has 20 FP, precision will be \( \frac{80}{80 + 20} = 0.8 \) or 80\%.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Understanding Model Evaluation Metrics - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Recall (Sensitivity)}:
            \begin{itemize}
                \item \textbf{Definition}: The ratio of true positives to the sum of true positives and false negatives.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Recall} = \frac{TP}{TP + FN}
                \end{equation}
                \item \textbf{Example}: If there are 90 actual positives, and the model predicts 70 correctly, recall is \( \frac{70}{90} = 0.78 \) or 78\%.
            \end{itemize}
        
        \item \textbf{F1 Score}:
            \begin{itemize}
                \item \textbf{Definition}: The harmonic mean of precision and recall, useful for imbalanced classes.
                \item \textbf{Formula}:
                \begin{equation}
                    F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
                \end{equation}
                \item \textbf{Example}: If Precision = 0.8 and Recall = 0.78, then:
                \begin{equation}
                    F1 = 2 \cdot \frac{0.8 \cdot 0.78}{0.8 + 0.78} \approx 0.79
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Common Pitfalls in Model Evaluation}
    \begin{itemize}
        \item \textbf{Overfitting}: A model performs well on training data but poorly on unseen data. Always validate with a separate test set.
        
        \item \textbf{Ignoring Class Imbalance}: A high accuracy in an imbalanced dataset can be misleading as it may be due to the majority class dominating.
        
        \item \textbf{Misinterpreting Metrics}: Focusing solely on one metric, like accuracy, without considering others can lead to suboptimal conclusions about a model’s performance.
        
        \item \textbf{Data Leakage}: Training a model on data and then evaluating it on the same data can inflate performance scores, leading to incorrect conclusions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Takeaways and Closing Thoughts}
    \begin{itemize}
        \item Always assess multiple metrics to get a comprehensive view of model performance.
        
        \item Be cautious of overfitting and class imbalance; they can skew results.
        
        \item Use cross-validation techniques to evaluate models reliably.
        
        \item Understanding and accurately interpreting model performance metrics is crucial for developing effective machine learning solutions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Part 1}
    \textbf{Understanding Class Imbalance}
    \begin{itemize}
        \item Class imbalance occurs when one class significantly outnumbers another.
        \item Example: In a binary classification (95\% "defaulted loans", 5\% "non-defaulted loans"), the model may become biased.
    \end{itemize}
    
    \textbf{Impact on Model Evaluation}
    \begin{itemize}
        \item \textbf{Skewed Metrics:} Accuracy can be misleading (e.g., an accuracy of 95\% might just reflect majority class predictions).
        \item \textbf{Overfitting:} Models may overfit to the majority class, impairing minority class predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Part 2}
    \textbf{Strategies for Addressing Class Imbalance}
    
    \begin{enumerate}
        \item \textbf{Resampling Techniques}
        \begin{itemize}
            \item \textbf{Oversampling:} Increase minority class instances by duplicating or synthetic generation (e.g., SMOTE).
            \item \textbf{Undersampling:} Reduce majority class instances, possibly losing critical data.
        \end{itemize}
        
        \item \textbf{Synthetic Data Generation}
        \begin{itemize}
            \item Create new instances resembling the minority class without simple duplication.
            \begin{itemize}
                \item Techniques: GANs, VAEs.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Algorithm-Level Approaches}
        \begin{itemize}
            \item \textbf{Cost-sensitive Training:} Penalizes misclassifications of minority class more heavily.
            \item \textbf{Ensemble Methods:} Techniques like bagging or boosting focus on different data subsets, aiding in minority class identification.
        \end{itemize}
    \end{enumerate}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Part 3}
    \textbf{Key Points to Remember}
    \begin{itemize}
        \item Always evaluate using appropriate metrics (e.g., F1 Score, AUC-ROC) rather than accuracy alone.
        \item Experiment with different techniques and combinations as approaches may vary based on dataset and model.
    \end{itemize}
    
    \textbf{Example Code Snippet (Python)}
    \begin{lstlisting}[language=Python]
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification
from collections import Counter

# Creating dummy dataset
X, y = make_classification(n_classes=2, class_sep=2, weights=[0.95, 0.05],
                           n_informative=3, n_redundant=1, flip_y=0,
                           n_features=20, n_clusters_per_class=1,
                           n_samples=1000, random_state=10)

print('Original dataset shape %s' % Counter(y))

# Applying SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

print('Resampled dataset shape %s' % Counter(y_res))
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Model Comparison}
    \begin{block}{Introduction to Model Comparison}
        Model comparison is a crucial step in the machine learning workflow that evaluates different models' performance on the same dataset using statistical and visual techniques.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts in Model Comparison}
    \begin{itemize}
        \item \textbf{Model Performance Metrics}:
        \begin{itemize}
            \item \textbf{Accuracy}: Fraction of correct predictions over total predictions.
            \item \textbf{Precision}: Ratio of true positives to the sum of true positives and false positives.
            \item \textbf{Recall (Sensitivity)}: Ratio of true positives to the sum of true positives and false negatives.
            \item \textbf{F1 Score}: Harmonic mean of precision and recall, useful for imbalanced datasets.
            \item \textbf{AUC-ROC}: Area under the ROC curve, illustrating trade-offs between true positive and false positive rates.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Statistical Methods for Model Comparison}
    \begin{itemize}
        \item \textbf{Paired t-Test}:
        \begin{itemize}
            \item A method to compare means of two related groups to assess if there is a statistically significant difference.
            \item \textbf{Formula}:
            \begin{equation}
            t = \frac{\bar{d}}{s_d / \sqrt{n}}
            \end{equation}
            Where:
            \begin{itemize}
                \item \( \bar{d} \) = mean difference between paired observations
                \item \( s_d \) = standard deviation of the differences
                \item \( n \) = number of pairs
            \end{itemize}
            \item \textbf{Use}: Evaluate two models on the same validation set for performance metric differences.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Visual Methods for Model Comparison}
    \begin{itemize}
        \item \textbf{Box Plots}: Show performance metrics across multiple iterations or folds, allowing side-by-side comparisons.
        \item \textbf{ROC Curves}: Plot true positive vs false positive rates for various thresholds; closer to the top-left indicates better performance.
        \item \textbf{Precision-Recall Curves}: Useful for imbalanced datasets, these curves show precision against recall for different thresholds.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Statistical significance from the paired t-test helps confirm if performance differences reflect true superiority.
        \item Visual tools enhance intuitive understanding of model performance across metrics.
        \item Choosing the right metrics is crucial for context-specific significance.
        \item Rigorous model evaluation is essential for validating machine learning algorithms and making informed deployment decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Python Code for Model Comparison}
    \begin{lstlisting}[language=Python]
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

# Simulated accuracy results for two models
model_a = np.random.rand(30) * 0.1 + 0.85  # Model A accuracies
model_b = np.random.rand(30) * 0.1 + 0.80  # Model B accuracies

# Paired t-test
t_stat, p_value = stats.ttest_rel(model_a, model_b)
print("T-statistic:", t_stat, "P-value:", p_value)

# Box plot
plt.boxplot([model_a, model_b], labels=['Model A', 'Model B'])
plt.title("Model Comparison: Accuracy Distribution")
plt.ylabel("Accuracy")
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Application - Case Study}
    \begin{block}{Real-World Context}
        A retail company aims to improve customer satisfaction through a recommendation system. The goal is to predict products customers are likely to purchase based on their previous behavior, utilizing historical purchase records, customer ratings, and demographic information.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics Used}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Samples}}
        \end{equation}

        \item \textbf{Precision}
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \end{equation}

        \item \textbf{Recall (Sensitivity)}
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}

        \item \textbf{F1 Score}
        \begin{equation}
            \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Application and Key Lessons}
    \begin{itemize}
        \item \textbf{Model Selection:}
        \begin{itemize}
            \item Various algorithms were tested with metrics computed, e.g., Neural Network achieved:
            \begin{itemize}
                \item Accuracy: 85\%
                \item Precision: 80\%
                \item Recall: 75\%
                \item F1: 0.77
            \end{itemize}
        \end{itemize}

        \item \textbf{Model Tuning:}
        \begin{itemize}
            \item Hyperparameters were optimized, reflecting the trade-off between precision and recall.
        \end{itemize}

        \item \textbf{Key Lessons Learned:}
        \begin{itemize}
            \item **Holistic Evaluation:** Use multiple metrics for a comprehensive performance view.
            \item **Iteration is Key:** Continuous adjustments based on evaluation feedback enhance performance.
            \item **Stakeholder Collaboration:** Engaging non-technical stakeholders promotes transparency.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The case study illustrates that evaluation metrics are essential for:
    \begin{itemize}
        \item Assessing performance.
        \item Identifying improvement areas.
        \item Ensuring models remain relevant over time.
    \end{itemize}
    By understanding and applying various evaluation metrics, data scientists can create robust models that drive real-world value, leading to better decision-making within organizations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Overview}
    \begin{itemize}
        \item Importance of ethical implications in model evaluation
        \item Key areas: Fairness, Bias, Transparency
        \item Impacts on individuals and communities
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Part 1: Fairness}
    \begin{block}{Definition}
        Fairness means ensuring that models treat all individuals equally, without unjust discrimination.
    \end{block}
    \begin{itemize}
        \item Example: Hiring algorithm favoring one gender or ethnic group
        \item Key Points:
            \begin{itemize}
                \item Metrics: demographic parity, equal opportunity, disparate impact
                \item Countermeasures: re-sampling data, re-weighting examples, adjusting thresholds
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Part 2: Bias}
    \begin{block}{Definition}
        Bias refers to systematic errors affecting groups, leading to unjust outcomes.
    \end{block}
    \begin{itemize}
        \item Causes of Bias:
            \begin{itemize}
                \item Data Bias: Poor representation leads to unequal performance
                \item Algorithmic Bias: Certain algorithms may favor specific data patterns
            \end{itemize}
        \item Example: Facial recognition bias against darker skin tones
        \item Key Points:
            \begin{itemize}
                \item Detection: Auditing with confusion matrices, fairness assessments
                \item Mitigation: De-biasing techniques, ensemble methods
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Part 3: Transparency}
    \begin{block}{Definition}
        Transparency involves making model workings clear to stakeholders.
    \end{block}
    \begin{itemize}
        \item Importance: Enhances trust and accountability
        \item Example: Explainable AI providing insights into decision-making
        \item Key Points:
            \begin{itemize}
                \item Model Interpretability: Use simpler models, LIME, SHAP
                \item Documentation: Keep thorough records of development and evaluation
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Conclusion}
    \begin{itemize}
        \item Vital for creating responsible machine learning systems
        \item Emphasize:
            \begin{itemize}
                \item Fairness
                \item Detecting and mitigating bias
                \item Ensuring transparency
            \end{itemize}
        \item Key Takeaway: Regular evaluation of ethical implications enhances trust and accountability in AI systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Learning Points}
    \begin{enumerate}
        \item \textbf{Understanding Model Performance Metrics}
        \begin{itemize}
            \item Evaluating a model’s performance is critical. Key metrics: Accuracy, Precision, Recall, F1 Score, AUC-ROC.
            \item \textbf{Example}: In a binary classification task with 90 out of 100 positives identified, recall is 0.9.
        \end{itemize}
        
        \item \textbf{Importance of Cross-Validation}
        \begin{itemize}
            \item Cross-validation techniques assess how statistical analyses generalize to independent datasets.
            \item \textbf{Illustration}: K-Fold (K=5) splits the dataset into 5 parts, training on 4 and evaluating on the last.
        \end{itemize}
        
        \item \textbf{Addressing Bias and Fairness}
        \begin{itemize}
            \item Ethical implications must be critically considered. Evaluate models for disparate impact on different demographic groups.
            \item \textbf{Key Point}: A seemingly accurate model can propagate bias, needing investigation.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Emerging Trends}
    \begin{enumerate}
        \item \textbf{Explainable AI (XAI)}
        \begin{itemize}
            \item Demand for transparency in predictions is increasing. XAI techniques clarify decision-making processes.
            \item \textbf{Future Direction}: SHAP and LIME are becoming essential tools in model evaluation.
        \end{itemize}
        
        \item \textbf{Automated Model Evaluation}
        \begin{itemize}
            \item Rise of AutoML platforms is automating the evaluation process and model selection through benchmarks.
            \item \textbf{Example}: These platforms can perform thousands of evaluations in parallel, streamlining model selection.
        \end{itemize}
        
        \item \textbf{Contextual Evaluation Metrics}
        \begin{itemize}
            \item Traditional metrics may not suffice for all data types. Emerging trends focus on context-specific metrics.
            \item \textbf{Key Point}: In healthcare, a false negative is often more critical than a false positive.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Future Challenges}
    \begin{enumerate}
        \item \textbf{Scalability and Efficiency}
        \begin{itemize}
            \item As datasets grow, efficient evaluation methodologies are crucial to maintain quality while reducing computational overhead.
        \end{itemize}
        
        \item \textbf{Adapting to Real-World Changes}
        \begin{itemize}
            \item Models based on historical data may fail in evolving trends. Continuous evaluation and adaptation are necessary.
        \end{itemize}
        
        \item \textbf{Integrating Multimodal Data}
        \begin{itemize}
            \item Incorporating multiple data types (text, images, audio) introduces unique challenges in accuracy and interpretability.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Summary}
        Effective model evaluation merges traditional methodologies with modern challenges and technologies. Focus on ethics, automation, and context-aware evaluations is essential for future assessments of model performance.
    \end{block}
\end{frame}


\end{document}