\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Model Deployment and Maintenance]{Chapter 12: Model Deployment and Maintenance}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Deployment and Maintenance}
    \begin{block}{Overview}
        Model deployment is a critical step in the machine learning lifecycle. It involves taking a trained machine learning model and making it accessible for end-users or systems to derive insights or outcomes. Maintenance ensures that the deployed model continues to perform optimally over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Model Deployment - Part 1}
    \begin{itemize}
        \item \textbf{Bridging Research to Application}:
            \begin{itemize}
                \item Deploying models transforms theoretical models into practical applications, allowing organizations to leverage data-driven decision-making.
                \item \textit{Example}: A sentiment analysis model trained on customer reviews can be deployed in a customer service chatbot to provide instant feedback.
            \end{itemize}
        
        \item \textbf{Real-Time Predictions}:
            \begin{itemize}
                \item Models provide immediate insights which can enhance operational efficiency.
                \item \textit{Example}: Fraud detection models can scan transactions in real-time to identify and flag suspicious activity, reducing financial losses.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Model Deployment - Part 2}
    \begin{itemize}
        \item \textbf{Scalability}:
            \begin{itemize}
                \item Deployment facilitates the scaling of solutions to handle large volumes of data and users.
                \item \textit{Example}: A recommendation system can be deployed on an e-commerce platform to serve millions of users simultaneously.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Model Maintenance - Part 1}
    \begin{itemize}
        \item \textbf{Performance Monitoring}:
            \begin{itemize}
                \item Regularly tracking the model’s performance helps identify any degradation in accuracy.
                \item \textit{Key Metric}: Monitor metrics such as accuracy, precision, recall, and F1-score post-deployment.
            \end{itemize}

        \item \textbf{Data Drift and Concept Drift}:
            \begin{itemize}
                \item Models may become less effective over time due to changes in data distributions or underlying relationships in data.
                \item \textit{Example}: If a model trained on historical sales data is not updated, it may fail to recognize changing consumer preferences, leading to poorer predictions.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Model Maintenance - Part 2}
    \begin{itemize}
        \item \textbf{Regular Updates}:
            \begin{itemize}
                \item Continuous learning and retraining of models can help adapt to new incoming data.
                \item \textit{Approach}: Implement a schedule for regular retraining intervals, such as monthly or quarterly, based on business needs and model usage.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Additional Resources}
    \begin{itemize}
        \item \textbf{Key Points to Remember}:
            \begin{itemize}
                \item Deployment is not the end, but the beginning of a continuous process; models require ongoing evaluations to stay relevant and accurate.
                \item Effective maintenance strategies may include automated monitoring systems that notify the team of performance dips.
                \item Collaboration across teams (data scientists, engineers, business stakeholders) ensures smooth deployment and effective maintenance.
            \end{itemize}
        
        \item \textbf{Additional Resources}:
            \begin{itemize}
                \item \textit{Case Studies}: Review deployment case studies from companies to see successful implementation and maintenance strategies.
                \item \textit{Tools \& Frameworks}: Familiarize yourself with tools like Docker for containerization or Kubernetes for orchestration to simplify deployment processes.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Strategies - Introduction}
    Deployment strategies are critical in transitioning a trained machine learning model into an operational system where it can provide predictions on new data. 
    The deployment method can significantly affect:
    \begin{itemize}
        \item Performance
        \item Scalability
        \item Maintainability
    \end{itemize}
    There are two primary deployment approaches:
    \begin{itemize}
        \item On-Premises
        \item Cloud-Based
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Strategies - On-Premises}
    \textbf{Definition:} On-premises deployment refers to hosting the model within the physical infrastructure of an organization, using its own servers and hardware.

    \begin{block}{Advantages}
        \begin{itemize}
            \item \textbf{Control:} Full control over hardware, software, and data.
            \item \textbf{Security:} Data remains within the organization's premises, reducing potential security risks.
            \item \textbf{Compliance:} Easier to meet regulatory compliance in certain industries.
        \end{itemize}
    \end{block}

    \begin{block}{Disadvantages}
        \begin{itemize}
            \item \textbf{Cost:} High initial setup cost for infrastructure and ongoing maintenance.
            \item \textbf{Scalability:} Scaling can be complex and time-consuming.
            \item \textbf{Resource Utilization:} Requires in-house expertise for management and operation.
        \end{itemize}
    \end{block}

    \textbf{Example:} A financial institution might choose this strategy for their risk assessment model to maintain strict data privacy compliance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Strategies - Cloud-Based}
    \textbf{Definition:} Cloud-based deployment utilizes third-party cloud services (e.g., AWS, Google Cloud, Microsoft Azure) to host and run machine learning models.

    \begin{block}{Advantages}
        \begin{itemize}
            \item \textbf{Scalability:} Easily scale resources up or down based on demand.
            \item \textbf{Cost-Effective:} Pay-as-you-go pricing without large upfront costs for hardware.
            \item \textbf{Maintenance:} Cloud providers handle infrastructure management, freeing up internal resources.
        \end{itemize}
    \end{block}

    \begin{block}{Disadvantages}
        \begin{itemize}
            \item \textbf{Security Concerns:} Potential exposure of sensitive data to third parties.
            \item \textbf{Dependency:} Reliance on internet connectivity and service provider's uptime.
            \item \textbf{Compliance Challenges:} Adhering to data regulations can be more complex.
        \end{itemize}
    \end{block}

    \textbf{Example:} A retail company may deploy a recommendation system on AWS to quickly respond to changing consumer behavior without worrying about infrastructure management.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Strategies - Key Points}
    \begin{itemize}
        \item \textbf{Choose the Right Strategy:} Consider factors such as budget, regulatory requirements, and technical expertise.
        \item \textbf{Hybrid Models:} Many organizations opt for hybrid deployment strategies, combining both approaches to leverage strengths and mitigate weaknesses.
        \item \textbf{Monitoring and Maintenance:} Continuous monitoring and maintenance are crucial for ensuring model performance and reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Strategies - Conclusion}
    Understanding deployment strategies is essential for effectively integrating machine learning models into operational environments. 
    Organizations can:
    \begin{itemize}
        \item Optimize model performance
        \item Enhance security
        \item Meet business objectives
    \end{itemize}

    \textbf{Additional Note:} Consideration of real-time vs. batch predictions based on deployment strategy can impact system design and implementation.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Pipeline - Introduction}
    \begin{block}{Definition}
        A deployment pipeline is a crucial process in the lifecycle of machine learning models. It automates the stages of software delivery, facilitating the rapid and reliable deployment of code changes.
    \end{block}
    The pipeline manages the complexity of transitioning from development to production, ensuring high-quality outcomes, reduced risks, and timely updates.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Pipeline - Stages}
    \begin{enumerate}
        \item \textbf{Versioning}
        \begin{itemize}
            \item \textbf{Definition}: Practice of assigning unique identifiers to each iteration of the model.
            \item \textbf{Example}: Semantic versioning (e.g., v1.0.0, v1.1.0) for easier management.
        \end{itemize}
        
        \item \textbf{Continuous Integration (CI)}
        \begin{itemize}
            \item \textbf{Definition}: Automatically integrates and tests code changes in a shared repository.
            \item \textbf{Process}:
            \begin{enumerate}
                \item Developers commit code.
                \item Automated tests validate changes.
                \item Code merges into the main branch if tests pass.
            \end{enumerate}
            \item \textbf{Example}: Tools like Jenkins or GitHub Actions automate these processes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Pipeline - Testing and Visualization}
    \begin{enumerate}[resume]
        \item \textbf{Testing}
        \begin{itemize}
            \item \textbf{Definition}: Essential for ensuring the deployed model’s success.
            \item \textbf{Types of Tests}:
            \begin{itemize}
                \item \textbf{Unit Tests}: Validate individual components.
                \item \textbf{Integration Tests}: Ensure components work together.
                \item \textbf{Performance Testing}: Assess speed and scalability.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \begin{block}{Visual Representation}
        \begin{center}
            \begin{verbatim}
+-----------------+
|   Versioning    | ← Tracks and manages model changes
+-----------------+
        ↓
+-----------------+
| Continuous       |
|   Integration    | ← Automates testing and merging code
+-----------------+
        ↓
+-----------------+
|    Testing      | ← Validates model performance and functionality
+-----------------+
        ↓
+-----------------+
|   Deployment    | ← Pushes the new model to production 
+-----------------+
            \end{verbatim}
        \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deployment Pipeline - Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Automation}: Reduces human error and speeds up delivery.
        \item \textbf{Feedback Loop}: Quick feedback addresses issues proactively.
        \item \textbf{Best Practices}: Regular updates of the pipeline to incorporate new tools.
    \end{itemize}
    \begin{block}{Conclusion}
        A well-structured deployment pipeline is essential for model deployment and maintenance.
    \end{block}
    \textbf{Next Steps:} Explore the importance of model monitoring post-deployment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Monitoring}
    \begin{block}{What is Model Monitoring?}
        Model Monitoring refers to the ongoing process of tracking the performance of machine learning models post-deployment to ensure they maintain expected performance metrics. It is vital because various factors can affect a model's effectiveness over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Model Monitoring Important?}
    \begin{itemize}
        \item \textbf{Data Drift:} Changes in data distributions can affect model performance, e.g., models trained on past economic conditions may fail under new conditions.
        \item \textbf{Concept Drift:} The relationship between features and targets may evolve, leading to changes in feature importance and performance decline.
        \item \textbf{Performance Degradation:} Accuracy and other metrics may degrade over time, necessitating continuous monitoring to identify declines.
        \item \textbf{Regulatory Compliance:} Many industries require ongoing model validation for compliance, thus monitoring is essential to meet these requirements.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics to Monitor}
    \begin{itemize}
        \item \textbf{Accuracy:} Proportion of true results among total cases.
        \item \textbf{Precision and Recall:} Important in classification tasks to evaluate false positives and negatives.
        \item \textbf{F1 Score:} Harmonic mean of precision and recall.
        \item \textbf{ROC-AUC:} Measures model's ability to distinguish between classes, especially useful in binary classification.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Monitoring}
    Consider a customer churn prediction model:
    \begin{enumerate}
        \item Set baseline metrics (accuracy, precision, recall) from pre-deployment tests.
        \item Automate regular metric calculations (e.g., daily).
        \item Trigger alerts if accuracy drops below threshold (e.g., 80%).
        \item Periodically sample incoming data for drift using statistical tests (e.g., Kolmogorov-Smirnov).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simple Monitoring Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score

# Function to monitor model performance
def monitor_model_performance(true_labels, predictions):
    accuracy = accuracy_score(true_labels, predictions)
    
    # Log the performance
    print(f'Model Accuracy: {accuracy:.2f}')
    
    return accuracy

# Example usage with true and predicted labels
true_labels = [1, 0, 1, 0, 1]
predictions = [1, 0, 0, 0, 1]

monitor_model_performance(true_labels, predictions)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Model monitoring is vital in the machine learning lifecycle to maintain model integrity and effectiveness. Regular monitoring supports timely interventions and continuous improvements through retraining and updates as necessary.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Overview}
    \begin{block}{Overview}
        Performance metrics are crucial for assessing the effectiveness of machine learning models after deployment. Monitoring these metrics helps ensure that models continue to deliver the intended results and adapt to changes in data patterns over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Key Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: The proportion of correct predictions out of all predictions made.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Samples}}
            \end{equation}
            \item \textbf{Example}: In a binary classification model predicting disease presence, if there are 80 correct predictions out of 100, the accuracy is 80\%.
        \end{itemize}

        \item \textbf{Precision}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of true positive predictions to the total predicted positives.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Example}: If a model identifies 70 positive cases, of which 50 are true positives and 20 are false positives, precision is \( \frac{50}{70} \approx 0.71 \).
        \end{itemize}

        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of true positive predictions to all actual positives.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Example}: If there were actually 80 positive cases, recall would be \( \frac{50}{80} \approx 0.625 \).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Continued}
    \begin{enumerate}[resume]
        \item \textbf{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of precision and recall; useful for imbalanced datasets.
            \item \textbf{Formula}:
            \begin{equation}
            F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Example}: For precision = 0.71 and recall = 0.625, the F1 score is approximately 0.67.
        \end{itemize}

        \item \textbf{Area Under the ROC Curve (AUC-ROC)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the ability of the model to distinguish between classes. AUC ranges from 0 to 1.
            \item \textbf{Key Point}: A higher AUC indicates better model performance.
        \end{itemize}

        \item \textbf{Mean Absolute Error (MAE)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the average magnitude of errors in a set of predictions, without considering their direction.
            \item \textbf{Formula}:
            \begin{equation}
            \text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
            \end{equation}
            \item \textbf{Example}: If actual values are [3, -0.5, 2, 7] and predicted values are [2.5, 0.0, 2, 8], then MAE = 0.5.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Importance and Conclusion}
    \begin{block}{Importance of Tracking Metrics}
        \begin{itemize}
            \item \textbf{Responsiveness to Data Drift}: Regularly monitoring these metrics helps identify data drift, where the data distribution changes over time.
            \item \textbf{Model Reliability}: Ensures the model continues to meet business and user expectations.
            \item \textbf{Informed Decisions}: Enables practitioners to make data-driven decisions about model updates, retraining, or replacement.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Maintaining high performance in machine learning models is an ongoing process that requires diligent tracking of performance metrics. By consistently monitoring these metrics, organizations can enhance model reliability and adaptability to changing environments.
    \end{block}
    
    \begin{block}{Takeaways}
        Regularly track accuracy, precision, recall, F1 score, AUC-ROC, and MAE to ensure models perform well in production and adapt to new data trends.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Retraining}
    \begin{block}{Introduction}
        Model retraining is a crucial process in machine learning to maintain model performance as new data becomes available. This process involves updating the model's parameters to adapt to any changes in data patterns.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Retrain a Model}
    Model retraining should be considered under the following circumstances:
    \begin{enumerate}
        \item \textbf{Performance Degradation:} Declines in accuracy or other metrics may indicate decreased effectiveness.
              \begin{itemize}
                  \item \textit{Example:} A customer segmentation model misclassifying groups.
              \end{itemize}
        \item \textbf{New Data Availability:} Incorporating new data can enhance robustness.
              \begin{itemize}
                  \item \textit{Example:} E-commerce recommendations improved with latest sales trends.
              \end{itemize}
        \item \textbf{Concept Drift \& Data Drift:} Mismatch between training and real-world data due to changes in distribution.
              \begin{itemize}
                  \item \textit{Example:} Spam detection models adapting to new tactics.
              \end{itemize}
        \item \textbf{Seasonal Trends:} Regular intervals indicating when retraining is needed.
              \begin{itemize}
                  \item \textit{Example:} Predictive maintenance needing seasonal adjustments.
              \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How to Retrain a Model}
    \textbf{Step-by-Step Process:}
    \begin{enumerate}
        \item \textbf{Monitor Performance:} Continuously track performance metrics.
        \item \textbf{Collect New Data:} Gather relevant data to enhance the model.
        \item \textbf{Preprocess New Data:} Clean and process the new data as done for the training data.
        \item \textbf{Evaluate Retrained Model:} Use training, validation, and test datasets; measure with previous metrics.
        \item \textbf{Deploy the Updated Model:} Replace old model if performance improves.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    Here’s a simple Python snippet using scikit-learn to demonstrate model retraining:
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load previous model and new data
model = load_model('my_model.pkl')
new_data = load_new_data('new_data.csv')

# Preprocess new data
X_new, y_new = preprocess(new_data)

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_new, y_new, test_size=0.2)

# Retrain the model
model.fit(X_train, y_train)

# Validate performance
y_pred = model.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)

print(f'Model Retrained Accuracy: {accuracy}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Effective monitoring is essential for early identification of retraining needs.
            \item Maintain a feedback loop for data collection and preprocessing based on performance.
            \item Consider automating retraining based on established performance thresholds.
        \end{itemize}
    \end{block}
    Model retraining is essential for keeping machine learning models relevant and effective by adapting to new data and trends.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Model Drift - Understanding Model Drift}
    
    \begin{block}{Definition}
        Model drift refers to the deterioration of machine learning model performance over time due to changes in data distribution.
    \end{block}
    
    \begin{block}{Types of Model Drift}
        \begin{enumerate}
            \item Covariate Shift: Changes in input data distribution while relationships remain constant.
            \item Prior Probability Shift: Changes in output class distribution without affecting input features.
            \item Concept Drift: Changes in relationships between input data and output classes.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Model Drift - Importance and Detection}
    
    \begin{block}{Importance of Model Drift}
        \begin{itemize}
            \item Maintaining Model Accuracy
            \item Ensuring Business Continuity
        \end{itemize}
    \end{block}
    
    \begin{block}{Detecting Model Drift}
        Key strategies include:
        \begin{itemize}
            \item \textbf{Statistical Tests:} Kolmogorov-Smirnov Test, Chi-Squared Test
            \item \textbf{Monitoring Performance:} Track metrics like accuracy, precision, and recall
            \item \textbf{Visualization Techniques:} Use histograms and box plots to observe feature distribution changes
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Model Drift - Responding and Example Code}
    
    \begin{block}{Responding to Model Drift}
        Corrective actions can include:
        \begin{itemize}
            \item Model Retraining
            \item Incremental Learning
            \item Feature Engineering
        \end{itemize}
    \end{block}
    
    \begin{block}{Example of a Drift Detection Process}
        \begin{lstlisting}[language=Python]
# Python Code Snippet for Monitoring Drift
from sklearn.metrics import log_loss
import numpy as np

# Actual vs Predicted probabilities
y_true = np.array([...])  # Actual labels
y_pred_probs = np.array([...])  # Model output probabilities

# Calculate log loss
current_log_loss = log_loss(y_true, y_pred_probs)

# Set a threshold for acceptable log loss
threshold = 0.5
if current_log_loss > threshold:
    print("Model drift detected: Consider retraining the model.")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    \begin{block}{Overview}
        Ethical considerations in model deployment and maintenance are critical due to:
        \begin{itemize}
            \item The impact of machine learning models on individuals and society.
            \item Issues of fairness, accountability, transparency, and bias mitigation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Implications}
    \begin{enumerate}
        \item \textbf{Bias Identification}
            \begin{itemize}
                \item Inadvertent learning of biases from training data can lead to unfair treatment.
                \item \textit{Example:} A hiring algorithm may favor candidates from a specific demographic.
            \end{itemize}
        \item \textbf{Fairness in Outcomes}
            \begin{itemize}
                \item Assessing whether model outcomes are equitable across demographics.
                \item Definitions of fairness include:
                    \begin{itemize}
                        \item \textbf{Equality of Opportunity}
                        \item \textbf{Equalized Odds}
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Transparency and Accountability}
    \begin{enumerate}[resume]
        \item \textbf{Transparency}
            \begin{itemize}
                \item Models should be interpretable to foster understanding among stakeholders.
                \item \textit{Example:} Clear reasoning for loan approvals enhances trust.
            \end{itemize}
        \item \textbf{Accountability}
            \begin{itemize}
                \item Developers must take responsibility for model decisions.
                \item Monitoring systems to assess model impact over time are essential.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Data Privacy and Community Engagement}
    \begin{itemize}
        \item \textbf{Data Privacy and Consent}
            \begin{itemize}
                \item Respect user privacy and obtain consent for data usage.
                \item Implement strong data governance to safeguard personal information.
            \end{itemize}
        \item \textbf{Community Engagement}
            \begin{itemize}
                \item Involve affected communities in the design and evaluation processes.
                \item Align models with societal values to enhance acceptance and trust.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Conclusion}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Ethical considerations are not merely regulatory; they build trust and ensure equitable outcomes.
            \item Integrate ethics into the entire model lifecycle from development to maintenance.
            \item Focus on the well-being of individuals and the community, not just performance metrics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies in Model Deployment and Maintenance}
    \begin{block}{Introduction}
        In this slide, we will explore real-world case studies that showcase successful deployment and maintenance strategies of machine learning models. 
        Understanding these case studies will help students grasp the practical challenges and solutions in deploying machine learning in various sectors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Netflix Recommendation System}
    \begin{itemize}
        \item \textbf{Deployment Strategy:}
            \begin{itemize}
                \item Netflix uses machine learning algorithms to personalize content recommendations.
                \item The model leverages collaborative filtering and deep learning techniques.
            \end{itemize}
        \item \textbf{Maintenance Approaches:}
            \begin{itemize}
                \item A/B Testing: Continuous evaluation of metrics to ensure updates lead to improvements.
                \item Data Retraining: Regular updates with new data to adapt to changing user preferences.
            \end{itemize}
        \item \textbf{Key Takeaways:}
            \begin{itemize}
                \item Continuous monitoring and feedback loops improve model relevance.
                \item Importance of experimentation in enhancing performance.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Uber's ETA Predictions}
    \begin{itemize}
        \item \textbf{Deployment Strategy:}
            \begin{itemize}
                \item Uber employs real-time models predicting estimated time of arrival (ETA) for rides.
                \item The model incorporates data such as traffic, weather, and historical ride data.
            \end{itemize}
        \item \textbf{Maintenance Approaches:}
            \begin{itemize}
                \item Dynamic Model Scaling: Models scaled based on demand for optimal performance.
                \item Real-Time Feedback Loop: User feedback adjusts predictions, enhancing accuracy.
            \end{itemize}
        \item \textbf{Key Takeaways:}
            \begin{itemize}
                \item Necessity of scalability and real-time adaptability in high-stakes environments.
                \item Use of feedback mechanisms to refine predictions actively.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 3: JPMorgan Chase's Fraud Detection}
    \begin{itemize}
        \item \textbf{Deployment Strategy:}
            \begin{itemize}
                \item Models detect fraudulent transactions by analyzing spending behavior.
                \item Combines supervised learning for known fraud and unsupervised learning for emerging patterns.
            \end{itemize}
        \item \textbf{Maintenance Approaches:}
            \begin{itemize}
                \item Continuous Learning: Models updated with new transaction data to adapt to evolving tactics.
                \item Anomaly Detection: Regular checks for transaction pattern shifts to catch new scams.
            \end{itemize}
        \item \textbf{Key Takeaways:}
            \begin{itemize}
                \item Advantage of hybrid models (supervised \& unsupervised) in complex scenarios.
                \item Importance of proactive measures in maintaining robustness against threats.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Discussion Questions}
    \begin{block}{Conclusion}
        The reviewed case studies illustrate that successful model deployment extends beyond initial implementation—it necessitates an ongoing commitment to maintenance, feedback integration, and continuous learning. 
        Applying strategies from these cases ensures that machine learning solutions remain effective, relevant, and ethical over time.
    \end{block}
    
    \begin{enumerate}
        \item How can continuous learning be effectively implemented in different industries?
        \item What are the ethical considerations that should be kept in mind while deploying these models?
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Best Practices - Overview}
    \begin{block}{Summary of Best Practices}
        Deploying and maintaining machine learning (ML) models in a production environment is a critical phase in the data science lifecycle. A successful deployment requires careful planning and ongoing management to ensure the model remains effective and reliable. Here are key best practices to consider:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Best Practices - Best Practices Overview}
    \begin{enumerate}
        \item \textbf{Version Control}
        \begin{itemize}
            \item Manage model versions using systems like Git to track changes and facilitate collaboration.
            \item Example: Rollback to the last stable version if a new model underperforms.
        \end{itemize}
        
        \item \textbf{Monitoring and Logging}
        \begin{itemize}
            \item Implement monitoring for tracking performance metrics, user interactions, and system health.
            \item Log predictions and errors for troubleshooting and analysis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Best Practices - More Practices}
    \begin{enumerate}[resume]
        \item \textbf{Automated Retraining}
        \begin{itemize}
            \item Schedule regular retraining of models with new data to combat model drift.
            \item Formula: Trigger retraining if accuracy dips below a certain threshold.
        \end{itemize}

        \item \textbf{A/B Testing}
        \begin{itemize}
            \item Test different versions of models in real-time before full deployment.
            \item Example: Deploy a new recommendation algorithm to a subset of users to evaluate engagement changes.
        \end{itemize}

        \item \textbf{Documentation}
        \begin{itemize}
            \item Maintain documentation of deployment processes, assumptions, and architecture to assist team understanding over time.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Best Practices - Final Practices}
    \begin{enumerate}[resume]
        \item \textbf{Scalability Considerations}
        \begin{itemize}
            \item Ensure deployment architecture can scale to meet increased loads, utilizing cloud services like AWS and Azure.
        \end{itemize}
        
        \item \textbf{Security Measures}
        \begin{itemize}
            \item Protect sensitive data through encryption and secure APIs, mindful of data privacy laws such as GDPR.
        \end{itemize}
        
        \item \textbf{Continuous Feedback Loop}
        \begin{itemize}
            \item Gather user feedback to assess model performance and inform future iterations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Best Practices - Key Takeaways}
    \begin{itemize}
        \item Successful deployment and maintenance require strategic planning, continuous monitoring, and adaptability.
        \item Regular updates ensure the model meets evolving business needs and user expectations.
    \end{itemize}
    \begin{block}{Final Thoughts}
        By implementing these best practices, ML practitioners can create robust models that deliver consistent value while being adaptable to change.
    \end{block}
\end{frame}

\begin{frame}[fragile,plain]{Conclusion and Best Practices - Example Code}
    \begin{lstlisting}[language=Python]
import logging

# Set up logging
logging.basicConfig(filename='model_performance.log', level=logging.INFO)

def log_prediction(y_true, y_pred):
    accuracy = sum(y_true == y_pred) / len(y_true)
    logging.info(f'Accuracy: {accuracy:.2f}')

# Simulated usage
y_true = [1, 0, 1, 1]
y_pred = [1, 0, 0, 1]
log_prediction(y_true, y_pred)
    \end{lstlisting}
    \begin{block}{Purpose}
        This code snippet tracks model performance, providing invaluable data for maintenance and future improvements.
    \end{block}
\end{frame}


\end{document}