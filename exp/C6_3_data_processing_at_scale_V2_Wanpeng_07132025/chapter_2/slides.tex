\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Apache Spark Introduction]{Week 2: Introduction to Apache Spark}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark}
    \begin{block}{Overview}
        Apache Spark is an open-source distributed computing system designed for fast and flexible big data processing. It offers an interface for programming entire clusters with implicit data parallelism and fault tolerance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Big Data Processing}
    \begin{itemize}
        \item \textbf{Speed:} High processing speed due to in-memory data storage, making it faster than traditional frameworks like Hadoop.
        \item \textbf{Versatility:} Supports multiple programming languages (Python, Java, Scala, R), allowing developers to use the language they prefer.
        \item \textbf{Support for Multiple Data Sources:} Capable of handling real-time data streams, batch processing, and interactive querying.
        \item \textbf{Machine Learning \& Graph Processing:} Includes libraries such as MLlib for machine learning and GraphX for graph processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relevance to Industry Practices}
    \begin{itemize}
        \item \textbf{Data-Driven Decision Making:} Facilitates quick analysis of large volumes of data for informed business strategies.
        \item \textbf{Cloud Integration:} Compatible with cloud platforms like AWS, Google Cloud, and Azure for scalable big data analytics.
        \item \textbf{Real-Time Analytics:} Utilized in finance, healthcare, and e-commerce for improved customer experiences and operational efficiency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{enumerate}
        \item \textbf{In-Memory Computation:} Speeds up tasks compared to Hadoop's disk-based storage.
        \item \textbf{Unified Engine:} Integrates batch processing, stream processing, and iterative processing in one framework.
        \item \textbf{Community and Ecosystem:} A robust ecosystem supported by an active community that fosters continuous improvement.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case}
    \begin{block}{Retail Example}
        A retail company uses Apache Spark to analyze customer buying patterns in real-time. By processing data from transactions and social media, Spark helps identify trends, allowing the company to tailor marketing strategies and optimize inventory rapidly.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet (in PySpark)}
    \begin{lstlisting}[language=Python]
from pyspark import SparkContext

# Initialize SparkContext
sc = SparkContext("local", "WordCount")

# Read data
text_file = sc.textFile("hdfs://path_to_file.txt")

# Count words
word_counts = text_file.flatMap(lambda line: line.split(" ")) \
                        .map(lambda word: (word, 1)) \
                        .reduceByKey(lambda a, b: a + b)

# Collect results
results = word_counts.collect()
for word, count in results:
    print(f"{word}: {count}")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture Diagram}
    \begin{block}{Diagram}
        Consider including a diagram that illustrates the architecture of Apache Spark. Highlight its components:
        \begin{itemize}
            \item Spark Core
            \item Spark SQL
            \item Spark Streaming
            \item MLlib
            \item GraphX
        \end{itemize}
        Show how these components interact within a big data ecosystem.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Apache Spark?}
    \begin{block}{Definition}
        Apache Spark is an open-source distributed computing system designed for high-speed large-scale data processing.
        It enables processing of big data in-memory, significantly enhancing performance compared to traditional disk-based processing engines.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{History of Apache Spark}
    \begin{itemize}
        \item \textbf{Launched:} Developed at the University of California, Berkeley's AMPLab in 2009.
        \item \textbf{Apache Project:} Became an Apache Software Foundation project in 2014, leading to widespread adoption for its performance and ease of use.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Apache Spark}
    \begin{itemize}
        \item \textbf{Speed:} In-memory computing capabilities allow processing to be up to 100 times faster than Hadoop MapReduce for some workloads.
        \item \textbf{Ease of Use:} High-level APIs in Java, Scala, Python, and R make it accessible for developers and data scientists.
        \item \textbf{Unified Engine:} Supports batch processing, real-time streaming, machine learning, and graph processing.
        \item \textbf{Flexible:} Can run on various cluster managers such as Hadoop YARN and Apache Mesos.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Apache Spark Differs from Hadoop}
    \begin{center}
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Feature} & \textbf{Apache Spark} & \textbf{Hadoop MapReduce} \\ \hline
            \textbf{Processing Model} & In-memory processing & Disk-based processing \\ \hline
            \textbf{Speed} & Up to 100 times faster & Slower due to disk I/O \\ \hline
            \textbf{Ease of Use} & Interactive queries (Spark SQL) & Longer development time \\ \hline
            \textbf{Data Processing} & Batch, Streaming, Interactive, ML & Primarily Batch processing \\ \hline
            \textbf{Built-in Libraries} & Extensive libraries (Spark SQL, MLlib) & Limited to MapReduce libraries \\ \hline
        \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Use Cases}
    \begin{itemize}
        \item \textbf{Data Analytics:} Big data analytics in industries like finance and retail.
        \item \textbf{Machine Learning:} Building scalable machine learning models using MLlib.
        \item \textbf{Real-Time Processing:} Analyzing streaming data from IoT devices or social media feeds.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Note}
    Apache Spark has revolutionized how we handle big data challenges. It offers improved speed, flexibility, and an easier learning curve for developers and data professionals. As we advance in this course, we will delve deeper into its architecture and practical applications.
\end{frame}

\begin{frame}[fragile]{Week 2: Introduction to Apache Spark}
    % Overview of Spark Architecture
    Apache Spark is a powerful open-source distributed computing system designed for speed and ease of use. Its architecture is built to efficiently handle big data processing, providing a robust platform for handling complex data workloads.
\end{frame}

\begin{frame}[fragile]{Spark Architecture - Key Components}
    \begin{itemize}
        \item \textbf{Driver Program}
        \begin{itemize}
            \item Manages execution of applications.
            \item Converts user code into jobs and schedules tasks.
        \end{itemize}

        \item \textbf{Cluster Manager}
        \begin{itemize}
            \item Handles resource allocation across the Spark cluster.
            \item Types: Standalone, Apache Mesos, Hadoop YARN.
        \end{itemize}

        \item \textbf{Executors}
        \begin{itemize}
            \item Worker nodes that execute data processing tasks.
            \item Store intermediate data in memory.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Spark Architecture - Workflow Summary}
    \begin{enumerate}
        \item \textbf{Job Submission:} User submits a job to the Driver.
        \item \textbf{Job Scheduling:} Driver communicates with Cluster Manager for resource allocation.
        \item \textbf{Task Execution:} Executors process data utilizing in-memory computing.
        \item \textbf{Result Collection:} Executors send results back to the Driver.
    \end{enumerate}

    % Insert Diagram - Placeholder
    \begin{block}{Diagram of Spark Architecture}
        % Insert your diagram illustrating the interaction here
        (Insert diagram here)
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Points and Real-World Application}
    \begin{itemize}
        \item The Driver orchestrates the application and task coordination.
        \item The Cluster Manager controls resource allocation and scheduling.
        \item Executors perform data processing and execute tasks.
    \end{itemize}
    
    \textbf{Real-World Application Example:} \\
    An online streaming service utilizes Spark’s architecture to quickly analyze user behavior and provide real-time personalized show recommendations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Abstractions in Spark - Overview}
    \begin{block}{Introduction to Core Abstractions}
        Apache Spark is built around several core abstractions essential for distributed data processing. 
        The three primary abstractions are:
        \begin{itemize}
            \item Resilient Distributed Datasets (RDDs)
            \item DataFrames
            \item Datasets
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Abstractions in Spark - RDDs}
    \begin{block}{Resilient Distributed Datasets (RDDs)}
        \begin{itemize}
            \item \textbf{Definition:} 
                RDDs are an immutable distributed collection of objects that can be processed in parallel.
            \item \textbf{Key Features:}
                \begin{itemize}
                    \item \textbf{Fault Tolerance:} Automatically recover lost data using lineage.
                    \item \textbf{Lazy Evaluation:} Transformations are queued until an action is called.
                    \item \textbf{Optimized for Speed:} Efficient in-memory computation.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        \begin{lstlisting}[language=Python]
# Example of creating an RDD and performing a transformation
activities_rdd = spark.parallelize([("user1", "active"), ("user2", "inactive"), ("user3", "active")])
active_users = activities_rdd.filter(lambda x: x[1] == "active").collect()
print(active_users)  # Output: [('user1', 'active'), ('user3', 'active')]
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Abstractions in Spark - DataFrames and Datasets}
    \begin{block}{DataFrames}
        \begin{itemize}
            \item \textbf{Definition:} 
                A distributed collection of data organized into named columns, similar to a table.
            \item \textbf{Key Features:}
                \begin{itemize}
                    \item \textbf{Schema Information:} Defined schema makes it easier to work with structured data.
                    \item \textbf{Optimized Execution:} Uses Catalyst optimizer for better performance.
                    \item \textbf{Interoperability with SQL:} Execute SQL queries on DataFrames.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        \begin{lstlisting}[language=Python]
# Example of creating a DataFrame from a CSV file
employees_df = spark.read.csv("employees.csv", header=True, inferSchema=True)
active_employees = employees_df.filter(employees_df.status == "active")
active_employees.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Abstractions in Spark - Datasets}
    \begin{block}{Datasets}
        \begin{itemize}
            \item \textbf{Definition:}
                A combination of RDDs and DataFrames, providing strong type safety.
            \item \textbf{Key Features:}
                \begin{itemize}
                    \item \textbf{Type Safety:} Catch errors at compile-time.
                    \item \textbf{Combines Functional and SQL APIs:} Supports both functional programming and SQL queries.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        \begin{lstlisting}[language=Scala]
// Example in Scala - converting a DataFrame to a Dataset
case class Employee(name: String, status: String)
val employeesDS = employees_df.as[Employee]
val activeEmployeesDS = employeesDS.filter(emp => emp.status == "active")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resilient Distributed Datasets (RDDs) - Introduction}
    \begin{block}{What are RDDs?}
        Resilient Distributed Datasets (RDDs) are the fundamental data structure in Apache Spark. 
        They represent an immutable collection of objects distributed across a computing cluster. 
        RDDs facilitate parallel processing and provide fault tolerance, making them essential for big data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resilient Distributed Datasets (RDDs) - Key Features}
    \begin{enumerate}
        \item \textbf{Immutable}: Once created, RDDs cannot be changed. Transformations create a new RDD, ensuring data integrity.
        
        \item \textbf{Distributed}: RDDs are partitioned across different nodes in a cluster for parallel computation.

        \item \textbf{Fault Tolerance}: RDDs track their lineage, allowing for recomputation if a partition is lost.

        \item \textbf{In-Memory Processing}: RDDs can be cached for faster access, improving performance for iterative algorithms.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How RDDs Enable Fault Tolerance}
    \begin{block}{Lineage Graph}
        Each RDD keeps track of the sequence of operations (transformations) that created it. If a partition is lost, Spark can reconstruct it using its lineage information.
    \end{block}
    \begin{exampleblock}{Example}
        If RDD1 is transformed into RDD2 by applying a \texttt{map} function, and RDD2 loses a partition, Spark traces back to RDD1 and reapplies the \texttt{map} function.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Checkpointing and Example Code}
    \begin{block}{Checkpointing}
        - For long lineage graphs, RDDs can be checkpointed, saving a snapshot to reliable storage. 
        - This helps reduce lineage length and prevent recomputation.
    \end{block}

    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
from pyspark import SparkContext

# Initialize Spark Context
sc = SparkContext("local", "RDD Example")

# Create RDD from a collection
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Apply a transformation
squared_rdd = rdd.map(lambda x: x ** 2)

# Collect the results
results = squared_rdd.collect()
print(results)  # Output: [1, 4, 9, 16, 25]
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item RDDs are the backbone of Spark, providing flexibility, scalability, and resilience.
        \item They are ideal for high-throughput data processing applications such as batch processing, real-time analytics, and machine learning.
        \item Understanding RDDs lays the groundwork for learning higher-level abstractions like DataFrames and Datasets.
    \end{itemize}
    \begin{block}{Conclusion}
        Mastering RDDs enables crucial insights into distributed data processing and prepares students for advanced topics in Spark.
    \end{block}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Overview}
    In this section, we will explore \textbf{DataFrames} and \textbf{Datasets} in Apache Spark, focusing on their structure, advantages over \textbf{Resilient Distributed Datasets (RDDs)}, and their usefulness in data manipulation tasks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are DataFrames?}
    \begin{itemize}
        \item \textbf{Definition}: A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database or a data frame in Python's Pandas library.
        \item \textbf{Schema}: Each DataFrame has a schema that defines the column names and types, which enables Spark to optimize execution plans.
    \end{itemize}
    \begin{block}{Example}
        \begin{lstlisting}[language=python]
# Creating a DataFrame in Spark using Python (PySpark)
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Example").getOrCreate()
data = [("Alice", 1), ("Bob", 2)]
df = spark.createDataFrame(data, ["Name", "Id"])
df.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Datasets?}
    \begin{itemize}
        \item \textbf{Definition}: A Dataset is a distributed collection of data providing the benefits of both RDDs and DataFrames. Datasets are strongly-typed, allowing for compile-time type safety.
        \item \textbf{Type Safety}: While DataFrames use untyped columns, Datasets leverage static types to catch errors at compile-time instead of runtime.
    \end{itemize}
    \begin{block}{Example}
        \begin{lstlisting}[language=scala]
// Creating a Dataset in Spark using Scala
import spark.implicits._

case class Person(name: String, age: Int)
val ds = Seq(Person("Alice", 1), Person("Bob", 2)).toDS()
ds.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Comparison with RDDs}
    \begin{center}
        \begin{tabular}{|l|l|l|l|}
            \hline
            \textbf{Feature} & RDDs & DataFrames & Datasets \\
            \hline
            \textbf{Type Safety} & Not Type Safe & Not Type Safe & Type Safe \\
            \hline
            \textbf{Schema} & No & Yes (schema defined) & Yes (type-defined schema) \\
            \hline
            \textbf{Optimization} & Limited optimization & Catalyst Optimizer applied & Catalyst and Tungsten optimizations \\
            \hline
            \textbf{Ease of Use} & Complex to use for structured data & User-friendly with SQL-like queries & Combined benefits of RDDs and DataFrames \\
            \hline
            \textbf{Performance} & Slower due to less optimization & Faster due to query optimization & Faster than RDDs with safety features \\
            \hline
        \end{tabular}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Advantages of DataFrames and Datasets}
    \begin{itemize}
        \item \textbf{Performance}: Both benefit from Spark's Catalyst optimizer which optimizes query plans.
        \item \textbf{Ease of Use}: High-level abstractions reduce the need for complex boilerplate code.
        \item \textbf{Interoperability}: Easily read and write various data formats (Parquet, Avro, JSON, etc.)
        \item \textbf{Expression API}: Support for SQL queries and DataFrame API allows easier complex data manipulations.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item DataFrames and Datasets provide a more efficient and user-friendly way to handle structured data compared to RDDs.
        \item Combination of optimization techniques enhances performance and productivity significantly.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    By leveraging DataFrames and Datasets, you can streamline your data processing tasks through improved performance and easier syntax, making data manipulation much more efficient in Apache Spark.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Operations in Spark - Overview}
    Apache Spark provides two fundamental types of operations:
    \begin{itemize}
        \item \textbf{Transformations} - create a new dataset from an existing one.
        \item \textbf{Actions} - trigger execution of transformations and return results.
    \end{itemize}
    Understanding these operations is crucial for effective data manipulation and processing in Spark.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Operations in Spark - Transformations}
    \textbf{Transformations} are operations that produce a new dataset from an existing one. They are \textit{lazy} and \textit{immutable}.

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Lazy Evaluation}: Not executed until an action is called.
            \item \textbf{Immutable}: Each transformation generates a new dataset.
        \end{itemize}
    \end{block}

    \textbf{Common Transformations:}
    \begin{itemize}
        \item \textbf{map(func)}: Applies a function to each element.
        \item \textbf{filter(func)}: Returns a new dataset with elements meeting criteria.
    \end{itemize}

    \textbf{Examples:}
    \begin{lstlisting}[language=Python]
    rdd = spark.sparkContext.parallelize([1, 2, 3, 4])
    squared_rdd = rdd.map(lambda x: x ** 2)
    even_rdd = rdd.filter(lambda x: x % 2 == 0)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Operations in Spark - Actions}
    \textbf{Actions} are operations that execute the transformations and return results, finalizing and running the logical plan created by transformations.

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Execution}: Forces the evaluation and returns results.
            \item \textbf{Triggering Computation}: Completes and runs the logical plan.
        \end{itemize}
    \end{block}

    \textbf{Common Actions:}
    \begin{itemize}
        \item \textbf{count()}: Returns the number of elements in a dataset.
        \item \textbf{collect()}: Retrieves all elements as an array to the driver program.
        \item \textbf{saveAsTextFile(path)}: Saves the dataset to a text file at the specified path.
    \end{itemize}

    \textbf{Examples:}
    \begin{lstlisting}[language=Python]
    num_employees = rdd.count()
    squared_values = squared_rdd.collect()
    even_rdd.saveAsTextFile("even_numbers.txt")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation and Action Operations - Overview}
    \begin{itemize}
        \item Apache Spark processes data through two main operations: 
        \textbf{Transformations} and \textbf{Actions}.
        \item \textbf{Transformations:} Create a new dataset from an existing one 
        without immediate computation.
        \item \textbf{Actions:} Trigger execution and return a value to the driver program.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformations}
    Transformations are lazy operations that create a new dataset.
    
    \begin{block}{Key Transformations}
        \begin{itemize}
            \item \textbf{Map:}
            \begin{lstlisting}[language=Python]
celsius = [0, 10, 20, 30, 40]
fahrenheit = sc.parallelize(celsius).map(lambda x: (x * 9/5) + 32).collect()
# Output: [32.0, 50.0, 68.0, 86.0, 104.0]
            \end{lstlisting}

            \item \textbf{Filter:}
            \begin{lstlisting}[language=Python]
numbers = [1, 2, 3, 4, 5, 6]
even_numbers = sc.parallelize(numbers).filter(lambda x: x % 2 == 0).collect()
# Output: [2, 4, 6]
            \end{lstlisting}

            \item \textbf{FlatMap:}
            \begin{lstlisting}[language=Python]
sentences = ["Hello World", "Apache Spark"]
words = sc.parallelize(sentences).flatMap(lambda x: x.split(" ")).collect()
# Output: ['Hello', 'World', 'Apache', 'Spark']
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions}
    Actions trigger execution of transformations and return results.

    \begin{block}{Key Actions}
        \begin{itemize}
            \item \textbf{Count:}
            \begin{lstlisting}[language=Python]
rdd = sc.parallelize([1, 2, 3, 4])
count = rdd.count()
# Output: 4
            \end{lstlisting}

            \item \textbf{Collect:}
            \begin{lstlisting}[language=Python]
rdd = sc.parallelize([1, 2, 3, 4])
data = rdd.collect()
# Output: [1, 2, 3, 4]
            \end{lstlisting}

            \item \textbf{First:}
            \begin{lstlisting}[language=Python]
rdd = sc.parallelize([1, 2, 3, 4])
first_element = rdd.first()
# Output: 1
            \end{lstlisting}
        \end{itemize}
    \end{block}
    
    \textbf{Summary:} Transformations are lazy, creating new datasets, while actions trigger computations and return results.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Working with Spark SQL - Overview}
    \begin{itemize}
        \item Spark SQL integrates relational data processing with Spark's programming model.
        \item Allows execution of SQL queries on DataFrames.
        \item Benefits from Spark's in-memory computing capabilities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Spark SQL}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{DataFrames:}
                \begin{itemize}
                    \item Immutable distributed collection of data with named columns.
                    \item Supports various data sources (e.g., Hive, Parquet, JSON).
                \end{itemize}
            \item \textbf{SQL Queries:}
                \begin{itemize}
                    \item Run SQL directly or create temporary views.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: SQL Queries with DataFrames}
    \begin{block}{Sample Code}
        \begin{lstlisting}[language=Python]
# Creating a DataFrame
df = spark.read.csv("data.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("data_table")

# Running SQL Query
sql_result = spark.sql("SELECT column1, COUNT(*) AS count FROM data_table GROUP BY column1")
        \end{lstlisting}
    \end{block}

    \begin{block}{Use Case}
        \begin{itemize}
            \item Calculate total sales per customer from a transactions dataset:
            \begin{lstlisting}[language=Python]
result = spark.sql("""
SELECT customer_id, SUM(sales_amount) AS total_sales
FROM transactions
GROUP BY customer_id
ORDER BY total_sales DESC
""")
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Performance:} Utilizes Catalyst query optimizer to speed up execution.
        \item \textbf{Interoperability:} Combines SQL with DataFrame operations for flexibility.
        \item \textbf{Support for Data Formats:} Reads and writes in JSON, Avro, Parquet, and ORC.
    \end{itemize}
    
    \begin{block}{Summary}
        Spark SQL is essential for analyzing structured data with SQL commands alongside Spark's powerful data processing abilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Cases of Apache Spark - Introduction}
    \begin{block}{Introduction}
        Apache Spark is a versatile, open-source distributed computing system designed for fast data processing and analytics. 
        Its ability to manage large datasets makes it a favored option in various industries. 
        Below, we explore real-world applications that leverage Spark’s powerful capabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Case 1: Data Processing and Analytics}
    \begin{itemize}
        \item \textbf{Industry:} Retail
        \item \textbf{Example:} 
            A retail company uses Spark to process massive sales data in real-time. 
            By integrating Spark with Apache Kafka, they can analyze live stream data to generate immediate insights into customer behavior, identify trends, and optimize inventory management.
        \item \textbf{Key Point:} Real-time processing allows businesses to make quick, data-driven decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Case 2: Machine Learning}
    \begin{itemize}
        \item \textbf{Industry:} Finance
        \item \textbf{Example:} 
            A bank employs Spark's MLlib to detect fraudulent transactions. 
            Using historical transaction data, Spark helps build and train machine learning models that can identify anomalies in real-time, reducing fraud losses significantly.
        \item \textbf{Key Point:} Supports scalability for complex machine learning algorithms across large datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Case 3: Batch Processing and Data Integration}
    \begin{itemize}
        \item \textbf{Industry:} Healthcare
        \item \textbf{Example:} 
            A healthcare provider uses Spark to process patient records for predictive analytics. 
            By running batch jobs that analyze millions of records, they can identify patterns that help in early disease detection and treatment optimization.
        \item \textbf{Key Point:} Efficient batch processing capabilities for heavy-duty analytics workloads.
    \end{itemize}
    
    \begin{itemize}
        \item \textbf{Industry:} Telecommunications
        \item \textbf{Example:} 
            A telecom company utilizes Spark for ETL (Extract, Transform, Load) processes to consolidate data from various departments (billing, customer service, etc.). 
        \item \textbf{Key Point:} Streamlines data from heterogeneous sources, making it easier to extract actionable insights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Case 4: Graph Processing}
    \begin{itemize}
        \item \textbf{Industry:} Social Media
        \item \textbf{Example:} 
            A social media platform leverages Spark's GraphX library to analyze user connections and recommend friends, content, and advertisements based on user behavior.
        \item \textbf{Key Point:} Powerful graph processing capabilities that allow for deeper relationship insights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Code Snippet}
    \begin{block}{Conclusion}
        Apache Spark plays a pivotal role across various sectors by enabling efficient data processing and analytics. 
        From real-time insights in retail to predictive analytics in healthcare, Spark’s applications are versatile, demonstrating its significant value in handling big data challenges.
    \end{block}
    
    \begin{block}{Code Snippet: Sample Spark SQL Query}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("Retail Sales Analysis").getOrCreate()

# Load data into DataFrame
sales_data = spark.read.csv("sales_data.csv", header=True, inferSchema=True)

# Example Spark SQL query to analyze total sales by product
sales_data.createOrReplaceTempView("sales")
total_sales_by_product = spark.sql("SELECT product, SUM(sales_amount) as total_sales FROM sales GROUP BY product")

# Show results
total_sales_by_product.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Considerations - Introduction}
    \begin{block}{Introduction to Performance Optimization in Spark}
        Apache Spark is a powerful tool for processing large datasets, but achieving peak performance requires an understanding of best practices and common pitfalls. Optimizing Spark applications can lead to significant gains in execution speed and resource efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Considerations - Best Practices}
    \begin{block}{Best Practices for Optimizing Performance}
        \begin{enumerate}
            \item \textbf{Data Partitioning}
            \begin{itemize}
                \item Partitioning controls data division across the cluster.
                \item Use \texttt{repartition()} or \texttt{coalesce()} strategically.
            \end{itemize}
            
            \item \textbf{Caching and Persistence}
            \begin{itemize}
                \item Use caching to store frequently accessed RDDs or DataFrames.
                \item Utilize \texttt{persist()} with appropriate storage levels.
            \end{itemize}
            
            \item \textbf{Optimizing Spark Configuration}
            \begin{itemize}
                \item Adjust Spark's settings to match your workload.
                \item Key settings include \texttt{spark.executor.memory} and \texttt{spark.executor.cores}.
            \end{itemize}
            
            \item \textbf{Use the Right Data Format}
            \begin{itemize}
                \item Choose formats like Parquet or ORC for I/O efficiency.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Considerations - Code Examples}
    \begin{block}{Code Examples}
        \textbf{Data Partitioning Example:}
        \begin{lstlisting}[language=Python]
        df = df.repartition("customer_id")
        \end{lstlisting}

        \textbf{Caching Example:}
        \begin{lstlisting}[language=Python]
        df.cache()
        \end{lstlisting}

        \textbf{Optimizing Spark Configuration Example:}
        \begin{lstlisting}[language=Python]
        spark = SparkSession.builder \
            .appName("OptimizedApp") \
            .config("spark.executor.memory", "4g") \
            .getOrCreate()
        \end{lstlisting}

        \textbf{Data Format Example:}
        \begin{lstlisting}[language=Python]
        df.write.parquet("output_data.parquet")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Considerations - Common Pitfalls}
    \begin{block}{Common Pitfalls to Avoid}
        \begin{enumerate}
            \item \textbf{Excessive Shuffling}
            \begin{itemize}
                \item Shuffling can slow down processing. Minimize operations like \texttt{groupBy} or joins.
            \end{itemize}
            \item \textbf{Not Using Built-in Functions}
            \begin{itemize}
                \item Use Spark's optimized built-in functions instead of custom transformations.
            \end{itemize}
            \item \textbf{Overlooking Serialization}
            \begin{itemize}
                \item Use Kryo serialization for faster and more compact data transfer.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Further Learning - Part I}
    \textbf{I. Recap of Key Points:}
    \begin{enumerate}
        \item \textbf{Introduction to Apache Spark:}
        \begin{itemize}
            \item Apache Spark is an open-source, distributed computing system designed for big data processing, known for its speed and ease of use.
            \item It runs tasks in-memory, significantly improving performance compared to traditional disk-based processing frameworks.
        \end{itemize}
        
        \item \textbf{Key Components of Spark:}
        \begin{itemize}
            \item \textbf{Spark Core:} The foundational engine that manages distributed tasks.
            \item \textbf{Spark SQL:} Enables querying of structured data via SQL.
            \item \textbf{Spark Streaming:} Processes live data streams for real-time analytics.
            \item \textbf{MLlib:} A library for scalable machine learning.
            \item \textbf{GraphX:} For graph-parallel computations.
        \end{itemize}
        
        \item \textbf{Advantages of Apache Spark:}
        \begin{itemize}
            \item \textbf{Speed:} Processes data in-memory leading to faster execution.
            \item \textbf{Flexibility:} Supports multiple programming languages (Scala, Python, R, and Java) and integrates with various data sources.
            \item \textbf{Unified Engine:} Capable of running various workloads, from batch processing to streaming analytics.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Further Learning - Part II}
    \textbf{II. Practical Example:}
    \begin{itemize}
        \item \textbf{Data Processing Pipeline:} Imagine working with a large dataset of customer transactions. You can use Spark to:
        \begin{itemize}
            \item Read data from a distributed storage system like HDFS.
            \item Perform transformations such as filtering out fraudulent transactions.
            \item Aggregate remaining transactions to summarize total sales per product category.
        \end{itemize}
    \end{itemize}

    \textbf{Example Code Snippet:}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('SalesAnalysis').getOrCreate()
df = spark.read.csv('transactions.csv', header=True, inferSchema=True)
sales_summary = df.groupBy("product_category").sum("sales").show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Further Learning - Part III}
    \textbf{III. Further Learning Resources:}
    \begin{itemize}
        \item \textbf{Books:}
        \begin{itemize}
            \item "Learning Spark: Lightning-Fast Data Analytics" by Holden Karau et al.
            \item "Spark: The Definitive Guide" by Bill Chambers and Matei Zaharia.
        \end{itemize}
        
        \item \textbf{Online Courses:}
        \begin{itemize}
            \item Coursera: "Big Data Analysis with Spark."
            \item edX: "Introduction to Apache Spark" offered by various institutions.
        \end{itemize}
        
        \item \textbf{Documentation and Community:}
        \begin{itemize}
            \item \texttt{Apache Spark Official Documentation:} Extensive guides and API references: \url{https://spark.apache.org/docs/latest/}
            \item Join forums like the Spark User Mailing List or platforms like Stack Overflow for community interactions and troubleshooting.
        \end{itemize}
    \end{itemize}

    \textbf{IV. Key Points to Emphasize:}
    \begin{itemize}
        \item Apache Spark enables real-time insights and scalable machine learning.
        \item Understanding its components and best practices optimizes performance and avoids pitfalls.
        \item Continuous learning is vital for deepening understanding and practical skills.
    \end{itemize}

    \textbf{V. Conclusion:}
    \begin{itemize}
        \item Mastering Apache Spark opens opportunities in big data analytics, enabling professionals to harness data insights on a larger scale and speed.
    \end{itemize}
\end{frame}


\end{document}