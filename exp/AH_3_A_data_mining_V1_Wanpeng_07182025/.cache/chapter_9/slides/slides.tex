\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation Techniques - Overview}
    \begin{block}{Overview of Model Evaluation}
        Model evaluation techniques are essential in data mining, ensuring the effectiveness and reliability of predictive models. They help ascertain whether a model is performing well or needs adjustments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation Techniques - Significance}
    \begin{block}{Significance of Model Evaluation}
        \begin{itemize}
            \item \textbf{Quality Assurance}: Validates models against known outcomes.
            \item \textbf{Improving Model Performance}: Helps identify limitations and enhance predictive accuracy through methods like cross-validation.
            \item \textbf{Selection of the Best Model}: Aids in ranking multiple models based on performance metrics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation Techniques - Objectives}
    \begin{block}{Objectives of Model Evaluation}
        \begin{itemize}
            \item \textbf{Generalization Assessment}: Evaluates model performance on unseen data.
            \item \textbf{Performance Metrics Usage}: Uses metrics like accuracy, precision, recall, and F1-score to quantify performance.
            \item \textbf{Algorithm Comparison}: Compares different algorithms to find the best fit for specific data types.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation Techniques - Key Techniques}
    \begin{block}{Key Evaluation Techniques}
        \begin{enumerate}
            \item \textbf{Train-Test Split}: Split data into training and testing sets.
                \begin{itemize}
                    \item \textbf{Example}: 1000 instances - Train: 800, Test: 200.
                \end{itemize}

            \item \textbf{Cross-Validation}: Divides data into subsets for training and testing.
                \begin{equation}
                \text{Mean Cross-Validation Score} = \frac{1}{k} \sum_{i=1}^{k} \text{Score}_i
                \end{equation}

            \item \textbf{Performance Metrics}:
                \begin{itemize}
                    \item \textbf{Accuracy}: 
                    \[
                    \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Observations}}
                    \]
                    \item \textbf{Precision}:
                    \[
                    \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
                    \]
                    \item \textbf{Recall (Sensitivity)}:
                    \[
                    \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
                    \]
                    \item \textbf{F1-Score}:
                    \[
                    F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
                    \]
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation Techniques - Conclusion}
    \begin{block}{Conclusion}
        Model evaluation techniques are crucial for the successful application of data mining. They assist in model selection, quality improvement, and ensure robust performance across datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Model Evaluation - Overview}
    \begin{block}{Understanding Model Evaluation}
        Model evaluation is a critical process in data mining, allowing us to determine how well our predictive models perform when applied to unseen data. Proper evaluation helps us confirm that a model performs well on the training data and is also effective in real-world situations.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Model Evaluation - Part 1}
    \begin{block}{Why Model Evaluation Matters}
        \begin{enumerate}
            \item \textbf{Performance Validation}
            \begin{itemize}
                \item Ensures predictions are reliable and accurate.
                \item Example: A model predicting customer churn should deliver high accuracy in identifying actual churners and non-churners.
            \end{itemize}

            \item \textbf{Avoiding Overfitting}
            \begin{itemize}
                \item Prevents models from capturing noise instead of the underlying pattern.
                \item Example: A complex model may perform well on training data but poorly on validation data, indicating memorization rather than generalization.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Model Evaluation - Part 2}
    \begin{block}{Why Model Evaluation Matters (Continued)}
        \begin{enumerate}
            \setcounter{enumi}{2}
            \item \textbf{Enhancing Decision-Making}
            \begin{itemize}
                \item Informs business decisions based on model outcomes.
                \item Example: A predictive model in healthcare should undergo rigorous evaluation to provide accurate intervention recommendations.
            \end{itemize}

            \item \textbf{Model Comparison}
            \begin{itemize}
                \item Enables comparison of different models and algorithms.
                \item Example: Comparing logistic regression against decision trees or neural networks for fraud detection using evaluation metrics like precision and recall.
            \end{itemize}

            \item \textbf{Identifying Improvement Areas}
            \begin{itemize}
                \item Highlights strengths and weaknesses for iterative improvements.
                \item Example: A low recall rate suggests many actual positive cases are missed, prompting adjustments in model tuning or feature selection.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Evaluation Metrics}
    \begin{block}{Key Evaluation Metrics to Consider}
        \begin{itemize}
            \item \textbf{Accuracy}: Ratio of correctly predicted instances to total instances.
            \item \textbf{Precision}: Ratio of correctly predicted positives to total predicted positives.
            \item \textbf{Recall (Sensitivity)}: Ratio of correctly predicted positives to all actual positives.
            \item \textbf{F1 Score}: Harmonic mean of precision and recall, balancing both metrics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Summary of Model Evaluation}
    \begin{block}{Summary}
       Model evaluation is an indispensable step in the data mining process. It validates performance, prevents overfitting, enhances decision-making, enables model comparison, and identifies areas for improvement, ensuring robust models that effectively meet business needs. In the next slide, we will delve deeper into common evaluation metrics, which will serve as tools for assessing model performance systematically.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Evaluation Metrics: Introduction}
    \begin{block}{Overview}
        In model evaluation, selecting the right metric is essential for quantifying the effectiveness of predictive models, particularly in classification tasks. This slide covers various common evaluation metrics, their definitions, contexts, and examples.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Evaluation Metrics: Accuracy}
    \begin{itemize}
        \item \textbf{Definition:} Ratio of correctly predicted instances to total instances.
        \item \textbf{Formula:}
        \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \end{equation}
        \item \textbf{Where:}
        \begin{itemize}
            \item TP = True Positives
            \item TN = True Negatives
            \item FP = False Positives
            \item FN = False Negatives
        \end{itemize}
        \item \textbf{Context:} Best when classes are balanced.
        \item \textbf{Example:} In a dataset with 90 positives and 10 negatives, an accuracy of 95\% may mislead if all are predicted as positive.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Evaluation Metrics: Precision, Recall \& F1-Score}
    \begin{block}{Precision}
        \begin{itemize}
            \item \textbf{Definition:} Proportion of true positive predictions among all positive predictions.
            \item \textbf{Formula:}
            \begin{equation}
                \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \end{equation}
            \item \textbf{Context:} Useful when the cost of false positives is high, such as in spam detection.
            \item \textbf{Example:} With 80 true positives and 20 false positives, precision = 80\%.
        \end{itemize}
    \end{block}

    \begin{block}{Recall}
        \begin{itemize}
            \item \textbf{Definition:} Ability of a model to find all relevant cases (true positives).
            \item \textbf{Formula:}
            \begin{equation}
                \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \end{equation}
            \item \textbf{Context:} Crucial when false negatives carry a high cost, e.g., in medical diagnoses.
            \item \textbf{Example:} 70 out of 100 actual positives predicted correctly gives recall = 70\%.
        \end{itemize}
    \end{block}

    \begin{block}{F1-Score}
        \begin{itemize}
            \item \textbf{Definition:} Combines precision and recall into a single metric.
            \item \textbf{Formula:}
            \begin{equation}
                \text{F1-Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Context:} Useful for imbalanced class distribution.
            \item \textbf{Example:} Precision = 0.8 and Recall = 0.6 lead to F1-Score ≈ 0.69.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Context Matters:} Different metrics provide insights suited to varied scenarios (e.g., fraud detection may prioritize recall).
        \item \textbf{Trade-offs:} The balance between precision and recall is crucial since increasing one may decrease the other.
    \end{itemize}

    \begin{block}{Conclusion}
        Understanding these metrics allows practitioners to better evaluate models and inform decisions based on specific goals and data characteristics. Choosing the appropriate metric can significantly impact perceived model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix}
    \begin{block}{What is a Confusion Matrix?}
        A confusion matrix is a performance measurement tool for machine learning classification algorithms. It is a table used to evaluate a model's performance by comparing predicted classifications against actual classifications.
    \end{block}
    \begin{itemize}
        \item **True Positives (TP)**: Correctly predicted positive cases.
        \item **True Negatives (TN)**: Correctly predicted negative cases.
        \item **False Positives (FP)**: Incorrectly predicted positive cases (Type I error).
        \item **False Negatives (FN)**: Incorrectly predicted negative cases (Type II error).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of a Confusion Matrix}
    \begin{block}{Binary Classification Confusion Matrix}
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline
                & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
                \hline
                \textbf{Actual Positive} & TP & FN \\
                \hline
                \textbf{Actual Negative} & FP & TN \\
                \hline
            \end{tabular}
        \end{center}
    \end{block}
    \begin{block}{Visual Representation}
        Higher values in the TP and TN cells indicate better model performance. Higher FP and FN values indicate potential weaknesses.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Confusion Matrix}
    \begin{block}{Email Spam Prediction}
        Consider the following confusion matrix from a spam prediction model:
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline
                & \textbf{Predicted Spam} & \textbf{Predicted Not Spam} \\
                \hline
                \textbf{Actual Spam} & 50 (TP) & 10 (FN) \\
                \hline
                \textbf{Actual Not Spam} & 5 (FP) & 35 (TN) \\
                \hline
            \end{tabular}
        \end{center}
    \end{block}
    \begin{block}{Key Metrics}
        \begin{itemize}
            \item Accuracy: \( \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \)
            \item Precision: \( \text{Precision} = \frac{TP}{TP + FP} \)
            \item Recall: \( \text{Recall} = \frac{TP}{TP + FN} \)
            \item F1 Score: \( F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall} \)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{ROC and AUC}
  The \textbf{Receiver Operating Characteristic (ROC) curve} is a tool for evaluating binary classification model performance, illustrating the trade-off between sensitivity (True Positive Rate) and specificity (1 - False Positive Rate).
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is the ROC Curve?}
  The ROC curve plots:
  \begin{itemize}
    \item **True Positive Rate (Sensitivity)** on the Y-axis
    \item **False Positive Rate (1 - Specificity)** on the X-axis
  \end{itemize}
  
  Key metrics:
  \begin{equation}
  \text{Sensitivity (Recall)} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
  \end{equation}
  
  \begin{equation}
  \text{False Positive Rate} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}
  \end{equation}
  
  Understanding thresholds shows how changing them affects the rates of true positives and false positives.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Understanding the Area Under the Curve (AUC)}
  The \textbf{Area Under the Curve (AUC)} quantifies the overall performance of a binary classifier:
  \begin{itemize}
    \item **AUC = 1**: Perfect classification.
    \item **AUC = 0.5**: No discrimination.
    \item **AUC < 0.5**: Worse than random guessing.
  \end{itemize}
  
  Key points:
  \begin{itemize}
    \item AUC values provide insights into classifier effectiveness regardless of threshold.
    \item Useful for imbalanced datasets.
    \item Visual comparisons of models can be made using ROC curves.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Medical Testing}
  Consider a medical test for disease detection:
  \begin{itemize}
    \item Setting a low threshold results in high sensitivity (many true positives) but increases false positives.
    \item Adjusting the threshold moves along the ROC curve, impacting overall performance.
  \end{itemize}

  \textbf{Important Formulas:}
  \begin{equation}
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
  \end{equation}
  
  \begin{equation}
  \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
  \end{equation}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Visualizing ROC and AUC}
  To visualize the ROC curve, consider the following Python code using libraries `matplotlib` and `sklearn`:
  \begin{block}{Python Code}
    \begin{lstlisting}
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assume y_true are the actual labels and y_scores are the predicted probabilities
fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label='ROC Curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Model Comparison Techniques}
  Model comparison techniques are essential for evaluating the performance of different predictive models and determining the best one that captures underlying patterns in the data.
\end{frame}

\begin{frame}[fragile]
  \frametitle{1. Common Model Comparison Metrics}
  \begin{itemize}
    \item \textbf{Accuracy}: Proportion of correctly predicted instances.
    \begin{equation}
        \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}}
    \end{equation}
    
    \item \textbf{Precision}: Accuracy of positive predictions.
    \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
    
    \item \textbf{Recall (Sensitivity)}: Ability to identify relevant instances.
    \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
    
    \item \textbf{F1 Score}: Harmonic mean of precision and recall.
    \begin{equation}
        F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
    
    \item \textbf{Area Under the ROC Curve (AUC-ROC)}: Summarizes model performance across thresholds.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{2. Statistical Tests for Model Comparison}
  \begin{itemize}
    \item \textbf{Paired t-test}: Compares performance of two models on the same dataset.
    \item \textbf{Wilcoxon Signed-Rank Test}: A non-parametric test for varied data distributions.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3. Validation Techniques}
  \begin{itemize}
    \item \textbf{Cross-Validation}: Splits dataset into subsets for reliable model performance estimation.
    
    \begin{itemize}
      \item \textit{K-Fold Cross-Validation}: Divides data into K subsets (folds), training on K-1 folds and validating on the remaining one.
      \begin{equation}
          \text{Cross-Validation Error} = \frac{1}{K}\sum_{i=1}^{K} \text{Error}(i)
      \end{equation}
    \end{itemize}
    
    \item \textbf{Hold-Out Method}: Randomly splits dataset into training and testing sets.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{4. Example: Comparing Two Models}
  Consider two models, Model A and Model B, predicting binary outcomes. After K-Fold Cross-Validation:
  \begin{itemize}
    \item Model A: Average accuracy of 85% (SD: 3%).
    \item Model B: Average accuracy of 82% (SD: 2%).
  \end{itemize}
  
  \textbf{Interpretation}: If tests (e.g., paired t-test) show significant difference (p < 0.05), Model A performs better statistically.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Takeaways}
  \begin{itemize}
    \item Use various metrics for model evaluation.
    \item Employ statistical tests for performance differences.
    \item Use validation techniques like cross-validation for robustness.
    \item Continuous model comparison refines predictive capabilities.
  \end{itemize}
  
  Mastering these techniques allows for confident model selection and better outcomes in predictive analytics.
\end{frame}

\begin{frame}
    \frametitle{Cross-Validation}
    \begin{block}{What is Cross-Validation?}
        Cross-validation is a statistical technique used to assess the predictive performance of a model. It involves partitioning the dataset into complementary subsets, training on one subset, and testing on the other.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Importance of Cross-Validation}
    \begin{itemize}
        \item \textbf{Model Robustness}: Provides a reliable measure of performance by reducing variability from a single train-test split.
        \item \textbf{Avoiding Overfitting}: Helps in detecting when a model is too complex and fitting noise in the training data.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Common Cross-Validation Methods}
    \begin{block}{K-Fold Cross-Validation}
        \begin{itemize}
            \item The dataset is divided into 'k' equally sized folds.
            \item The model is trained on 'k-1' folds and tested on the remaining fold, repeating this process 'k' times.
            \item \textbf{Example}: For \(k=5\), the dataset is split into 5 parts (4 for training and 1 for testing).
            \item \textbf{Formula}:
            \begin{equation}
                \text{Mean Accuracy} = \frac{1}{k} \sum_{i=1}^{k} \text{Accuracy on Fold } i
            \end{equation}
        \end{itemize}
    \end{block}

    \begin{block}{Leave-One-Out Cross-Validation (LOOCV)}
        \begin{itemize}
            \item A special case where \(k\) equals the number of data points.
            \item Trains the model on all but one data point, which is used for testing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    Here’s how to implement K-Fold cross-validation using Python’s Scikit-Learn:
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import numpy as np

# Sample dataset
X = [[...]]  # Feature set
y = [...]    # Labels

kf = KFold(n_splits=5)
model = RandomForestClassifier()

scores = []
for train_index, test_index in kf.split(X):
    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]
    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    scores.append(accuracy_score(y_test, predictions))

# Calculate mean accuracy
mean_accuracy = np.mean(scores)
print(f'Mean Accuracy: {mean_accuracy}')
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{When to Use Cross-Validation}
    \begin{itemize}
        \item When the dataset is small and maximizing data usage is crucial.
        \item To compare the predictive performance of different models or configurations.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Cross-validation is a vital technique in machine learning that helps build more robust models that generalize well to unseen data. Proper implementation ensures effective model evaluation and selection, essential in any data-driven decision-making process.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting vs Underfitting - Definitions}
    \begin{block}{Overfitting}
        \begin{itemize}
            \item \textbf{Definition}: Learning both the underlying patterns and noise in the training data. 
            \item \textbf{Signs}:
                \begin{itemize}
                    \item High training accuracy
                    \item Low validation/test accuracy
                    \item Complex model relative to data size
                    \item Visuals: Divergence of training and validation curves
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Underfitting}
        \begin{itemize}
            \item \textbf{Definition}: Model is too simple to capture underlying trends.
            \item \textbf{Signs}:
                \begin{itemize}
                    \item Low training accuracy
                    \item Low validation/test accuracy
                    \item Inappropriate models or insufficient training
                    \item Visuals: Low training and validation curve
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting vs Underfitting - Effects and Causes}
    \begin{block}{Impact of Overfitting}
        \begin{itemize}
            \item Performs well on training data but poorly on new data.
            \item Inaccurate predictions and resource wastage.
            \item Cross-validation helps identify overfitting.
        \end{itemize}
    \end{block}
    
    \begin{block}{Impact of Underfitting}
        \begin{itemize}
            \item Fails to learn adequately leading to poor predictions.
            \item Results in overall lack of performance.
            \item Arises from overly simplistic models or insufficient training time.
        \end{itemize}
    \end{block}
    
    \begin{block}{Causes}
        \begin{itemize}
            \item \textbf{Overfitting Causes}:
                \begin{itemize}
                    \item Complex models with excessive parameters
                    \item Lack of regularization
                \end{itemize}
            \item \textbf{Underfitting Causes}:
                \begin{itemize}
                    \item Inappropriate models for the data trends
                    \item Insufficient feature representation
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting vs Underfitting - Examples and Summary}
    \begin{block}{Illustrative Example}
        \begin{enumerate}
            \item \textbf{Overfitting}: 
                Fitting a 10th-degree polynomial to a linear pattern captures noise instead of trend.
            \item \textbf{Underfitting}: 
                A linear model applied to quadratic data yields inaccurate representations.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Summary Points}
        \begin{itemize}
            \item \textbf{Balance}: Essential to avoid both overfitting and underfitting.
            \item \textbf{Strategies}: Use techniques such as cross-validation and regularization.
            \item \textbf{Evaluation}: Always validate model performance to gauge generalization capability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Residual Analysis}
    \begin{block}{Introduction to Residual Analysis}
        Residuals are the differences between observed and predicted values. For \( n \) observations:
        \[
        e_i = y_i - \hat{y}_i
        \]
        where \( y_i \) is the actual value, and \( \hat{y}_i \) is the predicted value.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Residual Analysis}
    \begin{itemize}
        \item \textbf{Understanding Model Errors:}
        \begin{itemize}
            \item Detect systematic patterns indicating model mis-specification.
        \end{itemize}
        
        \item \textbf{Improving Predictions:}
        \begin{itemize}
            \item Identify observations with larger residuals to enhance model accuracy.
        \end{itemize}
        
        \item \textbf{Assessment of Model Fit:}
        \begin{itemize}
            \item Random residuals suggest a good model fit.
            \item Non-random patterns may indicate:
            \begin{itemize}
                \item **Non-linearity**
                \item **Outliers**
                \item **Heteroscedasticity**
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing and Testing Residuals}
    \begin{itemize}
        \item \textbf{Randomness of Residuals:}
        \begin{itemize}
            \item Use scatter plots or histograms to visualize distributions.
        \end{itemize}
        
        \item \textbf{Residual Plots:}
        \begin{itemize}
            \item **Versus Fitted Values:** Detect non-linearity and unequal variance.
            \item **Histogram/QQ Plot:** Assess residual normality.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Statistical Tests}
        \begin{itemize}
            \item **Durbin-Watson Test:** Tests independence of residuals.
            \item **Breusch-Pagan Test:** Assesses heteroscedasticity.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Residual analysis is crucial for model evaluation, helping refine models and enhance predictive performance. Recognizing patterns in residuals facilitates a deeper understanding of the modeling process.
   
    \begin{block}{Remember!}
        Always conduct residual analysis as part of your evaluation strategy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Understanding Hyperparameters}
    \begin{itemize}
        \item \textbf{Definition}: Hyperparameters are configurations that control the learning process of a machine learning algorithm, set before training.
        \item \textbf{Examples}:
        \begin{itemize}
            \item Learning rate in gradient descent
            \item Number of hidden layers and neurons in a neural network
            \item Maximum depth of a decision tree
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - The Tuning Process}
    \begin{enumerate}
        \item \textbf{Identification of Hyperparameters}: Determine which hyperparameters impact model performance.
        \item \textbf{Selection of Tuning Method}:
        \begin{itemize}
            \item \textbf{Grid Search}: An exhaustive search over specified hyperparameter values.
                \begin{itemize}
                    \item \textbf{Pros}: Comprehensive results.
                    \item \textbf{Cons}: Computationally expensive.
                \end{itemize}
            \item \textbf{Random Search}: Randomly samples from parameter space.
                \begin{itemize}
                    \item \textbf{Pros}: Often effective with less cost.
                    \item \textbf{Cons}: May overlook optimal settings due to randomness.
                \end{itemize}
            \item \textbf{Bayesian Optimization}: Iteratively selects hyperparameters using probability based on past evaluations.
                \begin{itemize}
                    \item \textbf{Pros}: Generally more efficient.
                    \item \textbf{Cons}: More complex to implement.
                \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Cross-Validation and Evaluation}
    \begin{itemize}
        \item \textbf{Cross-Validation}: Split data into parts to assess performance for each hyperparameter configuration.
            \begin{itemize}
                \item \textbf{K-Fold Cross-Validation}: Divides dataset into K subsets, utilizing each fold as a validation set.
            \end{itemize}
        \item \textbf{Evaluation Metric}: Choose metrics (e.g., accuracy, precision) to assess model performance.
        \item \textbf{Selection of Optimal Hyperparameters}: Analyze tuning results to determine the best-performing configuration.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Impact on Model Performance}
    \begin{itemize}
        \item \textbf{Increased Accuracy}: Proper tuning significantly enhances accuracy, allowing better pattern capture.
        \item \textbf{Reduced Overfitting}: Optimizing hyperparameters prevents fitting noise, improving generalization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Example Scenario}
    \begin{itemize}
        \item \textbf{Algorithm}: Random Forest
        \item \textbf{Hyperparameters to Tune}:
        \begin{itemize}
            \item Number of trees ($n\_estimators$)
            \item Maximum depth ($max\_depth$)
            \item Minimum samples per leaf ($min\_samples\_leaf$)
        \end{itemize}
        \item \textbf{Possible Code Snippet for Grid Search}:
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize model
rf = RandomForestClassifier()

# Initialize GridSearch
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=5)

# Fit the model
grid_search.fit(X_train, y_train)

# Best parameters
print(grid_search.best_params_)
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Key Points}
    \begin{itemize}
        \item Hyperparameter tuning is essential for improving model accuracy and robustness.
        \item Different tuning methods have distinct advantages and disadvantages.
        \item Proper evaluation techniques like cross-validation lead to more reliable performance estimates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Model Evaluation Techniques}
    \begin{block}{Introduction}
        Model evaluation techniques are crucial for assessing the performance and reliability of predictive models across various industries. 
        These evaluations ensure that models not only fit the training data but also generalize well to new, unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Healthcare: Predicting Patient Outcomes}
    \begin{itemize}
        \item \textbf{Application:} Models predicting patient readmission rates.
        \item \textbf{Evaluation Techniques:}
        \begin{itemize}
            \item \textbf{ROC Curve \& AUC:} Measures sensitivity vs specificity.
            \item \textbf{Cross-Validation:} Validates performance on multiple data subsets.
        \end{itemize}
        \item \textbf{Example:} A confusion matrix evaluates false positives and negatives in readmission predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Finance: Credit Scoring}
    \begin{itemize}
        \item \textbf{Application:} Models assessing the likelihood of default on loans.
        \item \textbf{Evaluation Techniques:}
        \begin{itemize}
            \item \textbf{Precision-Recall Curve:} Important for imbalanced datasets.
            \item \textbf{F1 Score:} Balances precision and recall for evaluation.
        \end{itemize}
        \item \textbf{Example:} Bank uses F1 score to prioritize candidates in credit scoring.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Marketing: Customer Segmentation}
    \begin{itemize}
        \item \textbf{Application:} Models to identify potential high-value customers.
        \item \textbf{Evaluation Techniques:}
        \begin{itemize}
            \item \textbf{Silhouette Score:} Measures clustering effectiveness.
            \item \textbf{Adjusted Rand Index:} Compares similarity between clusterings.
        \end{itemize}
        \item \textbf{Example:} Marketing team uses silhouette score for effective customer grouping.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. E-commerce: Recommendation Systems}
    \begin{itemize}
        \item \textbf{Application:} Models suggesting products based on user activity.
        \item \textbf{Evaluation Techniques:}
        \begin{itemize}
            \item \textbf{MAE \& RMSE:} Evaluate predicted vs actual user ratings.
            \item \textbf{A/B Testing:} Compares recommendation algorithms in real time.
        \end{itemize}
        \item \textbf{Example:} E-commerce platform uses RMSE to improve recommendation accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Autonomous Vehicles: Object Detection}
    \begin{itemize}
        \item \textbf{Application:} Models identifying pedestrians and obstacles.
        \item \textbf{Evaluation Techniques:}
        \begin{itemize}
            \item \textbf{IoU:} Measures overlap between predicted bounding boxes and actual objects.
            \item \textbf{True Positive Rate:} Ensures detection accuracy while minimizing false alarms.
        \end{itemize}
        \item \textbf{Example:} Company uses IoU to enhance object detection algorithms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Real-world applications highlight the importance of effective model evaluation.
        \item Tailoring techniques to industry needs enhances model reliability and user trust.
        \item Understanding metrics is critical for optimizing model performance.
    \end{itemize}
    \begin{block}{Conclusion}
        Investing in evaluation techniques ensures better performance and safer applications across various fields.
        Proper application facilitates advanced data-driven decision-making processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Introduction}
    \begin{itemize}
        \item Evaluation of models extends beyond accuracy and efficiency.
        \item Ethical aspects include:
        \begin{itemize}
            \item Bias
            \item Fairness
            \item Transparency
        \end{itemize}
        \item Scrutinizing impacts on individuals and communities is crucial to prevent inequalities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Key Concepts}
    \begin{block}{1. Bias}
        \begin{itemize}
            \item Definition: Systematic errors in model predictions due to training data or model design.
            \item Example: A hiring algorithm favoring candidates from a specific demographic.
        \end{itemize}
    \end{block}

    \begin{block}{Types of Bias}
        \begin{itemize}
            \item Sample Bias: Training data not representing the broader population.
            \item Prejudice Bias: Societal biases reflected in the data.
            \item Measurement Bias: Errors in data collection or labeling.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Fairness and Transparency}
    \begin{block}{2. Fairness}
        \begin{itemize}
            \item Definition: Equitable treatment and outcomes across different groups.
            \item Example: A credit model ensuring fair loan access for low-income communities.
        \end{itemize}
        
        \begin{itemize}
            \item Approaches:
            \begin{itemize}
                \item Group Fairness: Equal performance across defined groups.
                \item Individual Fairness: Similar individuals treated similarly.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{3. Transparency}
        \begin{itemize}
            \item Definition: Understanding model processes and decisions.
            \item Example: Healthcare algorithms explaining treatment recommendations.
            \item Practices: Model explainability using tools (e.g. LIME, SHAP) and comprehensive documentation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Evaluation - Key Takeaways}
    \begin{itemize}
        \item Ethical evaluation is essential for bias mitigation, fairness, and transparency.
        \item Regular audits and updates of models are necessary to align with changing societal norms.
        \item Engaging diverse stakeholder backgrounds helps identify potential ethical issues early.
    \end{itemize}

    \begin{block}{Conclusion}
        \begin{itemize}
            \item Prioritizing ethical implications in model evaluation contributes to equitable technology.
            \item Reflect on the societal impact of automated systems and obtain feedback from affected communities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Evaluation Techniques - Overview}
    \begin{itemize}
        \item Understanding emerging trends in model evaluation is crucial for robust, fair, and effective models.
        \item This presentation will cover three key trends:
        \begin{enumerate}
            \item Automated Evaluation Techniques
            \item Enhanced Interpretability
            \item Continuous Learning and Evaluation
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Evaluation Techniques - Automated Evaluation}
    \begin{block}{1. Automated Evaluation Techniques}
        \begin{itemize}
            \item \textbf{Description:} Automation aims to reduce human bias and improve efficiency in model evaluations.
            \item \textbf{Examples:}
            \begin{itemize}
                \item \textit{AutoML Tools}: E.g., Google Cloud AutoML selects evaluation metrics based on problem type.
                \item \textit{Hyperparameter Optimization}: E.g., Optuna assesses and optimizes model performance automatically.
            \end{itemize}
            \item \textbf{Key Point:} Automation accelerates the model development cycle and ensures consistent evaluation methodologies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Evaluation Techniques - Enhanced Interpretability}
    \begin{block}{2. Enhanced Interpretability}
        \begin{itemize}
            \item \textbf{Description:} Increased complexity in models raises the demand for interpretability.
            \item \textbf{Examples:}
            \begin{itemize}
                \item \textit{SHAP (SHapley Additive exPlanations)}: Insights into feature contributions to predictions.
                \item \textit{LIME (Local Interpretable Model-agnostic Explanations)}: Generates local, faithful explanations by using simpler models.
            \end{itemize}
            \item \textbf{Key Point:} Enhanced interpretability builds trust in AI systems and promotes ethical models.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Evaluation Techniques - Continuous Learning}
    \begin{block}{3. Continuous Learning and Evaluation}
        \begin{itemize}
            \item \textbf{Description:} Continuous learning involves adapting models to new data with ongoing evaluation.
            \item \textbf{Examples:}
            \begin{itemize}
                \item \textit{Drift Detection}: Methods like Kolmogorov-Smirnov tests identify performance drops due to data drift.
                \item \textit{Continuous Monitoring}: Metrics in production can trigger model retraining to ensure effectiveness.
            \end{itemize}
            \item \textbf{Key Point:} Continuous evaluation practices allow proactive adjustments to maintain model relevance and accuracy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item By adapting to these emerging trends—automation, enhanced interpretability, and continuous learning—data mining practitioners can improve evaluation frameworks.
        \item These advancements are vital for the ethical and effective application of machine learning across domains.
    \end{itemize}
    \begin{block}{Note}
        Always integrate advanced techniques with ethical considerations, such as fairness, to ensure responsible AI development.
    \end{block}
\end{frame}


\end{document}