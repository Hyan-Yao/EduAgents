\frametitle{Common Evaluation Metrics: Precision, Recall \& F1-Score}
    \begin{block}{Precision}
        \begin{itemize}
            \item \textbf{Definition:} Proportion of true positive predictions among all positive predictions.
            \item \textbf{Formula:}
            \begin{equation}
                \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \end{equation}
            \item \textbf{Context:} Useful when the cost of false positives is high, such as in spam detection.
            \item \textbf{Example:} With 80 true positives and 20 false positives, precision = 80\%.
        \end{itemize}
    \end{block}

    \begin{block}{Recall}
        \begin{itemize}
            \item \textbf{Definition:} Ability of a model to find all relevant cases (true positives).
            \item \textbf{Formula:}
            \begin{equation}
                \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \end{equation}
            \item \textbf{Context:} Crucial when false negatives carry a high cost, e.g., in medical diagnoses.
            \item \textbf{Example:} 70 out of 100 actual positives predicted correctly gives recall = 70\%.
        \end{itemize}
    \end{block}

    \begin{block}{F1-Score}
        \begin{itemize}
            \item \textbf{Definition:} Combines precision and recall into a single metric.
            \item \textbf{Formula:}
            \begin{equation}
                \text{F1-Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Context:} Useful for imbalanced class distribution.
            \item \textbf{Example:} Precision = 0.8 and Recall = 0.6 lead to F1-Score â‰ˆ 0.69.
        \end{itemize}
    \end{block}
