\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 4: Introduction to Apache Spark]{Week 4: Introduction to Apache Spark}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark}
    \begin{block}{Overview of Apache Spark}
        \begin{itemize}
            \item Apache Spark is an open-source, distributed computing system for fast processing of large-scale data.
            \item Significantly more efficient than traditional technologies like Hadoop MapReduce.
            \item Capable of both batch and real-time data processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Apache Spark}
    \begin{enumerate}
        \item \textbf{Speed:}
        \begin{itemize}
            \item Processes data in-memory for increased speed.
            \item Example: Operations that took hours with MapReduce can often complete in minutes.
        \end{itemize}

        \item \textbf{Ease of Use:}
        \begin{itemize}
            \item High-level APIs in Java, Scala, Python, and R for intuitive application development.
            \item Example: A simple word count can be completed in just a few lines of code.
        \end{itemize}

        \item \textbf{Unified Engine:}
        \begin{itemize}
            \item Unified framework for SQL queries, streaming, machine learning, and graph processing.
            \item Example: Use Spark SQL for structured queries and Spark Streaming for real-time data.
        \end{itemize}

        \item \textbf{Rich Ecosystem:}
        \begin{itemize}
            \item Strong libraries for machine learning (MLlib), graph processing (GraphX), and Spark SQL.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance and Example of Apache Spark}
    \begin{block}{Importance in Data Processing at Scale}
        \begin{itemize}
            \item \textbf{Big Data Capability:} Scales from terabytes to petabytes across clusters.
            \item \textbf{Versatility Across Domains:} Used in finance, healthcare, retail for applications like fraud detection and recommendations.
        \end{itemize}
    \end{block}

    \begin{block}{Illustrative Example}
        \textbf{A Simple Spark Application}
        \begin{lstlisting}[language=Python]
from pyspark import SparkContext

# Initialize SparkContext
sc = SparkContext("local", "WordCount")

# Load the text file
text_file = sc.textFile("hdfs://path/to/textfile.txt")

# Process the data to count words
word_counts = text_file.flatMap(lambda line: line.split(" ")) \
                       .map(lambda word: (word, 1)) \
                       .reduceByKey(lambda a, b: a + b)

# Collect and display the results
for word, count in word_counts.collect():
    print(f"{word}: {count}")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Apache Spark is crucial for large-scale data processing with speed, ease-of-use, and flexibility.
        \item Its ability to handle big data and integration with diverse applications defines its value in modern data analytics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Spark? - Definition}
    \begin{block}{Definition of Apache Spark}
        Apache Spark is an open-source, distributed computing system designed for processing large volumes of data efficiently and quickly. It provides a unified analytics engine capable of handling:
        \begin{itemize}
            \item Batch processing
            \item Streaming data
            \item Machine learning
            \item Graph processing
        \end{itemize}
        All in a single platform, originally developed at UC Berkeley's AMPLab.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Spark? - Architecture}
    \begin{block}{Architecture of Apache Spark}
        The architecture comprises several key components:
        \begin{enumerate}
            \item \textbf{Driver}: Initiates the application and coordinates task distribution.
            \item \textbf{Cluster Manager}: Can be standalone, Mesos, or YARN; manages resources in the cluster.
            \item \textbf{Workers}: Nodes where tasks are executed, running multiple executors.
            \item \textbf{Executor}: Processes tasks assigned by the driver; stores data in-memory.
            \item \textbf{Job}: Each action leads to a job, consisting of multiple stages executed in parallel.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Spark? - Components}
    \begin{block}{Components of Apache Spark}
        Key components include:
        \begin{itemize}
            \item \textbf{Resilient Distributed Datasets (RDDs)}: Immutable, parallel processing, fault tolerance.
            \item \textbf{DataFrames}: Higher-level abstraction, similar to database tables.
            \item \textbf{Spark SQL}: Integrates SQL queries with data manipulation tasks.
            \item \textbf{MLlib}: Scalable machine learning library.
            \item \textbf{Spark Streaming}: Processes live data streams in real-time.
            \item \textbf{GraphX}: API for graphs and graph-parallel computation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Spark? - Key Points and Example}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Performance}: In-memory processing speeds up job execution.
            \item \textbf{Unified Processing}: Combines batch, streaming, and real-time processing.
            \item \textbf{Scalability}: Handles big data across clusters of machines.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Imagine a retail company analyzing customer purchase behavior. Using Spark, they can:
        \begin{itemize}
            \item Collect data from various sources
            \item Process data in real-time to identify trends
            \item Build machine learning models to predict future purchases
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Spark? - Code Example}
    \begin{block}{Example Spark Code}
        \begin{lstlisting}[language=Python]
# Example Spark code to create an RDD and perform a transformation
from pyspark import SparkContext

sc = SparkContext("local", "Simple App")
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
squared_rdd = rdd.map(lambda x: x ** 2)

print(squared_rdd.collect())  # Output: [1, 4, 9, 16, 25]
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resilient Distributed Datasets (RDDs) - Introduction}
    Resilient Distributed Datasets (RDDs) are the core abstraction in Apache Spark for distributed data. 
    \begin{itemize}
        \item They facilitate efficient work with large datasets across a cluster.
        \item RDDs offer fault tolerance, support parallel processing, and promote immutable data structures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of RDDs}
    \begin{enumerate}
        \item \textbf{Resilient}:
            \begin{itemize}
                \item Automatically recover from node failures.
                \item Lost partitions can be recomputed through transformations.
            \end{itemize}
        
        \item \textbf{Distributed}:
            \begin{itemize}
                \item Data is partitioned across multiple nodes.
                \item Parallel processing of partitions enhances performance.
            \end{itemize}
        
        \item \textbf{Immutable}:
            \begin{itemize}
                \item Once created, RDDs cannot be modified.
                \item New RDDs are formed through transformations, maintaining data integrity.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Operations on RDDs}
    \textbf{Transformations:}
    \begin{itemize}
        \item Create new RDDs from existing ones (lazy evaluation).
        \item Example code:
        \begin{lstlisting}[language=Python]
        from pyspark import SparkContext

        sc = SparkContext("local", "Example")
        rdd = sc.parallelize([1, 2, 3, 4])
        rdd2 = rdd.map(lambda x: x * 2)  # Transformation
        \end{lstlisting}
    \end{itemize}

    \textbf{Actions:}
    \begin{itemize}
        \item Trigger execution and return results.
        \item Example code:
        \begin{lstlisting}[language=Python]
        result = rdd2.collect()  # Action
        print(result)  # Outputs: [2, 4, 6, 8]
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Resilient Distributed Datasets (RDDs)}
    \begin{itemize}
        \item Fault Tolerance
        \item Immutability
        \item Distributed Nature
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fault Tolerance}
    \begin{block}{Definition}
        RDDs are designed to automatically recover from failures. If a partition of an RDD is lost, Spark can reconstruct it using the lineage information.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} In a large dataset spread across nodes, if one node fails, Spark only re-executes the transformations for the lost partition.
        \item \textbf{Illustration:} Visualize this as a tree where each node represents a transformation; if a leaf node goes missing, only the path back to the root needs recalculation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features Continued: Immutability and Distributed Nature}
    \begin{enumerate}
        \item \textbf{Immutability}
            \begin{itemize}
                \item Once created, RDDs cannot be changed; new operations produce new RDDs.
                \item \textbf{Example:} Using \texttt{RDD1.filter(x => x > 10)} produces a new RDD \texttt{RDD2}, leaving \texttt{RDD1} unchanged.
                \item This property allows safe concurrent processing and simplifies debugging.
            \end{itemize}
        
        \item \textbf{Distributed Nature}
            \begin{itemize}
                \item RDDs are distributed across multiple machines, enabling parallel processing and scalability.
                \item \textbf{Example:} For a dataset of 1 million records, Spark can split it into 100 partitions, processing them simultaneously.
                \item \textbf{Code Snippet:}
                \begin{lstlisting}[language=Python]
                from pyspark import SparkContext
                sc = SparkContext("local", "Example")
                data = range(1, 1000001)
                rdd = sc.parallelize(data, numSlices=100)  # 100 partitions
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs - Introduction}
    \begin{itemize}
        \item RDDs (Resilient Distributed Datasets) are a fundamental abstraction in Apache Spark.
        \item They enable parallel processing of large datasets.
        \item Characteristics:
            \begin{itemize}
                \item Immutable and distributed collections of objects.
                \item Resilient to failures.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs - Methods Overview}
    \begin{block}{Methods for Creating RDDs}
        RDDs can be created from various existing data sources. Here are some common methods:
    \end{block}
    \begin{enumerate}
        \item From Existing Data Files
        \item From Hadoop Input Formats
        \item From Collections in Driver Program
        \item From External Data Sources
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs - From Existing Data Files}
    \begin{itemize}
        \item **Text Files**: RDDs can be created from external text files stored in HDFS, S3, etc.
        \item \textbf{Example Code}:
        \begin{lstlisting}[language=Python]
from pyspark import SparkContext
sc = SparkContext("local", "Create RDD Example")
rdd = sc.textFile("hdfs://path/to/yourfile.txt")
        \end{lstlisting}
        \item \textbf{Key Point}: Use \texttt{sc.textFile()} for loading text files.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs - From Hadoop Input Formats}
    \begin{itemize}
        \item Leverage Hadoop's input formats (e.g., Sequence Files, Avro Files) for RDDs.
        \item \textbf{Example Code}:
        \begin{lstlisting}[language=Python]
rdd = sc.newAPIHadoopFile("hdfs://path/to/yourfile", 
                          "org.apache.hadoop.mapreduce.lib.input.TextInputFormat")
        \end{lstlisting}
        \item \textbf{Key Point}: This method is beneficial in a Hadoop ecosystem.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs - From Collections in Driver Program}
    \begin{itemize}
        \item Create RDDs from existing Python collections (e.g., lists, sets).
        \item \textbf{Example Code}:
        \begin{lstlisting}[language=Python]
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
        \end{lstlisting}
        \item \textbf{Key Point}: Useful for small datasets or prototyping.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs - From External Data Sources}
    \begin{itemize}
        \item RDDs can be created from SQL databases or NoSQL systems.
        \item \textbf{Using JDBC}:
        \begin{lstlisting}[language=Python]
jdbcDF = spark.read.format("jdbc").option("url", 
    "jdbc:mysql://hostname:port/dbname").option("dbtable", "tablename").load()
rdd = jdbcDF.rdd
        \end{lstlisting}
        \item \textbf{Key Point}: Facilitates integration with enterprise data systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating RDDs - Summary of Key Points}
    \begin{itemize}
        \item RDDs are versatile for big data processing, created from various data sources.
        \item Choosing the method depends on data source type and size:
            \begin{itemize}
                \item Files
                \item Hadoop input formats
                \item Collections
                \item Databases
            \end{itemize}
        \item Understanding these methods sets the foundation for RDD transformations and actions in subsequent lessons.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformations and Actions in Apache Spark}
    \begin{block}{Overview of RDD Operations}
        In Apache Spark, Resilient Distributed Datasets (RDDs) are the foundational data structure that allows for parallel processing of data. RDD operations are categorized into two main types:
        \begin{itemize}
            \item \textbf{Transformations}
            \item \textbf{Actions}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformations}
    Transformations are operations that create a new RDD from an existing one. They are \textbf{lazy}, meaning they do not execute until an action is called. This allows for building a data processing pipeline without immediate execution.

    \begin{block}{Common Examples}
        \begin{itemize}
            \item \textbf{Map:} Transforms each element in the RDD and returns a new RDD.
            \begin{lstlisting}[language=Python]
numbers = sc.parallelize([1, 2, 3, 4])
squared_numbers = numbers.map(lambda x: x ** 2)
            \end{lstlisting}
            
            \item \textbf{Filter:} Returns a new RDD containing only elements that satisfy a given condition.
            \begin{lstlisting}[language=Python]
odd_numbers = numbers.filter(lambda x: x % 2 != 0)
            \end{lstlisting}
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        Transformations are \textbf{lazy}:
        \begin{itemize}
            \item Examples include \textbf{map}, \textbf{filter}, \textbf{flatMap}, \textbf{union}, and \textbf{distinct}.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions}
    Actions trigger the execution of the transformations and return results to the driver program. Unlike transformations, actions force computations.

    \begin{block}{Common Examples}
        \begin{itemize}
            \item \textbf{Collect:} Returns all elements of the RDD to the driver as an array.
            \begin{lstlisting}[language=Python]
result = squared_numbers.collect()
            \end{lstlisting}
            
            \item \textbf{Count:} Returns the number of elements in the RDD.
            \begin{lstlisting}[language=Python]
count_of_numbers = numbers.count()
            \end{lstlisting}
            
            \item \textbf{First:} Returns the first element of the RDD.
            \begin{lstlisting}[language=Python]
first_number = numbers.first()
            \end{lstlisting}
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        Actions will trigger the execution of all transformations defined. Examples of actions include \textbf{count}, \textbf{first}, \textbf{take(n)}, and \textbf{saveAsTextFile(path)}.
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Lazy Evaluation}
    \begin{block}{Overview}
        Lazy evaluation is a key concept in Apache Spark that enhances performance by delaying the execution of transformations until an action is called. This allows Spark to optimize the computation plan and reduce the amount of data shuffling across the network.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Lazy Evaluation?}
    \begin{itemize}
        \item \textbf{Definition:} 
        Operations on Resilient Distributed Datasets (RDDs) are divided into two categories: transformations and actions. Transformations are lazy, meaning they do not compute their results immediately but rather record the transformation for later execution upon an action.
        \item \textbf{Purpose:} 
        This design minimizes overhead by consolidating operations and optimizing resource usage.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Lazy Evaluation Works}
    \begin{itemize}
        \item \textbf{Pipeline Creation:}
        When transformations like \texttt{map} or \texttt{filter} are called on an RDD, Spark builds a Directed Acyclic Graph (DAG) that defines the sequence of transformations without executing them immediately.
        
        \item \textbf{Execution Trigger:}
        Once an action, such as \texttt{collect} or \texttt{count}, is invoked, Spark evaluates the pipeline, executing the transformations in a single pass to produce the final output efficiently.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Lazy Evaluation}
    \begin{lstlisting}[language=Python]
# Initializing a SparkContext
from pyspark import SparkContext

sc = SparkContext("local", "Lazy Evaluation Example")

# Creating an RDD
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Applying transformations (lazy)
transformed_rdd = rdd.map(lambda x: x * 2).filter(lambda x: x > 5)

# This action triggers the computation
result = transformed_rdd.collect()  # The transformation occurs here
print(result)
    \end{lstlisting}
    \begin{itemize}
        \item \textbf{Explanation:} 
        In this example, the transformations \texttt{map} and \texttt{filter} are defined but not executed until \texttt{collect()} is called, allowing Spark to optimize the execution plan.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Performance Implications}
    \begin{itemize}
        \item \textbf{Optimization:} 
        Minimizes the amount of data read/written and the number of operations through optimized workflows.
        
        \item \textbf{Fault Tolerance:} 
        If a computation fails, only the necessary transformations need to be recomputed.
        
        \item \textbf{Resource Management:} 
        Reduces intermediate data, leading to lower memory usage and faster execution.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Transformations are lazy; actions trigger execution.
        \item Lazy evaluation allows Spark to optimize performance through pipeline execution.
        \item Understanding lazy evaluation is crucial for efficient Spark programming.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Lazy evaluation is a fundamental feature of Apache Spark that enhances performance by delaying execution, allowing optimizations, and improving efficiency. Understanding this concept is vital for effective big data processing in Spark applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Spark Context - Overview}
    
    \begin{block}{What is SparkContext?}
        The SparkContext is the entry point to any Spark application. 
        It allows your program to connect to the Spark cluster and utilize Spark's capabilities for distributed data processing.
    \end{block}
    
    \begin{block}{Functionality}
        It provides the necessary environment for running RDDs (Resilient Distributed Datasets) and performing transformations and actions on them.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Spark Context - Role in Initiating Applications}
    
    \begin{enumerate}
        \item \textbf{Cluster Connection}: Establishes a connection to the Spark cluster manager, working with Standalone, YARN, or Mesos.
        
        \item \textbf{Resource Allocation}: Specifies the number of executors, memory per executor, and processing cores for optimization.
        
        \item \textbf{RDD Creation}: Enables creation of RDDs from various data sources (HDFS, local file systems, etc.).
        
        \item \textbf{Job Submission}: Manages the execution of jobs across the cluster once the application is built.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Spark Context - Example Code Snippet}
    
    \begin{lstlisting}[language=Python]
from pyspark import SparkContext

# Initialize SparkContext
sc = SparkContext("local", "Example Application")

# Create an RDD from a text file
data = sc.textFile("hdfs://path/to/data.txt")

# Perform a transformation
word_counts = data.flatMap(lambda line: line.split(" ")) \
                  .map(lambda word: (word, 1)) \
                  .reduceByKey(lambda a, b: a + b)

# Action to collect results
results = word_counts.collect()

print(results)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Initialization}: Always initialize SparkContext at the beginning of your Spark applications.
        
        \item \textbf{Single Instance}: Only one active SparkContext per JVM; make sure to stop the previous one to create a new one.
        
        \item \textbf{Configuration}: Use \texttt{SparkConf} to define parameters like application name and cluster URL.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing Spark Context}
    
    \begin{block}{Diagrams and Illustrations}
        \begin{itemize}
            \item \textbf{Application Lifecycle}: A flowchart showing Spark application lifecycle from initialization, resource allocation, to job execution.
            \item \textbf{Cluster Connection}: A visual representation of SparkContext establishing connections with worker nodes in the cluster.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration with Other Tools - Overview}
    \begin{block}{Overview}
        Apache Spark is a powerful big data framework known for its speed and ease of use. 
        One of its standout features is its ability to integrate seamlessly with a variety of tools and frameworks in the big data ecosystem.
        This facilitates enhanced functionality, improved data processing, and richer analytic capabilities for users.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration with Other Tools - Key Integrations}
    \begin{itemize}
        \item \textbf{Hadoop Ecosystem}
            \begin{itemize}
                \item \textbf{YARN}: Runs on Hadoop's YARN, allowing efficient resource sharing.
                \item \textbf{HDFS}: Natively supports HDFS for easy data integration.
            \end{itemize}
        \item \textbf{Data Sources}
            \begin{itemize}
                \item \textbf{Apache Kafka}: Enables real-time consumption for Spark Streaming.
                \item \textbf{Cassandra}: Integrates directly for analysis of large datasets.
                \item \textbf{MongoDB}: Provides connectors for document-based interaction.
            \end{itemize}
        \item \textbf{Machine Learning and Graph Processing}
            \begin{itemize}
                \item \textbf{MLlib}: Built-in library for distributed machine learning.
                \item \textbf{GraphX}: Efficiently handles graph data and queries.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration with Other Tools - Example Scenarios}
    \begin{itemize}
        \item \textbf{Real-time Analytics}:
            \begin{itemize}
                \item Using Spark Streaming with Kafka to analyze streaming data from social media.
            \end{itemize}
        \item \textbf{Batch Processing}:
            \begin{itemize}
                \item Running ETL jobs that pull data from HDFS for predictive analytics.
            \end{itemize}
    \end{itemize}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Spark serves as a bridge between various tools, enhancing capabilities.
            \item Integration with Hadoop leverages existing infrastructure.
            \item Libraries like MLlib and GraphX add significant analytic value.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Apache Spark}
    Apache Spark is an open-source unified analytics engine designed for big data processing, providing essential tools for handling large-scale data across various industries.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Apache Spark}
    \begin{itemize}
        \item \textbf{Speed}: Processes data in memory, significantly improving performance over traditional disk-based systems.
        \item \textbf{Ease of Use}: High-level APIs available in Java, Scala, Python, and R for versatile usage by data professionals.
        \item \textbf{Unified Framework}: Supports batch, stream, and machine learning processing all within a single platform.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Use Cases of Apache Spark}
    \begin{enumerate}
        \item \textbf{Data Analytics in Retail}
            \begin{itemize}
                \item \textbf{Example}: Walmart uses Spark for predictive analytics to manage inventory and customer behavior.
            \end{itemize}
        \item \textbf{Real-time Fraud Detection}
            \begin{itemize}
                \item \textbf{Example}: PayPal leverages Spark to detect fraudulent transactions in real-time.
            \end{itemize}
        \item \textbf{Healthcare Data Processing}
            \begin{itemize}
                \item \textbf{Example}: The NHS utilizes Spark for managing patient data and improving care strategies.
            \end{itemize}
        \item \textbf{Social Media Analytics}
            \begin{itemize}
                \item \textbf{Example}: LinkedIn employs Spark for job recommendations and targeted advertising.
            \end{itemize}
        \item \textbf{Telecommunications}
            \begin{itemize}
                \item \textbf{Example}: Comcast analyzes user data using Spark to enhance service reliability.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Flexibility}: Spark can be deployed on various platforms, including Hadoop and Kubernetes.
        \item \textbf{Scalability}: Handles petabytes of data effortlessly, suitable for enterprises.
        \item \textbf{Community and Ecosystem}: Large community support with numerous connectors for integration.
    \end{itemize}
    Apache Spark's versatility is crucial in the modern data landscape, enhancing operational efficiency and delivering actionable business insights.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Sample Spark Job (Python)}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("ExampleApp") \
    .getOrCreate()

# Load data
df = spark.read.csv("data.csv", header=True)

# Perform transformation
result = df.groupBy("category").agg({"sales": "sum"})

# Show the result
result.show()
    \end{lstlisting}
    This sample demonstrates how easy it is to load, manipulate, and analyze data with Apache Spark's APIs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary of Core Concepts}
    
    \begin{enumerate}
        \item \textbf{What is Apache Spark?}
        \begin{itemize}
            \item Open-source, distributed computing system.
            \item Offers in-memory computing and fault tolerance.
        \end{itemize}

        \item \textbf{Key Features:}
        \begin{itemize}
            \item \textbf{Fast Processing:} 100x faster in memory and 10x faster on disk than MapReduce.
            \item \textbf{Ease of Use:} High-level APIs in Java, Scala, Python, and R.
            \item \textbf{Unified Engine:} Supports batch, stream processing, interactive queries, and ML.
        \end{itemize}
        
        \item \textbf{Spark Components:}
        \begin{itemize}
            \item \textbf{Spark Core:} Task scheduling, memory management, fault recovery.
            \item \textbf{Spark SQL:} Execute SQL queries; connect with data sources.
            \item \textbf{Spark Streaming:} Real-time data processing.
            \item \textbf{MLlib:} Machine learning algorithms for big data.
            \item \textbf{GraphX:} Graph processing framework.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Future Potential of Apache Spark}
    
    \begin{itemize}
        \item \textbf{Growing Adoption:} Increased data generation leads to higher demand for Spark.
        
        \item \textbf{Innovative Use Cases:}
        \begin{itemize}
            \item Financial modeling and fraud detection.
            \item Healthcare data analysis.
            \item Real-time social media analytics.
        \end{itemize}

        \item \textbf{Integration with AI and ML:} 
        \begin{itemize}
            \item MLlib evolving with better AI integration.
            \item Supported in predictive analytics.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Points and Example Code}
    
    \textbf{Key Points:}
    \begin{itemize}
        \item Efficient processing of vast datasets enables advanced analytics.
        \item Accessibility through multiple programming languages fosters innovation.
    \end{itemize}

    \textbf{Example Code Snippet:}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("ExampleApp").getOrCreate()

# Load a DataFrame
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Perform a transformation
result = df.groupBy("category").sum("sales")

# Show the result
result.show()
    \end{lstlisting}
\end{frame}


\end{document}