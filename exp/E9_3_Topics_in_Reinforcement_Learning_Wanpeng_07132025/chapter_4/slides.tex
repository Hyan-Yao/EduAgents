\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Deep Reinforcement Learning]{Chapter 4: Deep Reinforcement Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Deep Reinforcement Learning (DRL)}
    \begin{itemize}
        \item Deep Reinforcement Learning (DRL) combines Reinforcement Learning (RL) with deep learning techniques.
        \item Enables agents to make decisions based on high-dimensional sensory inputs.
        \item Particularly effective in complex environments such as robotics, game playing, and autonomous vehicles.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Reinforcement Learning}:
            \begin{itemize}
                \item An agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
                \item Agents take actions based on their current state.
            \end{itemize}
        \item \textbf{Deep Learning}:
            \begin{itemize}
                \item Utilizes neural networks with multiple layers to process data.
                \item Allows for the approximation of complex mappings between states and actions.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance in AI}
    \begin{itemize}
        \item \textbf{Complex Problem Solving}:
            \begin{itemize}
                \item Effective in domains with sparse data and dynamic environments.
            \end{itemize}
        \item \textbf{Generalization}:
            \begin{itemize}
                \item Deep neural networks help DRL generalize from past experiences.
            \end{itemize}
        \item \textbf{Real-world Applications}:
            \begin{itemize}
                \item Used in finance, healthcare, and natural language processing for automated decision-making systems.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Examples}
    \begin{itemize}
        \item Combines exploration and exploitation to optimize performance.
        \item Handles continuous and discrete action spaces.
        \item Examples include:
            \begin{itemize}
                \item Training AI agents in chess and video games.
                \item Optimizing resource management in real time.
                \item Enhancing user experience in recommendation systems.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Concepts}
    \begin{block}{Q-Learning Formula}
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
        \end{equation}
        where:
        \begin{itemize}
            \item \( Q(s, a) \): action-value function indicating expected return of taking action \( a \) in state \( s \).
            \item \( r \): immediate reward received after taking action \( a \).
            \item \( \alpha \): learning rate (0 < \( \alpha \) ≤ 1).
            \item \( \gamma \): discount factor (0 ≤ \( \gamma \) < 1) balancing immediate and future rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamental Concepts of Reinforcement Learning - Key Definitions}
    \begin{enumerate}
        \item \textbf{Agent:}
        \begin{itemize}
            \item The learner or decision-maker in the reinforcement learning framework. 
            \item Example: In a game of chess, the player (agent) makes decisions (actions) to win the game (goal).
        \end{itemize}

        \item \textbf{Environment:}
        \begin{itemize}
            \item Everything that the agent interacts with, providing feedback based on actions taken.
            \item Example: The chessboard and the opponent's move are part of the environment. 
        \end{itemize}

        \item \textbf{State (s):}
        \begin{itemize}
            \item A specific situation of the environment at a particular time.
            \item Example: A particular arrangement of pieces on the chessboard represents a state.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamental Concepts of Reinforcement Learning - Actions and Rewards}
    \begin{enumerate}
        \setcounter{enumi}{3} % Start from 4
        
        \item \textbf{Action (a):}
        \begin{itemize}
            \item A choice made by the agent that affects the environment.
            \item Example: Moving a pawn forward or capturing an opponent's piece in chess.
        \end{itemize}

        \item \textbf{Reward (r):}
        \begin{itemize}
            \item Scalar feedback received by the agent after taking an action, indicating the quality of the action.
            \item Example: Capturing a queen in chess may yield a positive reward, while losing a piece could yield a negative reward.
        \end{itemize}

        \item \textbf{Policy ($\pi$):}
        \begin{itemize}
            \item A strategy used by the agent to decide the next action based on the current state.
            \item Example: A policy in chess could dictate responses based on the opponent's moves.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Interconnectedness:} The interaction between agents and environments is crucial; agents learn from their environments through rewards.
        \item \textbf{Objective:} The goal of an agent is to develop a policy maximizing cumulative rewards over time.
        \item \textbf{Exploration vs. Exploitation:} Balancing the exploration of new actions against exploiting known high-reward actions is essential.
    \end{itemize}

    \begin{block}{Illustration of Concepts}
        Consider a maze-solving scenario: 
        - Agent: A robot trying to navigate.
        - Environment: The maze itself.
        - State: The robot's current location.
        - Action: Moving up, down, left, right.
        - Reward: Positive for reaching the exit; negative for hitting a wall.
        - Policy: The strategy for navigating effectively.
    \end{block}

    \begin{equation}
        G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots 
    \end{equation}
    where $G_t$ is the cumulative reward from time $t$, $R_t$ represents the immediate reward, and $\gamma$ (0 \leq \gamma < 1) is the discount factor.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differentiating Learning Paradigms - Overview}
    \begin{itemize}
        \item Primary paradigms in machine learning:
        \begin{itemize}
            \item Supervised Learning
            \item Unsupervised Learning
            \item Reinforcement Learning
        \end{itemize}
        \item Each paradigm has:
        \begin{itemize}
            \item Distinct characteristics
            \item Unique methods
            \item Specific applications
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differentiating Learning Paradigms - Supervised Learning}
    \begin{block}{Definition}
        In supervised learning, an algorithm learns from labeled training data, aiming to learn a mapping from inputs to outputs.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} Image Classification (labels: "cat" or "dog")
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item Requires labeled dataset
            \item Effective for classification and regression tasks
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differentiating Learning Paradigms - Unsupervised Learning}
    \begin{block}{Definition}
        Unsupervised learning involves training an algorithm on data without labeled outputs. The goal is to identify patterns or groupings.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} Clustering customer behaviors using K-means
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item No labeled data required
            \item Useful for discovering inherent data structures
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differentiating Learning Paradigms - Reinforcement Learning}
    \begin{block}{Definition}
        Reinforcement Learning (RL) involves an agent learning to make decisions by taking actions to maximize cumulative rewards.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Concepts:}
        \begin{itemize}
            \item \textbf{Agent:} The learner or decision-maker
            \item \textbf{Environment:} The system that the agent interacts with
            \item \textbf{State (s):} Current situation of the agent
            \item \textbf{Action (a):} Possible moves the agent can make
            \item \textbf{Reward (r):} Feedback signal after actions
        \end{itemize}
        \item \textbf{Example:} An RL agent playing chess to maximize overall score through strategic learning
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differentiating Learning Paradigms - Key Differences}
    \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            \textbf{Feature} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} & \textbf{Reinforcement Learning} \\
            \hline
            Data Type & Labeled data & Unlabeled data & Interaction with environment \\
            \hline
            Feedback Mechanism & Direct (correct labels) & None (pattern recognition) & Delayed (cumulative rewards) \\
            \hline
            Objective & Predict outcomes & Discover patterns & Maximize cumulative rewards \\
            \hline
            Use Cases & Classification, Regression & Clustering, Association & Game AI, Robotics, Resource Management \\
            \hline
        \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differentiating Learning Paradigms - Summary}
    \begin{itemize}
        \item \textbf{Supervised Learning:} Needs labeled data, focuses on input-output mapping
        \item \textbf{Unsupervised Learning:} Extracts patterns from unlabeled data
        \item \textbf{Reinforcement Learning:} Learns through interaction to maximize rewards
    \end{itemize}
    \begin{block}{Conclusion}
        Understanding these distinctions helps in applying the right paradigm for specific problems, optimizing model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Reinforcement Learning Algorithms}
    \begin{block}{Introduction to Foundational Algorithms}
        Reinforcement Learning (RL) is a paradigm where an agent learns to make decisions by interacting with an environment. 
        Two of the foundational algorithms in RL are \textbf{Q-learning} and \textbf{SARSA} (State-Action-Reward-State-Action).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning}
    \begin{block}{Concept}
        Q-learning is an off-policy reinforcement learning algorithm that aims to determine the best action given a certain state.
    \end{block}

    \begin{block}{Key Formula}
        The Q-value is updated using the following formula:
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
        \end{equation}
        Where:
        \begin{itemize}
            \item $s$ = current state 
            \item $a$ = action taken 
            \item $r$ = reward received 
            \item $s'$ = next state 
            \item $a'$ = possible actions from the next state 
            \item $\alpha$ = learning rate ($0 < \alpha \leq 1$) 
            \item $\gamma$ = discount factor ($0 \leq \gamma < 1$)
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        In a simple grid world:
        \begin{itemize}
            \item The agent receives a reward of +10 for reaching the goal.
            \item If it starts from state $s$ and takes action $a$, it may receive a reward $r$ based on its current state and action.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA}
    \begin{block}{Concept}
        SARSA is an on-policy algorithm where the action taken is determined by the current policy. It updates Q-values based on actions actually taken.
    \end{block}

    \begin{block}{Key Formula}
        The Q-value update in SARSA is given by:
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
        \end{equation}
    \end{block}
    
    \begin{block}{Example}
        In the same grid world:
        \begin{itemize}
            \item If the agent follows a policy that consistently explores, it might take action $a'$ in state $s'$ leading to reward $r$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    \begin{itemize}
        \item \textbf{Exploration vs. Exploitation:} Both algorithms balance the trade-off between exploring new actions and exploiting known high-reward actions.
        \item \textbf{Convergence:} With adequate decaying of $\alpha$ and $\epsilon$, both algorithms can converge to the optimal policy.
        \item \textbf{Applications:} These foundational algorithms are used in various fields including game playing, robotics, and automated trading.
    \end{itemize}

    \begin{block}{Conclusion}
        Understanding Q-learning and SARSA is vital for building deeper reinforcement learning models, forming the basis for more complex strategies.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Q-learning - Introduction}
    \begin{block}{Introduction to Q-learning}
        Q-learning is a model-free reinforcement learning technique used to find the optimal action-selection policy for an agent. It learns a value function that estimates the expected utility of taking a given action in a specific state and following a certain policy thereafter.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Q-learning - Key Concepts}
    \begin{itemize}
        \item \textbf{State (S)}: A specific situation in the environment.
        \item \textbf{Action (A)}: A choice or decision made by the agent that affects its state.
        \item \textbf{Reward (R)}: Feedback from the environment based on the action taken.
        \item \textbf{Q-value (Q(S, A))}: Represents the expected utility of taking action A in state S, followed by the optimal policy.
        \item \textbf{Learning Rate ($\alpha$)}: Determines how much new information overrides the old information (0 < $\alpha$ ≤ 1).
        \item \textbf{Discount Factor ($\gamma$)}: Represents the importance of future rewards (0 ≤ $\gamma$ < 1).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Q-learning - Step-by-Step}
    \begin{enumerate}
        \item \textbf{Initialize Q-values:} Create a Q-table initialized to zero for all state-action pairs.
        \begin{lstlisting}[language=Python]
   import numpy as np

   states = <number_of_states>
   actions = <number_of_actions>
   Q = np.zeros((states, actions))
        \end{lstlisting}
        
        \item \textbf{Parameters Setup:} Define the parameters for learning.
        \begin{lstlisting}[language=Python]
   learning_rate = 0.1  # α
   discount_factor = 0.95  # γ
   exploration_exploitation_balance = 0.1  # ε
        \end{lstlisting}

    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Q-learning - Action & Update}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Choose an Action:} Implement an epsilon-greedy policy.
        \begin{lstlisting}[language=Python]
   def choose_action(state):
       if np.random.rand() < exploration_exploitation_balance:
           return np.random.choice(actions)  # Explore
       else:
           return np.argmax(Q[state])  # Exploit
        \end{lstlisting}

        \item \textbf{Update Q-values:} Update the Q-value based on the received reward.
        \begin{lstlisting}[language=Python]
   def update_q_value(state, action, reward, next_state):
       best_future_q = np.max(Q[next_state])
       Q[state, action] += learning_rate * (reward + discount_factor * best_future_q - Q[state, action])
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Q-learning - Training Loop}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Implementation of the Training Loop:}
        \begin{lstlisting}[language=Python]
   for episode in range(total_episodes):
       state = <initial_state>
       done = False
       
       while not done:
           action = choose_action(state)
           next_state, reward, done = <environment_step>(state, action)
           update_q_value(state, action, reward, next_state)
           state = next_state
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Implementing Q-learning - Example Scenario}
    \begin{block}{Example Scenario: Grid World}
        Imagine a grid where an agent needs to learn how to reach a goal while avoiding obstacles.
        The states represent each cell in the grid, and actions could be moving up, down, left, or right. The agent receives rewards for reaching the goal or penalties for hitting walls.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Q-learning - Key Points}
    \begin{itemize}
        \item Q-learning is effective in unknown environments due to its model-free nature.
        \item Balancing exploration and exploitation is crucial for convergence to the optimal policy.
        \item Proper tuning of learning parameters can significantly affect performance.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementing Q-learning - Conclusion}
    \begin{block}{Conclusion}
        Q-learning is a foundational algorithm in reinforcement learning due to its simplicity and effectiveness. Understanding this algorithm prepares you for more advanced techniques in deep reinforcement learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing SARSA - Introduction}
    \begin{block}{What is SARSA?}
        SARSA stands for State-Action-Reward-State-Action. It is an on-policy reinforcement learning algorithm that learns the value of state-action pairs. 
    \end{block}
    \begin{itemize}
        \item Unlike Q-learning (off-policy), SARSA takes into account the next action performed by the current policy.
        \item It aids in understanding the performance of the policy being followed.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing SARSA - Key Concepts}
    \begin{block}{Key Concepts:}
        \begin{enumerate}
            \item \textbf{On-Policy Algorithm:} SARSA learns the value based on the same policy that generates the behaviors.
            \item \textbf{State-Action Value Function (Q-value):} Denoted as $Q(s, a)$, it indicates the expected return for taking action 'a' in state 's' and following the current policy thereafter.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing SARSA - Algorithm Steps}
    \begin{enumerate}
        \item \textbf{Initialization:}
            \begin{itemize}
                \item Initialize $Q(s, a)$ arbitrarily for all state-action pairs.
                \item Set the action selection policy (e.g., $\epsilon$-greedy policy).
            \end{itemize}
        \item \textbf{For each episode:} 
            \begin{itemize}
                \item Initialize starting state (s) and select action (a) based on current policy.
                \item Repeat for each step in the episode:
                    \begin{itemize}
                        \item Take action (a) and observe reward (r) and new state (s').
                        \item Select next action (a') from new state (s') using the current policy.
                        \item Update the action-value function:
                        \begin{equation}
                        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
                        \end{equation}
                        \item Update state and action: $(s, a) \leftarrow (s', a')$.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing SARSA - Example Implementation}
    \begin{block}{Python Example}
        \begin{lstlisting}[language=Python]
import numpy as np
import random

# Initialize parameters
alpha = 0.1     # Learning rate
gamma = 0.9     # Discount factor
epsilon = 0.1   # Exploration rate
num_episodes = 1000
num_actions = 4 # Example: 4 possible actions
num_states = 10 # Example: 10 possible states

# Initialize Q-table
Q = np.zeros((num_states, num_actions))

def choose_action(state):
    if random.uniform(0, 1) < epsilon:
        return random.choice(range(num_actions))  # Explore
    else:
        return np.argmax(Q[state])  # Exploit
\end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing SARSA - Conclusion}
    \begin{itemize}
        \item \textbf{Exploration vs. Exploitation:} Balancing new actions and known rewards is crucial for effective learning.
        \item \textbf{Policy Improvement:} As Q-values are updated, the policy improves, yielding higher expected rewards.
        \item \textbf{Use Cases:} Applicable in various environments, such as grid-world scenarios, robotic navigation, and gaming.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Algorithm Performance - Overview}
    \begin{itemize}
        \item Importance of evaluating reinforcement learning (RL) algorithms
        \item Methods to visualize and interpret performance
        \item Key performance metrics:
        \begin{itemize}
            \item Reward Curves
            \item Learning Curves
            \item Policy Visualization
            \item Parameter Sensitivity Analysis
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics in Reinforcement Learning - Key Concepts}
    \begin{enumerate}
        \item \textbf{Reward Curves}:
            \begin{itemize}
                \item Plots total rewards over time or episodes.
                \item Indicates learning progression.
                \item \textit{Example:} Average reward of 50 over 100 episodes indicates improvement.
            \end{itemize}
        \item \textbf{Learning Curves}:
            \begin{itemize}
                \item Metrics like average cumulative reward over time.
                \item \textit{Example:} A plateau indicates maximum performance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Continuation}
    \begin{enumerate}
        \setcounter{enumi}{2} % to continue numbering
        \item \textbf{Policy Visualization}:
            \begin{itemize}
                \item Visualizes learned policies in environments.
                \item \textit{Example:} Heatmaps indicating action preferences.
            \end{itemize}
        \item \textbf{Parameter Sensitivity Analysis}:
            \begin{itemize}
                \item Varies parameters to assess performance impact.
                \item \textit{Example:} Different learning rates plotted for optimal value.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Formulas}
    \begin{block}{Average Reward Formula}
        \begin{equation}
            \text{Average Reward} = \frac{1}{N} \sum_{t=1}^{N} r_t
        \end{equation}
        \text{where } N \text{ is the number of episodes, and } r_t \text{ is the reward at time } t.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Reward Plotting}
    \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

def plot_rewards(episode_list, reward_list):
    plt.plot(episode_list, reward_list)
    plt.xlabel('Episodes')
    plt.ylabel('Total Reward')
    plt.title('Reward Curve')
    plt.grid()
    plt.show()

# Example usage:
episodes = list(range(1, 101))
rewards = [random.randint(0, 100) for _ in range(100)]  # Dummy data
plot_rewards(episodes, rewards)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Importance of performance evaluation in RL
        \item Utilization of various metrics for comprehensive understanding
        \item Consistency in representation across experiments is key for comparison
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Deep Reinforcement Learning - Overview}
    \begin{block}{Definition}
        Deep Reinforcement Learning (DRL) combines Reinforcement Learning (RL) with Deep Learning (DL) techniques.
    \end{block}
    
    \begin{itemize}
        \item Enables processing of high-dimensional sensory inputs.
        \item Facilitates learning in complex environments.
        \item A pivotal method in artificial intelligence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Deep Reinforcement Learning - Key Concepts}
    \begin{block}{Reinforcement Learning (RL) Basics}
        \begin{itemize}
            \item \textbf{Agent}: The learner or decision-maker.
            \item \textbf{Environment}: The external system or problem to be solved.
            \item \textbf{State (s)}: Representation of the environment at a specific time.
            \item \textbf{Action (a)}: Choices made by the agent affecting future states.
            \item \textbf{Reward (r)}: Feedback from the environment encouraging behaviors that maximize future rewards.
        \end{itemize}
    \end{block}
    
    \begin{block}{Deep Learning Basics}
        \begin{itemize}
            \item \textbf{Neural Networks}: Models mimicking brain functioning with layers of interconnected nodes (neurons).
            \item \textbf{Representation Learning}: Automatically discovers the features needed for a task from data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration of Deep Learning with Reinforcement Learning}
    \begin{block}{Why Use Deep Learning in RL?}
        \begin{itemize}
            \item \textbf{High-Dimensional Inputs}: DRL processes raw sensory data effectively.
            \item \textbf{Feature Extraction}: Reduces the need for handcrafted features through robust representation learning.
        \end{itemize}
    \end{block}
    
    \begin{block}{How it is Done}
        \begin{itemize}
            \item \textbf{Function Approximation}: Deep learning models approximate value functions or policy functions.
            \item \textbf{Experience Replay}: Using past experiences enhances learning efficiency and stability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Introduction}
    Deep Q-Networks (DQN) are a pivotal component in Deep Reinforcement Learning (DRL).
    
    \begin{itemize}
        \item Combine Q-learning with deep learning architectures.
        \item Enable handling of high-dimensional state spaces.
        \item Effective for solving complex problems across various applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Architecture}
    
    \begin{enumerate}
        \item \textbf{Input Layer:}
        \begin{itemize}
            \item Represents the state of the environment (e.g., frames from a game).
            \item Processed by convolutional layers to extract features.
        \end{itemize}
        
        \item \textbf{Convolutional Neural Network (CNN):}
        \begin{itemize}
            \item Convolutional layers extract spatial hierarchies from input.
            \item ReLU is commonly used as the activation function.
        \end{itemize}
        
        \item \textbf{Fully Connected Layers:}
        \begin{itemize}
            \item Outputs are flattened and passed through to determine Q-values.
        \end{itemize}
        
        \item \textbf{Output Layer:}
        \begin{itemize}
            \item Provides the Q-values for all possible actions in the given state.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Key Concepts and Applications}
    
    \textbf{Key Concepts:}
    \begin{itemize}
        \item \textbf{Experience Replay:}
        \begin{itemize}
            \item Uses a buffer to store experiences, improving stability.
        \end{itemize}

        \item \textbf{Target Network:}
        \begin{itemize}
            \item Maintains a separate target network to reduce learning oscillations.
        \end{itemize}
    \end{itemize}
    
    \textbf{Applications:}
    \begin{itemize}
        \item \textbf{Atari Games:} Excels in playing games using pixel values.
        \item \textbf{Robotics:} Used in robotic control for navigation and manipulation.
        \item \textbf{Autonomous Vehicles:} Aids in decision-making, obstacle avoidance, and path planning.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Policy Gradient Methods}
    \begin{block}{Introduction}
        Policy Gradient Methods are algorithms in Deep Reinforcement Learning (DRL) that optimize the policy directly rather than the value function. 
        This is beneficial for high-dimensional or continuous action spaces where traditional methods may fail.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Policy}: A function defining the probability of an action given a state. Can be either deterministic or stochastic.
        
        \item \textbf{Objective}: Maximize expected cumulative reward \( J(\theta) \):
        \begin{equation}
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} r_t \right]
        \end{equation}
        where \( r_t \) is the reward at time \( t \).

        \item \textbf{Gradient Estimate}: To compute the gradient of expected return with respect to policy parameters:
        \begin{equation}
            \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla \log \pi_\theta(a_t | s_t) G_t \right]
        \end{equation}
        where \( G_t \) is the return from time \( t \).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of Policy Gradient Methods}
    \begin{itemize}
        \item \textbf{Direct Optimization}: Optimizes policy directly, improving performance in complex environments.
        
        \item \textbf{Stochastic Policies}: Naturally handles stochastic environments, allowing for exploration of better strategies.
        
        \item \textbf{Continuous Action Spaces}: Suitable for actions chosen from continuous distributions, vital in real-world applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: REINFORCE Algorithm}
    The REINFORCE algorithm updates policy after collecting a complete trajectory of states, actions, and rewards. The update rule is based on the policy gradient theorem:
    
    \begin{lstlisting}[language=Python, basicstyle=\footnotesize]
    # Pseudocode for updating policy using the REINFORCE algorithm
    for episode in range(num_episodes):
        states, actions, rewards = collect_trajectory()  # Simulate episode
        for t in range(len(rewards)):
            G = calculate_return(rewards[t:])  # Calculate return
            update_policy(log_pi_theta(actions[t], states[t]), G)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Policy gradient methods offer a framework for effective RL strategies in complex environments.
        
        \item Understanding the mathematical formulation enables better implementation of these methods.
        
        \item The policy parameters can encode exploratory behavior, enhancing versatility in reinforcement learning problems.
    \end{itemize}
    \vfill
    \textbf{Connection to Next Slide:} Transitioning to Actor-Critic Methods which combine strengths of policy gradients and value function approximations for more robust learning strategies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Part 1}
    \begin{block}{Understanding Actor-Critic Architectures}
        Actor-critic methods represent a hybrid approach in deep reinforcement learning (DRL) that combines the strengths of both value-based and policy-based methods. In this architecture, two primary components operate concurrently: the **Actor** and the **Critic**.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Actor:}
        \begin{itemize}
            \item Selects actions based on the current policy.
            \item Outputs probability distribution over actions.
            \item Can employ deterministic or stochastic policies.
            \item \textit{Example:} Decides to move left or right in a game.
        \end{itemize}
        
        \item \textbf{Critic:}
        \begin{itemize}
            \item Evaluates the action taken by the actor.
            \item Calculates the value function representing expected returns.
            \item Estimates the Temporal Difference (TD) Error.
            \item \textit{Example:} Assesses action quality based on future rewards.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Part 2}
    \begin{block}{Mathematical Formulation}
        \begin{itemize}
            \item \textbf{Critic Update:} 
            \begin{equation}
                \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
            \end{equation}
            where $r_t$ is the reward at time $t$, $\gamma$ is the discount factor, $V(s_t)$ is the value of the current state, and $V(s_{t+1})$ is the value of the next state.
        
            \item \textbf{Actor Update:} 
            \begin{equation}
                \nabla J(\theta) \approx \nabla \log \pi_\theta(a_t | s_t) \cdot \delta_t
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Part 3}
    \begin{block}{Advantages of Actor-Critic Methods}
        \begin{itemize}
            \item \textbf{Reduced Variance:} Combines actor and critic to minimize variance compared to standard methods.
            \item \textbf{Improved Stability:} Separation of value function and policy leads to stable learning.
            \item \textbf{Flexibility:} Can enact both on-policy (e.g., A3C) and off-policy (e.g., DDPG) learning strategies.
            \item \textbf{Scalability:} Efficiently handles large state and action spaces for complex environments.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Effectively combines policies and value functions for powerful DRL.
            \item Understanding the actor and critic interplay is essential for effective algorithms.
            \item Facilitates advanced architectures such as Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Reinforcement Learning}
    \begin{block}{Overview}
        Deep Reinforcement Learning (DRL) merges reinforcement learning and deep learning to address complex decision-making problems. Its adaptability has led to significant advancements across various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Areas of Application}
    \begin{enumerate}
        \item \textbf{Gaming}
        \item \textbf{Robotics}
        \item \textbf{Healthcare}
        \item \textbf{Finance}
        \item \textbf{Transportation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies of DRL Applications}
    \begin{itemize}
        \item \textbf{AlphaGo (Gaming)}: Defeated a world champion Go player using DRL via supervised learning and self-play.
        \item \textbf{Robotic Hand Control (Robotics)}: Robots learn to grasp and manipulate objects through trial and error.
        \item \textbf{Personalized Treatment Plans (Healthcare)}: DRL models optimize treatment strategies based on patient data.
        \item \textbf{Stock Trading (Finance)}: Algorithms predict stock price movements and make buying/selling decisions in high-frequency trading.
        \item \textbf{Autonomous Vehicles (Transportation)}: DRL helps navigate complex driving conditions and adapt to traffic.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in DRL}
    \begin{itemize}
        \item \textbf{Trial and Error}: Agents learn optimal actions through environmental interactions.
        \item \textbf{Function Approximation}: Deep learning serves as function approximators for action-value estimations.
        \item \textbf{Exploration vs. Exploitation}: Balancing exploration of new strategies and leveraging known strategies for effective learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Formula in DRL}
    The value function \( V(s) \) can be represented as follows:
    \begin{equation}
        V(s) = \mathbb{E} [R_t | s_t = s]
    \end{equation}
    where \( R_t \) is the expected return following state \( s \).
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Deep Reinforcement Learning has transformative implications across various sectors, enabling systems to learn and adapt in real-time. Understanding its applications is critical for harnessing the full potential of DRL technologies in diverse industries.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Deep Reinforcement Learning (DRL)}
    \begin{itemize}
        \item Importance of addressing ethical implications in DRL.
        \item Focus on:
        \begin{itemize}
            \item Fairness
            \item Accountability
            \item Transparency
            \item Societal Impact
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Fairness and Bias}
    \begin{block}{Explanation}
        DRL algorithms can learn biases from training data, leading to unfair treatment of different groups, perpetuating social biases or creating new inequalities.
    \end{block}
    \begin{block}{Example}
        In job recruitment, a DRL model may favor candidates based on biased historical data, disadvantaging those from underrepresented backgrounds.
    \end{block}
    \begin{block}{Key Point}
        Regular audits and diverse datasets are essential to mitigate biases in DRL models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Accountability}
    \begin{block}{Explanation}
        Identifying responsibility for decisions made by DRL systems is complex. When algorithms cause negative outcomes, accountability becomes challenging.
    \end{block}
    \begin{block}{Example}
        In self-driving cars, determining whether developers, operators, or the algorithm itself is accountable for an accident is crucial.
    \end{block}
    \begin{block}{Key Point}
        Clear guidelines and legal frameworks for accountability are necessary as DRL evolves.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Transparency and Explainability}
    \begin{block}{Explanation}
        DRL models act as "black boxes," making it difficult for users to comprehend decision-making processes. This can erode trust in AI systems.
    \end{block}
    \begin{block}{Example}
        In healthcare, recommendations made by DRL systems without rational explanations may lead practitioners to hesitate in following them.
    \end{block}
    \begin{block}{Key Point}
        Striving for explainable AI models is essential for fostering user trust and informed decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Societal Impact}
    \begin{block}{Explanation}
        DRL technologies can significantly impact society, influencing job markets, privacy, and ethical standards, leading to both positive and negative effects.
    \end{block}
    \begin{block}{Example}
        The introduction of DRL in manufacturing can enhance efficiency but may displace unskilled workers.
    \end{block}
    \begin{block}{Key Point}
        Ethical considerations must assess the broader societal implications, promoting responsible innovation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Integrate fairness, accountability, and transparency into DRL frameworks.
        \item Use continuous monitoring, diverse data, and explainable algorithms to address ethical concerns.
        \item Engage stakeholders in discussions about societal impacts for responsible deployment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item As Deep Reinforcement Learning capabilities grow, so do the ethical responsibilities.
        \item Proactive approaches are essential to foster trust and maximize societal benefits.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Trends in Reinforcement Learning Research}
    \begin{block}{Overview of Advancements}
        Reinforcement Learning (RL) has seen significant advancements, leading to breakthroughs in applications like robotics, gaming, and healthcare. Key trends include:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Trends - Part 1}
    \begin{enumerate}
        \item \textbf{Integration of Deep Learning with RL}
            \begin{itemize}
                \item Deep Q-Networks (DQN) leverage deep learning for handling high-dimensional state spaces.
                \item \textit{Example}: AlphaGo, which used this combination to master the game of Go.
            \end{itemize}
        
        \item \textbf{Meta Reinforcement Learning}
            \begin{itemize}
                \item Focuses on agents adapting to new tasks rapidly using past experiences.
                \item \textit{Key Technique}: Model-Agnostic Meta-Learning (MAML) allows fast learning of new tasks.
                \item \textit{Example}: Robots learning new manipulation tasks after exposure to similar tasks.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Trends - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Multi-Agent Reinforcement Learning}
            \begin{itemize}
                \item Studies interactions among multiple agents in shared environments.
                \item \textit{Challenges}: Coordination, communication, and competition.
                \item \textit{Example}: Cooperative driving systems that prevent collisions.
            \end{itemize}
        
        \item \textbf{Exploration Strategies}
            \begin{itemize}
                \item Focus on methods to enhance learning efficiency:
                    \begin{itemize}
                        \item Intrinsic Reward Methods 
                        \item Thompson Sampling - probabilistic exploration vs. exploitation balance.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Applications in Real-World Problems}
            \begin{itemize}
                \item \textit{Healthcare}: Personalized treatment plans and resource optimization.
                \item \textit{Finance}: Automated trading adjusts to market dynamics.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Trends - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{5}
        \item \textbf{Safety and Robustness in RL}
            \begin{itemize}
                \item Increasing focus on safe RL systems for reliable performance.
                \item \textit{Methods}: Incorporating constraints and employing adversarial training.
            \end{itemize}

        \item \textbf{Conclusion}
            \begin{itemize}
                \item RL is an evolving field integrated with other AI paradigms.
                \item Meta-learning promotes agent adaptation for dynamic applications.
                \item Multi-agent research emphasizes collaboration in complex systems.
                \item Safety focus ensures responsible RL deployment.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Summary and Future Directions}
    \begin{block}{Key Learnings in Deep Reinforcement Learning}
        \begin{enumerate}
            \item \textbf{Understanding Deep Reinforcement Learning (DRL)}:  
            DRL merges reinforcement learning with deep learning, utilizing neural networks for value functions, policies, and models.
            
            \item \textbf{Key Concepts}:
            \begin{itemize}
                \item \textbf{Agent and Environment}: The agent interacts with its environment, observing states and selecting actions based on policies.
                \item \textbf{Reward}: A feedback signal from the environment that influences the agent's learning.
                \item \textbf{Policy}: A strategy that maps states to actions; can be deterministic or stochastic.
                \item \textbf{Value Function}: Estimates the expected returns (future rewards) for states or state-action pairs.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Popular Algorithms in DRL}
    \begin{block}{Algorithm Overview}
        \begin{itemize}
            \item \textbf{Deep Q-Networks (DQN)}: Uses deep learning to approximate Q-values and benefits from experience replay.
            \item \textbf{Proximal Policy Optimization (PPO)}: A policy gradient method that stabilizes policy updates to avoid drastic changes.
            \item \textbf{Actor-Critic Methods}: Combine both a value function (critic) and a policy (actor) for action evaluation and enhancement.
        \end{itemize}
    \end{block}
    
    \begin{block}{Applications of DRL}
        \begin{itemize}
            \item Game Playing (e.g., AlphaGo, OpenAI Five)
            \item Robotics (e.g., navigation, manipulation)
            \item Autonomous Vehicles
            \item Personalized Recommendations
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in DRL Research}
    \begin{block}{Research Opportunities}
        \begin{enumerate}
            \item \textbf{Sample Efficiency}: Enhance learning speed with fewer interactions with the environment.
            \item \textbf{Exploration-Exploitation Trade-off}: Develop strategies to balance exploration of new strategies with exploitation of known ones.
            \item \textbf{Multimodal Learning}: Combine multiple types of data (e.g., visual and audio) for better agent performance.
            \item \textbf{Transfer Learning}: Transfer knowledge between tasks to accelerate learning.
            \item \textbf{Safety and Robustness}: Ensure DRL algorithms are reliable in unpredictable environments.
            \item \textbf{Ethics and Fairness}: Address biases in policies for equitable decision-making.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Takeaway}
        Deep reinforcement learning holds the potential to significantly advance intelligent decision-making across various sectors, paving the way for innovative solutions.
    \end{block}
\end{frame}


\end{document}