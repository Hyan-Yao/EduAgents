\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Midterm Presentations]{Chapter 8: Midterm Presentations}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Overview}
    \begin{block}{What is Reinforcement Learning (RL)?}
        Reinforcement Learning is a type of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards over time.
    \end{block}
    
    \begin{block}{Key Concepts:}
        \begin{enumerate}
            \item **Agent**: The learner or decision maker (e.g., a robot, software program).
            \item **Environment**: Everything that the agent interacts with (e.g., game environment).
            \item **State**: Current situation representation of the agent (e.g., chess position).
            \item **Action**: Choices made by the agent (e.g., moving a piece).
            \item **Reward**: Feedback signal received post-action (e.g., points earned).
            \item **Policy**: Strategy defining agent actions based on current state.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Importance and Applications}
    \begin{block}{Importance of Reinforcement Learning}
        \begin{itemize}
            \item **Adaptability**: RL algorithms adapt to changes in the environment over time.
            \item **Autonomous Decision Making**: Enables systems to make independent decisions like human learning.
        \end{itemize}
    \end{block}
    
    \begin{block}{Applications of Reinforcement Learning}
        \begin{enumerate}
            \item **Gaming**: RL agents play video games (e.g., AlphaGo).
            \item **Robotics**: Robots learn tasks autonomously (e.g., assembly line).
            \item **Autonomous Vehicles**: Make driving decisions based on traffic.
            \item **Finance**: Optimize trading strategies through market learning.
            \item **Healthcare**: Personalize treatment plans based on patient data.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Example and Key Points}
    \begin{block}{Example: Training an RL Agent}
        Consider an RL agent playing a simple video game:
        \begin{itemize}
            \item **Goal**: Collect as many coins as possible.
            \item **Rewards**: Positive for coins collected, negative for obstacles hit.
            \item Through trial and error, the agent learns an effective strategy over time.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item RL is about **learning from consequences** of actions.
            \item Differs from supervised learning (learning from labels).
            \item Powerful tool in **decision-making** and **strategy formulation** contexts.
        \end{itemize}
    \end{block}
    This overview sets the stage for exploring key concepts of reinforcement learning in the next slides.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Reinforcement Learning - Overview}
    \begin{itemize}
        \item \textbf{Reinforcement Learning (RL)}: A paradigm where an agent learns optimal behaviors by interacting with an environment through trial and error.
        \item \textbf{Goal}: Maximize cumulative rewards over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Reinforcement Learning - Key Terms}
    \begin{block}{Key Terms Defined}
        \begin{itemize}
            \item \textbf{Agent}: The decision-maker that interacts with the environment. \\
                  \textit{Example: A robot navigating a maze.}
            \item \textbf{Environment}: The context of the agent's operations; it responds to actions. \\
                  \textit{Example: The maze's structure, including walls and pathways.}
            \item \textbf{State}: The current situation of the agent in its environment. \\
                  \textit{Example: The robot's location in the maze.}
            \item \textbf{Action}: The choices available to the agent that influence the state. \\
                  \textit{Example: Moving forward or turning.}
            \item \textbf{Reward}: A feedback signal received after taking an action in a state. \\
                  \textit{Example: +10 points for reaching an exit.}
            \item \textbf{Policy}: A strategy mapping states to actions. \\
                  \textit{Example: Rules for navigating the maze.}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Reinforcement Learning - Key Points and Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Interdependence}: All concepts are interconnected.
            \item \textbf{Trial and Error}: RL relies on exploration to uncover rewarding actions.
            \item \textbf{Dynamic Nature}: Both agents and environments continuously change.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Scenario}
        \textbf{Self-Driving Car in City Streets}
        \begin{itemize}
            \item \textbf{Agent}: The self-driving car.
            \item \textbf{Environment}: City streets, traffic, pedestrians.
            \item \textbf{State}: Car's position and speed, nearby obstacles.
            \item \textbf{Action}: Accelerating, braking, turning.
            \item \textbf{Reward}: Points for safe navigation; penalties for collisions.
            \item \textbf{Policy}: Rules for navigation based on state (e.g., slowing near pedestrians).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Differences from Other Machine Learning Paradigms - Overview}
    \begin{itemize}
        \item Machine learning paradigms: supervised, unsupervised, and reinforcement learning.
        \item Importance of understanding differences for problem solving.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Differences from Other Machine Learning Paradigms - Supervised Learning}
    \frametitle{Supervised Learning}
    \begin{itemize}
        \item \textbf{Definition}: Learns from labeled data (input-output pairs).
        \item \textbf{Goals}: Predict output for new, unseen inputs.
        \item \textbf{Examples}:
            \begin{itemize}
                \item \textbf{Classification}: Email spam detection.
                \item \textbf{Regression}: House pricing prediction.
            \end{itemize}
        \item \textbf{Key Points}:
            \begin{itemize}
                \item Requires labeled data.
                \item Trains on past examples.
                \item Common algorithms: linear regression, decision trees, neural networks.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Differences from Other Machine Learning Paradigms - Unsupervised and Reinforcement Learning}
    \frametitle{Unsupervised and Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Unsupervised Learning}:
            \begin{itemize}
                \item \textbf{Definition}: Works with unlabeled data to find patterns.
                \item \textbf{Goals}: Discover structures within the data.
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item Clustering customer behavior.
                        \item Dimensionality Reduction (e.g., PCA).
                    \end{itemize}
                \item \textbf{Key Points}:
                    \begin{itemize}
                        \item No labeled data required.
                        \item Useful for exploratory analysis.
                        \item Common algorithms: k-means, hierarchical clustering.
                    \end{itemize}
            \end{itemize}
    
        \item \textbf{Reinforcement Learning}:
            \begin{itemize}
                \item \textbf{Definition}: Agent learns through interaction with the environment.
                \item \textbf{Goals}: Maximize cumulative rewards over time.
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item Game playing (e.g., chess).
                        \item Robotics (e.g., maze navigation).
                    \end{itemize}
                \item \textbf{Key Points}:
                    \begin{itemize}
                        \item Trial and error learning.
                        \item Relies on rewards for feedback.
                        \item Common algorithms: Q-learning, SARSA.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Summary of Key Differences}
    \begin{tabular}{|l|l|l|l|}
        \hline
        & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} & \textbf{Reinforcement Learning} \\
        \hline
        \textbf{Data Type} & Labeled data & Unlabeled data & Exploratory interaction \\
        \hline
        \textbf{Learning Process} & Mapping inputs to outputs & Finding patterns in data & Maximizing cumulative rewards \\
        \hline
        \textbf{Feedback Mechanism} & Direct feedback (labels) & No explicit feedback & Delayed feedback (rewards) \\
        \hline
        \textbf{Typical Applications} & Classification, regression & Clustering, dimensionality reduction & Game AI, robotics \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]{Conclusion}
    \begin{itemize}
        \item Understanding these paradigms aids in selecting appropriate methods.
        \item Each paradigm serves distinct roles in solving diverse problems effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Q-learning and SARSA}
    \begin{itemize}
        \item \textbf{Reinforcement Learning (RL):}
        \begin{itemize}
            \item Agents aim to maximize rewards through interactions with their environment.
            \item Learning is achieved by exploring actions and observing outcomes.
        \end{itemize}
        \item \textbf{Focus Algorithms:}
        \begin{itemize}
            \item Q-learning and SARSA are model-free algorithms for solving RL problems.
            \item The goal is to learn a policy that maximizes cumulative rewards.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning}
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Off-Policy Learning:} Learns the value of the optimal policy independently of the agent's actions.
            \item Updates Q-values using the maximum future reward, irrespective of the chosen action.
        \end{itemize}
    \end{block}

    \begin{block}{Q-learning Formula}
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
        \end{equation}
        Where:
        \begin{itemize}
            \item $Q(s, a)$: Current value of action $a$ in state $s$.
            \item $\alpha$: Learning rate (0 < $\alpha$ ≤ 1).
            \item $r$: Reward received for action $a$.
            \item $\gamma$: Discount factor (0 ≤ $\gamma$ < 1).
            \item $s'$: New state after action $a$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of Q-learning}
    \begin{enumerate}
        \item Initialize $Q(s, a)$ for all state-action pairs to arbitrary values.
        \item For each episode:
        \begin{enumerate}
            \item Choose action $a$ based on an exploration strategy (e.g., $\epsilon$-greedy).
            \item Take action, observe reward $r$ and next state $s'$.
            \item Update Q-value using the Q-learning formula.
        \end{enumerate}
        \item Repeat until convergence.
    \end{enumerate}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Exploration vs. Exploitation: A balance is crucial in learning.
            \item Convergence: Q-learning guarantees convergence to the optimal policy under certain conditions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA (State-Action-Reward-State-Action)}
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{On-Policy Learning:} Learns the value of the policy being followed.
            \item Updates Q-values based on the action actually taken in the next state.
        \end{itemize}
    \end{block}

    \begin{block}{SARSA Formula}
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
        \end{equation}
        Where:
        \begin{itemize}
            \item $a'$ is the action taken from the new state $s'$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of SARSA}
    \begin{enumerate}
        \item Initialize $Q(s, a)$ for all state-action pairs.
        \item For each episode:
        \begin{enumerate}
            \item Choose action $a$ using an exploration strategy.
            \item Take action, receive reward $r$, and observe next state $s'$.
            \item Select next action $a'$ based on the current policy.
            \item Update Q-value using the SARSA formula.
        \end{enumerate}
        \item Repeat until convergence.
    \end{enumerate}
    
    \begin{block}{Applications}
        \begin{itemize}
            \item Widely used in gaming, robotics, and optimization problems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Pseudocode}
    \begin{lstlisting}[language=Python]
# Initialize Q-table
Q = initialize_Q_table()

# Q-learning algorithm
for each episode:
    s = initial_state()
    while not done:
        a = choose_action(s)  # ε-greedy action
        s', r = take_action(s, a)  # Environment response
        Q[s, a] += alpha * (r + gamma * max(Q[s', :]) - Q[s, a])
        s = s'

# SARSA algorithm
for each episode:
    s = initial_state()
    a = choose_action(s)
    while not done:
        s', r = take_action(s, a)
        a' = choose_action(s')  # Next action using policy
        Q[s, a] += alpha * (r + gamma * Q[s', a'] - Q[s, a])
        s = s'; a = a'
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation Metrics - Introduction}
    To quantitatively assess the effectiveness of reinforcement learning algorithms such as Q-learning and SARSA, it's essential to use appropriate performance evaluation metrics. This allows for consistent comparison and understanding of how well the algorithms learn and make decisions based on their environment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation Metrics - Key Metrics}
    \begin{block}{1. Key Performance Metrics}
        \begin{itemize}
            \item \textbf{Cumulative Reward (Total Reward)}:
                The main goal of reinforcement learning is to maximize the cumulative reward over time.
                \begin{equation}
                R_t = \sum_{k=0}^{T} r_{t+k}
                \end{equation}
                Where \( r_{t+k} \) is the reward received at time step \( t+k \).

            \item \textbf{Average Reward}:
                Calculated over a series of episodes to understand the average performance.
                \begin{equation}
                \text{Average Reward} = \frac{R_{total}}{N}
                \end{equation}
                Where \( R_{total} \) is the cumulative reward across \( N \) episodes.

            \item \textbf{Time to Convergence}:
                Measures how quickly an algorithm reaches an optimal policy.
                
            \item \textbf{Success Rate}:
                The fraction of episodes where the algorithm achieves the desired outcome.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing Performance}
    \begin{block}{2. Visualizing Performance}
        \begin{itemize}
            \item \textbf{Learning Curves}:
                Plot the cumulative reward vs. episodes to visualize improvements in the algorithm over time.
                \begin{itemize}
                    \item X-axis: Number of episodes
                    \item Y-axis: Cumulative reward
                \end{itemize}

            \item \textbf{Comparison Plots}:
                Use bar charts or line graphs to compare the performance metrics of different algorithms (e.g., Q-learning vs. SARSA).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Metrics}
    \begin{block}{3. Importance of Metrics}
        Emphasizing the right metrics is crucial as they provide insights into algorithm behavior:
        \begin{itemize}
            \item Understanding \textbf{instabilities} in learning.
            \item Assessing performance in \textbf{varied environments}.
            \item Selecting the best-performing model during \textbf{hyperparameter tuning}.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Conclusion}
    \begin{block}{4. Examples in Practice}
        \begin{itemize}
            \item \textbf{Example 1}:
                If Q-learning consistently gets higher cumulative rewards over a set number of episodes compared to SARSA, one might conclude Q-learning is more effective for that particular environment.

            \item \textbf{Example 2}:
                By measuring the time to convergence, a practitioner can decide if an algorithm is feasible for real-time applications.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Evaluating and visualizing algorithm performance using metrics like cumulative reward, average reward, and convergence time enhances our understanding of reinforcement learning algorithms. Proper use of these metrics allows for informed decisions regarding algorithm optimization and deployment in practical scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Key Points}
    \begin{itemize}
        \item Use \textbf{cumulative reward} and \textbf{average reward} as foundational metrics.
        \item Visualize performance through \textbf{learning curves} and \textbf{comparison plots}.
        \item Metrics help reveal insights into learning stability and algorithm choice.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advanced Techniques: Deep Reinforcement Learning - Overview}
    \begin{block}{Definition}
        Deep Reinforcement Learning (DRL) combines Reinforcement Learning (RL) strategies with Deep Learning methods.
    \end{block}
    \begin{itemize}
        \item Enhances the ability to learn complex policies.
        \item Suitable for high-dimensional state and action spaces.
        \item Applications include robotics and game playing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advanced Techniques: Deep Reinforcement Learning - Key Concepts}
    \begin{enumerate}
        \item \textbf{Reinforcement Learning (RL)}
        \begin{itemize}
            \item Agent learns by interacting with an environment.
            \item Key components:
            \begin{itemize}
                \item Agent, Environment, State (S), Action (A), Reward (R), Policy ($\pi$).
            \end{itemize}
        \end{itemize}
        \item \textbf{Deep Learning}
        \begin{itemize}
            \item Uses deep neural networks to model complex patterns.
            \item Common architectures: CNNs, RNNs, Feedforward networks.
        \end{itemize}
        \item \textbf{Deep Reinforcement Learning}
        \begin{itemize}
            \item Utilizes deep neural networks for policy or value function approximation.
            \item Goal: Maximum cumulative reward over time.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advanced Techniques: Deep Reinforcement Learning - Example Algorithms}
    \begin{enumerate}
        \item \textbf{Deep Q-Network (DQN)}
        \begin{itemize}
            \item Combines Q-Learning with neural networks.
            \item Neural network approximates Q-value function.
            \item Update rule: 
            \begin{equation}
                Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right)
            \end{equation}
            \item Experience Replay for improved learning stability.
        \end{itemize}
        \item \textbf{Proximal Policy Optimization (PPO)}
        \begin{itemize}
            \item A policy gradient method for balancing exploration and exploitation.
            \item Objective function:
            \begin{equation}
                L(\theta) = \mathbb{E}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right]
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods}
    \begin{block}{Overview}
        Explore and apply policy gradient techniques to improve learning strategies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Policy Gradient Methods?}
    \begin{itemize}
        \item Directly optimize the policy instead of the value function.
        \item Improve policies by following the gradients of expected rewards.
        \item \textbf{Key Concept:} 
            \begin{itemize}
                \item \textbf{Policy ($\pi$)}: Maps states to actions (deterministic or stochastic).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Policy Gradient Methods?}
    \begin{itemize}
        \item \textbf{Continuous Action Spaces:} Effective for high-dimensional environments.
        \item \textbf{Complex Policies:} Can approximate any probability distribution over actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Principle}
    The goal of policy gradient methods is to maximize the expected return by adjusting the policy parameters:
    \begin{equation}
        J(\theta) = E_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
    \end{equation}
    Where:
    \begin{itemize}
        \item $R(\tau)$ is the total reward from trajectory $\tau$.
        \item $\pi_\theta$ is the parameterized policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Gradient Ascent Formulation}
    To improve the policy, we perform gradient ascent:
    \begin{equation}
        \nabla J(\theta) = E_{\tau \sim \pi_\theta} \left[ \nabla \log(\pi_\theta(a|s)) R(\tau) \right]
    \end{equation}
    This shows how to adjust parameters $\theta$ based on actions and rewards.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Policy Gradient Algorithms}
    \begin{enumerate}
        \item \textbf{REINFORCE Algorithm:}
            \begin{itemize}
                \item Updates the policy after each episode.
                \item \textbf{Update Rule:}
                \begin{equation}
                    \theta \leftarrow \theta + \alpha \nabla \log(\pi_\theta(a|s)) R
                \end{equation}
            \end{itemize}
        \item \textbf{Trust Region Policy Optimization (TRPO):} 
            \begin{itemize}
                \item Ensures safe and monotonic policy improvements.
            \end{itemize}
        \item \textbf{Proximal Policy Optimization (PPO):}
            \begin{itemize}
                \item Utilizes a clipped objective for stable updates.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    An agent learning to play a game by navigating a maze:
    \begin{itemize}
        \item Starts with a random policy.
        \item Explores the maze, receiving rewards and penalties.
        \item Adjusts policy parameters to maximize average reward, refining strategy over episodes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Direct Optimization:} Beneficial for complex action spaces.
        \item \textbf{Stochastic vs. Deterministic:} Policies can either output probabilities or be point estimates.
        \item \textbf{Sample Efficiency:} Policy gradients tend to be less sample-efficient compared to value-based methods.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    Policy Gradient Methods are powerful tools in reinforcement learning that enable effective learning in various environments. 
    Understanding their implementation and differences from value-based methods is crucial for real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Overview}
    \begin{itemize}
        \item Actor-Critic methods combine two key components in reinforcement learning: the \textbf{Actor} and the \textbf{Critic}.
        \item \textbf{Actor}: Determines actions based on the current policy; maps states to actions ($\pi(s) \rightarrow a$).
        \item \textbf{Critic}: Evaluates actions by estimating the value function; helps reduce variance by providing feedback ($V(s)$ or $Q(s,a)$).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Process}
    \begin{enumerate}
        \item \textbf{Initialization}: Start with random parameters for both Actor and Critic.
        \item \textbf{Interaction with the Environment}: Actor selects actions; Critic evaluates actions based on feedback from rewards and new states.
        \item \textbf{Updating the Actor and Critic}:
            \begin{block}{Critic Update}
                \begin{equation}
                    V(s) \leftarrow V(s) + \alpha \cdot (r + \gamma V(s') - V(s))
                \end{equation}
            \end{block}
            \begin{block}{Actor Update}
                \begin{equation}
                    \theta \leftarrow \theta + \beta \cdot \nabla \log \pi(a|s; \theta) \cdot A
                \end{equation}
                where \quad $A = r + \gamma V(s') - V(s)$.
            \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Summary and Key Points}
    \begin{itemize}
        \item \textbf{Example}: A robotic agent learning to walk uses the Actor for movements while the Critic evaluates their effectiveness.
        \item \textbf{Key Points}:
            \begin{itemize}
                \item Dual approaches: Combines value-based and policy-based elements.
                \item Variance reduction: Provides a baseline for stable training.
                \item Flexibility: Effective in continuous action spaces, unlike traditional Q-learning.
            \end{itemize}
        \item \textbf{Summary}: Actor-Critic methods enable simultaneous action and learning in reinforcement learning, crucial for complex environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Reinforcement Learning - Overview}
    \begin{block}{Overview of Reinforcement Learning (RL)}
        Reinforcement Learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. It is particularly useful for sequential decision-making problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Reinforcement Learning - Key Applications}
    \begin{itemize}
        \item \textbf{Robotics:}
        \begin{itemize}
            \item Example: Autonomous robots for warehouse logistics (e.g., Amazon's Kiva robots)
            \item Application: Learn optimal paths for picking and delivering items
            \item Key Algorithm: Q-Learning for path optimization
        \end{itemize}
        
        \item \textbf{Healthcare:}
        \begin{itemize}
            \item Example: Personalized treatment regimes
            \item Application: Optimize treatment plans based on patient responses
            \item Key Algorithm: Deep Q-Networks (DQN)
        \end{itemize}
        
        \item \textbf{Finance:}
        \begin{itemize}
            \item Example: Algorithmic trading systems
            \item Application: Adapt trading strategies based on market conditions
            \item Key Algorithm: Policy Gradient methods
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Reinforcement Learning - Continued}
    \begin{itemize}
        \item \textbf{Game Playing:}
        \begin{itemize}
            \item Example: AlphaGo, which outperformed human champions in Go
            \item Application: Master complex games through self-play
            \item Key Algorithm: Monte Carlo Tree Search (MCTS) with neural networks
        \end{itemize}

        \item \textbf{Recommendation Systems:}
        \begin{itemize}
            \item Example: Content recommendation on platforms like Netflix or YouTube
            \item Application: Learn user preferences and suggest content
            \item Key Algorithm: Multi-Armed Bandit approaches
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Reinforcement Learning - Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item RL is increasingly used in various fields, solving complex dynamic problems.
            \item The adaptability of RL algorithms allows for performance improvement through feedback.
            \item Collaborative applications with other AI methods can yield synergistic benefits.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Reinforcement Learning is a powerful tool for solving real-world problems through adaptive learning. Its applications span various industries, transforming tasks through data-driven decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Framework Illustration}
    \begin{center}
        \texttt{
        [Environment] $\leftarrow$ Observe $\rightarrow$ [Agent] $\leftarrow$ Take Action $\rightarrow$ [Reward] \\
                     $\uparrow$ \\
          Update Policy (Learning) $\leftarrow$ Feedback
        }
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Research in Reinforcement Learning - Overview}
    \begin{block}{Overview of Reinforcement Learning (RL)}
        Reinforcement Learning is a subfield of machine learning where an agent learns to make decisions by interacting with an environment to optimize its behavior over time.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Agent}: The learner or decision-maker (e.g., a robot).
        \item \textbf{Environment}: The system with which the agent interacts.
        \item \textbf{Actions (A)}: The set of all possible moves the agent can make.
        \item \textbf{States (S)}: All possible situations the agent can find itself.
        \item \textbf{Rewards (R)}: Feedback from the environment based on the agent’s actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Research in Reinforcement Learning - Innovations}
    \begin{block}{Recent Innovations in Reinforcement Learning}
        \begin{enumerate}
            \item \textbf{Deep Reinforcement Learning (DRL)}:
                \begin{itemize}
                    \item Combines deep learning with reinforcement learning for high-dimensional state spaces.
                    \item \textit{Example}: AlphaGo, which defeated a professional Go player.
                \end{itemize}
            
            \item \textbf{Multi-Agent Reinforcement Learning (MARL)}:
                \begin{itemize}
                    \item Focuses on multiple agents in the same environment, enabling cooperative or competitive strategies.
                    \item \textit{Example}: Self-driving cars coordinating in traffic.
                \end{itemize}
            
            \item \textbf{Model-Based Reinforcement Learning}:
                \begin{itemize}
                    \item Creates models of the environment for better action planning.
                    \item \textit{Example}: Robotic arms simulating tasks before executing them.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Research in Reinforcement Learning - Critiques and Gaps}
    \begin{block}{Critiques of Current Research}
        \begin{itemize}
            \item \textbf{Sample Inefficiency}: Algorithms often require vast amounts of data.
            \item \textbf{Generalization}: Struggles to transfer learned skills across tasks.
            \item \textbf{Sparse Rewards}: Performance can degrade with infrequent rewards.
        \end{itemize}
    \end{block}

    \begin{block}{Gaps in Research}
        \begin{enumerate}
            \item \textbf{Transfer Learning}: Methods to apply knowledge from one task to another.
            \item \textbf{Explainability and Interpretability}: Clarifying decision-making processes of RL systems.
            \item \textbf{Robustness and Safety}: Creating agents reliable in dynamic environments.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Research in Reinforcement Learning - Conclusion}
    \begin{block}{Conclusion}
        The field of Reinforcement Learning is rapidly advancing through innovations that tackle traditional challenges. Yet notable gaps remain that researchers must address for enhanced efficiency, applicability, and safety in various domains.
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Learning through interaction is fundamental in RL.
            \item Innovations like DRL and MARL expand RL's capabilities.
            \item Addressing critiques and gaps is vital for further growth.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Reinforcement Learning}
    \begin{block}{Formula Snippet for Key Concepts}
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
        \end{equation}
        Where:
        \begin{itemize}
            \item \(Q(s, a)\): Estimated value of action \(a\) in state \(s\)
            \item \(\alpha\): Learning rate
            \item \(\gamma\): Discount factor
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in AI - Introduction}
    \begin{block}{Introduction to Ethical Challenges in Reinforcement Learning (RL)}
        Reinforcement Learning (RL) enables agents to learn optimal actions through interactions with their environment. However, deploying RL technologies introduces ethical challenges that require careful consideration to promote responsible AI development.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in AI - Key Ethical Challenges}
    \begin{enumerate}
        \item \textbf{Bias in Training Data}
            \begin{itemize}
                \item RL agents learn from data that may carry societal biases.
                \item \textit{Example:} An RL system for hiring could perpetuate historical biases in the training data.
            \end{itemize}
        \item \textbf{Lack of Transparency}
            \begin{itemize}
                \item RL algorithms may operate as "black boxes," making decisions hard to understand.
                \item \textit{Example:} An autonomous vehicle may make dangerous decisions without clear explanations.
            \end{itemize}
        \item \textbf{Unintended Consequences}
            \begin{itemize}
                \item Agents can find loopholes in reward structures, leading to harmful actions.
                \item \textit{Example:} A factory optimization RL agent could bypass safety protocols.
            \end{itemize}
        \item \textbf{Accountability and Responsibility}
            \begin{itemize}
                \item Clarity is needed on who is responsible for actions taken by RL systems.
                \item \textit{Example:} Financial losses due to a trading algorithm raise issues about liability.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in AI - Proposed Solutions}
    \begin{enumerate}
        \item \textbf{Fairness in Data Collection}
            \begin{itemize}
                \item Ensure training data represents diverse demographics to reduce bias.
            \end{itemize}
        \item \textbf{Explainable AI (XAI) Techniques}
            \begin{itemize}
                \item Utilize transparency layers and interpretability tools for better decision insight.
            \end{itemize}
        \item \textbf{Robust Reward Structures}
            \begin{itemize}
                \item Design ethical reward systems that prevent harmful behaviors.
                \item \textit{Example:} Penalties for risky actions to promote safety.
            \end{itemize}
        \item \textbf{Clear Legal Frameworks}
            \begin{itemize}
                \item Establish guidelines to ensure accountability for AI systems' actions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in AI - Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Continuous evaluation of RL systems is necessary.
            \item Stakeholder engagement across various fields is crucial for comprehensive solutions.
            \item User education on RL systems is vital for informed engagement.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Addressing ethical considerations in RL is crucial for societal benefit, promoting trust and safety in AI applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Midterm Presentation Guidelines}
    \begin{block}{Objectives}
        \begin{itemize}
            \item \textbf{Demonstrate Understanding:} Showcase your grasp of the research topic and its ethical relevance in AI, particularly reinforcement learning.
            \item \textbf{Engage Your Audience:} Connect with peers to spark discussion on the ethical dimensions of your topic.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Midterm Presentation Structure}
    \begin{enumerate}
        \item \textbf{Introduction (1-2 minutes):}
            \begin{itemize}
                \item Briefly introduce the selected research topic.
                \item Explain its importance in ethical AI.
            \end{itemize}

        \item \textbf{Research Findings (3-5 minutes):}
            \begin{itemize}
                \item Present main findings concisely.
                \item Use statistics or case studies, e.g.,
                    \begin{itemize}
                        \item "75\% of AI researchers believe ethical guidelines are essential."
                        \item "Case Study: GPT-3's biases highlight ethical implications."
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continuing the Structure}
    \begin{enumerate}[resume]
        \item \textbf{Ethical Considerations (2-3 minutes):}
            \begin{itemize}
                \item Identify specific ethical challenges.
                \item Offer potential solutions or frameworks to address these challenges.
                    \item For example, "Implementing transparent algorithms can mitigate biases."
            \end{itemize}

        \item \textbf{Conclusion (1-2 minutes):}
            \begin{itemize}
                \item Recap key points and findings.
                \item Suggest future directions for better ethical standards in AI.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Grading Criteria and Tips for Success}
    \begin{block}{Grading Criteria}
        \begin{itemize}
            \item \textbf{Content Mastery:} Depth and relevance of research.
            \item \textbf{Organization:} Clarity and logical flow.
            \item \textbf{Engagement:} Audience involvement.
            \item \textbf{Presentation Skills:} Clear speech and effective use of visuals.
        \end{itemize}
    \end{block}
    \begin{block}{Tips for Success}
        \begin{itemize}
            \item \textbf{Practice:} Rehearse multiple times.
            \item \textbf{Seek Feedback:} Present to peers or mentors.
            \item \textbf{Engage:} Foster interaction and encourage questions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Preparation is Key:} Research and structure presentation effectively.
        \item \textbf{Ethical AI is Crucial:} Focus on ethics for responsible innovation.
        \item \textbf{Expect Questions:} Engage with peers post-presentation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points Covered in the Course}
    \begin{enumerate}
        \item \textbf{Introduction to Reinforcement Learning (RL)}:
        \begin{itemize}
            \item Agents learn to make decisions to maximize cumulative rewards.
            \item Key elements: agent, environment, states, actions, rewards, and policy.
        \end{itemize}
        
        \item \textbf{Core Concepts}:
        \begin{itemize}
            \item \textbf{Markov Decision Processes (MDPs)}: Formalizes RL environment (states, actions, transition probabilities, rewards).
            \item \textbf{Q-Learning and Value Iteration}:
            \begin{itemize}
                \item Model-free approach to find optimal policies.
                \item Q-learning update rule:
                \begin{equation}
                    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_a Q(s', a) - Q(s, a) \right)
                \end{equation}
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts and Advanced RL Techniques}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Exploration vs. Exploitation}:
        \begin{itemize}
            \item Balancing exploration of new actions and exploitation of known actions.
        \end{itemize}
        
        \item \textbf{Advanced RL Techniques}:
        \begin{itemize}
            \item \textbf{Deep Q-Networks (DQN)}:
            \begin{itemize}
                \item Integrates deep learning with Q-learning for high-dimensional states.
            \end{itemize}
            \item \textbf{Policy Gradient Methods}:
            \begin{itemize}
                \item Direct optimization of policy function, examples include REINFORCE.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Applications of RL}:
        \begin{itemize}
            \item Game playing, robotics, autonomous vehicles, finance, recommendation systems.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Research Directions in Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Sample Efficiency}:
        \begin{itemize}
            \item Algorithms that learn from fewer interactions with the environment.
        \end{itemize}
        
        \item \textbf{Transfer Learning}:
        \begin{itemize}
            \item Improve learning in similar tasks by transferring knowledge.
        \end{itemize}
        
        \item \textbf{Exploration Strategies}:
        \begin{itemize}
            \item Enhancing methods to improve learning efficiency in sparse reward environments.
        \end{itemize}
        
        \item \textbf{Multi-Agent RL}:
        \begin{itemize}
            \item Investigate interaction and learning among multiple agents.
        \end{itemize}
        
        \item \textbf{Safety and Robustness}:
        \begin{itemize}
            \item Design RL algorithms ensuring safe behavior under uncertainty.
        \end{itemize}
        
        \item \textbf{Explainability}:
        \begin{itemize}
            \item Increase understanding of RL models to build trust in critical applications.
        \end{itemize}
    \end{enumerate}

    \textbf{Conclusion:} Embracing the challenges of RL is crucial for innovation and practical implementations.
\end{frame}


\end{document}