\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Title Page Information
\title[Chapter 5: Policy Gradient Methods]{Chapter 5: Policy Gradient Methods}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Policy Gradient Methods}
    \begin{block}{What are Policy Gradient Methods?}
        Policy gradient methods are algorithms in reinforcement learning that optimize the policy directly by mapping states to actions and improving the policy based on gradients of expected rewards.
    \end{block}
    
    \begin{block}{Why are Policy Gradient Methods Important?}
        \begin{itemize}
            \item \textbf{Direct Steering}: Suitable for continuous action spaces in complex environments like robotics and gaming.
            \item \textbf{High-Dimensional Action Spaces}: Effective where optimal actions are difficult to determine.
            \item \textbf{Flexibility}: Can be combined with value function approaches to enhance overall learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How do Policy Gradient Methods Work?}
    \begin{enumerate}
        \item \textbf{Policy Representation}:
            \begin{itemize}
                \item Denoted as \( \pi(a|s) \): probability of taking action \( a \) in state \( s \).
                \item Can be deterministic or stochastic.
            \end{itemize}
        
        \item \textbf{Objective Function}:
            \begin{equation}
                J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} r_t \right]
            \end{equation}
            where \( \tau \) is a trajectory and \( r_t \) is the reward at time \( t \).

        \item \textbf{Gradient Estimate}:
            \begin{equation}
                \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla \log \pi_\theta(a_t|s_t) R(\tau) \right]
            \end{equation}
            \( R(\tau) \) is the total return following actions taken by the policy.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Application}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Versatility}: Adaptable for both discrete and continuous action spaces.
            \item \textbf{End-to-End Training}: Policies can be trained independently across episodes.
            \item \textbf{Drawbacks}: Variance in gradient estimates; often requires techniques like baselines or actor-critic methods for stabilization.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Application}
        Consider a robot navigating a maze using policy gradient methods, exploring various actions and adjusting its policy based on rewards (e.g., reaching goals).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Policy Gradients - Overview}
    \begin{itemize}
        \item Definition of policy gradients
        \item Contrast with value-based methods
        \item Key concepts and motivations
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Policy Gradients?}
    \begin{itemize}
        \item Policy Gradient methods optimize the policy directly.
        \item They parameterize the policy and improve it using gradient ascent based on rewards received.
    \end{itemize}
    \begin{block}{Key Concept: Policy}
        \begin{itemize}
            \item \textbf{Policy ($\pi$)}: A function mapping states ($s$) to probabilities of actions ($a$).
            \item Example: $\pi(a|s)$ is the probability of taking action $a$ in state $s$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Motivation for Policy Gradients}
    \begin{itemize}
        \item Suitable for large or continuous action spaces where value-based methods face challenges.
        \item Facilitate effective exploration and exploitation in complex environments.
    \end{itemize}
    
    \begin{block}{Contrasting with Value-Based Methods}
        \begin{itemize}
            \item \textbf{Value-Based Methods}: Focus on estimating value functions (e.g., Q-values).
            \item Examples include Q-Learning and Deep Q-Networks (DQN).
            \item Optimization is done via maximizing the value function using techniques like Temporal Difference (TD) learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods Overview}
    \begin{itemize}
        \item \textbf{Definition}: Parameterize the policy directly, optimizing it by maximizing expected returns using gradient ascent.
        \item \textbf{Examples}: REINFORCE algorithm, Proximal Policy Optimization (PPO).
        \item \textbf{Objective Function}:
        \begin{equation}
            J(\theta) = E_{\tau \sim \pi_\theta} [R(\tau)]
        \end{equation}
        where $R(\tau)$ is the total reward of trajectory $\tau$, and $\theta$ are the policy parameters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Challenges of Policy Gradients}
    \begin{block}{Advantages}
        \begin{itemize}
            \item Suitable for continuous action spaces.
            \item Allow for stochastic policies, representing uncertainty in action selection.
            \item Directly optimize high-dimensional action spaces.
        \end{itemize}
    \end{block}

    \begin{block}{Challenges}
        \begin{itemize}
            \item High variance in gradient estimations, often needing variance reduction techniques.
            \item Lower sample efficiency compared to value-based methods.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Formula}
    \begin{equation}
        \nabla J(\theta) = E_{s_t \sim \pi_\theta} \left[ \nabla \log \pi_\theta(a_t | s_t) R_t \right]
    \end{equation}
    \begin{itemize}
        \item Where $R_t$ is the total expected return from time $t$ onward.
        \item $\nabla \log \pi_\theta(a_t | s_t)$ is the gradient of the log probability of the chosen action.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaway}
    \begin{itemize}
        \item Policy gradient methods represent a powerful framework in reinforcement learning.
        \item They focus on optimizing policies directly, beneficial for complex and high-dimensional problems.
        \item Despite challenges like variance and sample inefficiency, mastering these methods is crucial for reinforcement learning.
    \end{itemize}
    \begin{block}{Key Takeaway}
        Policy gradients enable a direct approach to policy optimization, essential for tackling various challenges in reinforcement learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Theoretical Foundations - Part 1}
    % Introduction to Policy Gradient Methods
    \begin{block}{Introduction to Policy Gradient Methods}
        \begin{itemize}
            \item Policy gradient methods optimize policies directly in reinforcement learning.
            \item Focus on maximizing expected cumulative reward by adjusting policy parameters.
            \item Differ from value-based methods that estimate value functions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Theoretical Foundations - Part 2}
    % Mathematical formulation of Expected Rewards
    \begin{block}{Mathematical Formulation}
        \begin{itemize}
            \item \textbf{Expected Rewards:}
            \[
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
            \]
            \begin{itemize}
                \item \(J(\theta)\): expected reward.
                \item \(\tau\): trajectory (sequence of states and actions).
                \item \(R(\tau)\): total reward over trajectory.
            \end{itemize}

            \item \textbf{Policy Objective Function:}
            \[
            \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \nabla_\theta \log \pi_\theta(a|s) \right]
            \]
            \begin{itemize}
                \item How changes in \(\theta\) affect expected rewards.
                \item \(\nabla_\theta \log \pi_\theta(a|s)\): score function indicating parameter adjustment direction.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Theoretical Foundations - Part 3}
    % Key Points and Example
    \begin{block}{Key Points}
        \begin{itemize}
            \item Direct policy optimization vs value-based methods.
            \item Exploration vs exploitation in policy adjustment.
            \item Variance reduction techniques: reward normalization, baselines.
        \end{itemize}
    \end{block}

    \begin{block}{Example: Visualizing the Objective Function}
        \begin{itemize}
            \item Simple environment with actions A and B.
            \item If action A yields higher rewards than B, gradient indicates increasing action A's probability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE Algorithm - Overview}
    \begin{block}{Overview}
        The REINFORCE algorithm is a foundational method in Reinforcement Learning (RL) 
        representing a type of policy gradient method. This algorithm directly optimizes the policy 
        by adjusting its parameters in the direction that increases expected rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE Algorithm - Structure}
    \begin{enumerate}
        \item \textbf{Initialization:} Start with a policy parameterized by $\theta$.
        \item \textbf{Episode Generation:} Collect states, actions, and rewards: $(s_t, a_t, r_t)$.
        \item \textbf{Return Calculation:} Compute the return $G_t$ as:
        \begin{equation}
            G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
        \end{equation}
        \item \textbf{Policy Update:} Update the policy parameters using:
        \begin{equation}
            \theta \leftarrow \theta + \alpha \nabla J(\theta)
        \end{equation}
        where 
        \begin{equation}
            \nabla J(\theta) = \frac{1}{T} \sum_{t=0}^{T} \nabla \log \pi(a_t | s_t; \theta) G_t
        \end{equation}
        \item \textbf{Iteration:} Repeat until convergence.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE Algorithm - Key Points}
    \begin{itemize}
        \item \textbf{Stochastic Policy:} Works with stochastic policies, outputting a distribution over actions.
        \item \textbf{Advantages:}
        \begin{itemize}
            \item Simple to implement.
            \item Directly optimizes expected rewards.
        \end{itemize}
        \item \textbf{Limitations:}
        \begin{itemize}
            \item High variance in returns, leading to unstable updates.
            \item Sample inefficiency due to high episode requirements.
            \item Lacks bootstrapping capabilities.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE Algorithm - Closing Thoughts}
    The REINFORCE algorithm serves as a stepping stone to more advanced policy gradient methods. 
    It lays the groundwork for enhancements such as Actor-Critic approaches, which help mitigate 
    the limitations observed in REINFORCE. Understanding this algorithm is crucial for further 
    exploration in reinforcement learning techniques.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Overview}
    \begin{block}{Definition}
        Actor-Critic methods are a class of reinforcement learning algorithms that combine policy-based and value-based approaches.
    \end{block}
    \begin{itemize}
        \item \textbf{Actor}: Selects actions based on the current policy.
        \item \textbf{Critic}: Evaluates the actions taken and provides feedback on their value.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Components}
    \begin{block}{Policy (Actor)}
        The policy is the strategy determining action selection, often represented as:
        \begin{equation}
        \pi(a|s; \theta)
        \end{equation}
        where \( \theta \) are the parameters.
    \end{block}
    
    \begin{block}{Value Function (Critic)}
        The value function estimates the expected return from a state or state-action pair, typically represented by:
        \begin{equation}
        V(s; w)
        \end{equation}
        where \( w \) denote the critic's parameters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Learning Process}
    \begin{enumerate}
        \item \textbf{Interaction with Environment}:
            \begin{itemize}
                \item The agent takes actions based on the actor and receives rewards.
            \end{itemize}
        
        \item \textbf{Critic Feedback}:
            \begin{itemize}
                \item Evaluates action using the advantage function:
                \begin{equation}
                A_t = r_t + \gamma V(s_{t+1}; w) - V(s_t; w)
                \end{equation}
            \end{itemize}
        
        \item \textbf{Policy Update}:
            \begin{equation}
            \theta \leftarrow \theta + \alpha \cdot A_t \nabla_{\theta} \log \pi(a_t | s_t; \theta)
            \end{equation}
        
        \item \textbf{Value Function Update}:
            \begin{equation}
            L(w) = \frac{1}{2}(v_t - V(s_t; w))^2
            \end{equation}
        \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Variations of Policy Gradient Methods}
    \begin{block}{Overview}
        Policy Gradient Methods are a class of reinforcement learning techniques that directly optimize the policy used to select actions. This presentation focuses on two prominent variations: 
        \begin{itemize}
            \item \textbf{Proximal Policy Optimization (PPO)}
            \item \textbf{Trust Region Policy Optimization (TRPO)}
        \end{itemize}
        Both address key challenges in policy optimization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Proximal Policy Optimization (PPO)}
    \begin{block}{Concept}
        PPO balances performance and training stability using a clipped objective function to restrict policy updates.
    \end{block}

    \begin{block}{Clipped Objective Function}
        \begin{equation}
            L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
        \end{equation}
        where:
        \begin{itemize}
            \item \( r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \)
            \item \( \hat{A}_t \) is the estimated advantage at time \(t\).
            \item \( \epsilon \) controls the range of policy updates.
        \end{itemize}
    \end{block}

    \begin{block}{Key Features}
        \begin{itemize}
            \item Simple to implement and tune.
            \item Robust and high sample efficiency.
            \item Effective across various environments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Trust Region Policy Optimization (TRPO)}
    \begin{block}{Concept}
        TRPO uses a theoretical framework ensuring policy updates do not move too far from the current policy by constraining the KL divergence.
    \end{block}

    \begin{block}{Objective and Constraint}
        \begin{equation}
            \text{maximize } \mathbb{E}_t \left[ \hat{A}_t \right] \quad \text{subject to } \mathbb{E}_t \left[ D_{KL}\left( \pi_{\theta_{old}} || \pi_\theta \right) \right] \leq \delta
        \end{equation}
        where \( D_{KL} \) measures divergence and \( \delta \) is a threshold.
    \end{block}

    \begin{block}{Key Features}
        \begin{itemize}
            \item Conservative policy changes due to the "trust region."
            \item More computationally intensive; requires solving constrained optimization.
            \item Mitigates policy collapse and instability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Policy Gradients - Introduction}
    \begin{itemize}
        \item Policy Gradient Methods are reinforcement learning algorithms that optimize policies by gradient ascent on expected rewards.
        \item Effective in complex environments where traditional value-based methods struggle.
        \item Capable of handling:
        \begin{itemize}
            \item High-dimensional action spaces
            \item Continuous action spaces
            \item Partially observable states
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Policy Gradients - Key Applications}
    \begin{enumerate}
        \item \textbf{Robotics}
        \begin{itemize}
            \item Example: Training robotic arms to grasp objects.
            \item Policy gradients allow learning intricate tasks through trial and error.
        \end{itemize}
        
        \item \textbf{Game Playing}
        \begin{itemize}
            \item Example: Playing games like Atari and Go.
            \item State-of-the-art performance achieved via self-play and exploration through algorithms like PPO and TRPO.
        \end{itemize}
        
        \item \textbf{Autonomous Driving}
        \begin{itemize}
            \item Example: Navigation in complex environments.
            \item Learn driving policies by interacting with realistic simulations.
        \end{itemize}
        
        \item \textbf{Finance}
        \begin{itemize}
            \item Example: Portfolio management.
            \item Reinforcement learning agents learn trading strategies based on market simulations.
        \end{itemize}
        
        \item \textbf{Natural Language Processing}
        \begin{itemize}
            \item Example: Dialogue systems and machine translation.
            \item Policy gradients help maximize user engagement and contextual relevance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Policy Gradients - Importance and Challenges}
    \begin{block}{Importance and Advantages}
        \begin{itemize}
            \item \textbf{Flexibility:} Handles stochastic policies for exploration in uncertain environments.
            \item \textbf{Direct Policy Optimization:} Avoids complications from value function estimation.
            \item \textbf{Scalability:} Suitable for high-dimensional spaces without easy discretization.
        \end{itemize}
    \end{block}

    \begin{block}{Challenges and Considerations}
        \begin{itemize}
            \item \textbf{High Variance:} Estimates of gradients can lead to unstable training; techniques like Baseline subtraction help.
            \item \textbf{Sample Inefficiency:} Requires significant training data which can be costly.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementation of Policy Gradient Algorithms}
    \begin{block}{Overview}
        This presentation covers the implementation of a policy gradient method in reinforcement learning. 
        Key topics include:
        \begin{itemize}
            \item Understanding Policy Gradient Methods
            \item Key Concepts
            \item Implementation Steps using Python (PyTorch)
            \item Summary and Key Points
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Understanding Policy Gradient Methods}
    \begin{itemize}
        \item Policy Gradient methods optimize the policy directly.
        \item They adjust parameters based on expected returns.
        \item Unlike value-based methods, these methods choose actions based on a parameterized probability distribution.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Policy Representation:} 
        A policy $\pi$ is represented as a neural network. 
        Input: state, Output: action probabilities.
        \item \textbf{Objective Function:} 
        The aim is to maximize the expected return 
        \begin{equation}
            J(\theta) = E[R]
        \end{equation}
        where $R$ is the return from a state following the policy.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation Steps in Python (Part 1)}
    \begin{enumerate}
        \item \textbf{Install Required Libraries:}
        \begin{lstlisting}
        pip install torch gym
        \end{lstlisting}
        
        \item \textbf{Environment Setup:}
        \begin{lstlisting}
        import gym
        env = gym.make('CartPole-v1')  # Example environment
        \end{lstlisting}
        
        \item \textbf{Define the Policy Network:}
        \begin{lstlisting}
        import torch.nn as nn
        import torch.optim as optim

        class PolicyNetwork(nn.Module):
            def __init__(self, input_dim, output_dim):
                super(PolicyNetwork, self).__init__()
                self.fc = nn.Linear(input_dim, 128)
                self.relu = nn.ReLU()
                self.output = nn.Linear(128, output_dim)
                self.softmax = nn.Softmax(dim=-1)

            def forward(self, x):
                x = self.relu(self.fc(x))
                return self.softmax(self.output(x))
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation Steps in Python (Part 2)}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Define the Training Function:}
        \begin{lstlisting}
        def train(env, policy_net, optimizer, num_episodes=1000, max_steps=500):
            for episode in range(num_episodes):
                state = env.reset()
                rewards = []
                log_probs = []
                
                for step in range(max_steps):
                    state_tensor = torch.FloatTensor(state)
                    action_probs = policy_net(state_tensor)
                    action = torch.multinomial(action_probs, 1).item()  # Sampling action
                    next_state, reward, done, _ = env.step(action)
                    
                    log_probs.append(torch.log(action_probs[action]))
                    rewards.append(reward)
                    
                    if done:
                        break
                    state = next_state

                # Update Policy
                optimizer.zero_grad()
                returns = compute_returns(rewards)
                loss = -torch.stack(log_probs).sum() * returns  # Policy gradient loss
                loss.backward()
                optimizer.step()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation Steps in Python (Part 3)}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Compute Returns:}
        \begin{lstlisting}
        def compute_returns(rewards, discount_factor=0.99):
            returns = []
            R = 0
            for r in reversed(rewards):
                R = r + discount_factor * R
                returns.insert(0, R)
            return torch.FloatTensor(returns)
        \end{lstlisting}
        
        \item \textbf{Initialization and Execution:}
        \begin{lstlisting}
        input_dim = env.observation_space.shape[0]
        output_dim = env.action_space.n
        policy_net = PolicyNetwork(input_dim, output_dim)
        optimizer = optim.Adam(policy_net.parameters(), lr=0.01)

        train(env, policy_net, optimizer)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item Policy Gradient Basics:
        Focus on adjusting policy parameters using gradients from sampled returns.
        \item Neural Network Structure:
        A neural network enables flexible policy approximation.
        \item Exploration vs. Exploitation:
        Important to balance exploring diverse actions and exploiting known benefits.
    \end{itemize}
    
    \begin{block}{Summary}
        Implementing policy gradient methods involves:
        \begin{itemize}
            \item defining a policy,
            \item collecting trajectories,
            \item updating the policy based on the returns received.
        \end{itemize}
        This serves as a foundation for advanced reinforcement learning techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Performance Evaluation - Introduction}
    Performance evaluation of policy gradient methods is crucial to understand their effectiveness in solving reinforcement learning (RL) problems. This section discusses:
    \begin{itemize}
        \item Key metrics for assessment
        \item Sampling efficiency
        \item Convergence properties
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Performance Evaluation - Key Metrics}
    \begin{enumerate}
        \item \textbf{Average Return:}
        \begin{itemize}
            \item Measures cumulative reward over episodes.
            \item Formula:  
            \begin{equation}
                R = \frac{1}{N} \sum_{i=1}^{N} R_i  
            \end{equation}
            (where \( R_i \) is the return of episode \( i \) and \( N \) is total episodes)
        \end{itemize}

        \item \textbf{Sample Efficiency:}
        \begin{itemize}
            \item Evaluates number of samples needed for performance level.
            \item Higher efficiency indicates quicker learning.
        \end{itemize}

        \item \textbf{Convergence Properties:}
        \begin{itemize}
            \item Speed and reliability of reaching optimal solutions.
            \begin{itemize}
                \item \textbf{Rate of Convergence:} How fast the algorithm approaches optimal policy.
                \item \textbf{Stability:} Resilience against variations.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Performance Evaluation - Techniques}
    Techniques for evaluating policy gradient methods:
    \begin{enumerate}
        \item \textbf{Multiple Experiments:}
        \begin{itemize}
            \item Conduct runs with different random seeds and average results.
        \end{itemize}

        \item \textbf{Learning Curves:}
        \begin{itemize}
            \item Visualize average return vs. number of episodes.
            \item \textit{Python Code Example:}
            \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

episodes = [i for i in range(len(average_returns))]
plt.plot(episodes, average_returns)
plt.title('Learning Curve')
plt.xlabel('Episodes')
plt.ylabel('Average Return')
plt.show()
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Hyperparameter Sensitivity Analysis:}
        \begin{itemize}
            \item Analyze impact of hyperparameters on performance.
        \end{itemize}

        \item \textbf{Comparison with Baselines:}
        \begin{itemize}
            \item Evaluate against simpler algorithms for benchmarking.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Future Directions in Policy Gradient Methods}
    \begin{block}{Overview}
        Policy Gradient Methods (PGMs) have shown great promise in reinforcement learning (RL) but face several challenges. This presentation discusses the hurdles in deploying PGMs effectively and outlines future research directions to enhance their performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Challenges}
    \begin{enumerate}
        \item \textbf{High Variance in Gradient Estimates}
            \begin{itemize}
                \item Explanation: High variance can lead to unstable learning and convergence issues.
                \item Example: The REINFORCE algorithm offers unbiased estimates with high variance, causing slow convergence.
                \item Mitigation: Variance reduction techniques (e.g., using baselines).
            \end{itemize}
        \item \textbf{Sample Inefficiency}
            \begin{itemize}
                \item Explanation: Requires a large number of samples, which can be computationally expensive.
                \item Example: Limited data environments make sufficient sample acquisition impractical.
                \item Future Direction: Explore methods to enhance sample efficiency using advanced function approximation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Challenges (continued)}
    \begin{enumerate}
        \setcounter{enumi}{2} % continue numbering from previous frame
        \item \textbf{Suboptimal Local Optima}
            \begin{itemize}
                \item Explanation: PGMs may converge to suboptimal policies due to local minima.
                \item Example: Complex environments may trap the policy in a less optimal strategy.
                \item Future Direction: Investigate methods like simulated annealing to escape local minima.
            \end{itemize}
        \item \textbf{Combining Exploration and Exploitation}
            \begin{itemize}
                \item Explanation: Balancing new action exploration and leveraging known actions is challenging.
                \item Example: Over-exploitation can lead to missing better strategies.
                \item Future Direction: Integrate exploration strategies (e.g., $\epsilon$-greedy methods) to improve learning.
            \end{itemize}
        \item \textbf{Complexity of Policy Representation}
            \begin{itemize}
                \item Explanation: Designing efficient architectures that generalize across states is difficult.
                \item Example: Deep reinforcement learning struggles with creating efficient networks while maintaining performance.
                \item Future Direction: Research on expressive policy representations, like hierarchical reinforcement learning.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Research Directions}
    \begin{enumerate}
        \item \textbf{Hybrid Approaches}
            \begin{itemize}
                \item Integrating PGMs with value-based methods to create robust frameworks.
            \end{itemize}
        \item \textbf{Improved Algorithms}
            \begin{itemize}
                \item Developing adaptive algorithms that adjust based on training performance feedback.
            \end{itemize}
        \item \textbf{Addressing Computational Demand}
            \begin{itemize}
                \item Optimizing performance on resource-constrained devices through lightweight models.
            \end{itemize}
        \item \textbf{Robustness to Environment Changes}
            \begin{itemize}
                \item Exploring transfer learning and meta-RL for quick adaptability to new tasks.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item PGMs present unique challenges, including variance in gradient estimates and sample inefficiency.
        \item Future research should focus on hybrid methods, improved algorithms, and enhancing robustness to realize the full potential of PGMs in practical applications.
    \end{itemize}
    \begin{block}{Note}
        Understanding these challenges and future directions is vital for advancing the applications of policy gradient methods in various domains, from robotics to game AI.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in RL - Introduction}
    \begin{itemize}
        \item \textbf{Ethics in AI}: The integration of ethical considerations into AI technologies is crucial for positive societal impact.
        \item \textbf{Importance of Responsible AI}: Understanding the consequences of decisions made by RL systems is vital to prevent harm and promote fairness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in RL - Key Ethical Concerns}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}:
            \begin{itemize}
                \item RL algorithms may learn biased behaviors from training data or reward structures.
                \item \textit{Example}: An RL agent in a game may favor certain strategies, leading to unfair experiences.
            \end{itemize}
        \item \textbf{Transparency and Explainability}:
            \begin{itemize}
                \item Complex models from policy gradient methods obscure understanding, affecting trust and accountability.
                \item \textit{Example}: In accidents caused by self-driving cars, understanding decision-making is crucial.
            \end{itemize}
        \item \textbf{Safety and Control}:
            \begin{itemize}
                \item Ensuring safe operations in unpredictable environments is essential.
                \item \textit{Example}: A robot arm optimizing for speed may risk human safety.
            \end{itemize}
        \item \textbf{Long-term Consequences}:
            \begin{itemize}
                \item RL agents should consider long-term impacts and potential adverse outcomes.
                \item \textit{Example}: Maximizing short-term engagement in apps can lead to addiction.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in RL - Best Practices}
    \begin{itemize}
        \item \textbf{Define Clear Ethical Guidelines}: Establish a framework for ethical decision-making, addressing bias and transparency.
        \item \textbf{Continuous Monitoring and Evaluation}: Ongoing assessments to identify and correct ethical issues.
        \item \textbf{Stakeholder Involvement}: Engage diverse stakeholders for a comprehensive understanding of impacts.
        \item \textbf{Adopt Explainable AI (XAI) Techniques}: Techniques that improve explainability enhance user trust and accountability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in RL - Conclusion and Key Takeaways}
    \begin{itemize}
        \item Ensuring ethical standards enhances trustworthiness and aligns AI with societal values.
        \item \textbf{Key Takeaways}:
            \begin{itemize}
                \item Ethics is integral to RL framework design.
                \item Transparency, fairness, and safety are critical ethical pillars.
                \item Engaging with stakeholders enhances ethical outcomes.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Recap of Policy Gradient Methods}
    
    \begin{itemize}
        \item Policy Gradient methods are crucial in reinforcement learning (RL) as they directly optimize the policy.
        \item They differ from value-based methods by modeling the probability distribution over actions given states.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Advantages of Policy Gradient Methods}
    
    \begin{enumerate}
        \item \textbf{Continuous Action Spaces:} Effective in environments like robotic control.
        \item \textbf{High-Dimensional Action Spaces:} Efficient in scenarios such as game playing and autonomous driving.
        \item \textbf{Stochastic Policies:} Able to explore and exploit simultaneously, enhancing performance in uncertain environments.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Example and Future Impact}
    
    \begin{block}{Example: REINFORCE Algorithm}
        The REINFORCE algorithm updates policy parameters \( \theta \) using:
        \[
        \theta \leftarrow \theta + \alpha \cdot \nabla J(\theta)
        \]
        where
        \[
        \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla \log \pi_\theta(a_t | s_t) G_t \right]
        \]
    \end{block}
    
    \textbf{Future Advancements:}
    \begin{itemize}
        \item \textbf{Healthcare:} Personalized treatment strategies.
        \item \textbf{Gaming:} Intelligent non-playable characters (NPCs).
        \item \textbf{Autonomous Systems:} Enhanced performance in dynamic environments.
    \end{itemize}
\end{frame}


\end{document}