\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 7: Supervised Learning Algorithms}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning}
    \begin{block}{Overview}
        Supervised learning is a type of machine learning where the model is trained on a labeled dataset. This enables the model to learn the relationship between input features and output targets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Supervised Learning?}
    \begin{itemize}
        \item \textbf{Features (Inputs)}: Characteristics or attributes of the data used for prediction.
        \item \textbf{Labels (Outputs)}: Known results or target values that the model is trying to predict.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Machine Learning}
    \begin{block}{Importance}
        Supervised learning is foundational due to its wide range of applications, effectiveness, and ease of interpretation.
    \end{block}
    \begin{itemize}
        \item \textbf{Healthcare}: Diagnosing diseases based on patient data.
        \item \textbf{Finance}: Predicting stock prices or credit risks.
        \item \textbf{Marketing}: Classifying customer behaviors and preferences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Does It Work?}
    \begin{enumerate}
        \item \textbf{Data Collection}: Gather a substantial amount of labeled data relevant to the task.
        \item \textbf{Training Phase}: Use algorithms to learn from the training data by identifying patterns.
        \item \textbf{Evaluation}: Test the model against unseen data to evaluate accuracy and performance.
        \item \textbf{Prediction}: Apply the trained model to new data to generate predictions.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms}
    \begin{itemize}
        \item \textbf{Linear Regression}: For predicting continuous values.
        \item \textbf{Logistic Regression}: For binary classification tasks.
        \item \textbf{Decision Trees}: Useful for both classification and regression; offers a visual representation of decisions and their possible consequences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Spam Detection}
    \begin{block}{Example}
        Imagine developing a model to predict whether an email is \textbf{spam} or \textbf{not spam}. 
    \end{block}
    \begin{itemize}
        \item \textbf{Features}: Frequency of certain words, sender's address, subject line.
        \item \textbf{Label}: "spam" or "not spam".
    \end{itemize}
    The algorithm learns from a training set of emails labeled accordingly and then predicts the label of new incoming emails.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Supervised learning depends on \textbf{labeled data}.
        \item It is most effective when ample and representative data is available.
        \item It encompasses a variety of algorithms suited for different types of tasks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Resources}
    \begin{block}{Hands-on Experience}
        Platforms like \textbf{Kaggle} and \textbf{Google Colab} provide datasets and Python libraries (e.g., Scikit-learn) to practice supervised learning techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Supervised learning serves as a powerful framework for machine learning, enabling models to make informed predictions based on previous examples. Its applications span numerous industries, making it integral to advancements in artificial intelligence. Understanding this concept is crucial as we explore different types of supervised learning algorithms in detail in the next section.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning Algorithms - Introduction}
    \begin{block}{Introduction to Supervised Learning}
        Supervised learning is a type of machine learning where the model is trained on labeled data, meaning that both the input features and the corresponding output labels are provided. 
    \end{block}
    The goal is to learn a mapping from inputs to outputs, enabling the model to make predictions on new, unseen data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning Algorithms - Main Types}
    \begin{itemize}
        \item \textbf{Regression}
        \item \textbf{Classification}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning Algorithms - Regression}
    \begin{block}{Definition}
        Regression algorithms are used when the output variable is continuous. The goal is to predict a numerical value based on input data.
    \end{block}
    
    \textbf{Common Algorithms:}
    \begin{itemize}
        \item \textbf{Linear Regression}
            \begin{itemize}
                \item Formula: \( y = mx + b \)
                \item Example: Predicting house prices based on features like size, number of bedrooms, and location.
            \end{itemize}
        \item \textbf{Polynomial Regression}
        \item \textbf{Support Vector Regression (SVR)}
    \end{itemize}
    
    \textbf{Key Points:}
    \begin{itemize}
        \item Outputs are continuous values.
        \item Performance measured using metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning Algorithms - Classification}
    \begin{block}{Definition}
        Classification algorithms predict categorical outcomes, assigning input data into discrete classes.
    \end{block}
    
    \textbf{Common Algorithms:}
    \begin{itemize}
        \item \textbf{Logistic Regression}
            \begin{itemize}
                \item Formula: \( P(y=1 | x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + ... + \beta_nx_n)}} \)
                \item Example: Classifying emails as spam or not spam.
            \end{itemize}
        \item \textbf{Decision Trees}
        \item \textbf{Random Forest}
        \item \textbf{Support Vector Machines (SVM)}
    \end{itemize}
    
    \textbf{Key Points:}
    \begin{itemize}
        \item Outputs are discrete classes.
        \item Evaluation metrics include accuracy, precision, recall, and F1-score.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the types of supervised learning algorithms is crucial as they lay the foundation for building predictive models. 

    Choosing between regression and classification depends on the nature of the target variable, guiding the selection of the most appropriate algorithm for a given problem.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Introduction}
    \begin{block}{Definition}
        Linear regression is a fundamental algorithm in supervised learning used to model the relationship between a dependent variable (outcome) and one or more independent variables (predictors).
    \end{block}
    
    \begin{itemize}
        \item Primarily used to predict continuous outcomes
        \item Important in fields like economics, biology, and engineering
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Formula}
    \begin{block}{Simple Linear Regression}
        \begin{equation}
            y = \beta_0 + \beta_1 x + \epsilon 
        \end{equation}
    \end{block}
    
    Where:
    \begin{itemize}
        \item \( y \): dependent variable (what we want to predict)
        \item \( x \): independent variable (the predictor variable)
        \item \( \beta_0 \): y-intercept (value of \( y \) when \( x = 0 \))
        \item \( \beta_1 \): slope (impact of \( x \) on \( y \))
        \item \( \epsilon \): error term (difference between observed and predicted values)
    \end{itemize}

    \begin{block}{Multiple Linear Regression}
        \begin{equation}
            y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon 
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Key Concepts}
    \begin{enumerate}
        \item \textbf{Modeling Relationship:} Assumes a linear relationship between input variables (\( x \)) and output variable (\( y \)).
        \item \textbf{Fitting the Model:} The objective is to find coefficients (\( \beta_0, \beta_1, \ldots, \beta_n \)) that minimize prediction errors.
    \end{enumerate}
    
    \begin{block}{Example}
        \begin{itemize}
            \item Predicting house prices based on their size.
            \item Sample Data:
            \end{itemize}
            \[
            \begin{array}{|c|c|}
            \hline
            \text{Size (sq ft)} & \text{Price (\$)} \\
            \hline
            1500 & 300,000 \\
            2000 & 400,000 \\
            2500 & 500,000 \\
            \hline
            \end{array}
            \]
        \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Linear Regression - Part 1}
    \begin{block}{1. Cost Function}
        \begin{itemize}
            \item **Definition**: Quantifies model performance by measuring the difference between predicted and actual values.
            \item **Formula**: For a dataset with \( n \) data points:
            \[
            J(\theta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2
            \]
            \item \( y_i \) = actual value, \( \hat{y_i} \) = predicted value
            \item **Key Point**: The goal is to minimize the cost function to enhance prediction accuracy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Linear Regression - Part 2}
    \begin{block}{2. Gradient Descent}
        \begin{itemize}
            \item **Definition**: An optimization algorithm to minimize the cost function.
            \item **Process**:
                \begin{itemize}
                    \item Start with initial parameters.
                    \item Compute the gradient of the cost function.
                    \item Update parameters:
                    \[
                    \theta_j = \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
                    \]
                    \end{itemize}
            \item **Key Point**: The learning rate \( \alpha \) is critical for convergence speed.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Linear Regression - Part 3}
    \begin{block}{3. Model Evaluation Metrics}
        \begin{itemize}
            \item **R-squared (R²)**:
                \begin{itemize}
                    \item Measures the proportion of variability explained by the model.
                    \item Formula:
                    \[
                    R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
                    \]
                    \item **Interpretation**: Closer to 1 = good fit; closer to 0 = poor fit.
                \end{itemize}
            \item **Mean Squared Error (MSE)**:
                \begin{itemize}
                    \item Measures average squared difference between predicted and actual values.
                    \item Formula:
                    \[
                    MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2
                    \]
                    \item **Key Point**: Lower MSE = better model performance.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps}
    \begin{block}{Summary}
        \begin{itemize}
            \item Understanding cost function, gradient descent optimization, and evaluation metrics (R² and MSE) is essential for effective linear regression modeling.
        \end{itemize}
    \end{block}
    
    \begin{block}{Next Steps}
        \begin{itemize}
            \item Next, we will explore implementing linear regression in Python with practical examples and code snippets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Linear Regression - Overview}
    \begin{itemize}
        \item Linear regression is a foundational algorithm in supervised learning.
        \item Used to model relationships between a dependent variable and independent variables.
        \item This guide covers implementation using Python with practical code snippets and examples.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Step 1: Import Necessary Libraries}
    \begin{block}{Import Required Libraries}
        To begin, import essential libraries for data manipulation, visualization, and machine learning.
    \end{block}
    \begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Steps 2 to 7}
    \begin{enumerate}
        \item Load the dataset
        \begin{lstlisting}[language=Python]
data = pd.read_csv('house_prices.csv')
X = data[['Size']]
y = data['Price']
        \end{lstlisting}
        
        \item Split the data
        \begin{lstlisting}[language=Python]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        \end{lstlisting}
        
        \item Create and fit the model
        \begin{lstlisting}[language=Python]
model = LinearRegression()
model.fit(X_train, y_train)
        \end{lstlisting}
        
        \item Make predictions
        \begin{lstlisting}[language=Python]
y_pred = model.predict(X_test)
        \end{lstlisting}
        
        \item Evaluate the model
        \begin{lstlisting}[language=Python]
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')
print(f'R-squared: {r2:.2f}')
        \end{lstlisting}
        
        \item Visualize the results
        \begin{lstlisting}[language=Python]
plt.scatter(X_test, y_test, color='blue', label='Actual Prices')
plt.scatter(X_test, y_pred, color='red', label='Predicted Prices')
plt.xlabel('Size')
plt.ylabel('Price')
plt.title('Actual vs Predicted House Prices')
plt.legend()
plt.show()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Implementing Linear Regression - Key Points}
    \begin{itemize}
        \item \textbf{Cost Function}: Linear regression minimizes the cost function measuring the difference between actual and predicted values.
        \item \textbf{Gradient Descent}: This optimization algorithm minimizes the cost function by iteratively adjusting model parameters.
        \item \textbf{Model Evaluation}: Use metrics like MSE and R-squared for accuracy. Lower MSE indicates a better fit; R-squared closer to 1 indicates a strong model.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementing Linear Regression - Conclusion}
    By following these steps, you can effectively implement a linear regression model in Python, analyze data, and visualize results. This foundational knowledge paves the way for exploring more complex supervised learning algorithms.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Decision Trees}
    
    \begin{block}{Overview}
        A \textbf{Decision Tree} is a flowchart-like structure used in machine learning for making decisions based on the values of input features. It serves as a model that maps observations about data to conclusions, either as classifications (categorical outcomes) or predictions (continuous outcomes).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of Decision Trees}
    
    \begin{enumerate}
        \item \textbf{Root Node}: The topmost node that represents the entire dataset or population. It is split into sub-nodes based on a feature.
        
        \item \textbf{Internal Nodes}: Represent features of the dataset. Each denotes a test on a feature, with branches representing the outcome of the test.
        
        \item \textbf{Leaf Nodes}: Terminal nodes that represent the final outcome (class labels for classification tasks or predicted values for regression tasks).
        
        \item \textbf{Edges/Branches}: The lines connecting nodes, indicating the flow from question to answer.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work}
    
    \begin{itemize}
        \item Use a recursive partitioning approach:
        \begin{itemize}
            \item Start from the root node and make decisions based on feature splits.
            \item Continue until stopping criteria are met (e.g., all data in the node belong to a single class, or depth limit reached).
        \end{itemize}

        \item \textbf{Example}: Predicting whether a person will play tennis based on weather conditions:
        \begin{itemize}
            \item \textbf{Root Node}: Weather conditions (e.g., sunny, overcast, rainy).
            \item Splits based on humidity (high or normal) or wind (weak or strong) leading to different leaf nodes: \textbf{“Play”} or \textbf{“Do Not Play”}.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Utilization for Classification and Regression}
    
    \begin{enumerate}
        \item \textbf{Classification Tasks}:
        \begin{itemize}
            \item Classify data into distinct categories.
            \item E.g., classifying emails as \textbf{Spam} or \textbf{Not Spam} based on features like subject line and sender.
        \end{itemize}
        
        \item \textbf{Regression Tasks}:
        \begin{itemize}
            \item Predict numerical values.
            \item E.g., predicting house prices based on features like size (sq ft), number of bedrooms, etc.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item Intuitive and easy to interpret.
        \item Can handle both categorical and continuous data.
        \item Prone to overfitting; pruning and setting a maximum depth can be beneficial.
        \item Support both classification and regression tasks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formulas}
    
    \begin{block}{Gini Impurity}
        \begin{equation}
        Gini(D) = 1 - \sum_{i=1}^{C} p_i^2
        \end{equation}
        Where \(C\) is the number of classes and \(p_i\) is the probability of class \(i\).
    \end{block}
    
    \begin{block}{Entropy}
        \begin{equation}
        Entropy(D) = - \sum_{i=1}^{C} p_i \log_2(p_i)
        \end{equation}
    \end{block}

    \begin{itemize}
        \item Each node's split maximizes the information gain (reducing uncertainty in the target variable).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Decision Trees - Introduction}
    \begin{block}{Introduction to Decision Trees}
        Decision trees are a popular supervised learning algorithm used for both classification and regression tasks. They model decisions and their possible consequences as a tree-like structure, enabling clear visualization and interpretation. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Decision Trees - Splitting Criteria}
    \begin{block}{Splitting Criteria}
        To determine how to split the data at each node, two primary measures of impurity are commonly used:
        \begin{enumerate}
            \item \textbf{Gini Impurity:}
                \begin{itemize}
                    \item \textbf{Definition:} Measures the impurity of a node; a lower Gini score indicates a purer node.
                    \item \textbf{Formula:}
                    \begin{equation}
                        Gini = 1 - \sum (p_i)^2
                    \end{equation}
                    \item \textbf{Example:} For a node with 80\% of instances in class A and 20\% in class B:
                    \begin{equation}
                        Gini = 1 - (0.8^2 + 0.2^2) = 0.32
                    \end{equation}
                \end{itemize}
            \item \textbf{Entropy:}
                \begin{itemize}
                    \item \textbf{Definition:} Measures the amount of randomness in the data; quantifies uncertainty in predicting the output class.
                    \item \textbf{Formula:}
                    \begin{equation}
                        Entropy = -\sum (p_i \log_2 p_i)
                    \end{equation}
                    \item \textbf{Example:} For the same node:
                    \begin{equation}
                        Entropy \approx 0.72
                    \end{equation}
                \end{itemize}
        \end{enumerate}
        \textbf{Key Point:} The goal in using these criteria is to maximize the purity of the nodes after every split.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Decision Trees - Tree Pruning}
    \begin{block}{Tree Pruning Techniques}
        Pruning helps to reduce the size of the decision tree and combat overfitting by removing parts of the tree that provide little predictive power.
        \begin{enumerate}
            \item \textbf{Pre-Pruning (Early Stopping):}
                \begin{itemize}
                    \item Stops tree growth early based on defined criteria, such as minimum sample size at a node.
                    \item Example: If Gini impurity decrease is below a threshold, no split is made.
                \end{itemize}
            \item \textbf{Post-Pruning:}
                \begin{itemize}
                    \item Involves growing the full tree first and then removing branches based on validation data.
                    \item Techniques:
                    \begin{itemize}
                        \item Cost Complexity Pruning: Balances tree size against performance through penalties on added nodes.
                    \end{itemize}
                \end{itemize}
        \end{enumerate}
        \textbf{Key Point:} Pruning enhances generalization and improves predictions on unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Decision Trees - Overview}
    \begin{itemize}
        \item \textbf{What is a Decision Tree?}
        \begin{itemize}
            \item A flowchart-like structure for classification and regression.
            \item Splits data into subsets based on input feature values.
            \item Consists of decision nodes and leaf nodes (outcomes).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Decision Trees - Key Steps}
    \begin{enumerate}
        \item \textbf{Import Required Libraries}
            \begin{lstlisting}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt
            \end{lstlisting}

        \item \textbf{Load and Prepare the Dataset}
            \begin{lstlisting}
from sklearn.datasets import load_iris

# Load dataset and create a DataFrame
iris = load_iris()
X = pd.DataFrame(data=iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)
            \end{lstlisting}

        \item \textbf{Split the Data}
            \begin{lstlisting}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
            \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Decision Trees - Modeling and Evaluation}
    \begin{enumerate}[resume]
        \item \textbf{Building the Decision Tree Model}
            \begin{lstlisting}
# Initialize and fit the decision tree classifier
clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
clf.fit(X_train, y_train)
            \end{lstlisting}

        \item \textbf{Visualizing the Decision Tree}
            \begin{lstlisting}
plt.figure(figsize=(12,8))
tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.title("Decision Tree Visualization")
plt.show()
            \end{lstlisting}

        \item \textbf{Making Predictions}
            \begin{lstlisting}
y_pred = clf.predict(X_test)
            \end{lstlisting}

        \item \textbf{Evaluating the Model}
            \begin{lstlisting}
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
            \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Decision Trees - Key Points & Summary}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize}
        \begin{itemize}
            \item Visualization helps in understanding decision-making processes.
            \item Adjust hyperparameters like \texttt{max\_depth} and \texttt{criterion} for better performance.
            \item Understanding impurity measures (Gini, entropy) is crucial for splits.
        \end{itemize}
        
        \item \textbf{Summary}
        \begin{itemize}
            \item Steps include loading data, splitting, fitting the model, and evaluation.
            \item Tools like \texttt{sklearn} make the process accessible for all skill levels.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Model Evaluation for Supervised Learning}
    \begin{block}{Introduction to Model Evaluation}
        Model evaluation is a critical step in the supervised learning process. It allows us to assess how well our model predicts outcomes based on training data.
    \end{block}
    \begin{block}{Importance}
        Effective evaluation ensures that our model will perform well on unseen data and provides insights into potential improvements.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Metrics for Evaluation}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \item \textbf{Precision}
        \item \textbf{Recall (Sensitivity)}
        \item \textbf{F1 Score}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics - Details}
    \begin{itemize}
        \item \textbf{Accuracy}:
        \begin{itemize}
            \item Definition: Proportion of correctly identified instances.
            \item Formula: 
            \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
            \end{equation}
            \item Example: If a model makes 80 correct predictions out of 100, accuracy = 80\%.
        \end{itemize}
        
        \item \textbf{Precision}:
        \begin{itemize}
            \item Definition: Ratio of correctly predicted positive observations to total predicted positives.
            \item Formula: 
            \begin{equation}
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \end{equation}
            \item Example: If 50 out of 70 predicted positive cases are actually positive, precision = 71.4\%.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics - More Details}
    \begin{itemize}
        \item \textbf{Recall (Sensitivity)}:
        \begin{itemize}
            \item Definition: Ratio of correctly predicted positive observations to actual positives.
            \item Formula:
            \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \end{equation}
            \item Example: If the model identifies 50 out of 100 actual positive cases, recall = 50\%.
        \end{itemize}

        \item \textbf{F1 Score}:
        \begin{itemize}
            \item Definition: Harmonic mean of precision and recall.
            \item Formula:
            \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item Example: If precision = 0.7 and recall = 0.5, F1 score ≈ 0.58.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Visual Representation and Conclusions}
    \begin{block}{Confusion Matrix}
        \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
            \hline
            \textbf{Actual Positive} & TP & FN \\
            \hline
            \textbf{Actual Negative} & FP & TN \\
            \hline
        \end{tabular}
        \end{center}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Trade-offs exist; use multiple metrics for evaluation.
            \item Evaluation metrics guide improvements in models.
            \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Application}
    \begin{block}{Python Code Snippet}
        \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Example predictions
y_true = [1, 0, 1, 1, 0, 0, 1, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 0, 1]

# Calculating metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{block}{Understanding Metrics}
        Understanding evaluation metrics is crucial for assessing the performance of supervised learning models. These metrics measure performance and guide improvements, ensuring models are reliable and effective for real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Supervised Learning}
    \begin{block}{Overview}
        Supervised learning, while powerful in making predictions based on input data, often raises significant ethical concerns. 
        As we develop and apply these algorithms, it is crucial to be aware of issues like bias and fairness that can impact individuals and communities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bias in Supervised Learning}
    \begin{itemize}
        \item \textbf{Definition:} Bias refers to systematic errors in the predictions of a model due to prejudiced data or assumptions.
        \item \textbf{Sources of Bias:}
        \begin{itemize}
            \item \textbf{Data Collection:} Non-representative samples can overlook certain groups, leading to skewed predictions.
            \item \textbf{Labeling Bias:} Different interpretations by human annotators can introduce bias.
        \end{itemize}
        \item \textbf{Example:} A recruitment algorithm trained on biased historical hiring data may favor certain demographic groups, perpetuating discrimination.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fairness in Model Predictions}
    \begin{itemize}
        \item \textbf{Definition:} Fairness seeks to ensure that model outcomes are equitable across different groups.
        \item \textbf{Types of Fairness:}
        \begin{itemize}
            \item \textbf{Individual Fairness:} Similar individuals should receive similar outcomes (e.g., candidates evaluated equally regardless of background).
            \item \textbf{Group Fairness:} Statistical measures of accuracy should be equal across groups (e.g., balanced positive predictions across genders).
        \end{itemize}
        \item \textbf{Fairness Metric:}
        \begin{equation}
        \text{Disparate Impact Ratio} = \frac{P(\text{positive outcome | group A})}{P(\text{positive outcome | group B})}
        \end{equation}
        A ratio below 0.8 typically indicates potential bias.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Addressing Ethical Issues}
    \begin{itemize}
        \item \textbf{Consequences of Ignoring Bias:}
        \begin{itemize}
            \item \textbf{Reinforcement of Inequality:} Ignoring bias can worsen social inequalities.
            \item \textbf{Legal Repercussions:} Organizations may face lawsuits for biased algorithms violating anti-discrimination laws.
        \end{itemize}
        \item \textbf{Trust and Acceptance of AI:} Ethical implementations enhance trust and facilitate broader acceptance of AI technologies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Always assess your data for bias before modeling.
        \item Employ fairness metrics to quantify and address potential discrimination in model predictions.
        \item Regularly evaluate and update models to adapt to changing societal norms and data distributions.
    \end{itemize}
    \begin{block}{Conclusion}
        Integrating ethical considerations into supervised learning processes is essential for creating fair and effective technologies, fostering accountability, and improving outcomes for all users.
    \end{block}
\end{frame}


\end{document}