\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 9: Unsupervised Learning Algorithms}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning}
    \begin{block}{What is Unsupervised Learning?}
        Unsupervised learning is a type of machine learning that identifies patterns in data without labeled outcomes. It explores the inherent structure of data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Discover Hidden Patterns}: Helps in discovering the underlying structure in data without pre-existing labels.
        \item \textbf{Data Preprocessing}: Useful for dimensionality reduction and feature extraction.
        \item \textbf{Anomaly Detection}: Identifies outliers or unusual observations that do not conform to expected patterns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Unsupervised Learning}
    \begin{enumerate}
        \item \textbf{Customer Segmentation}
            \begin{itemize}
                \item Businesses classify customers based on purchasing behavior to tailor marketing strategies.
                \item \textit{Example}: An online retailer grouping buyers who purchase similar items.
            \end{itemize}
        \item \textbf{Market Basket Analysis}
            \begin{itemize}
                \item Finding relationships between items purchased together.
                \item \textit{Example}: Customers who buy bread often also buy butter.
            \end{itemize}
        \item \textbf{Image Compression}
            \begin{itemize}
                \item Reduces the size of image files by identifying similar pixels.
                \item \textit{Example}: Clustering techniques group similar pixel values.
            \end{itemize}
        \item \textbf{Text Analysis and Topic Modeling}
            \begin{itemize}
                \item Grouping documents into topics without explicit labels.
                \item \textit{Example}: Algorithms like Latent Dirichlet Allocation (LDA) categorize articles.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms in Unsupervised Learning}
    \begin{itemize}
        \item \textbf{K-Means Clustering}
            \begin{itemize}
                \item Partitions n observations into k clusters based on feature similarity.
                \item \textit{Formula}: Minimize the sum of squared distances between points and centroid.
            \end{itemize}
        \item \textbf{Hierarchical Clustering}
            \begin{itemize}
                \item Builds a hierarchy of clusters using agglomerative or divisive approaches.
            \end{itemize}
        \item \textbf{Principal Component Analysis (PCA)}
            \begin{itemize}
                \item Reduces dimensionality while preserving variance for visualization and analysis.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Unsupervised learning does not rely on labeled data.
            \item It is essential for exploratory data analysis.
            \item Empowers organizations to make data-driven decisions by uncovering new insights.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering?}
    \begin{block}{Definition of Clustering}
        Clustering is an unsupervised learning technique that aims to group a set of objects such that objects in the same group (or cluster) are more similar to each other than to those in other groups.
    \end{block}
    
    \begin{itemize}
        \item Similarity can be defined using various metrics (e.g., Euclidean, Manhattan).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role in Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Data Exploration:} Understanding structure when labeled outcomes are not available, revealing inherent groupings.
        \item \textbf{Pre-processing:} Used as a preliminary step in other machine learning tasks to simplify datasets.
        \item \textbf{Anomaly Detection:} Identifying unusual data points that do not fit any cluster.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differences Between Clustering and Classification}
    \begin{enumerate}
        \item \textbf{Objective:} 
        \begin{itemize}
            \item Clustering: No predefined labels; goal is to find structures or patterns.
            \item Classification: Uses labeled data to categorize instances into predefined categories.
        \end{itemize}
        
        \item \textbf{Supervision:}
        \begin{itemize}
            \item Clustering: Unsupervised; no prior knowledge about group definitions.
            \item Classification: Supervised; relies on a training dataset with existing labels.
        \end{itemize}

        \item \textbf{Methods:} 
        \begin{itemize}
            \item Clustering: K-means, hierarchical clustering, DBSCAN.
            \item Classification: Decision trees, random forests, support vector machines.
        \end{itemize}

        \item \textbf{Application:}
        \begin{itemize}
            \item Clustering: Customer segmentation, image compression, organizing computing clusters.
            \item Classification: Spam detection, sentiment analysis, medical diagnosis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Clustering vs Classification}
    \begin{block}{Example}
        Consider a dataset of various fruits with attributes like color, weight, and sweetness:
        
        \begin{itemize}
            \item \textbf{Clustering:} Discovering distinct clusters; e.g., citrus fruits (oranges, lemons) vs. berries (strawberries, blueberries).
            \item \textbf{Classification:} Using a labeled dataset to learn distinguishing features and categorize new unlabeled fruits.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    \begin{itemize}
        \item Clustering is vital for exploratory data analysis.
        \item Unlike classification, clustering does not require predefined labels.
        \item It plays a significant role in various real-world applications, providing insights and informing further analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Overview}
    \begin{block}{Introduction to Clustering Techniques}
        Clustering is an unsupervised learning method that groups a set of objects such that objects in the same group (or cluster) are more similar to each other than to those in other groups. 
    \end{block}
    
    \begin{itemize}
        \item **Partitioning Methods**: Divide the dataset into distinct non-overlapping clusters.
        \item **Hierarchical Methods**: Build a hierarchy of clusters using either a bottom-up (agglomerative) or top-down (divisive) approach.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Partitioning Methods}
    \begin{block}{1. Partitioning Methods}
        Partitioning methods create distinct non-overlapping subsets (clusters) so that each data point is assigned to exactly one cluster.
    \end{block}
    
    \begin{itemize}
        \item **Key Characteristics**:
        \begin{itemize}
            \item Each cluster is represented by a centroid.
            \item The process is iterative, refining clusters until convergence.
        \end{itemize}
        
        \item **Example: K-Means Clustering**
        \begin{enumerate}
            \item Initialize: Select K initial centroids randomly.
            \item Assign: Allocate each data point to the nearest centroid.
            \item Update: Recalculate the centroid based on assigned points.
            \item Repeat: Continue until centroids do not change significantly.
        \end{enumerate}
        
        \item **Formula**:
        \begin{equation}
            J = \sum_{i=1}^{K} \sum_{x \in C_i} \|x - \mu_i\|^2
        \end{equation}
        Where:
        \begin{itemize}
            \item \(J\) = total within-cluster variance
            \item \(C_i\) = \(i^{th}\) cluster
            \item \(\mu_i\) = centroid of cluster \(C_i\)
            \item \(x\) = points in cluster \(C_i\)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Hierarchical Methods}
    \begin{block}{2. Hierarchical Methods}
        Hierarchical clustering creates a hierarchy of clusters through either a bottom-up (agglomerative) or top-down (divisive) approach.
    \end{block}

    \begin{itemize}
        \item **Key Characteristics**:
        \begin{itemize}
            \item Result is represented as a dendrogram (tree structure).
            \item Clusters can be formed by cutting the dendrogram at certain levels.
        \end{itemize}
        
        \item **Example: Agglomerative Clustering**
        \begin{enumerate}
            \item Initial State: Treat each point as a single cluster.
            \item Combine: Merge closest pairs based on a distance metric.
            \item Repeat: Continue until all points form a single cluster or desired clusters are reached.
        \end{enumerate}
        
        \item **Common Distance Metrics**:
        \begin{itemize}
            \item Euclidean distance: \(\sqrt{\sum{(x_i - y_i)^2}}\)
            \item Manhattan distance: \(\sum{|x_i - y_i|}\)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering Overview}
    % Overview of K-Means Clustering
    K-Means Clustering is a widely-used unsupervised learning algorithm aimed at partitioning data into K distinct clusters based on the features of the data points. 
    \begin{itemize}
        \item Grouping similar data points together
        \item Maximizing the distance between different clusters
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of the K-Means Algorithm}
    % Steps of the K-Means Algorithm
    \begin{enumerate}
        \item \textbf{Initialization}:
            \begin{itemize}
                \item Choose number of clusters, \(K\)
                \item Randomly select \(K\) initial centroids from data points
            \end{itemize}
        
        \item \textbf{Assignment Step}:
            \begin{itemize}
                \item Calculate distance to each centroid and assign points to nearest centroid
                \item Distance formula:
                  \begin{equation}
                    D(x_i, C_j) = \sqrt{\sum_{k=1}^{n}(x_i^{(k)} - C_j^{(k)})^2}
                  \end{equation}
            \end{itemize}
        
        \item \textbf{Update Step}:
            \begin{itemize}
                \item Recalculate centroids as the mean of assigned points
                \item New centroid:
                  \begin{equation}
                    C_j = \frac{1}{N_j} \sum_{i=1}^{N_j} x_i
                  \end{equation}
            \end{itemize}
        
        \item \textbf{Convergence Check}:
            \begin{itemize}
                \item Repeat assignment and update until centroids stabilize
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of K-Means Clustering}
    % Example to illustrate K-Means Clustering
    Consider the following data points in a 2D space:
    \begin{itemize}
        \item (1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)
    \end{itemize}
    \begin{itemize}
        \item \textbf{Choosing K}: Assume \(K=2\)
        \item \textbf{Initial Centroids}: Randomly select (1, 2) and (10, 2)
        \item \textbf{Assignment}: Assign each point to the nearest centroid
        \item \textbf{Update}: Compute new centroids based on assigned points
        \item Repeat until centroids stabilize
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Consider}
    % Important points about K-Means Clustering
    \begin{itemize}
        \item \textbf{Distance Metrics}: Primarily uses Euclidean distance; other metrics can be applied based on data characteristics.
        \item \textbf{Random Initialization}: Different initial centroids may lead to varying results; it is advisable to run multiple iterations.
        \item \textbf{Applications}: Commonly used in market segmentation, document clustering, image compression, etc.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visual Representation}
    % Visual representation recommendation
    Consider including a simple scatterplot illustrating:
    \begin{itemize}
        \item Initial centroids
        \item Data points
        \item Final clusters after convergence
    \end{itemize}
    This visual helps in understanding the clustering process intuitively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Disadvantages of K-Means - Overview}
    K-Means is a popular unsupervised learning algorithm used for clustering data into K distinct groups based on feature similarities. It's widely recognized for its:
    \begin{itemize}
        \item Simplicity
        \item Computational efficiency
        \item Versatility in various applications
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of K-Means}
    \begin{enumerate}
        \item \textbf{Computational Efficiency}:
            \begin{itemize}
                \item Time complexity: $O(n \cdot K \cdot i)$
                    \begin{itemize}
                        \item $n$ = number of data points
                        \item $K$ = number of clusters
                        \item $i$ = number of iterations until convergence
                    \end{itemize}
            \end{itemize}

        \item \textbf{Simplicity and Ease of Implementation}:
            \begin{itemize}
                \item Steps: Initialize K centroids, assign points, update centroids until convergence.
            \end{itemize}

        \item \textbf{Versatility}:
            \begin{itemize}
                \item Applicable for customer segmentation, image compression, etc.
            \end{itemize}

        \item \textbf{Scalability}:
            \begin{itemize}
                \item Handles large datasets efficiently, suitable for real-time applications.
            \end{itemize}

        \item \textbf{Deterministic}:
            \begin{itemize}
                \item Same initial conditions yield the same clustering result.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Disadvantages of K-Means}
    \begin{enumerate}
        \item \textbf{Choice of K}:
            \begin{itemize}
                \item The required number of clusters K must be specified beforehand.
            \end{itemize}

        \item \textbf{Sensitivity to Initialization}:
            \begin{itemize}
                \item Initialization affects final clusters; K-Means++ for better initial formation.
            \end{itemize}

        \item \textbf{Assumes Spherical Clusters}:
            \begin{itemize}
                \item Assumption may not hold for all datasets.
            \end{itemize}

        \item \textbf{Outlier Sensitivity}:
            \begin{itemize}
                \item Sensitive to outliers, skewing centroids and misassigning clusters.
            \end{itemize}

        \item \textbf{Not Suitable for Non-Convex Shapes}:
            \begin{itemize}
                \item Struggles with clusters that aren't convex in shape.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \textbf{Key Points:}
    \begin{itemize}
        \item K-Means is efficient and simple but requires careful consideration of K and initialization.
        \item Understand limitations related to data structure (shape, outliers).
    \end{itemize}

    \textbf{Example:}
    Consider a dataset of customer purchase behavior:
    \begin{itemize}
        \item \textbf{Ideal Situation}: K-Means groups similar customers for targeted marketing.
        \item \textbf{Challenging Situation}: Incorrect K leads to ineffective strategies.
    \end{itemize}

    \textbf{Conclusion:} K-Means is powerful when applied correctly, but understanding its limitations is crucial for effective clustering.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering}
    \begin{block}{Overview}
        Hierarchical clustering is an unsupervised learning technique that organizes data into nested clusters based on their characteristics. 
        This method is fundamentally divided into two approaches: 
        \begin{itemize}
            \item Agglomerative (bottom-up)
            \item Divisive (top-down)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Approach}
    \begin{block}{Definition}
        The agglomerative method starts with each data point as its own cluster and merges them into larger clusters.
    \end{block}
    
    \begin{block}{Process}
        \begin{enumerate}
            \item Start with \( n \) clusters (each data point is its own cluster).
            \item Calculate distances between all pairs of clusters using a metric (e.g., Euclidean distance).
            \item Merge the two closest clusters.
            \item Repeat until only one cluster remains or a designated number is reached.
        \end{enumerate}
    \end{block}

    \begin{block}{Linkage Methods}
        \begin{itemize}
            \item Single Linkage: Distance between closest points.
            \item Complete Linkage: Distance between farthest points.
            \item Average Linkage: Average distance between all points.
            \item Ward's Linkage: Minimizes total within-cluster variance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Divisive Approach}
    \begin{block}{Definition}
        The divisive method starts with all data points in one cluster and recursively splits them into smaller clusters.
    \end{block}
    
    \begin{block}{Process}
        \begin{enumerate}
            \item Start with all items in one cluster.
            \item Select a cluster to split based on a criterion (e.g., variance).
            \item Split the cluster into two sub-clusters.
            \item Repeat until each cluster contains a single item or designated number is reached.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item No predefined number of clusters.
            \item Results can be represented using dendrograms.
            \item Computationally intensive for larger datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Distance Calculation and Example Code}
    \begin{block}{Distance Calculation (Euclidean Distance)}
        For two points \( A(x_1, y_1) \) and \( B(x_2, y_2) \):
        \begin{equation}
            \text{Distance}(A, B) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
        \end{equation}
    \end{block}
    
    \begin{block}{Python Code Sample}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import AgglomerativeClustering
import numpy as np

# Sample Data
data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])

# Applying Agglomerative Clustering
model = AgglomerativeClustering(n_clusters=2, linkage='ward')
clusters = model.fit_predict(data)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation: Introduction to Dendrograms}
    A \textbf{dendrogram} is a tree-like diagram used to visualize the arrangement of clusters formed during hierarchical clustering. 
    It provides a graphical representation of the data's clustering process, illustrating how clusters are formed step-by-step or bottom-up.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation: How Dendrograms Work}
    \begin{itemize}
        \item \textbf{Hierarchical Clustering}: The dendrogram results from hierarchical clustering, either \textit{agglomerative} (bottom-up) or \textit{divisive} (top-down).
        \item In \textit{agglomerative clustering}, each data point starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation: Interpretation of a Dendrogram}
    \begin{enumerate}
        \item \textbf{Branches}: Each branch represents a cluster of points that have been merged together.
        \item \textbf{Height}: The height indicates the distance (or dissimilarity) at which clusters are merged.
        \item \textbf{Leaf Nodes}: The endpoints of the branches (leaf nodes) represent individual data points or observations.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation: Example}
    Consider a dataset with 5 data points. When plotting the dendrogram for this set:
    \begin{itemize}
        \item At the bottom, you would see 5 leaves, each representing one data point.
        \item Clusters might merge: points A and B at height 1.5, points C and D at height 2.0.
        \item One cluster containing all points merges at height 4.0.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation: Determining the Number of Clusters}
    A key functionality of a dendrogram is to help decide the appropriate number of clusters. This can be done by:
    \begin{itemize}
        \item \textbf{Cutting the Dendrogram}: A horizontal line can be drawn across the dendrogram to determine how many clusters exist below that line.
        \item \textbf{Elbow Method}: Look for “elbows” where the distance between merges significantly increases, indicating a natural division between clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation: Summary and Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Dendrograms provide a visual means of evaluating and understanding cluster structures.
            \item Height in the dendrogram represents dissimilarity; higher branches indicate more distinct clusters.
            \item An optimal number of clusters can be determined visually by assessing where to cut the dendrogram.
        \end{itemize}
    \end{block}

    Dendrograms are a powerful tool in hierarchical clustering that visually illustrates the clustering process and aids in determining an appropriate number of clusters.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation: Visual Aid Suggestion}
    Consider including a simple dendrogram sketch in your presentation. Label the height of clusters and show potential cut lines for cluster determination.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Clustering Results}
    \begin{block}{Overview}
        Evaluating clustering performance is essential for determining the quality of formed clusters. Since clustering is an unsupervised learning method, traditional metrics like accuracy are not applicable. We discuss two significant metrics: the \textbf{Silhouette Score} and the \textbf{Davies-Bouldin Index}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Silhouette Score}
    \begin{itemize}
        \item \textbf{Definition}: Measures how similar an object is to its own cluster versus other clusters. Range is from -1 to 1:
            \begin{itemize}
                \item \textbf{1}: Well-clustered points
                \item \textbf{0}: Points on the boundary
                \item \textbf{-1}: Points possibly misclassified
            \end{itemize}
        \item \textbf{Formula}:
        \begin{equation}
        s = \frac{b - a}{\max(a, b)}
        \end{equation}
        Where:
        \begin{itemize}
            \item \(a\): Average distance to points in the same cluster
            \item \(b\): Average distance to points in the nearest cluster
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Davies-Bouldin Index}
    \begin{itemize}
        \item \textbf{Definition}: Evaluates cluster separation and compactness. A lower index indicates better clustering.
        \item \textbf{Formula}:
        \begin{equation}
        DB = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \left( \frac{s_i + s_j}{d_{ij}} \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item \(k\): Number of clusters
            \item \(s_i\): Average distance of points in cluster \(i\) to its centroid (compactness)
            \item \(d_{ij}\): Distance between centroids of clusters \(i\) and \(j\) (separation)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering - Introduction}
    \begin{itemize}
        \item Clustering is an unsupervised learning technique that groups a set of objects based on similarity.
        \item Provides insights facilitating data-driven decision-making across various sectors.
        \item Applications span fields such as marketing, social networking, and image processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering - Key Applications}
    \begin{enumerate}
        \item \textbf{Market Segmentation}
            \begin{itemize}
                \item Divides a market into distinct buyer groups with different needs.
                \item Example: Retail company uses K-means to segment customers by purchasing behavior.
                \item \textbf{Benefits}: Enhanced satisfaction and increased sales.
            \end{itemize}
        
        \item \textbf{Social Network Analysis}
            \begin{itemize}
                \item Examines social structures using network and graph theories.
                \item Example: Platforms like Facebook group users based on interactions.
                \item \textbf{Benefits}: Better understanding of user behavior and targeted advertising.
            \end{itemize}

        \item \textbf{Image Processing}
            \begin{itemize}
                \item Groups pixels based on color similarity or intensity in images.
                \item Example: K-means segmentation separates objects in photographs.
                \item \textbf{Benefits}: Advanced editing, object recognition, and video analysis.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering - Conclusion}
    \begin{itemize}
        \item Clustering techniques enable insights from large datasets.
        \item Enhances strategic decision-making across various domains.
        \item Summarized Key Points:
            \begin{itemize}
                \item Powerful for identifying natural groupings in data.
                \item Utilized widely in marketing, networking, and image processing sectors.
                \item Promotes improved customer experiences and informed business strategies.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering Example}
    \begin{block}{Pseudocode}
    \begin{lstlisting}[language=python]
# K-means clustering example
def k_means(data, k):
    centroids = initialize_centroids(data, k)
    while not converged:
        labels = assign_clusters(data, centroids)
        centroids = update_centroids(data, labels, k)
    return labels, centroids
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Updating Centroids Formula}
    The new centroid for each cluster is calculated as:
    \begin{equation}
        C_j = \frac{1}{|S_j|} \sum_{x_i \in S_j} x_i
    \end{equation}
    Where \( C_j \) is the centroid of cluster \( j \), and \( S_j \) is the set of points in cluster \( j \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Importance of Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Definition}: Unsupervised learning is a type of machine learning where the model learns from unlabeled data, identifying patterns without explicit instructions.
        \item \textbf{Purpose}: Aims to explore data's underlying structure to extract meaningful insights, making it powerful for exploratory data analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Significance of Clustering Techniques}
    \begin{itemize}
        \item \textbf{Clustering}: A key unsupervised learning method that groups similar data points into clusters, aiding in pattern identification.
        \item \textbf{Common Algorithms}:
            \begin{itemize}
                \item \textbf{K-Means}: Partitions data into $K$ distinct clusters based on feature similarity.
                \item \textbf{Hierarchical Clustering}: Creates a tree-like structure by building a hierarchy of clusters.
                \item \textbf{DBSCAN}: Groups points closely packed together and marks outliers in low-density regions.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Applications and Final Thoughts}
    \begin{itemize}
        \item \textbf{Real-World Applications}:
            \begin{itemize}
                \item \textbf{Market Segmentation}: Tailors marketing strategies by identifying customer groups.
                \item \textbf{Social Network Analysis}: Identifies communities within networks, enhancing understanding of connectivity.
                \item \textbf{Image Processing}: Organizes image pixels into segments for simplified representation and object recognition.
            \end{itemize}
        \item \textbf{Key Takeaways}:
            \begin{itemize}
                \item Unsupervised learning reveals hidden structures, leading to deeper insights.
                \item Versatile across various domains: marketing, healthcare, and image analysis.
                \item Clustering serves as a preliminary step for supervised learning, aiding in feature engineering.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Final Remarks}
    \begin{itemize}
        \item Unsupervised learning and clustering techniques are vital in machine learning, allowing for exploration of data beyond predefined categories.
        \item Understanding these concepts empowers practitioners to effectively leverage modern datasets, fostering innovation and informed decision-making across various industries.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Example of K-Means}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

# Sample data
data = np.array([[1, 2], [1, 4], [1, 0],
                 [4, 2], [4, 4], [4, 0]])

# Apply K-Means
kmeans = KMeans(n_clusters=2, random_state=0).fit(data)

# Predicted cluster for each point
print(kmeans.labels_)
    \end{lstlisting}
\end{frame}


\end{document}