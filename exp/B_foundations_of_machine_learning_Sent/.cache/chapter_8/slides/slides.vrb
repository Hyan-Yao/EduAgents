\frametitle{Key Points & Python Code Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Model Selection:} Cross-validation is crucial for comparing different models and selecting the best one based on their performance metrics.
            \item \textbf{Performance Variability:} Using different folds can reveal inconsistencies that may not be visible through a simple train-test split.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

# Sample dataset
X, y = load_data()  # Placeholder for the actual data
kf = KFold(n_splits=5)

accuracies = []
model = RandomForestClassifier()

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    accuracies.append(accuracy_score(y_test, predictions))

average_accuracy = sum(accuracies) / len(accuracies)
print(f'Average Accuracy: {average_accuracy}')
    \end{lstlisting}
