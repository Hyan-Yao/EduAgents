\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 10: Unsupervised Learning - Clustering]{Week 10: Unsupervised Learning - Clustering}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning}
    \begin{block}{What is Unsupervised Learning?}
        Unsupervised learning is a category of machine learning where the algorithm operates on data without supervised labels.
        The model learns underlying patterns, structures, and relationships in the data without explicit outcome guidance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Data without Labels}:
        \begin{itemize}
            \item Unlike supervised learning, unsupervised learning deals with datasets that do not have labels.
            \item The algorithm discovers hidden structures in the data.
        \end{itemize}
        
        \item \textbf{Goal of Unsupervised Learning}:
        \begin{itemize}
            \item Identify patterns or groupings within the data.
            \item Examples include clustering, reducing dimensionality, or discovering associations among data features.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Exploratory Data Analysis (EDA)}:
        \begin{itemize}
            \item Reveals patterns in the dataset, like customer segments based on purchasing behavior.
        \end{itemize}

        \item \textbf{Pattern Recognition}:
        \begin{itemize}
            \item Classifies incoming data without labeled datasets; useful in healthcare, marketing, and finance.
        \end{itemize}

        \item \textbf{Feature Engineering}:
        \begin{itemize}
            \item Transforms raw data into structured forms for supervised learning models.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Customer Segmentation}:
        Businesses use clustering to identify different customer segments based on behavior or preferences.
        
        \item \textbf{Anomaly Detection}:
        In fraud detection, unsupervised learning discovers unusual patterns to identify potential fraud cases without predefined rules.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques in Unsupervised Learning}
    \begin{enumerate}
        \item \textbf{Clustering}:
        \begin{itemize}
            \item Groups data points into clusters based on similarity (e.g., K-means, Hierarchical Clustering).
        \end{itemize}
        
        \item \textbf{Association Rule Learning}:
        \begin{itemize}
            \item Discovers interesting relationships between variables in large databases (e.g., Market Basket Analysis).
        \end{itemize}
        
        \item \textbf{Dimensionality Reduction}:
        \begin{itemize}
            \item Reduces the number of variables under consideration (e.g., Principal Component Analysis - PCA).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Unsupervised learning plays a crucial role in machine learning by deriving insights from unlabeled data. Its applications span multiple fields, enabling businesses and researchers to unlock the hidden potential of their datasets.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Definition}
    \begin{block}{Definition}
        Clustering is a fundamental technique in unsupervised learning where the goal is to group a set of objects such that objects in the same group (or cluster) are more similar to each other than to those in other groups. 
        It helps in discovering natural groupings in data without prior labels or categories.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Purpose}
    \begin{itemize}
        \item \textbf{Data Exploration}: Identify patterns and structures in data sets.
        \item \textbf{Data Reduction}: Simplify data by reducing the number of individual data points to clusters, making it easier to analyze.
        \item \textbf{Feature Engineering}: Enable the creation of new features from raw data by working with clusters that represent shared characteristics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Key Concepts}
    \begin{itemize}
        \item \textbf{Similarity Measures}:
        \begin{itemize}
            \item \textbf{Euclidean Distance}: Measures straight-line distance between two points.
            \item \textbf{Cosine Similarity}: Measures the cosine of the angle between two vectors, useful in high-dimensional spaces.
        \end{itemize}
        
        \item \textbf{Types of Clustering}:
        \begin{itemize}
            \item \textbf{Partitioning Methods}: Divide data into non-overlapping subsets (e.g., K-means clustering).
            \item \textbf{Hierarchical Methods}: Create a tree-like structure to represent data groupings (e.g., Agglomerative clustering).
            \item \textbf{Density-Based Methods}: Identify clusters based on the density of data points in a region (e.g., DBSCAN).
        \end{itemize}
        
        \item \textbf{Example}:
        Imagine a dataset of customer purchase behaviors where clustering might reveal:
        \begin{itemize}
            \item Group A: Frequent buyers of luxury items.
            \item Group B: Occasional buyers who prefer discounts.
            \item Group C: Seasonal shoppers focusing on holiday sales.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Key Points}
    \begin{itemize}
        \item Clustering is unsupervised; no labeled outputs are needed.
        \item It reveals hidden structures in data, facilitating informed decision-making.
        \item The choice of clustering algorithm and distance measurement significantly impacts results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Applications of Clustering - Overview}
    \begin{itemize}
        \item Clustering is an unsupervised learning technique that groups similar data points based on inherent patterns.
        \item Significant applications include:
        \begin{enumerate}
            \item Market Segmentation
            \item Image Compression
            \item Social Network Analysis
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Applications of Clustering - Market Segmentation}
    \begin{block}{Market Segmentation}
        \begin{itemize}
            \item \textbf{Explanation:} Clustering groups consumers based on characteristics like demographics and buying behavior.
            \item \textbf{Example:} Retail companies can identify customer segments such as "budget shoppers," "brand loyalists," and "tech enthusiasts."
            \item \textbf{Key Point:} Effective market segmentation leads to targeted strategies, increased sales, and improved customer satisfaction.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Applications of Clustering - Image Compression}
    \begin{block}{Image Compression}
        \begin{itemize}
            \item \textbf{Explanation:} Clustering reduces image file size by grouping similar pixel colors.
            \item \textbf{Example:} The K-means algorithm can cluster pixel colors, reducing thousands of colors to a few representatives without significant quality loss.
            \item \textbf{Key Point:} Image compression enhances storage efficiency and improves transmission speed, crucial for web and mobile applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Applications of Clustering - Social Network Analysis}
    \begin{block}{Social Network Analysis}
        \begin{itemize}
            \item \textbf{Explanation:} Clustering helps identify communities within social structures based on shared interests or behaviors.
            \item \textbf{Example:} Social media platforms can discover user clusters (e.g., fitness enthusiasts) for content or ad recommendations.
            \item \textbf{Key Point:} This analysis allows organizations to target communication strategies effectively and understand user interactions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Summary and Important Considerations}
    \begin{itemize}
        \item Clustering is fundamental across various domains, providing insights that drive decisions and enhance efficiencies.
        \item Common algorithms include K-means, Hierarchical clustering, and DBSCAN.
        \item Selecting the appropriate method is crucial based on data nature and desired outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example Code Snippet for K-means}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.cluster import KMeans

# Sample customer data
data = pd.read_csv('customers.csv')
X = data[['Age', 'Annual Income (k$)']]

# Apply K-means clustering
kmeans = KMeans(n_clusters=3)
data['Segment'] = kmeans.fit_predict(X)

# View the clustered data
print(data.head())
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Clustering Methods Overview - Introduction}
    \begin{block}{Introduction to Clustering}
        Clustering is an essential technique in unsupervised learning where the goal is to group similar data points together based on certain characteristics. Understanding the different clustering methods helps in selecting the right approach for various data scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Clustering Methods Overview - Centroid-Based Clustering}
    \frametitle{Centroid-Based Clustering}
    \begin{itemize}
        \item \textbf{Concept:} Clusters are represented by a central point (centroid), typically the mean of all points in the cluster.
        \item \textbf{Key Algorithm:} K-Means
        \item \textbf{Process:}
        \begin{enumerate}
            \item Choose K initial centroids randomly.
            \item Assign each data point to the nearest centroid based on distance (most commonly Euclidean distance).
            \item Recalculate centroids as the mean of assigned points.
            \item Repeat steps 2-3 until convergence (no change in cluster assignments).
        \end{enumerate}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Sensitive to initial centroid placement.
            \item Efficient for large datasets.
            \item Works best when clusters are spherical and equally sized.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Clustering Methods Overview - Connectivity-Based Clustering}
    \frametitle{Connectivity-Based Clustering}
    \begin{itemize}
        \item \textbf{Concept:} Clustering based on the connectivity between data points, forming trees or hierarchical structures (dendrograms).
        \item \textbf{Key Algorithm:} Hierarchical Clustering
        \item \textbf{Process:}
        \begin{enumerate}
            \item Start with all points as individual clusters.
            \item Iteratively merge the closest clusters until a stopping criterion is met.
            \item Can produce a dendrogram visualizing the cluster formation.
        \end{enumerate}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Does not require a preset number of clusters.
            \item Produces a hierarchy useful for understanding the data structure.
            \item Computationally intensive for large datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Clustering Methods Overview - Distribution-Based Clustering}
    \frametitle{Distribution-Based Clustering}
    \begin{itemize}
        \item \textbf{Concept:} Assumes data points are generated from a mixture of underlying probability distributions, typically Gaussian distributions.
        \item \textbf{Key Algorithm:} Gaussian Mixture Model (GMM)
        \item \textbf{Process:}
        \begin{enumerate}
            \item Assume a certain number of distribution components.
            \item Use algorithms such as Expectation-Maximization (EM) to fit these distributions to the data.
            \item Each data point is assigned to the cluster that likely generated it.
        \end{enumerate}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Can model clusters of different shapes and sizes.
            \item More flexible than K-Means, accommodating non-spherical shapes.
            \item Requires more complex parameter tuning.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Clustering Methods Overview - Conclusion}
    \begin{block}{Conclusion}
        Understanding these clustering methods allows us to strategically choose the appropriate technique for analyzing data based on its characteristics and the research question at hand. Each method has specific advantages that make it suitable for certain types of analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to k-Means Clustering}
    \begin{block}{What is k-Means Clustering?}
        k-Means Clustering is a popular unsupervised learning algorithm used to classify data into distinct groups based on their similarity by partitioning data points into 'k' clusters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Clusters}: Groups of similar data points in the dataset.
        \item \textbf{Centroids}: The center point of each cluster, calculated as the average of all points within the cluster.
        \item \textbf{Distance Metrics}: Typically, Euclidean distance is used to find the closest centroid.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The k-Means Algorithm Process}
    \begin{enumerate}
        \item \textbf{Initialization}: Select 'k' initial centroids randomly from the data points.
        \item \textbf{Assignment Step}: Assign each data point to the nearest centroid based on the distance metric.
        \item \textbf{Update Step}: Re-calculate the centroids as the mean of all points assigned to each cluster.
        \item \textbf{Iteration}: Repeat the assignment and update steps until centroid positions stabilize.
    \end{enumerate}
    \begin{block}{Illustration}
        \includegraphics[width=0.8\linewidth]{assignment_to_centroid.png} % Placeholder for the illustration
        \caption{This illustration demonstrates the assignment of data points to the nearest centroid.}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strengths of k-Means Clustering}
    \begin{itemize}
        \item \textbf{Simplicity}: Easy to implement and understand.
        \item \textbf{Efficiency}: Converges quickly, suitable for large datasets.
        \item \textbf{Flexibility}: Works well with spherical clusters and various dimensions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Requires defining the number of clusters ($k$) beforehand.
        \item Sensitivity to initial centroid positions may lead to different clustering results.
        \item Can struggle with non-globular cluster shapes or varying cluster sizes/densities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    k-Means clustering is a foundational technique in machine learning. Understanding its process and strengths allows students to appreciate more complex clustering algorithms in advanced applications.
\end{frame}

\begin{frame}[fragile]{k-Means Algorithm Steps - Overview}
    \begin{block}{Overview of k-Means Clustering}
        K-means clustering is an unsupervised learning algorithm used to partition data into distinct groups (clusters) based on feature similarity. The objective is to minimize the variance within each cluster.
    \end{block}
\end{frame}

\begin{frame}[fragile]{k-Means Algorithm Steps - Initialization and Assignment}
    \begin{enumerate}
        \item \textbf{Initialization}:
            \begin{itemize}
                \item \textbf{Choose the Number of Clusters (k)}: 
                    Decide how many clusters to form, based on prior knowledge or methods like the Elbow method.
                \item \textbf{Randomly Initialize Centroids}: 
                    Select k random points from the dataset as the initial centroids.
                \item \textit{Example}: If k=3, pick 3 random points within the dataset.
            \end{itemize}

        \item \textbf{Assignment Phase}:
            \begin{itemize}
                \item \textbf{Assign Data Points to Nearest Centroid}: 
                    Compute the distance from each data point to each centroid and assign the point to the nearest cluster.
                \item \textbf{Formula for Euclidean distance}:
                    \begin{equation}
                        D(x_i, c_k) = \sqrt{\sum_{j=1}^{n}(x_{ij} - c_{kj})^2}
                    \end{equation}
                \item \textit{Example}: If a point is closer to centroid C1, it will be assigned to Cluster 1.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{k-Means Algorithm Steps - Update and Convergence}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Update Phase}:
            \begin{itemize}
                \item \textbf{Recalculate Centroids}: 
                    After assigning points, recalculate each cluster's centroid as the mean of all points in that cluster.
                \item \textbf{Formula for Updating Centroid}:
                    \begin{equation}
                        c_k = \frac{1}{N_k} \sum_{x_i \in Cluster_k} x_i
                    \end{equation}
                    where \( N_k \) is the number of points in cluster \( k \).
                \item \textit{Example}: If Cluster 1 has points A, B, and C, calculate their mean position for the new centroid C1.
            \end{itemize}

        \item \textbf{Convergence Check}:
            \begin{itemize}
                \item Repeat steps 2 and 3 until convergence: when point assignments no longer change or centroids remain stable.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{k-Means Algorithm Steps - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The k-means algorithm is sensitive to the initial placement of centroids; different initializations can lead to varied results.
            \item The choice of k is significant and influences the clustering structure. Techniques like the Elbow method can help.
            \item The algorithm is best suited for spherical-shaped clusters and may struggle with non-spherical shapes.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding these steps establishes a foundation for effectively implementing the k-means algorithm in data clustering applications. Next, we will explore how to determine the optimal number of clusters (k).
    \end{block}
\end{frame}

\begin{frame}[fragile]{Choosing the Right Number of Clusters (k) - Overview}
    \begin{itemize}
        \item The challenge: Determining the optimal number of clusters, \(k\).
        \item Consequences of improper choice:
        \begin{itemize}
            \item \(k\) too small: Oversimplification, merging distinct groups.
            \item \(k\) too large: Overfitting, creating artificial clusters.
        \end{itemize}
        \item Two effective methods:
        \begin{itemize}
            \item The Elbow Method
            \item The Silhouette Score
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Choosing the Right Number of Clusters (k) - The Elbow Method}
    \begin{block}{Concept}
        The Elbow Method involves plotting the Within-Cluster Sum of Squares (WCSS) against the number of clusters \(k\).
    \end{block}
    
    \begin{itemize}
        \item WCSS decreases as \(k\) increases (points closer to centroids).
        \item Steps:
        \begin{enumerate}
            \item Compute WCSS for different values of \(k\).
            \item Plot \(k\) on the x-axis and WCSS on the y-axis.
            \item Identify the "elbow" point where the decrease rate shifts.
        \end{enumerate}
        \item Example:
        \begin{itemize}
            \item For \(k=1\), high WCSS due to a single cluster.
            \item At \(k=4\), WCSS reduction begins to plateau, suggesting 4 clusters as optimal.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Choosing the Right Number of Clusters (k) - The Silhouette Score}
    \begin{block}{Concept}
        The Silhouette Score measures how close points in one cluster are to points in neighboring clusters.
    \end{block}

    \begin{itemize}
        \item Calculation for each point includes:
        \begin{itemize}
            \item \(a\): Average distance to points in the same cluster.
            \item \(b\): Average distance to points in the nearest cluster.
        \end{itemize}
        \item Silhouette coefficient \(s\):
        \begin{equation}
            s = \frac{b - a}{\max(a, b)}
        \end{equation}
        \item Interpretation:
        \begin{itemize}
            \item \(s = 1\): Well-clustered point far from nearest cluster.
            \item \(s = 0\): Point near the decision boundary of clusters.
            \item \(s = -1\): Likely misclassified to the wrong cluster.
        \end{itemize}
        \item Example: Plot average silhouette score against values of \(k\); optimal \(k\) corresponds to the highest score.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Choosing the Right Number of Clusters (k) - Key Points}
    \begin{itemize}
        \item Choosing \(k\) is crucial for effective clustering.
        \item Use the Elbow Method for visual guidance.
        \item Utilize the Silhouette Score for quantitative assessment.
        \item Both methods are complementary—should be used together.
    \end{itemize}

    \begin{block}{Useful Formulas}
        \begin{equation}
            \text{WCSS} = \sum_{i=1}^{k}\sum_{j=1}^{n_i} (x_j^{(i)} - \mu_i)^2
        \end{equation}
        where \(n_i\) is the number of points in cluster \(i\), \(x_j^{(i)}\) is the \(j\)-th point in cluster \(i\), and \(\mu_i\) is the centroid of cluster \(i\).
    \end{block}
\end{frame}

\begin{frame}[fragile]{Choosing the Right Number of Clusters (k) - Conclusion}
    \begin{itemize}
        \item Determining the optimal number of clusters is essential for successful clustering using k-Means.
        \item Engage with both the Elbow Method and Silhouette Score for informed decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Limitations of k-Means - Overview}
    \begin{block}{Introduction to k-Means}
        k-Means clustering is a widely-used unsupervised learning algorithm that partitions a dataset into \textbf{k distinct clusters} based on feature similarity. However, it has notable limitations.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Limitations of k-Means - Sensitivity to Initial Centroid Placement}
    \begin{enumerate}
        \item \textbf{Sensitivity to Initial Centroid Placement}
        \begin{itemize}
            \item The starting points (centroids) can significantly affect the final output.
            \item \textbf{Example}: Clustering circularly arranged data points can lead to poor groupings if centroids are chosen inside the circle.
            \item \textbf{Techniques to Mitigate}:
            \begin{itemize}
                \item Multiple Runs: Execute the algorithm multiple times with different initializations.
                \item k-Means++ Initialization: A smarter method to choose initial centroids.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Limitations of k-Means - Additional Concerns}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Inability to Form Non-Convex Shapes}
        \begin{itemize}
            \item k-Means assumes clusters are convex and isotropic.
            \item \textbf{Example}: Overlapping crescent-shaped clusters may be misrepresented as one.
            \item \textbf{Visual Illustration}: k-Means encircles them with spherical shapes, failing to capture their true structure.
        \end{itemize}

        \item \textbf{Fixed Number of Clusters (k)}
        \begin{itemize}
            \item The user must specify the number of clusters in advance.
            \item \textbf{Recommendation}: Use methods such as the Elbow method or Silhouette score to determine an appropriate value for k.
        \end{itemize}

        \item \textbf{Sensitivity to Outliers}
        \begin{itemize}
            \item Outliers can skew results significantly and lead to misclassification.
            \item \textbf{Example}: An outlier may pull the centroid away from the true center.
            \item \textbf{Mitigation}: Preprocess data to remove outliers before clustering.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Limitations of k-Means - Summary and Code Snippet}
    \begin{block}{Summary}
        - k-Means has limitations including sensitivity to initial centroids, inability to form non-convex clusters, predetermined cluster counts (k), and susceptibility to outliers.
        - Recognizing these limitations is vital for effective application of k-Means and may necessitate alternative methods.
    \end{block}
    
    \begin{block}{Code Snippet: k-Means Implementation Example (Python)}
        \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

# Generating random data
data = np.random.rand(100, 2)

# Applying k-Means
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300)
kmeans.fit(data)

# Getting cluster centers
print("Centroids:", kmeans.cluster_centers_)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Hierarchical Clustering - Overview}
    \begin{itemize}
        \item Hierarchical clustering is an unsupervised learning technique for grouping similar data points into a hierarchy of clusters.
        \item It allows for visualization of relationships and natural groupings within data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Two Main Types}
    \begin{enumerate}
        \item \textbf{Agglomerative Clustering (Bottom-Up Approach)}
        \begin{itemize}
            \item Starts with individual data points as clusters.
            \item Iteratively merges the closest pairs of clusters.
            \item \textbf{Key Concept: Linkage Criteria}
            \begin{itemize}
                \item Distance between clusters can be defined in various ways (single, complete, average linkage).
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Divisive Clustering (Top-Down Approach)}
        \begin{itemize}
            \item Begins with all data points in a single cluster.
            \item Iteratively splits clusters into smaller groups.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing Hierarchical Clustering}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Dendrogram Representation:} Hierarchical results are visualized as dendrograms to illustrate how clusters merge.
            \item \textbf{No Predefined Number of Clusters:} Offers flexibility in exploring data structures, unlike k-Means.
            \item \textbf{Use Cases:} Applications include bioinformatics, market research, and image analysis.
        \end{itemize}
    \end{block}

    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Sample data
data = [[1, 2], [2, 3], [5, 6], [8, 7]]

# Generate linkage matrix
Z = linkage(data, 'ward')

# Create dendrogram
dendrogram(Z)
plt.title('Dendrogram')
plt.xlabel('Data Points')
plt.ylabel('Euclidean Distance')
plt.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Clustering Process - Overview}
    \begin{block}{Overview of Agglomerative Clustering}
        Agglomerative Clustering is a type of hierarchical clustering technique that builds the hierarchy from the individual elements by progressively merging clusters. This approach allows for the creation of a hierarchy or tree structure, known as a dendrogram, representing the data's clustering.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Clustering Process - Steps}
    \begin{enumerate}
        \item \textbf{Initialization}:
        \begin{itemize}
            \item Begin with each data point as its own cluster.
            \item For \( n \) data points, start with \( n \) clusters.
        \end{itemize}
        
        \item \textbf{Calculate Distance}:
        \begin{itemize}
            \item Compute the distance between every pair of clusters. 
            \item Distance metrics include:
                \begin{itemize}
                    \item \textbf{Euclidean distance}:  
                    \[
                    d(u, v) = \sqrt{\sum{(u_i - v_i)^2}}
                    \]
                    \item \textbf{Manhattan distance}:  
                    \[
                    d(u, v) = \sum{|u_i - v_i|}
                    \]
                    \item \textbf{Cosine similarity}:  
                    \[
                    \text{sim}(u, v) = \frac{u \cdot v}{||u|| \cdot ||v||}
                    \]
                \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Clustering Process - Continued Steps}
    \begin{enumerate}[resume]
        \item \textbf{Merge Clusters}:
        \begin{itemize}
            \item Identify the two clusters that are closest together.
            \item Merge these two clusters into a single cluster.
        \end{itemize}
        
        \item \textbf{Update Distances}:
        \begin{itemize}
            \item Recalculate distances between the new cluster and all other existing clusters using a chosen linkage criterion. Common criteria include:
                \begin{itemize}
                    \item \textbf{Single Linkage}: Distance between the closest points of two clusters.
                    \item \textbf{Complete Linkage}: Distance between the farthest points of two clusters.
                    \item \textbf{Average Linkage}: Average distance between all pairs of points from the two clusters.
                    \item \textbf{Ward’s Linkage}: Minimizes the total within-cluster variance.
                \end{itemize}
        \end{itemize}

        \item \textbf{Repeat}:
        \begin{itemize}
            \item Repeat steps 2-4 until only one cluster remains.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Clustering Process - Example}
    \begin{block}{Example}
        Consider a simple dataset with five points: A, B, C, D, E. The agglomerative clustering procedure would start with five clusters \(\{A\}, \{B\}, \{C\}, \{D\}, \{E\}\) and merge them in a stepwise manner based on the calculated distances using the selected linkage criterion.
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The process builds a hierarchy, allowing for various grouping levels.
            \item Different linkage criteria can lead to different clustering outcomes.
            \item The resulting dendrogram is an important tool for visualizing the merging of clusters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Clustering Process - Conclusion}
    \begin{block}{Conclusion}
        Agglomerative clustering is a systematic and flexible method for exploratory data analysis. Understanding the underlying process and criteria helps practitioners effectively analyze and interpret complex datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Clustering Process - Code Snippet}
    \begin{block}{Code Snippet (Python Example)}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import AgglomerativeClustering

# Sample Data
data = [[1, 2], [1, 4], [1, 0],
        [4, 2], [4, 4], [4, 0]]

# Agglomerative Clustering
model = AgglomerativeClustering(n_clusters=2, linkage='ward')
clusters = model.fit_predict(data)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Dendrogram Representation}
    \begin{block}{Understanding Dendrograms}
        A dendrogram is a tree-like diagram that illustrates the arrangement of the clusters formed through hierarchical clustering. It serves as a powerful visual representation allowing for easy interpretation of the data's structure.
    \end{block}
\end{frame}

\begin{frame}[fragile]{How Do Dendrograms Work?}
    \begin{enumerate}
        \item \textbf{Hierarchical Clustering}: Dendrograms are primarily used with agglomerative clustering, where clusters are formed by progressively merging smaller clusters into larger ones based on a distance metric.
        \item \textbf{Linkage Criteria}:
        \begin{itemize}
            \item \textbf{Single Linkage}: Distance between the closest points of two clusters.
            \item \textbf{Complete Linkage}: Distance between the farthest points of the clusters.
            \item \textbf{Average Linkage}: Average distance between all pairs of points in the two clusters.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Key Features and Example of Dendrograms}
    \begin{itemize}
        \item \textbf{Leaves}: Represent the individual data points or observations (starting clusters).
        \item \textbf{Branches}: Show how clusters merge based on similarity. The height at which branches merge indicates the distance or dissimilarity between clusters.
        \item \textbf{Height}: The y-axis typically represents the distance metric, indicating the similarity level between clusters—lower distances mean higher similarity.
    \end{itemize}
    
    \begin{block}{Example Illustration}
        Imagine you have 5 factories producing different products represented as leaves:
        \begin{itemize}
            \item Factory A
            \item Factory B
            \item Factory C
            \item Factory D
            \item Factory E
        \end{itemize}
        A dendrogram would show how these factories group based on similar production characteristics.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Reading and Benefits of Dendrograms}
    \begin{itemize}
        \item \textbf{Reading a Dendrogram}:
        \begin{enumerate}
            \item \textbf{Identifying Clusters}: You can "cut" the dendrogram at a specific height to determine how many clusters to identify.
            \item \textbf{Interpreting Distances}: The height at which two clusters connect reflects their dissimilarity. Cutting lower yields finer, more granular clusters.
        \end{enumerate}
        
        \item \textbf{Benefits of Using Dendrograms}:
        \begin{itemize}
            \item Intuitive Visualization: Easy to interpret and understand the relationships between clusters.
            \item Cluster Identification: Helps in determining the optimal number of clusters based on the level of dissimilarity.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Dendrogram Summary and Code Snippet}
    \begin{block}{Summary}
        \begin{itemize}
            \item Dendrograms are vital tools in hierarchical clustering, providing visual insights into the structure and relationships within data.
            \item By analyzing the dendrogram, you can gain a better understanding of how clusters formed and determine appropriate cluster segmentation.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Formula/Code Snippet}
        \begin{lstlisting}[language=Python]
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

# Assuming 'data' is a pre-processed dataset
Z = sch.linkage(data, 'ward')  # Using Ward's method for hierarchical clustering
dendrogram = sch.dendrogram(Z)
plt.title('Dendrogram')
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Distance Metrics in Clustering}
    
    \begin{block}{Introduction to Distance Metrics}
        In clustering, distance metrics are measures used to quantify how similar or dissimilar data points are to each other. The choice of distance metric can significantly affect the results of clustering algorithms.
    \end{block}
    In this section, we will explore three common distance metrics:
    \begin{itemize}
        \item Euclidean Distance
        \item Manhattan Distance
        \item Cosine Distance
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Euclidean Distance}
    
    \begin{block}{Definition}
        The Euclidean distance between two points in space is the length of the shortest path connecting them:
        \begin{equation}
        d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
        \end{equation}
    \end{block}
    
    \begin{block}{Example}
        For points \(A(1, 2)\) and \(B(4, 6)\):
        \begin{equation}
        d(A, B) = \sqrt{(4-1)^2 + (6-2)^2} = \sqrt{9 + 16} = \sqrt{25} = 5
        \end{equation}
    \end{block}
    
    \begin{block}{Key Point}
        Euclidean distance is sensitive to the scale of the data. Normalization may be necessary.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Manhattan Distance}
    
    \begin{block}{Definition}
        Also known as "Taxicab" or "City Block" distance:
        \begin{equation}
        d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} |x_i - y_i|
        \end{equation}
    \end{block}
    
    \begin{block}{Example}
        For points \(C(1, 2)\) and \(D(4, 6)\):
        \begin{equation}
        d(C, D) = |4 - 1| + |6 - 2| = 3 + 4 = 7
        \end{equation}
    \end{block}
    
    \begin{block}{Key Point}
        Manhattan distance is less sensitive to outliers than Euclidean distance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Cosine Distance}
    
    \begin{block}{Definition}
        Assessing the cosine of the angle between two non-zero vectors:
        \begin{equation}
        \text{cosine similarity} = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}
        \end{equation}
        Cosine distance is then computed as:
        \begin{equation}
        d(\mathbf{x}, \mathbf{y}) = 1 - \text{cosine similarity}
        \end{equation}
    \end{block}
    
    \begin{block}{Example}
        For vectors \(\mathbf{x} = (1, 1)\) and \(\mathbf{y} = (0, 1)\):
        \begin{equation}
        \text{cosine similarity} = \frac{(1 \cdot 0 + 1 \cdot 1)}{\sqrt{1^2 + 1^2} \cdot \sqrt{0^2 + 1^2}} = \frac{1}{\sqrt{2} \cdot 1} = \frac{1}{\sqrt{2}}
        \end{equation}
        Thus, cosine distance:
        \begin{equation}
        d(\mathbf{x}, \mathbf{y}) = 1 - \frac{1}{\sqrt{2}} \approx 0.2929
        \end{equation}
    \end{block}
    
    \begin{block}{Key Point}
        Cosine distance is effective for document clustering where orientation matters more than magnitude.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Final Note}
    
    \begin{block}{Conclusion}
        Choosing the correct distance metric is crucial for effective clustering. Accurate capture of relationships between data points is essential. 
    \end{block}
    
    \begin{block}{Final Note}
        Always consider the scale and nature of your data when selecting a distance metric. Normalization techniques can significantly improve clustering outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Clustering Results}
    \begin{itemize}
        \item Importance of evaluating clustering algorithms
        \item Challenges due to lack of ground truth labels
        \item Key metrics for assessing clustering:
        \begin{itemize}
            \item Silhouette Score
            \item Davies-Bouldin Index (DBI)
            \item Within-Cluster Sum of Squares (WCSS)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Silhouette Score}
    \begin{block}{Definition}
        Measures how similar an object is to its own cluster compared to other clusters.
    \end{block}
    \begin{itemize}
        \item \textbf{Range}: -1 to +1
            \begin{itemize}
                \item +1: well-clustered points
                \item 0: points on the border of two clusters
                \item -1: points may be in the wrong cluster
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Formula}
        \[
        s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
        \]
        where:
        \begin{itemize}
            \item \(a(i)\): average distance to points in the same cluster
            \item \(b(i)\): average distance to points in the nearest cluster
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Clustering animals (dogs, cats, rabbits): A dog with score close to +1 indicates proximity to other dogs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Davies-Bouldin Index (DBI)}
    \begin{block}{Definition}
        Evaluates clustering quality by comparing distances between clusters and the sizes of the clusters.
    \end{block}
    \begin{itemize}
        \item \textbf{Range}: Lower values indicate better clustering.
    \end{itemize}
    
    \begin{block}{Formula}
        \[
        DB = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i}\left\{ \frac{s_i + s_j}{d_{ij}} \right\}
        \]
        where:
        \begin{itemize}
            \item \(k\): number of clusters
            \item \(s_i\): average distance of points in cluster \(i\) to the cluster center
            \item \(d_{ij}\): distance between cluster centers \(i\) and \(j\)
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        In market segmentation: Lower DBI suggests a clearer separation between customer groups.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Within-Cluster Sum of Squares (WCSS)}
    \begin{block}{Definition}
        Measures variability within each cluster by summing the squared distances of each point to its cluster centroid.
    \end{block}
    
    \begin{block}{Formula}
        \[
        WCSS = \sum_{i=1}^{k} \sum_{x \in C_i} (x - \mu_i)^2
        \]
        where:
        \begin{itemize}
            \item \(k\): number of clusters
            \item \(C_i\): set of points in cluster \(i\)
            \item \(\mu_i\): centroid of cluster \(i\)
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Clustering customers based on purchase behavior with lower WCSS indicates closer alignment to average buying habits within segments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Evaluation metrics ensure clusters are meaningful and distinguishable.
        \item A combination of metrics provides a comprehensive view of clustering quality.
        \item Understanding these metrics enables effective iteration and refinement of clustering approaches.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Proper evaluation of clustering results is critical for deriving actionable insights from unsupervised learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Use Cases of Hierarchical Clustering - Introduction}
    \begin{itemize}
        \item Hierarchical clustering builds a hierarchy of clusters.
        \item It can be agglomerative (bottom-up) or divisive (top-down).
        \item Widely applicable due to its ability to reveal nested data structures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Use Cases of Hierarchical Clustering - Applications Part 1}
    \begin{enumerate}
        \item \textbf{Biology and Genetics:}
            \begin{itemize}
                \item \textbf{Example: Phylogenetic Trees}
                \begin{itemize}
                    \item Analyze genetic data to construct phylogenetic trees.
                    \item Groups species based on genetic similarities, showing evolutionary relationships.
                \end{itemize}
            \end{itemize}
        \item \textbf{Market Research:}
            \begin{itemize}
                \item \textbf{Example: Consumer Segmentation}
                \begin{itemize}
                    \item Segment customers based on purchasing behavior.
                    \item Use dendrograms for visualizing and identifying distinct consumer groups.
                \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Use Cases of Hierarchical Clustering - Applications Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue enumeration from the last frame
        \item \textbf{Image Processing:}
            \begin{itemize}
                \item \textbf{Example: Image Segmentation}
                \begin{itemize}
                    \item Group similar colors to enhance image analysis.
                    \item Categorize pixels based on color clusters.
                \end{itemize}
            \end{itemize}
        \item \textbf{Social Network Analysis:}
            \begin{itemize}
                \item \textbf{Example: Community Detection}
                \begin{itemize}
                    \item Identify communities within networks based on user interactions.
                    \item Improve understanding of user behaviors and relationships.
                \end{itemize}
            \end{itemize}
        \item \textbf{Document Clustering:}
            \begin{itemize}
                \item \textbf{Example: Text Categorization}
                \begin{itemize}
                    \item Organize documents based on content for better information retrieval.
                \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Use Cases of Hierarchical Clustering - Summary and Key Points}
    \begin{itemize}
        \item Provides visual representation of data structures through dendrograms.
        \item Beneficial for exploratory data analysis without predefined clusters.
        \item Reveals groupings and relationships between clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Mathematical Framework}
    \begin{block}{Distance Measure}
        The distance measure often used in hierarchical clustering is the Euclidean distance:
        \[
        d(x, y) = \sqrt{\sum (x_i - y_i)^2}
        \]
    \end{block}
    \begin{block}{Agglomerative Clustering}
        Combines clusters based on:
        \begin{itemize}
            \item Minimum distance (Single-linkage)
            \item Maximum distance (Complete-linkage)
            \item Average distance (Average-linkage)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Comparison between k-Means and Hierarchical Clustering - Introduction}
    \begin{block}{Introduction}
        In unsupervised learning, clustering techniques like k-Means and Hierarchical Clustering help to identify patterns and group similar data points. 
        Both methods have their strengths and weaknesses, making them suitable for different types of problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Comparison between k-Means and Hierarchical Clustering - k-Means Clustering}
    \textbf{Overview:}
    \begin{itemize}
        \item Partitions data into \textbf{k} distinct clusters based on distance to the centroid of each cluster.
    \end{itemize}
    
    \textbf{Pros:}
    \begin{enumerate}
        \item \textbf{Scalability:} Efficient for large datasets (O(n * k * i), where n = number of points, k = number of clusters, and i = iterations).
        \item \textbf{Simplicity:} Easy to implement and interpret.
        \item \textbf{Speed:} Generally faster than hierarchical clustering, especially with large datasets.
    \end{enumerate}

    \textbf{Cons:}
    \begin{enumerate}
        \item \textbf{Choice of k:} Requires prior knowledge of the number of clusters.
        \item \textbf{Sensitivity to Initialization:} Different initial centroids can lead to different outcomes (can use k-Means++ to mitigate this).
        \item \textbf{Shape Limitations:} Assumes spherical clusters, which may not capture complex shapes.
    \end{enumerate}

    \textbf{Example:} Customer segmentation in a retail store where customers are grouped based on purchasing behavior.
\end{frame}

\begin{frame}[fragile]{Comparison between k-Means and Hierarchical Clustering - Hierarchical Clustering}
    \textbf{Overview:}
    \begin{itemize}
        \item Builds a tree-like structure (dendrogram) of clusters either through:
        \begin{itemize}
            \item \textbf{Agglomerative:} Bottom-up approach, starting with each point as its own cluster.
            \item \textbf{Divisive:} Top-down approach, starting with one cluster and dividing.
        \end{itemize}
    \end{itemize}

    \textbf{Pros:}
    \begin{enumerate}
        \item \textbf{No Need for k:} Automatically determines the number of clusters.
        \item \textbf{Dendrogram Visualization:} Provides insights into the data hierarchy.
        \item \textbf{Flexibility:} Can capture more complex shapes and structures.
    \end{enumerate}

    \textbf{Cons:}
    \begin{enumerate}
        \item \textbf{Time Complexity:} Slower than k-Means (O(n^3) for naive implementations).
        \item \textbf{Memory Intensive:} Requires more storage due to distance matrix.
        \item \textbf{Sensitive to Noise:} Outliers can significantly affect the clustering results.
    \end{enumerate}

    \textbf{Example:} Gene expression analysis where similar genes are grouped based on their expression patterns.
\end{frame}

\begin{frame}[fragile]{Comparison between k-Means and Hierarchical Clustering - When to Use Each Method}
    \textbf{Use k-Means when:}
    \begin{itemize}
        \item You have a large dataset and computational efficiency is critical.
        \item You have prior knowledge of the expected number of clusters (k).
        \item The data is fairly spherical and isotropic, or approximates such shapes.
    \end{itemize}

    \textbf{Use Hierarchical Clustering when:}
    \begin{itemize}
        \item You need a detailed structure of data relationships (dendrogram).
        \item The number of clusters is not known in advance.
        \item You are working with smaller datasets where computational resources allow.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Comparison between k-Means and Hierarchical Clustering - Conclusion}
    \begin{block}{Conclusion}
        Understanding the pros and cons of each clustering method allows for a more informed choice depending on the data characteristics, size, and the specific needs of your analysis. 
        By employing the appropriate technique, you can achieve better clustering results that drive insights from your data.
    \end{block}

    \textbf{Key Takeaways:}
    \begin{itemize}
        \item k-Means is preferred for large datasets with known cluster numbers, while Hierarchical Clustering is useful for its detailed output and flexibility.
        \item Evaluate and test different methods based on the specific context of your application to achieve optimal results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Takeaways on Clustering}
    \begin{enumerate}
        \item \textbf{Definition and Purpose}:
        \begin{itemize}
            \item Clustering is an \textbf{unsupervised learning technique} that groups similar data points based on features without prior labels.
            \item Essential for identifying underlying structures in data.
        \end{itemize}
        
        \item \textbf{Common Clustering Algorithms}:
        \begin{itemize}
            \item \textbf{k-Means Clustering}:
            \begin{itemize}
                \item Works well with large datasets and spherical clusters but is sensitive to outliers.
                \item \textit{Example:} Segmenting customers into distinct groups based on purchasing behavior.
            \end{itemize}
            \item \textbf{Hierarchical Clustering}:
            \begin{itemize}
                \item Produces a dendrogram and is ideal for small datasets with complex structures.
                \item \textit{Example:} Categorizing species in natural taxonomy based on genetic similarity.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Challenges and Evaluation Metrics}
    \begin{enumerate}
        \item \textbf{Challenges}:
        \begin{itemize}
            \item Choosing the right number of clusters influences outcomes (e.g., elbow method, silhouette score).
            \item Scalability: Some algorithms struggle with large datasets.
            \item Interpretability: Understanding cluster meanings requires domain knowledge.
        \end{itemize}
        
        \item \textbf{Evaluation Metrics}:
        \begin{itemize}
            \item Metrics such as Silhouette Score, Davies-Bouldin Index, and DBSCAN help assess clustering quality.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Future Trends in Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Incorporating Deep Learning}:
        \begin{itemize}
            \item Techniques like Deep Embedded Clustering (DEC) improve performance using neural networks.
        \end{itemize}
        \item \textbf{Handling Larger Datasets with Scalability}:
        \begin{itemize}
            \item Advancements in Distributed Computing will be significant as data grows exponentially.
        \end{itemize}
        \item \textbf{Integration with Other Learning Paradigms}:
        \begin{itemize}
            \item The rise of Semi-supervised Learning combines the strengths of unsupervised and supervised approaches.
        \end{itemize}
        \item \textbf{Real-time Clustering}:
        \begin{itemize}
            \item Essential for applications in streaming data processing (e.g., fraud detection).
        \end{itemize}
        \item \textbf{Ethics and Bias Consideration}:
        \begin{itemize}
            \item Focus on fairness and transparency, especially in sensitive applications.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Summary}
    Clustering remains a vital area of study within unsupervised learning, with diverse applications across many industries. As we adopt more sophisticated algorithms and techniques, the ways we analyze and interpret data will continue to evolve. The future of clustering will rely on innovation in computational methods and a conscientious approach to ethical implications. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Further Reading and Code Example}
    \begin{block}{Further Reading and Resources}
        \begin{itemize}
            \item Research papers on Deep Learning approaches to clustering.
            \item Online platforms for implementing clustering algorithms (e.g., Scikit-learn documentation).
            \item Discussions on ethical AI practices in unsupervised learning scenarios.
        \end{itemize}
    \end{block}
    
    \begin{block}{Code Snippet Example}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

# Sample data
data = np.array([[1, 2], [1, 4], [1, 0],
                 [4, 2], [4, 4], [4, 0]])

# Applying k-Means
kmeans = KMeans(n_clusters=2, random_state=0).fit(data)

# Output the cluster centers
print(kmeans.cluster_centers_)
    \end{lstlisting}
    \end{block}
\end{frame}


\end{document}