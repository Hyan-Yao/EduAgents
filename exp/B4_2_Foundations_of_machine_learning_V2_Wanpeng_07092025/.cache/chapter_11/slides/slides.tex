\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 11: Unsupervised Learning]{Week 11: Unsupervised Learning - Dimensionality Reduction}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Dimensionality Reduction}
    \begin{block}{Overview}
        Dimensionality Reduction (DR) is a crucial aspect of machine learning that aims to reduce the number of input variables in a dataset. It simplifies models, removes noise, and enhances the efficiency of data analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Dimensionality Reduction}
    \begin{itemize}
        \item \textbf{Reduces Complexity:} Simplifies analyses by decreasing the number of input variables.
        \item \textbf{Mitigates Overfitting:} Reduces noise in high-dimensional data, promoting better generalization.
        \item \textbf{Enhances Visualization:} Facilitates better data visualization using 2D and 3D plots.
        \item \textbf{Improves Performance:} Leads to faster model training and prediction times.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Dimensionality Reduction Techniques}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA)}
            \begin{itemize}
                \item Transforms data into linearly uncorrelated components, maximizing variance.
                \item \textbf{Formula:} 
                \begin{equation}
                Y = XW
                \end{equation}
                where \(Y\) is the reduced dataset, \(X\) is the original dataset, and \(W\) is the matrix of eigenvectors.
            \end{itemize}

        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
            \begin{itemize}
                \item Used primarily for visualization, transforming high-dimensional data into lower dimensions while preserving local structure.
            \end{itemize}
            
        \item \textbf{Singular Value Decomposition (SVD)}
            \begin{itemize}
                \item Decomposes a matrix into three matrices, revealing latent structures. Often used in recommender systems.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Dimensionality reduction is fundamental in unsupervised learning for model efficiency and interpretability.
        \item Techniques serve varied purposes:
            \begin{itemize}
                \item PCA for variance maximization
                \item t-SNE for visualization
                \item SVD for matrix factorization
            \end{itemize}
        \item Consider the trade-off between information loss and complexity reduction when applying DR techniques.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Dimensionality Reduction Defined - Overview}
    \begin{block}{What is Dimensionality Reduction?}
        Dimensionality Reduction refers to the process of reducing the number of features (or dimensions) in a dataset while preserving its essential information. This process transforms data from a high-dimensional space to a lower-dimensional space, making it easier to visualize and analyze.
    \end{block}

    \begin{block}{Why is it Important?}
        \begin{enumerate}
            \item \textbf{Simplicity}: Reduces complexity, making it easier to interpret data.
            \item \textbf{Efficiency}: Decreases storage and computation costs, speeding up the performance of machine learning models.
            \item \textbf{Noise Reduction}: Eliminates redundant and irrelevant features, which can improve model accuracy by reducing overfitting.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Dimensionality Reduction Techniques}
    \begin{block}{Key Techniques in Dimensionality Reduction}
        \begin{itemize}
            \item \textbf{Principal Component Analysis (PCA)}: 
            \begin{itemize}
                \item PCA identifies the directions (principal components) in which the data varies the most and projects it onto these axes.
                \item \textit{Example}: PCA can combine features like height, weight, and age of individuals to find a few new features capturing most of the variation.
                \item 
                \begin{equation}
                    Z = XW
                \end{equation}
                Where:
                \begin{itemize}
                    \item \( Z \): The reduced feature set
                    \item \( X \): Original feature set
                    \item \( W \): The matrix of principal components
                \end{itemize}
            \end{itemize}
            
            \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}: 
            \begin{itemize}
                \item A non-linear dimensionality reduction technique ideal for visualizing high-dimensional datasets.
                \item \textit{Illustration}: Captures local structures and groups similar instances closely on a two-dimensional map.
            \end{itemize}

            \item \textbf{Autoencoders}: 
            \begin{itemize}
                \item Neural networks designed to learn efficient codings of unsupervised input data.
                \item \textit{Example}: Autoencoders can compress images to fewer pixels while retaining visual features.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Dimensionality Reduction - Key Points & Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Dimensionality Reduction is fundamental in simplifying datasets while maintaining core information.
            \item It improves the performance and interpretability of machine learning models.
            \item Multiple techniques are available, each with unique strengths and appropriate use cases.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        In summary, dimensionality reduction is a crucial step in data preprocessing that equips us with the tools to manage high-dimensional datasets effectively. By embracing these techniques, we can reduce complexity and enhance insights from our data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Dimensionality Reduction?}
    \begin{block}{Introduction}
        Dimensionality reduction is a vital technique in machine learning and data analysis, transforming data from high-dimensional to lower-dimensional spaces while retaining relevant information.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Motivations for Dimensionality Reduction - Part 1}
    \begin{enumerate}
        \item \textbf{Improved Model Performance}
        \begin{itemize}
            \item \textbf{Overfitting Prevention:} Simplifies the model, making it more generalizable to unseen data.
            \item \textbf{Reduced Computational Cost:} Less memory and computational resources, faster training and evaluation.
            \item \textbf{Enhanced Interpretability:} Simpler models are easier to interpret; focuses on key features.
        \end{itemize}
        \item \textbf{Visualization}
        \begin{itemize}
            \item \textbf{Human Inspection:} Converts complex data into 2D or 3D formats for better understanding.
            \item \textbf{Pattern Recognition:} Makes clusters or trends visually apparent, aiding in identifying relationships within the data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Motivations for Dimensionality Reduction - Part 2}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Curse of Dimensionality:} Higher dimensions lead to sparsity in data, making analysis difficult.
            \item \textbf{Trade-offs:} Balancing between reducing dimensions and retaining essential information is crucial.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Dimensionality reduction enhances model performance and aids visualization, allowing for effective analysis and decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Dimensionality Reduction - Feature Space}
    \begin{block}{Feature Space}
        \begin{itemize}
            \item \textbf{Definition}: A multi-dimensional space where each dimension corresponds to a feature of the data.
            \item \textbf{Example}: 
                \begin{itemize}
                    \item Features: Height, Weight, Age
                    \item Visualization in 3D: 
                        \begin{itemize}
                            \item X-axis: Height
                            \item Y-axis: Weight
                            \item Z-axis: Age
                        \end{itemize}
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Dimensionality Reduction - Variance}
    \begin{block}{Variance}
        \begin{itemize}
            \item \textbf{Definition}: Measures how much data points deviate from their mean in feature space.
            \item \textbf{Importance}: We retain dimensions that capture the most variance.
            \item \textbf{Key Point}: Greater variance indicates more information in that dimension.
            \item \textbf{Example}: If feature A (height) has higher variance than feature B (weight), prefer keeping feature A.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Dimensionality Reduction - Curse of Dimensionality}
    \begin{block}{Curse of Dimensionality}
        \begin{itemize}
            \item \textbf{Definition}: Issues that arise from analyzing data in high-dimensional spaces. 
            \item \textbf{Key Points}:
                \begin{itemize}
                    \item \textbf{Data Sparsity}: High dimensions cause data to become sparse.
                    \item \textbf{Distance Metrics}: Distances lose meaning as data points become equidistant.
                    \item \textbf{Computational Issues}: Increased dimensions lead to higher computation time and resources.
                \end{itemize}
            \item \textbf{Illustration}: Moving from 2D to 10D drastically increases the volume of the space, complicating analysis.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Dimensionality Reduction - Summary and Tips}
    \begin{block}{Summary}
        \begin{itemize}
            \item Dimensionality reduction helps manage high-dimensional datasets effectively.
            \item Understanding feature space, variance, and the curse of dimensionality guides dimensionality reduction strategies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Visualization Tips}
        \begin{itemize}
            \item Use scatter plots for feature space demonstrations.
            \item Utilize visualizations like PCA or t-SNE to retain variance in reduced dimensions.
        \end{itemize}
    \end{block}
    
    \begin{equation}
        \text{Variance} (X) = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu)^2
    \end{equation}
    \begin{itemize}
        \item Where \( \mu \) is the mean of dataset \( X \) and \( N \) is the number of data points.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Principal Component Analysis (PCA) - Overview}
    \begin{block}{Overview of PCA}
        Principal Component Analysis (PCA) is a powerful statistical technique used in unsupervised learning for dimensionality reduction. Its primary goal is to reduce the number of variables (or dimensions) in a dataset while retaining as much variance (information) as possible. PCA transforms the original features into a new set of uncorrelated variables called "principal components."
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Principal Component Analysis (PCA) - Purpose}
    \begin{block}{Purpose of PCA}
        \begin{enumerate}
            \item \textbf{Dimensionality Reduction:} Simplifies datasets to enhance machine learning performance, especially in high dimensions.
            \item \textbf{Data Visualization:} Enables visualization of complex, multidimensional datasets in 2D or 3D.
            \item \textbf{Noise Reduction:} Focuses on significant variance components while discarding less important ones.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Principal Component Analysis (PCA) - Algorithmic Steps}
    \begin{enumerate}
        \item \textbf{Standardize the Data:}
        \begin{itemize}
            \item Center the data by subtracting the mean of each feature.
            \item Scale to have unit variance if necessary.
            \item Formula: 
            \begin{equation}
                X_{\text{standardized}} = \frac{X - \mu}{\sigma}
            \end{equation}
        \end{itemize}

        \item \textbf{Construct the Covariance Matrix:}
        \begin{itemize}
            \item Computes covariance to identify feature variability.
            \item Formula:
            \begin{equation}
                \text{Cov}(X) = \frac{1}{n-1}(X^TX) \quad \text{where } n \text{ is the number of samples}
            \end{equation}
        \end{itemize}

        \item \textbf{Calculate Eigenvalues and Eigenvectors:}
        \begin{itemize}
            \item Extract eigenvalues and corresponding eigenvectors from the covariance matrix.
        \end{itemize}

        \item \textbf{Sort Eigenvalues and Eigenvectors:}
        \begin{itemize}
            \item Rank eigenvalues and sort eigenvectors accordingly.
        \end{itemize}

        \item \textbf{Project the Original Data:}
        \begin{itemize}
            \item Transform the dataset into a lower-dimensional space.
            \item Formula for projection:
            \begin{equation}
                Z = X \cdot W \quad \text{where } W \text{ contains the top } k \text{ eigenvectors}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Principal Component Analysis (PCA) - Example and Key Points}
    \begin{block}{Example}
        Suppose we have a dataset with 10 features representing customer characteristics (age, income, spending score, etc.). Rather than analyzing all 10 features, PCA can help us find that 2 or 3 principal components summarize the patterns in the data effectively.
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item PCA aids in preprocessing for machine learning.
            \item Helps reduce overfitting by simplifying the model.
            \item The choice of the number of components (\(k\)) is crucial for balancing complexity and variance captured.
            \item Requires an understanding of linear algebra concepts, particularly eigenvalues and eigenvectors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA: Mathematical Foundations}
    \begin{block}{Understanding PCA's Mathematical Underpinnings}
        Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional form while retaining most of the variance (information).
        \begin{itemize}
            \item Core objective: Identify the directions (principal components) in which the data varies the most.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Mathematical Concepts}
    \begin{itemize}
        \item \textbf{Covariance Matrix}: PCA begins by calculating the covariance matrix \( \mathbf{C} \) of the dataset.
        \item \textbf{Formula}: Given a dataset \( \mathbf{X} \) (with mean-centered data):
        \begin{equation}
            \mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}
        \end{equation}
        \item \textbf{Eigenvalues and Eigenvectors}:
        \begin{itemize}
            \item \textbf{Eigenvectors}: Define the directions of the new feature space (the principal components).
            \item \textbf{Eigenvalues}: Indicate the magnitude of variance in the direction of the corresponding eigenvector.
        \end{itemize}
        
        \item \textbf{The Eigenvalue Equation}:
        \begin{equation}
            \mathbf{C} \mathbf{v} = \lambda \mathbf{v}
        \end{equation}
        where:
        \begin{itemize}
            \item \( \mathbf{C} \) = Covariance matrix
            \item \( \mathbf{v} \) = Eigenvector
            \item \( \lambda \) = Eigenvalue
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps in PCA Mathematics}
    \begin{enumerate}
        \item \textbf{Standardization}: Scale the dataset (mean = 0, variance = 1).
        \item \textbf{Covariance Matrix Calculation}: Assess the relationships between variables.
        \item \textbf{Eigen Decomposition}: Calculate eigenvalues and eigenvectors of the covariance matrix.
        \item \textbf{Selecting Principal Components}: Choose top \( k \) eigenvectors corresponding to the largest eigenvalues.
            \begin{itemize}
                \item These eigenvectors represent directions of maximum variance.
            \end{itemize}
        \item \textbf{Project Data}: Transform the original dataset using the selected eigenvectors:
        \begin{equation}
            \mathbf{Y} = \mathbf{X} \mathbf{W}
        \end{equation}
        where \( \mathbf{W} \) contains the selected eigenvectors.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA Implementation Steps - Overview}
    \begin{block}{Overview of PCA}
        PCA (Principal Component Analysis) is a powerful unsupervised learning technique used for dimensionality reduction. 
        It simplifies datasets for better analysis, visualization, and optimization of model performance by transforming data into a lower-dimensional space while retaining variance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA Implementation Steps - Step 1 to Step 3}
    \begin{enumerate}
        \item \textbf{Import Necessary Libraries}
        \begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
        \end{lstlisting}
        
        \item \textbf{Load Your Data}
        \begin{lstlisting}[language=Python]
data = pd.read_csv('your_data.csv')
        \end{lstlisting}

        \item \textbf{Preprocess the Data}
        \begin{itemize}
            \item Separate the features from the target (if applicable).
            \item Standardize the data because PCA is sensitive to data scale.
        \end{itemize}
        \begin{lstlisting}[language=Python]
features = data.drop('target_column', axis=1)
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA Implementation Steps - Step 4 to Step 6}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Apply PCA}
        \begin{lstlisting}[language=Python]
pca = PCA(n_components=2)  # Choose number of components
principal_components = pca.fit_transform(scaled_features)
pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
        \end{lstlisting}

        \item \textbf{Explained Variance Ratio}
        Analyze how much variance is explained by each principal component:
        \begin{lstlisting}[language=Python]
explained_variance = pca.explained_variance_ratio_
print(explained_variance)
        \end{lstlisting}

        \item \textbf{Visualization}
        Use a scatter plot to visualize PCA results:
        \begin{lstlisting}[language=Python]
plt.figure(figsize=(8, 6))
plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.6)
plt.title('PCA Result')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid()
plt.show()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing PCA Results}
    % Techniques for visualizing the results of PCA, including plots and interpretation of principal components.
    
    \begin{block}{Overview}
        Principal Component Analysis (PCA) is a powerful technique for dimensionality reduction and data visualization. Visualization is essential for understanding relationships and structure within the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques for Visualizing PCA Results}
    \begin{enumerate}
        \item \textbf{Scree Plot}
        \begin{itemize}
            \item Displays eigenvalues associated with each principal component.
            \item Helps determine the number of components to retain.
            \item Look for the "elbow" point indicating diminishing returns.
        \end{itemize}
        
        \item \textbf{Biplot}
        \begin{itemize}
            \item Combines scatter plot of the first two principal components with vectors from original features.
            \item Visualizes original variable influence on principal components.
        \end{itemize}
        
        \item \textbf{Scatter Plot of PC Scores}
        \begin{itemize}
            \item 2D scatter plot to visualize distribution in reduced feature space.
            \item Helps identify clusters, trends, and potential outliers.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Scree Plot Example}
    \begin{block}{Example Code}
    \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

pca = PCA()
pca.fit(data)  # 'data' is your dataset
plt.plot(range(1, len(pca.explained_variance_) + 1), pca.explained_variance_)
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance')
plt.title('Scree Plot')
plt.show()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Biplot Example}
    \begin{block}{Example Code}
    \begin{lstlisting}[language=Python]
import numpy as np

pca = PCA(n_components=2)
principal_components = pca.fit_transform(data)

plt.scatter(principal_components[:, 0], principal_components[:, 1])
for i in range(len(pca.components_)):
    plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.title('Biplot')
plt.grid()
plt.show()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Important Points and Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item PCA reduces dimensionality while retaining maximum variance.
            \item Data should be standardized before applying PCA.
            \item Principal components are linear combinations of original features.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Visualizing PCA results is critical for interpreting data structure. Tools like scree plots, biplots, and scatter plots assist in revealing underlying patterns.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Up}
    % Introducing the next topic
    Stay tuned for an introduction to t-Distributed Stochastic Neighbor Embedding (t-SNE) and an exploration of how it differs from PCA.
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-Distributed Stochastic Neighbor Embedding (t-SNE) - Introduction}
    \begin{itemize}
        \item \textbf{t-SNE} is a powerful dimensionality reduction technique.
        \item Excel in visualizing high-dimensional data in low-dimensional spaces (2D or 3D).
        \item Preserves local structures, capturing relationships between similar points.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-SNE - Purpose and Differences}
    \begin{block}{Purpose of t-SNE}
        \begin{itemize}
            \item Visualize complex, high-dimensional datasets (e.g., image features, text embeddings, gene expression).
            \item Converts similarity data into probability distributions, revealing clusters and data distributions.
        \end{itemize}
    \end{block}

    \begin{block}{Differences from Other Techniques}
        \begin{itemize}
            \item \textbf{PCA vs. t-SNE:}
                \begin{itemize}
                    \item PCA focuses on global structures and linear relationships.
                    \item t-SNE emphasizes local relationships, ideal for intricate cluster formations.
                \end{itemize}
            \item \textbf{UMAP vs. t-SNE:}
                \begin{itemize}
                    \item UMAP retains both local and some global structure; t-SNE focuses on local structures.
                    \item UMAP generally faster than t-SNE.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-SNE - Use Cases and Code Example}
    \begin{block}{When to Use t-SNE}
        \begin{itemize}
            \item Ideal for exploratory data analysis to uncover patterns and clusters.
            \item Applications: bioinformatics, image recognition, natural language processing.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item t-SNE preserves local structures effectively.
            \item Applies non-linear transformation; reveals complex patterns.
            \item Maps distances to probabilities for similarity visualization.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Consider visualizing a dataset of handwritten digits using t-SNE to see clusters of similar digits.
    \end{block}
    
    \begin{lstlisting}[language=Python]
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Assume `X` is your high-dimensional data
tsne = TSNE(n_components=2, random_state=42)
X_embedded = tsne.fit_transform(X)

plt.scatter(X_embedded[:, 0], X_embedded[:, 1])
plt.title('t-SNE Visualization')
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-SNE: Algorithm Overview}
    \begin{block}{Understanding t-SNE}
        t-Distributed Stochastic Neighbor Embedding (t-SNE) is a powerful technique for visualizing high-dimensional datasets in lower dimensions (typically 2D or 3D).
        Its effectiveness lies in uncovering patterns, clusters, and relationships within data during exploratory analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts - Similarity Measures}
    \begin{itemize}
        \item **Similarity Measures**: 
        \begin{itemize}
            \item t-SNE calculates similarities using conditional probabilities.
            \item For a data point \(x_i\), the probability of selecting \(x_j\) as a neighbor is given by:
            \begin{equation}
                P_{j|i} = \frac{exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} exp(-||x_i - x_k||^2 / 2\sigma_i^2)}
            \end{equation}
            \item Here, \(\sigma_i\) is the variance of the Gaussian distribution centered at \(x_i\).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts - Probability Distributions}
    \begin{itemize}
        \item **Probability Distributions**:
        \begin{itemize}
            \item In **High-Dimensional Space**, t-SNE computes a distribution of probabilities for neighbor relationships.
            \item In **Low-Dimensional Space**, a counterpart distribution \(Q\) is created. 
            \item The objective is to minimize the Kullback-Leibler divergence between distributions \(P\) and \(Q\):
            \begin{equation}
                KL(P || Q) = \sum_{i} P_{ij} \log \left( \frac{P_{ij}}{Q_{ij}} \right)
            \end{equation}
            \item This maintains the proximity of data points across dimensions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Applications}
    \begin{itemize}
        \item **Key Points**:
        \begin{itemize}
            \item t-SNE excels at finding complex, non-linear patterns, unlike PCA.
            \item Focuses more on preserving local relationships than global structure.
        \end{itemize}
        \item **Applications**:
        \begin{itemize}
            \item Popular in genomics, image processing, and natural language processing for exploratory analysis.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-SNE Implementation Steps: Introduction}
    \begin{block}{What is t-SNE?}
        t-distributed Stochastic Neighbor Embedding (t-SNE) is a powerful dimensionality reduction technique primarily used for visualizing high-dimensional data in a low-dimensional space. It helps to preserve the local structure of the data and reveals clusters that may exist.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-SNE Implementation Steps}
    \begin{enumerate}
        \item \textbf{Install Required Libraries}
        \begin{lstlisting}[language=bash]
        pip install numpy matplotlib scikit-learn
        \end{lstlisting}
        
        \item \textbf{Import Necessary Libraries}
        \begin{lstlisting}[language=python]
        import numpy as np
        import matplotlib.pyplot as plt
        from sklearn.manifold import TSNE
        from sklearn.datasets import load_iris
        \end{lstlisting}
        
        \item \textbf{Load and Prepare Data}
        \begin{lstlisting}[language=python]
        iris = load_iris()
        X = iris.data  # Features
        y = iris.target  # Labels or species
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying t-SNE and Visualization}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Apply t-SNE}
        \begin{lstlisting}[language=python]
        tsne = TSNE(n_components=2, random_state=0)
        X_embedded = tsne.fit_transform(X)
        \end{lstlisting}
        
        \item \textbf{Visualize the Results}
        \begin{lstlisting}[language=python]
        plt.figure(figsize=(8, 6))
        scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)
        plt.title('t-SNE Visualization of Iris Dataset')
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.colorbar(scatter, label='Species')
        plt.show()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Practical Takeaways}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Preserving Local Structure:} t-SNE aims to preserve the local small-scale structure of the data while capturing global structure.
            \item \textbf{Complexity:} It is computationally expensive for large datasets; consider preprocessing with PCA.
            \item \textbf{Parameters:} 
            \begin{itemize}
                \item $n\_components$: Number of dimensions (usually 2 for visualization).
                \item \text{perplexity}: Balances attention between local and global aspects; common values are between 5 and 50.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \vspace{0.5cm} % Add space for better visibility
    t-SNE is effective for visualizing complex data distributions, but might not always be optimal for all data types. Understanding its implementation is crucial for effective use in analysis and machine learning projects.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing PCA and t-SNE - Overview}
    \begin{itemize}
        \item This presentation compares PCA and t-SNE, two popular dimensionality reduction techniques.
        \item The focus is on their key differences and practical applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding PCA (Principal Component Analysis)}
    \begin{itemize}
        \item \textbf{Purpose}: Linear dimensionality reduction capturing variance.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Linear}: Captures only linear relationships.
            \item \textbf{Speed}: Faster and more efficient for large datasets.
            \item \textbf{Global Structure}: Preserves the overall structure for visualization.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding t-SNE (t-Distributed Stochastic Neighbor Embedding)}
    \begin{itemize}
        \item \textbf{Purpose}: Non-linear technique for visualizing high-dimensional data.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Non-linear}: Captures complex, non-linear relationships.
            \item \textbf{Local Focus}: Emphasizes local structures and discernible clusters.
            \item \textbf{Computationally Intensive}: Slower than PCA for large datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences Between PCA and t-SNE}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Feature} & \textbf{PCA} & \textbf{t-SNE} \\
        \hline
        Type & Linear & Non-linear \\
        \hline
        Scaling & Data should be standardized & Does not require standardized data \\
        \hline
        Dimensionality & Reduces to fewer dimensions & Typically reduces to 2 or 3 dimensions \\
        \hline
        Interpretability & Linear combinations & Less interpretable, focuses on similarities \\
        \hline
        Use Case & Exploratory analysis & Visualization and clustering \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Use PCA vs. t-SNE}
    \begin{block}{Use PCA When}
        \begin{itemize}
            \item A quick overview of data structure is needed.
            \item Data is linearly separable.
            \item Interested in retaining maximum variance.
        \end{itemize}
    \end{block}

    \begin{block}{Use t-SNE When}
        \begin{itemize}
            \item Visualizing high-dimensional data to uncover clusters.
            \item Data has complex, non-linear relationships.
            \item Afford computational expense, especially for larger datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenarios}
    \begin{itemize}
        \item \textbf{PCA Use Case}: Reducing image data to extract significant principal components for machine learning.
        \item \textbf{t-SNE Use Case}: Visualizing customer segmentation data to reveal complex interactions and insights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item PCA and t-SNE serve essential roles in dimensionality reduction and visualization.
        \item Selection of the appropriate method depends on analysis goals, data nature, and computational resources.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Use Cases for Dimensionality Reduction}
  \begin{block}{Understanding Dimensionality Reduction}
    Dimensionality reduction encompasses techniques that reduce the number of features in a dataset while preserving its essential characteristics and structure. This is crucial in many real-world scenarios where high-dimensional data can lead to inefficiencies, overfitting, and challenges in visualization.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Real-World Applications}
  \begin{enumerate}
    \item \textbf{Data Visualization:}
    \begin{itemize}
      \item \textit{Use Case:} Visualizing high-dimensional data in fields like social network analysis and bioinformatics.
      \item \textit{Example:} t-SNE used to visualize clusters in gene expression data.
    \end{itemize}
    
    \item \textbf{Image Processing:}
    \begin{itemize}
      \item \textit{Use Case:} Reducing image dimensions while retaining important features for classification.
      \item \textit{Example:} PCA compressing images for faster processing.
    \end{itemize}
    
    \item \textbf{Natural Language Processing (NLP):}
    \begin{itemize}
      \item \textit{Use Case:} Handling large feature spaces in text data.
      \item \textit{Example:} LSA reducing dimensions of term-document matrices to improve classification.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Real-World Applications (Cont.)}
  \begin{enumerate}
    \setcounter{enumi}{3}
    \item \textbf{Genomics and Bioinformatics:}
    \begin{itemize}
      \item \textit{Use Case:} Analyzing large-scale genomic data for feature selection.
      \item \textit{Example:} PCA identifying genes accounting for variance in cancer studies.
    \end{itemize}
    
    \item \textbf{E-commerce and Recommendation Systems:}
    \begin{itemize}
      \item \textit{Use Case:} Navigating large product recommendation datasets.
      \item \textit{Example:} Matrix factorization personalizing user recommendations.
    \end{itemize}
    
    \item \textbf{Finance:}
    \begin{itemize}
      \item \textit{Use Case:} Identifying significant assets in portfolio management.
      \item \textit{Example:} Factor analysis reducing financial indicators into key factors.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Benefits of Dimensionality Reduction}
  \begin{itemize}
    \item \textbf{Improved Performance:} Simpler models reduce computation time and enhance performance.
    \item \textbf{Noise Reduction:} Eliminating redundant data allows better model generalization.
    \item \textbf{Enhanced Interpretability:} Smaller datasets are easier to visualize and interpret.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Points}
  \begin{block}{Conclusion}
    Dimensionality reduction is integral across various fields, allowing for efficient data handling, insightful visualizations, and improved model performance. Understanding these applications equips us to tackle complex datasets effectively.
  \end{block}
  \begin{itemize}
    \item Dimensionality reduction is vital for visualization, compression, and feature extraction.
    \item Techniques like PCA, t-SNE, and LSA find practical applications in diverse fields.
    \item The impact extends from improving model performance to facilitating exploratory data analysis.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges and Limitations of Dimensionality Reduction}
  \begin{block}{Understanding Dimensionality Reduction}
    Dimensionality reduction is a technique used in data analysis that reduces the number of features in a dataset while preserving as much information as possible. While offering benefits like improved model performance and enhanced visualization, challenges arise.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Challenges - Part 1}
  \begin{enumerate}
    \item \textbf{Information Loss}
      \begin{itemize}
        \item Explanation: The primary challenge of dimensionality reduction is the potential loss of important information.
        \item Example: Reducing from 1,000 features to 100 might lead to poor model performance.
      \end{itemize}
      
    \item \textbf{Model Interpretability}
      \begin{itemize}
        \item Explanation: Reduced dimensions make it challenging to interpret relationships.
        \item Example: PCA creates principal components that may lack clear interpretations.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Challenges - Part 2}
  \begin{enumerate}
    \setcounter{enumi}{2} % Continue numbering from the previous frame
    \item \textbf{Choice of Technique and Parameters}
      \begin{itemize}
        \item Explanation: Selecting the right method (e.g., PCA, t-SNE) and hyperparameters is crucial.
        \item Example: PCA needs the number of components based on explained variance, while t-SNE's perplexity affects outcomes.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitations of Dimensionality Reduction}
  \begin{enumerate}
    \item \textbf{Computational Complexity}
      \begin{itemize}
        \item Explanation: Techniques can be computationally intensive for large datasets.
        \item Example: t-SNE can take significantly longer as dataset size increases.
      \end{itemize}

    \item \textbf{Sensitivity to Noise}
      \begin{itemize}
        \item Explanation: Methods can be influenced by noise, leading to misleading representations.
        \item Example: Noise affects distance metrics in methods like t-SNE, leading to non-representative clusters.
      \end{itemize}
    
    \item \textbf{Non-Gaussian Data}
      \begin{itemize}
        \item Explanation: Some techniques assume Gaussian distribution, which may not hold.
        \item Example: PCA on skewed data without preprocessing may yield unrepresentative components.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary Points}
  \begin{itemize}
    \item Dimensionality reduction techniques offer benefits but come with challenges.
    \item Key concerns include information loss and reduced interpretability.
    \item Choosing the appropriate technique requires consideration of computational demands and data nature.
    \item Domain knowledge is essential for effective application and validation of results.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{PCA Explained}
  \begin{block}{Key Formulas}
    1. \textbf{Covariance Matrix Calculation}:
    \[
    \text{Cov}(X) = \frac{1}{n-1} \sum (x_i - \bar{x})(x_i - \bar{x})^T
    \]

    2. \textbf{Eigenvalue Decomposition}:
    \[
    \text{Cov}(X) \cdot v = \lambda \cdot v
    \]

    3. \textbf{Select Top Components}: Choose top $k$ eigenvalues and corresponding eigenvectors.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example Code: PCA in Python}
  \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
import numpy as np

# Sample Data
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9],
              [1.9, 2.2], [3.1, 3.0], [2.3, 2.7],
              [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Applying PCA
pca = PCA(n_components=1)  # Reduce to 1 dimension
X_reduced = pca.fit_transform(X)
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations in Dimensionality Reduction}
  \begin{block}{Understanding Ethical Implications}
    Dimensionality reduction (DR) techniques, such as PCA and t-SNE, are essential for simplifying data while retaining important information. However, these methods also carry ethical responsibilities.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations - Bias and Information Loss}
  \begin{itemize}
    \item \textbf{Data Representation and Bias}:
      \begin{itemize}
        \item DR can amplify existing biases in datasets.
        \item \textbf{Example}: Facial recognition systems may perform poorly for underrepresented demographics due to bias in training data.
      \end{itemize}
    
    \item \textbf{Loss of Information}:
      \begin{itemize}
        \item DR seeks simplification but can lead to significant information loss.
        \item \textbf{Key Point}: Balance is necessary between simplification and retention of relevant features.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations - Transparency and Privacy}
  \begin{itemize}
    \item \textbf{Transparency and Accountability}:
      \begin{itemize}
        \item \textbf{Explainability}: Important for practitioners to document DR methods, especially in critical sectors like healthcare.
        \item \textbf{Reproducibility}: Techniques must be replicable to maintain trust.
      \end{itemize}
    
    \item \textbf{Data Privacy and Security}:
      \begin{itemize}
        \item DR may expose sensitive information, necessitating compliance with privacy regulations (e.g., GDPR).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example Scenario and Summary}
  \begin{block}{Example Scenario}
    Consider a bank using DR to assess credit risk. Reduced dimensions based solely on income and credit history may ignore critical socioeconomic factors, perpetuating inequality.
  \end{block}

  \begin{block}{Summary Points}
    \begin{itemize}
      \item \textbf{Review Data Bias}: Continually evaluate datasets for biases.
      \item \textbf{Ensure Explainability}: Clearly communicate data transformations.
      \item \textbf{Protect Privacy}: Adhere to ethical guidelines to secure data.
      \item \textbf{Balance Reduction and Importance}: Prioritize a mix of DR and critical feature retention.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Future Directions - Key Takeaways}
  \begin{enumerate}
    \item \textbf{Understanding Dimensionality Reduction:} 
    \begin{itemize}
      \item Technique to reduce the number of variables while retaining important patterns.
      \item Common methods include PCA, t-SNE, and Autoencoders.
    \end{itemize}
    
    \item \textbf{Applications in Real-World Scenarios:} 
    \begin{itemize}
      \item Enhances visualization and improves machine learning performance.
      \item Example: PCA in image processing for face recognition.
    \end{itemize}
    
    \item \textbf{Ethical Considerations:} 
    \begin{itemize}
      \item Risk of losing critical information leading to biased outcomes.
      \item Importance of evaluating the ethical impact of dimensionality reduction.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Future Directions - Future Trends}
  \begin{enumerate}
    \item \textbf{Integration of Deep Learning:}
    \begin{itemize}
      \item Combining dimensionality reduction with deep learning for automated feature extraction.
    \end{itemize}
    
    \item \textbf{Explainable AI:}
    \begin{itemize}
      \item Demand for interpretability drives innovative methods that reveal feature contributions.
    \end{itemize}
    
    \item \textbf{Advancements in Algorithms:}
    \begin{itemize}
      \item Research improving algorithms for scalability, e.g., UMAP for preserving local structures.
    \end{itemize}
    
    \item \textbf{Multimodal Data Handling:}
    \begin{itemize}
      \item Importance of dimensionality reduction on diverse data types (text, images, numbers).
    \end{itemize}
    
    \item \textbf{Real-time Applications:}
    \begin{itemize}
      \item Focus on online techniques for adapting to streaming data.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Future Directions - Summary}
  \begin{block}{Conclusion}
    Dimensionality reduction plays a vital role in simplifying complex data and enhancing machine learning performance. The evolution of this field promises exciting advancements aligning with technology and ethics.
  \end{block}

  \begin{block}{Key Formula}
    \textbf{PCA Transformation:} 
    \begin{equation}
      Z = XW
    \end{equation}
    Where:
    \begin{itemize}
      \item \( Z \) = projected data
      \item \( X \) = original data centered by subtracting the mean
      \item \( W \) = matrix with top principal components
    \end{itemize}
  \end{block}
\end{frame}


\end{document}