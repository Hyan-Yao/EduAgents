\frametitle{Common Activation Functions}
  \begin{enumerate}
    \item \textbf{Sigmoid Function}
      \begin{equation}
      \sigma(x) = \frac{1}{1 + e^{-x}}
      \end{equation}
      \begin{itemize}
        \item Output range: (0, 1)
        \item Good for binary classification, but can lead to saturation.
      \end{itemize}

    \item \textbf{ReLU (Rectified Linear Unit)}
      \begin{equation}
      \text{ReLU}(x) = \max(0, x)
      \end{equation}
      \begin{itemize}
        \item Output range: [0, âˆž)
        \item Introduces non-linearity, but may suffer from the "dying ReLU" issue.
      \end{itemize}

    \item \textbf{Tanh (Hyperbolic Tangent)}
      \begin{equation}
      \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
      \end{equation}
      \begin{itemize}
        \item Output range: (-1, 1)
        \item Centered around 0, leading to faster convergence.
      \end{itemize}
  \end{enumerate}
