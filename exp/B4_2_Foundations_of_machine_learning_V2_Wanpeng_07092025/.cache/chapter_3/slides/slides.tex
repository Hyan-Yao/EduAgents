\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Feature Engineering]{Week 3: Feature Engineering}
\subtitle{}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Feature Engineering}
    Feature engineering is the process of selecting, modifying, or creating features to enhance the performance of machine learning models. The quality of features is crucial as it significantly influences a model's accuracy and generalization capacity.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Feature Engineering}
    \begin{enumerate}
        \item \textbf{Improves Model Performance}:
        \begin{itemize}
            \item Enhanced features provide better signals for algorithms, resulting in more accurate predictions.
            \item \textit{Example}: In predicting house prices, features like "number of bedrooms" or "proximity to schools" improve predictive power compared to raw data alone.
        \end{itemize}
        
        \item \textbf{Reduces Overfitting}:
        \begin{itemize}
            \item Selecting informative features lowers the chances of models fitting noise, thus improving generalization.
            \item \textit{Key Point}: Fewer, well-chosen features simplify the model while maintaining performance.
        \end{itemize}
        
        \item \textbf{Facilitates Understanding of Data}:
        \begin{itemize}
            \item Good feature engineering reveals underlying data patterns, aiding in the communication of findings to stakeholders.
        \end{itemize}
        
        \item \textbf{Enhances Algorithm Efficiency}:
        \begin{itemize}
            \item High-quality features lead to quicker training processes and reduced resource usage.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques in Feature Engineering}
    \begin{itemize}
        \item \textbf{Feature Creation}: Constructing new features from existing ones (e.g., "price per square foot").
        \item \textbf{Feature Transformation}: Modifying existing features for better compatibility (e.g., applying log transformation to skewed features).
        \item \textbf{Feature Selection}: Using statistical techniques to identify the most relevant features (e.g., Recursive Feature Elimination or correlation matrices).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formula to Remember}
    Feature importance can be calculated using algorithms like Random Forest. For a linear model like Linear Regression, the relationship is given by:
    \begin{equation}
      Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_nX_n
    \end{equation}
    Where:
    \begin{itemize}
        \item \( Y \) = target variable
        \item \( \beta_0 \) = intercept
        \item \( \beta_i \) = coefficient of feature \( i \)
        \item \( X_i \) = ith feature
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Effective feature engineering is vital in machine learning workflows. By employing domain expertise to refine feature sets, practitioners can build stronger models that yield improved predictions and insights. Next, we will delve deeper into the role of features in model training.
\end{frame}

\begin{frame}[fragile]{Understanding Features - What are Features?}
    In machine learning, \textbf{features} are individual measurable properties or characteristics used as inputs for a model. They serve as the basis for making predictions or classifications.\\

    For example, in a dataset used to predict house prices, features could include:
    \begin{itemize}
        \item \textbf{Square footage}
        \item \textbf{Number of bedrooms}
        \item \textbf{Location (e.g., ZIP code)}
        \item \textbf{Age of the house}
    \end{itemize}
    
    These inputs help the model learn the relationship between the features and the target variable (in this case, house price).
\end{frame}

\begin{frame}[fragile]{Understanding Features - Types of Features}
    \textbf{Types of Features:}
    \begin{enumerate}
        \item \textbf{Numerical Features}: Continuous or discrete values.
        \begin{itemize}
            \item Examples: Age (in years), temperature (in Â°C), income (in \$).
        \end{itemize}
        
        \item \textbf{Categorical Features}: Represent categories or groups.
        \begin{itemize}
            \item Examples: Color (red, blue, green), type of property (house, apartment).
        \end{itemize}
        
        \item \textbf{Ordinal Features}: Categorical features with a defined order.
        \begin{itemize}
            \item Examples: Education level (high school, bachelor's, master's), satisfaction rating (poor, fair, good, excellent).
        \end{itemize}
        
        \item \textbf{Temporal Features}: Relate to time, can be captured as date/time stamps.
        \begin{itemize}
            \item Examples: Date of purchase, time spent on a website.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Understanding Features - Role of Features in Model Training}
    \textbf{Role of Features in Model Training:}
    \begin{itemize}
        \item \textbf{Input for Algorithms}: Features are what algorithms analyze to learn patterns and make predictions. 
        \item \textbf{Impact on Model Performance}: The choice and quality of features directly influence the model's accuracy and performance. 
        \begin{itemize}
            \item Example: Including relevant features like neighborhood crime rate when predicting house prices increases the model's predictive power.
        \end{itemize}
    \end{itemize}

    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Features are crucial for machine learning model performance. High-quality features lead to more accurate predictions.
        \item The selection of features should relate to the problem at hand and be based on domain knowledge to ensure relevance.
        \item Feature engineering and selection are essential processes to avoid introducing noise or irrelevant data into the model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Understanding Features - Example Feature Representation}
    \textbf{Example Feature Representation:}\\
    In Python, we represent features in a DataFrame using libraries such as Pandas:
    
    \begin{lstlisting}[language=Python]
import pandas as pd

# Sample DataFrame representing features of houses
data = {
    'SquareFootage': [1500, 2500, 1800],
    'Bedrooms': [3, 4, 3],
    'Location': ['Suburb', 'City', 'Suburb'],
    'Age': [10, 5, 15]
}

df = pd.DataFrame(data)
    \end{lstlisting}
    
    This DataFrame features four columns representing various properties of houses, succinctly illustrating how we can structure our features for analysis.
\end{frame}

\begin{frame}[fragile]{Feature Selection Techniques - Overview}
    \begin{block}{Overview}
        Feature selection is a crucial process in machine learning that involves selecting a subset of relevant features for building predictive models.
        Efficient feature selection helps improve model performance, reduce overfitting, and decrease computation time. 
        We'll explore three primary categories of feature selection techniques:
        \begin{itemize}
            \item \textbf{Filter Methods}
            \item \textbf{Wrapper Methods}
            \item \textbf{Embedded Methods}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Feature Selection Techniques - Filter Methods}
    \begin{block}{1. Filter Methods}
        \begin{itemize}
            \item \textbf{Definition:} Evaluate relevance based on intrinsic properties; score features independently.
            \item \textbf{Key Characteristics:}
            \begin{itemize}
                \item Fast and computationally inexpensive.
                \item Suitable for high-dimensional datasets.
            \end{itemize}
            \item \textbf{Examples:}
            \begin{itemize}
                \item Correlation Coefficient
                \item Chi-Square Test
            \end{itemize}
        \end{itemize}
        \textbf{Key Point:} Filter methods simplify feature selection before model training, serving as a preliminary step.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Feature Selection Techniques - Wrapper and Embedded Methods}
    \begin{block}{2. Wrapper Methods}
        \begin{itemize}
            \item \textbf{Definition:} Evaluate multiple models using different feature subsets based on their performance.
            \item \textbf{Key Characteristics:}
            \begin{itemize}
                \item More computationally expensive than filter methods.
                \item Directly tied to a specific model's performance.
            \end{itemize}
            \item \textbf{Examples:}
            \begin{itemize}
                \item Recursive Feature Elimination (RFE)
                \item Forward Selection
            \end{itemize}
        \end{itemize}
        \textbf{Key Point:} Wrapper methods can achieve better accuracy but come with higher computational costs.
    \end{block}

    \begin{block}{3. Embedded Methods}
        \begin{itemize}
            \item \textbf{Definition:} Incorporate feature selection as part of the model training process.
            \item \textbf{Key Characteristics:} Balance between filter and wrapper methods in computational efficiency and accuracy.
            \item \textbf{Examples:}
            \begin{itemize}
                \item Lasso Regression
                \item Decision Tree Algorithms
            \end{itemize}
        \end{itemize}
        \textbf{Key Point:} Embedded methods streamline the model-building process while automatically selecting relevant features.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Feature Selection Techniques - Conclusion and Formula}
    \begin{block}{Conclusion}
        Feature selection is critical in improving model efficacy. Understanding differences between techniques allows practitioners to choose the most appropriate.
    \end{block}
    
    \begin{block}{Formula Example for Correlation Coefficient}
        To illustrate a filter method:
        \begin{equation}
            r = \frac{n\left(\sum xy\right) - \left(\sum x\right)\left(\sum y\right)}{\sqrt{[n\sum x^2 - \left(\sum x\right)^2][n\sum y^2 - \left(\sum y\right)^2]}}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( n \) = number of observations
            \item \( x \) = feature values
            \item \( y \) = target variable values
        \end{itemize}
    \end{block}

    \begin{block}{Important Note}
        Different scenarios may warrant different feature selection techniques. The best approach often depends on the specific dataset and analysis goals.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Overview}
    \begin{block}{Overview of Filter Methods}
        \textbf{Filter methods} are feature selection techniques that pre-select features based on their statistical properties. They are executed independently of machine learning algorithms, making them computationally efficient.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Key Concepts}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Statistical Tests}
            \begin{itemize}
                \item \textbf{t-test}: Compares means of two groups (binary classification).
                \item \textbf{Chi-square test}: Assesses relationship between categorical variables.
                \item \textbf{ANOVA}: Compares means across multiple groups (multi-class classification).
            \end{itemize}
    
            \item \textbf{Correlation Coefficients}
            \begin{itemize}
                \item \textbf{Pearson Correlation Coefficient (r)}: Measures linear correlation, ranges from -1 to +1.
                \begin{equation}
                    r = \frac{n(\sum xy) - (\sum x)(\sum y)}{\sqrt{[n\sum x^2 - (\sum x)^2][n\sum y^2 - (\sum y)^2]}}
                \end{equation}
                \item \textbf{Spearman's Rank Correlation Coefficient}: Non-parametric measure of rank correlation (ordinal data).
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Benefits and Example}
    \begin{block}{Benefits of Filter Methods}
        \begin{itemize}
            \item \textbf{Efficiency}: Fast execution as features are analyzed independently.
            \item \textbf{Simplicity}: Straightforward to interpret and implement.
            \item \textbf{Preliminary Selection}: Eliminates irrelevant features, aiding complex selection methods.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Utilizing statistical tests and correlation:
        \begin{itemize}
            \item Perform a chi-square test between \textquotedblleft education level\textquotedblright\ and \textquotedblleft purchase behavior\textquotedblright. If p-value < 0.05, education is significant.
            \item Calculate Pearson correlation between \textquotedblleft income\textquotedblright\ and likelihood of purchasing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Wrapper Methods - Overview}
    \begin{block}{What are Wrapper Methods?}
        Wrapper methods are feature selection techniques that assess subsets of features based on their contribution to the model's predictive power. Unlike filter methods, they rely on a specific machine learning algorithm to evaluate feature subsets.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Wrapper Methods - How they Work}
    \begin{enumerate}
        \item \textbf{Subset Generation}: Select a subset of features:
            \begin{itemize}
                \item Random sampling
                \item All possible combinations
                \item Heuristic approaches
            \end{itemize}
        \item \textbf{Model Training}: Train a model using the selected features.
        \item \textbf{Evaluation}: Evaluate the model's performance using a metric on a validation dataset.
        \item \textbf{Iteration}: Repeat until the best performing subset is found.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Example: Recursive Feature Elimination (RFE)}
    \begin{block}{How RFE Works}
        \begin{enumerate}
            \item Start with all features.
            \item Train a model (e.g., SVM, decision tree) using all features.
            \item Identify the least significant feature based on importance.
            \item Remove the least significant feature.
            \item Repeat until a predetermined number of features remain.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example Code in Python}
        \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Load sample data
data = load_iris()
X, y = data.data, data.target

# Create a logistic regression model
model = LogisticRegression()

# Create RFE model and select the top 2 features
rfe = RFE(model, 2)
fit = rfe.fit(X, y)

# Print selected features
print("Num Features:", fit.n_features_)
print("Selected Features:", fit.support_)
print("Feature Ranking:", fit.ranking_)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Wrapper Methods - Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Model Dependency}: Wrapper methods are tied to a specific model, risking overfitting if the same model is used for selection and evaluation.
        \item \textbf{Computationally Intensive}: They involve training numerous models, which can be costly for large datasets or complex models.
        \item \textbf{Accuracy Focused}: They aim to maximize model accuracy, potentially reducing interpretability.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Wrapper methods like Recursive Feature Elimination can significantly improve model performance by selecting important features, but require careful consideration of computational costs and overfitting risks.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Embedded Methods - Overview}
    \begin{block}{What are Embedded Methods?}
        Embedded methods are a type of feature selection technique that perform feature selection during the model training process. They differ from wrapper methods and filter methods as they integrate feature selection directly into the learning algorithm.
    \end{block}
    
    \begin{itemize}
        \item Integrates with the learning process for efficiency
        \item Model-specific techniques
        \item Balances performance and interpretability
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Embedded Methods - Examples}
    \begin{enumerate}
        \item \textbf{LASSO (Least Absolute Shrinkage and Selection Operator)}:
            \begin{itemize}
                \item Adds penalty to loss function:
                \begin{equation}
                \text{Loss} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |w_j|
                \end{equation}
                \item Useful for high-dimensional datasets; simplifies models by zeroing out less important features.
            \end{itemize}
        
        \item \textbf{Decision Trees}:
            \begin{itemize}
                \item Create splits based on impurity measures (Gini impurity, Information Gain).
                \item Select features that minimize impurity during tree-building.
                \item Naturally provide ranking of feature importance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Key Points and Conclusion}
    \begin{itemize}
        \item Embedded methods merge feature selection with model training for efficient computation.
        \item LASSO is effective for high-dimensional datasets, helping reduce overfitting.
        \item Decision trees provide intuitive visualization of feature importance.
    \end{itemize}

    \begin{block}{Conclusion}
        Embedded methods enhance model performance and clarity. Utilizing techniques like LASSO and decision trees helps in building interpretable and accurate models.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Code Snippets for Implementation}
    \textbf{Example of LASSO in Python using Scikit-learn:}
    \begin{lstlisting}
from sklearn.linear_model import Lasso
model = Lasso(alpha=1.0)
model.fit(X_train, y_train)
important_features = X_train.columns[model.coef_ != 0]
print(important_features)
    \end{lstlisting}

    \textbf{Example of Decision Tree feature importance:}
    \begin{lstlisting}
from sklearn.tree import DecisionTreeClassifier
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)
importance = tree_model.feature_importances_
features = X_train.columns
feature_importance = pd.Series(importance, index=features).sort_values(ascending=False)
print(feature_importance)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction Techniques - Introduction}
    \begin{block}{Introduction to Dimensionality Reduction}
        Dimensionality reduction is the process of reducing the number of features (dimensions) in a dataset while preserving its important properties and relationships. This is a critical step in data preprocessing, especially when dealing with high-dimensional data, which can lead to problems such as:
        \begin{itemize}
            \item Overfitting
            \item Increased computational complexity
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction Techniques - Benefits}
    \begin{block}{Benefits of Dimensionality Reduction}
        \begin{enumerate}
            \item \textbf{Reduced Complexity:} Simplified models lead to faster training times.
            \item \textbf{Improved Visualization:} Easier to visualize data and identify clusters or patterns.
            \item \textbf{Mitigation of Overfitting:} Helps prevent models from fitting noise in the data.
            \item \textbf{Enhanced Performance:} Many algorithms perform better with fewer features due to reduced "curse of dimensionality."
            \item \textbf{Storage and Computational Efficiency:} Smaller datasets require less memory and computational power.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction Techniques - Common Techniques}
    \begin{block}{Common Dimensionality Reduction Techniques}
        \begin{itemize}
            \item \textbf{Principal Component Analysis (PCA):}
            \begin{itemize}
                \item Transforms data into new coordinate system where greatest variance lies on the first coordinate.
                \item Retain only the top k principal components.
            \end{itemize}
            \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE):}
            \begin{itemize}
                \item Converts similarities between data points to probabilities in lower-dimensional space.
            \end{itemize}
            \item \textbf{Linear Discriminant Analysis (LDA):}
            \begin{itemize}
                \item Supervised technique to maximize class separability; enhances classification.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction Techniques - Example and Key Points}
    \begin{block}{Example}
        Imagine a dataset with 100 features. Applying dimensionality reduction techniques:
        \begin{itemize}
            \item PCA may reduce it to 2D or 3D, retaining over 90\% of variance.
            \item Visualization aids in identifying customer segments.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Techniques simplify datasets without losing crucial information.
            \item Enhance model performance and reduce computational burden.
            \item Different techniques serve different purposes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction Techniques - Conclusion}
    By understanding and implementing dimensionality reduction techniques, 
    we can effectively manage the complexity of high-dimensional datasets and 
    enable more efficient data analysis and machine learning workflows.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Slide Teaser}
    Stay tuned for the next slide, where we will dive deeper into \textbf{Principal Component Analysis (PCA)} â its mathematical foundations and practical applications!
\end{frame}

\begin{frame}[fragile]{Principal Component Analysis (PCA)}
    \begin{block}{Overview}
        Principal Component Analysis (PCA) is a powerful statistical technique used for dimensionality reduction while preserving as much variability as possible. 
        It helps in simplifying data, removing noise, and visualizing high-dimensional datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Mathematical Foundations of PCA - Part 1}
    \begin{enumerate}
        \item \textbf{Data Centering}: 
        \begin{itemize}
            \item Before applying PCA, the data is centered by subtracting the mean of each feature.
            \item For a dataset \( X \):
            \begin{equation}
                \text{Centered Data} = X - \text{mean}(X)
            \end{equation}
        \end{itemize}
        
        \item \textbf{Covariance Matrix}:
        \begin{itemize}
            \item Compute the covariance matrix to understand how features vary together.
            \item The covariance matrix \( C \) for centered data \( X \) is given by:
            \begin{equation}
                C = \frac{1}{n-1} X^T X
            \end{equation}
            where \( n \) is the number of observations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Mathematical Foundations of PCA - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % continue numbering
        \item \textbf{Eigenvalues and Eigenvectors}:
        \begin{itemize}
            \item Eigenvalues measure the variance captured by each principal component, while eigenvectors define the direction of those components.
            \item Solve the characteristic equation:
            \begin{equation}
                |C - \lambda I| = 0
            \end{equation}
            where \( \lambda \) are the eigenvalues and \( I \) is the identity matrix.
        \end{itemize}
        
        \item \textbf{Select Principal Components}:
        \begin{itemize}
            \item Sort eigenvalues in descending order and select the top \( k \) eigenvectors (where \( k \) is the desired number of dimensions).
        \end{itemize}
        
        \item \textbf{Data Transformation}:
        \begin{itemize}
            \item Project the original data onto the new subspace:
            \begin{equation}
                Y = X W
            \end{equation}
            where \( W \) contains the selected eigenvectors.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Applications of PCA}
    \begin{itemize}
        \item \textbf{Data Visualization}: Reducing dimensions to 2D or 3D for easy plotting and interpretation.
        \item \textbf{Noise Reduction}: Filtering out less important features that contribute little to variance.
        \item \textbf{Feature Extraction}: Finding new features that capture significant patterns in the data.
        \item \textbf{Preprocessing for Machine Learning}: Improving algorithm performance by reducing overfitting and computational cost.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example of PCA}
    Consider a dataset with features such as height, weight, and age. 
    PCA can transform these correlated features into uncorrelated principal components that best explain the variance in the data, 
    allowing us to visualize the distribution of individuals in a 2D space instead of dealing with a 3D space.
\end{frame}

\begin{frame}[fragile]{Key Points to Emphasize}
    \begin{itemize}
        \item PCA is effective in simplifying datasets with many features.
        \item The key mathematical steps include centering, calculating the covariance matrix, and identifying eigenvalues/eigenvectors.
        \item While PCA retains the maximum variance, it may discard some information, thus appropriate feature selection is essential.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Code Snippet: PCA in Python}
    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
import numpy as np

# Example data
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], 
              [3.1, 3.0], [2.3, 2.7], [2.0, 1.6], [1.0, 1.1], 
              [1.5, 1.6], [1.1, 0.9]])

# Applying PCA
pca = PCA(n_components=1)  # Reduce to 1 dimension
X_reduced = pca.fit_transform(X)

print("Reduced data shape:", X_reduced.shape)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
    \begin{block}{What is t-SNE?}
        t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique used for visualizing high-dimensional data. It transforms data into a lower-dimensional space, preserving relative distances between data points.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How t-SNE Works}
    \begin{enumerate}
        \item Pairwise Similarity Calculation:
        \begin{itemize}
            \item Computes similarity between data points \( p_{j|i} \) using conditional probabilities.
            \item Formula: 
            \begin{equation}
                p_{j|i} = \frac{exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} exp(-||x_i - x_k||^2 / 2\sigma_i^2)}
            \end{equation}
        \end{itemize}
        
        \item Symmetrization:
        \begin{itemize}
            \item Joint probabilities \( P \) are calculated by combining \( p_{j|i} \) and \( p_{i|j} \).
            \begin{equation}
                P_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}
            \end{equation}
        \end{itemize}

        \item Low-Dimensional Mapping:
        \begin{itemize}
            \item Aim to create mapping \( Y \) where joint probabilities \( Q \) resemble \( P \).
            \begin{equation}
                q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}
            \end{equation}
        \end{itemize}

        \item Cost Function:
        \begin{itemize}
            \item Minimize Kullback-Leibler divergence.
            \begin{equation}
                C(Y) = KL(P || Q) = \sum_{i,j} P_{ij} \log \left( \frac{P_{ij}}{Q_{ij}} \right)
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Use t-SNE}
    \begin{itemize}
        \item High-Dimensional Data Visualization: Effective for datasets with many features.
        \item Exploratory Data Analysis (EDA): Identifies patterns, groupings, or anomalies.
        \item Understanding Model Outputs: Visualizes latent space of a trained model.
    \end{itemize}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Non-linear dimensionality reduction.
            \item Preserves local structure.
            \item Computationally intensive for large datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case}
    \begin{block}{Image Classification}
        t-SNE can visualize clustering in image datasets. For instance, it helps illustrate how similar images group based on their features, revealing natural categorizations like animals or vehicles.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        t-SNE is an invaluable tool for visualizing complex datasets in lower dimensions. Its ability to uncover intricate patterns makes it a favorite for exploratory data analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Feature Engineering Best Practices - Introduction}
    \begin{block}{Introduction to Feature Engineering}
        Feature engineering is the process of using domain knowledge to select, modify, or create features from raw data, making it more suitable for modeling. Effective feature engineering can significantly enhance the predictive performance of machine learning models.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Feature Engineering Best Practices - Common Best Practices}
    \begin{enumerate}
        \item \textbf{Handling Missing Values}:
            \begin{itemize}
                \item Identify missing data using descriptive statistics (e.g., \texttt{.isnull().sum()} in Python).
                \item Strategies for imputation:
                    \begin{itemize}
                        \item Mean/Median Imputation
                        \item Mode for categorical data
                        \item KNN Imputation
                    \end{itemize}
                \item Option to drop records with missing values when they are few.
            \end{itemize}
        
        \item \textbf{Normalization and Scaling}:
            \begin{itemize}
                \item Normalization: Scaling to a common scale (e.g., [0, 1]).
                \item Standardization: Transforming to have mean 0 and standard deviation 1.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Feature Engineering Best Practices - Examples and Techniques}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Creating Interaction Features}:
            \begin{itemize}
                \item Combine features like 'age' and 'income' for better predictions.
            \end{itemize}

        \item \textbf{Encoding Categorical Variables}:
            \begin{itemize}
                \item Label Encoding: Assign unique integers to categories.
                \item One-Hot Encoding: Create binary columns for each category.
                \begin{lstlisting}[language=python]
data = pd.get_dummies(data, columns=['category_feature'])
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Feature Selection}:
            \begin{itemize}
                \item Remove irrelevant features using feature importance analysis.
                \item Filter methods to eliminate multicollinear variables.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Feature Engineering Best Practices - Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Effective feature engineering can dramatically impact model performance.
            \item Always explore and preprocess your data thoroughly before modeling.
            \item Techniques can vary based on dataset type and complexity.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        By adhering to these best practices, you will enhance your model's accuracy and interpretability, providing a solid foundation for your data science projects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies on Feature Engineering - Introduction}
    \begin{block}{What is Feature Engineering?}
        Feature engineering is the process of transforming raw data into meaningful features that enhance the predictive performance of machine learning models.
    \end{block}
    \begin{itemize}
        \item Crucial for extracting valuable insights.
        \item Helps improve model accuracy across various domains.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies on Feature Engineering - Healthcare}
    \begin{block}{Case Study: Predicting Patient Readmissions}
        \textbf{Objective:} Reduce hospital readmission rates.
    \end{block}
    \begin{itemize}
        \item \textbf{Feature Engineering Techniques:}
            \begin{itemize}
                \item Temporal Features (time since previous admissions)
                \item Demographic Features (age, gender, socioeconomic factors)
                \item Comorbidity Index (additional diseases)
            \end{itemize}
        \item \textbf{Outcome:} Improved predictions allowed for better patient management.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies on Feature Engineering - Finance and E-commerce}
    \begin{block}{Case Study: Credit Scoring}
        \textbf{Objective:} Assess creditworthiness and minimize default risk.
    \end{block}
    \begin{itemize}
        \item \textbf{Feature Engineering Techniques:}
            \begin{itemize}
                \item Behavioral Features (transaction data)
                \item Aggregation Features (average balances, late payments)
                \item Financial Ratios (debt-to-income ratio)
            \end{itemize}
        \item \textbf{Outcome:} Enhanced models accurately identified high-risk customers.
    \end{itemize}
    
    \begin{block}{Case Study: Customer Retention}
        \textbf{Objective:} Increase customer retention rates.
    \end{block}
    \begin{itemize}
        \item \textbf{Feature Engineering Techniques:}
            \begin{itemize}
                \item RFM Analysis (recency, frequency, monetary)
                \item Engagement Features (website interactions)
            \end{itemize}
        \item \textbf{Outcome:} Targeted marketing strategies improved retention.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Feature engineering enhances model robustness and predictive power.
            \item Customization is essential across domains.
            \item Collaboration with domain experts enriches feature creation.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Feature engineering transforms raw datasets into effective predictors, integral to developing successful machine learning applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=python]
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load dataset
data = pd.read_csv('patient_data.csv')

# Feature engineering: create new features
data['days_since_last_admission'] = (data['current_admission_date'] - data['last_admission_date']).dt.days
data['comorbidity_index'] = data['num_conditions'].apply(lambda x: calculate_comorbidity_index(x))

# Normalize features
scaler = StandardScaler()
data[['feature1', 'feature2']] = scaler.fit_transform(data[['feature1', 'feature2']])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Feature Engineering on Model Performance - Introduction}
    \begin{itemize}
        \item \textbf{Feature Engineering}: Process of selecting, modifying, or creating features to improve model performance.
        \item Effective feature engineering enhances predictive accuracy and generalization to unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Feature Engineering on Model Performance - Key Performance Metrics}
    \begin{block}{Accuracy}
        \begin{itemize}
            \item Ratio of correctly predicted instances to total instances.
            \item Formula: 
            \[
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
            \]
        \end{itemize}
    \end{block}
    
    \begin{block}{F1 Score}
        \begin{itemize}
            \item Harmonic mean of Precision and Recall.
            \item Formula: 
            \[
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \]
            \item Where:
            \begin{itemize}
                \item \textbf{Precision}: Proportion of true positives among predicted positives.
                \item \textbf{Recall}: Proportion of true positives among actual positives.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Feature Engineering on Model Performance - Application and Examples}
    \begin{itemize}
        \item \textbf{Data Quality Improvement}: Relevant features enhance model fitting.
        \item \textbf{Complexity vs. Interpretability}: Adding features can complicate models; reducing dimensions may improve generalization.
        \item \textbf{Handling Imbalanced Datasets}: Techniques like resampling can improve F1 score.
    \end{itemize}

    \begin{block}{Example: Customer Churn Prediction}
        \begin{itemize}
            \item \textbf{Before Feature Engineering}:
                \begin{itemize}
                    \item Accuracy: 70\%
                    \item F1 Score: 0.60
                \end{itemize}
            \item \textbf{After Feature Engineering}:
                \begin{itemize}
                    \item Accuracy: 85\%
                    \item F1 Score: 0.78
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Feature Engineering on Model Performance - Conclusion}
    \begin{itemize}
        \item Effective feature engineering can significantly increase model performance metrics.
        \item The right features simplify complex patterns, improving predictive capabilities.
        \item Always assess model performance across multiple metrics for a holistic understanding.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Ethical Considerations - Introduction}
    \begin{block}{Understanding Ethical Implications in Feature Engineering}
        Feature engineering involves selecting and transforming variables (features) in a dataset to improve model performance. However, these choices can introduce ethical concerns, particularly through biases that affect fairness and equity.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Ethical Considerations - Key Implications}
    \begin{block}{Key Ethical Implications}
        \begin{enumerate}
            \item \textbf{Bias Introduction Through Feature Selection}  
            Certain features may reflect historical biases (e.g., gender, race) leading to discriminatory outcomes. For example, models predicting hiring decisions using biased historical data may perpetuate that bias.
            
            \item \textbf{Data Representation}  
            Features may not represent the diversity of the population, resulting in skewed results. For instance, facial recognition systems trained predominantly on lighter-skinned individuals may misidentify those with darker skin.
            
            \item \textbf{Feature Interaction Effects}  
            The interaction of features can amplify biases, as seen in combining age and gender where systemic discrimination may occur.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Ethical Considerations - Addressing Bias}
    \begin{block}{Strategies to Mitigate Bias}
        \begin{itemize}
            \item \textbf{Feature Auditing:} Regularly inspect features to identify potential biases.
            \item \textbf{Fairness Metrics:} Incorporate fairness metrics, such as demographic parity, into model evaluations.
            \item \textbf{Collaborative Approach:} Engage diverse stakeholders to review and provide feedback on feature engineering practices.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Incorporating ethical considerations in feature engineering is vital for building fair AI systems. Mitigating bias helps ensure equitable model performance for all individuals.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Ethical Considerations - Code Snippet}
    \frametitle{Feature Auditing with Python}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load dataset
data = pd.read_csv("dataset.csv")

# Check for potential biases in selected features
bias_summary = data[['gender', 'age', 'salary']].groupby('gender').agg(['mean', 'count'])

print(bias_summary)
    \end{lstlisting}
    \begin{block}{Remember:}
        Ethics in feature engineering is not just about compliance; it is about building trust and fairness in data-driven decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Practical Applications and Tools}
    \begin{block}{Introduction to Feature Engineering Tools}
        Feature engineering is a crucial step in the machine learning pipeline, as it directly influences the quality and performance of models. This presentation explores two widely-used libraries in Python for feature engineering: \textbf{Pandas} and \textbf{Scikit-Learn}.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Pandas - Overview}
    \begin{block}{Overview}
        Pandas is a powerful data manipulation and analysis library for Python. It offers data structures such as Series and DataFrames that make it easy to handle structured data.
    \end{block}
    
    \begin{block}{Key Functions for Feature Engineering}
        \begin{itemize}
            \item \textbf{Data Cleaning:} Handles missing data, duplicates, and improper data formats.
            \begin{lstlisting}[language=Python]
df.dropna()  # Removes missing values from a DataFrame
            \end{lstlisting}

            \item \textbf{Feature Creation:} Create new features from existing ones.
            \begin{lstlisting}[language=Python]
df['year'] = pd.to_datetime(df['date_column']).dt.year  # Extracts year from a date
            \end{lstlisting}

            \item \textbf{Encoding Categorical Variables:} Convert categorical variables into numerical format.
            \begin{lstlisting}[language=Python]
df = pd.get_dummies(df, columns=['categorical_column'])  # One-Hot Encoding
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Scikit-Learn - Overview and Key Functions}
    \begin{block}{Overview}
        Scikit-Learn is one of the most popular libraries for machine learning in Python. It simplifies feature selection, transformation, and evaluation.
    \end{block}

    \begin{block}{Key Functions for Feature Engineering}
        \begin{itemize}
            \item \textbf{Feature Scaling:} Normalize or standardize features to prevent bias.
            \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)  # Standard scaling
            \end{lstlisting}

            \item \textbf{Feature Selection:} Identify and select the most relevant features to reduce overfitting.
            \begin{lstlisting}[language=Python]
from sklearn.feature_selection import SelectKBest, f_classif
X_new = SelectKBest(f_classif, k=10).fit_transform(X, y)  # Selects top k features
            \end{lstlisting}

            \item \textbf{Pipeline for Feature Processing:} Streamline preprocessing steps.
            \begin{lstlisting}[language=Python]
from sklearn.pipeline import Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('selector', SelectKBest(f_classif, k=10))
])  # Creates a processing pipeline
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Importance of Feature Engineering:} Effective feature engineering can significantly enhance model performance.
            \item \textbf{Versatility of Tools:} Both Pandas and Scikit-Learn provide efficient methods for manipulating datasets and transforming features.
            \item \textbf{Integration in Workflows:} Understanding these tools is essential for seamless integration into the machine learning workflow.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Familiarity with Pandas and Scikit-Learn will empower you to perform robust feature engineering, facilitating better analysis and more accurate predictive models. Start experimenting with these libraries to enhance your data science toolkit!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assessment and Evaluation of Feature Engineering - Introduction}
    \begin{block}{Introduction}
        Feature engineering is a crucial step in building effective machine learning models, involving the creation, transformation, or selection of features to enhance model performance. 
        To yield the best results, we must evaluate and assess the techniques employed and their integration into the overall machine learning pipeline.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Feature Importance Evaluation}
            \begin{itemize}
                \item Assesses each feature's contribution to model predictive power.
                \item Common techniques:
                    \begin{itemize}
                        \item \textbf{Permutation Importance:} Measures the change in model performance by permuting a feature's values.
                        \item \textbf{SHAP Values:} Provides insights into each feature's contribution to individual predictions.
                    \end{itemize}
                \item \textit{Example:} If a feature like "Age" significantly increases model error when permuted, it indicates high importance.
            \end{itemize}
        
        \item \textbf{Cross-Validation for Feature Sets}
            \begin{itemize}
                \item Use k-fold cross-validation to evaluate the effectiveness of different feature sets.
                \item Assesses the generalization of chosen features to unseen data.
                \item \textit{Example:} Split the dataset into 5 parts, train on 4 parts, validate on the 5th, and repeat for all parts.
            \end{itemize}

        \item \textbf{Model Performance Metrics}
            \begin{itemize}
                \item Quantitative metrics to evaluate model performance after feature engineering:
                    \begin{itemize}
                        \item \textbf{Accuracy:} The proportion of true results among the examined cases.
                        \item \textbf{Precision \& Recall:} Especially important in imbalanced datasets.
                        \item \textbf{F1 Score:} The harmonic mean of precision and recall.
                    \end{itemize}
                \item \textbf{Formulae:}
                \begin{equation}
                    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
                \end{equation}
                \begin{equation}
                    F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration into the Machine Learning Pipeline}
    \begin{enumerate}
        \item \textbf{Development Phase}
            \begin{itemize}
                \item Iteratively conduct feature engineering and model training.
                \item Use \textbf{Feature Selection} methods like Recursive Feature Elimination (RFE) to refine feature sets.
            \end{itemize}

        \item \textbf{Deployment Phase}
            \begin{itemize}
                \item Monitor feature importance and data drift post-deployment.
                \item Adjust feature engineering techniques based on new data and performance insights.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points to Emphasize}
    \begin{itemize}
        \item Effective feature engineering can lead to substantial improvements in model performance.
        \item Regular evaluation of features is crucial as new data or methods may change their importance.
        \item Seamless integration of feature engineering into the machine learning pipeline is essential for ongoing improvement.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Assessment and evaluation of feature engineering techniques are vital to enhancing model performance. Using various methods and metrics ensures models remain robust and generalizable.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Importance of Feature Engineering}
    \begin{block}{Understanding Feature Engineering}
        Feature engineering is a crucial step in the machine learning pipeline. It involves the creation, selection, and transformation of variables (features) used in predictive models. Refined input data significantly impacts model performance and accuracy.
    \end{block}
    
    \begin{block}{Key Points of Importance}
        \begin{enumerate}
            \item Enhances Model Performance
            \item Reduces Overfitting
            \item Improves Interpretability
            \item Facilitates Data Efficiency
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Enhancements and Techniques}
    \begin{block}{Enhances Model Performance}
        Properly engineered features can lead to better accuracy. For example, breaking down a date into day, month, and year can help capture seasonal trends.
    
        \textbf{Example:} Predicting house prices often yields better results with the feature "age of the house" rather than just the "year built."
    \end{block}
    
    \begin{block}{Encouragement to Explore Techniques}
        Explore various feature engineering techniques:
        \begin{itemize}
            \item Polynomial Features
            \item Log Transformations
            \item One-Hot Encoding
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Practical Application and Call to Action}
    \begin{block}{Practical Applications}
        \textbf{Practice with Feature Engineering:}
        Consider undertaking mini-projects to apply feature engineering methods and enhance model performance.
    \end{block}
    
    \begin{block}{Call to Action}
        By recognizing the pivotal role of feature engineering, you can equip yourselves with the skills necessary to tackle complex data science problems effectively.
    \end{block}
\end{frame}


\end{document}