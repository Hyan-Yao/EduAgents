\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Supervised Learning]{Week 5: Supervised Learning - Classification}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Overview}
    \begin{block}{What is Supervised Learning?}
        Supervised learning is a core component of machine learning where an algorithm is trained using labeled data. The model learns from a dataset comprising input features (independent variables) and corresponding output labels (dependent variables).
    \end{block}
    \begin{itemize}
        \item Goal: Learn a mapping from inputs to outputs to make predictions on unseen data.
        \item Examples of labeled data: 
            \begin{itemize}
                \item Email spam detection (inputs: emails, outputs: "spam" or "not spam")
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Key Elements}
    \begin{itemize}
        \item \textbf{Labeled Data:} Input-output pairs that the model uses for training.
        \item \textbf{Training Phase:} The model analyzes relationships between inputs and outputs to minimize prediction errors.
        \item \textbf{Predictive Modeling:} The model predicts labels for new, unlabeled data based on learned patterns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Significance}
    \begin{block}{Significance in Machine Learning}
        Supervised learning is foundational due to:
        \begin{itemize}
            \item \textbf{Guided Learning:} The model improves predictions with correct outputs.
            \item \textbf{High Accuracy:} Achieves high prediction accuracy with sufficient quality labeled data.
            \item \textbf{Wide Applicability:} Used in various fields such as finance, healthcare, and marketing.
        \end{itemize}
    \end{block}
    
    \textbf{Examples:}
    \begin{enumerate}
        \item Spam Detection: Train on labeled emails to classify incoming messages.
        \item Image Recognition: Recognize objects in images (e.g., "cat," "dog").
        \item Sentiment Analysis: Classify reviews as "positive," "negative," or "neutral."
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Key Points}
    \begin{itemize}
        \item \textbf{Data Quality:} Model performance largely depends on the quality and quantity of labeled data.
        \item \textbf{Overfitting vs. Underfitting:} Balance is crucial; overfitting captures noise, underfitting fails to capture patterns.
        \item \textbf{Evaluation Metrics:} Use metrics like accuracy, precision, recall, and F1 score to evaluate model performance.
    \end{itemize}

    \textbf{Mathematical Notation:}
    \begin{equation}
        f: X \to y \quad \text{where} \quad X = \{x_1, x_2, \ldots, x_n\}, \quad y = \{y_1, y_2, \ldots, y_n\}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Conclusion}
    Supervised learning plays a pivotal role in machine learning, enabling impactful applications in daily life. 
    Understanding its foundations is essential for exploring classification and predictive modeling in upcoming discussions.
\end{frame}

\begin{frame}[fragile]{Classification Overview - Definition}
    \begin{block}{Definition of Classification in Supervised Learning}
        Classification is a fundamental task in supervised learning, where the objective is to categorize new observations into predefined classes or groups based on their features. 
        The algorithm learns from labeled training data, consisting of input-output pairs (features and corresponding class labels).
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Characteristics}:
        \begin{itemize}
            \item Uses labeled data: Each training instance comes with an associated class label.
            \item Predictive modeling: Classification predicts the class label for unlabeled data based on learned patterns.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Classification Overview - Role in Predictive Modeling}
    \begin{block}{The Role of Classification in Predictive Modeling}
        Classification plays a crucial role in predictive modeling by enabling decision-making processes across various fields:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Identification}: Helps identify the class to which a new instance belongs based on existing data patterns.
        \item \textbf{Segmentation}: Classifiers can segment the dataset into meaningful categories for analysis.
        \item \textbf{Decision Support}: Class predictions assist organizations in making informed decisions by providing insights derived from data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Classification Overview - Example and Key Points}
    \begin{block}{Example of Classification}
        Consider an email filtering system that classifies emails into "Spam" or "Not Spam." 
        \begin{itemize}
            \item \textbf{Training Data}: The model is trained on a dataset that consists of emails labeled either as "Spam" or "Not Spam."
            \item \textbf{Features}: Features could include the email's subject line, sender, and the presence of certain keywords.
            \item \textbf{Process}: Once trained, the model can classify incoming emails based on the patterns it learned.
        \end{itemize}
        \textbf{Illustration}: Imagine a scatter plot where points represent emails categorized by features (e.g., keyword frequency).
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Types of Classification}:
            \begin{itemize}
                \item Binary Classification: Two classes (e.g., yes/no).
                \item Multi-class Classification: More than two classes (e.g., categorizing animals as cat, dog, bird).
            \end{itemize}

            \item \textbf{Common Algorithms} include:
            \begin{itemize}
                \item Decision Trees
                \item Logistic Regression
                \item Support Vector Machines (SVM)
                \item Neural Networks
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Classification Overview - Formula and Conclusion}
    \begin{block}{Simple Example Formula}
        A common approach in classification is the Logistic Regression formula:
        \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
        Where:
        \begin{itemize}
            \item $P(Y=1|X)$ is the probability that the output class is 1 given input features $X$.
            \item $\beta_0, \beta_1, \ldots, \beta_n$ are the coefficients learned from the training data.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        In summary, classification in supervised learning is vital for constructing predictive models that assist in decision-making across various applications. By learning patterns from labeled data, classification algorithms offer insights that greatly enhance automated systems and human judgment alike.
    \end{block}
    
    \textbf{Next:} We will explore \textbf{Key Applications of Classification} in different industries, demonstrating its versatility and impact.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Classification}
    \begin{block}{Introduction to Classification Applications}
        Classification is a fundamental task in supervised learning, where we categorize data into predefined classes or labels. Its effectiveness is showcased across various industries, leading to impactful results.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications Across Industries}
    \begin{enumerate}
        \item \textbf{Healthcare}
        \begin{itemize}
            \item \textit{Disease Diagnosis}: Models classify medical images (e.g., X-rays) as 'normal' or 'abnormal'.
            \item \textit{Patient Risk Assessment}: Classifying records helps identify high-risk patients based on factors like age and medical history.
        \end{itemize}

        \item \textbf{Finance}
        \begin{itemize}
            \item \textit{Credit Scoring}: Classifying loan applicants as 'low risk' or 'high risk' based on financial information.
            \item \textit{Fraud Detection}: Classifying transactions as 'legitimate' or 'fraudulent' to enhance security.
        \end{itemize}

        \item \textbf{Social Media}
        \begin{itemize}
            \item \textit{Content Moderation}: Automatically detecting and classifying user content as 'appropriate' or 'inappropriate'.
            \item \textit{User Sentiment Analysis}: Classifying posts as 'positive', 'negative', or 'neutral' to gauge public opinion.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item \textbf{Versatility}: Applicable across numerous fields, showcasing adaptability and importance.
            \item \textbf{Data-Driven Decisions}: Supports informed decision-making and enhances efficiency.
            \item \textbf{Impact on Society}: Significant implications for public health, financial stability, and online safety.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding the practical applications of classification empowers us to realize its potential impact. As technology evolves, classification methods are expected to expand, ushering in innovations across industries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Decision Trees}
  
  \begin{block}{Introduction to Decision Trees}
    Decision Trees are a powerful and intuitive classification method used in machine learning. They model decisions and their possible consequences, including chance event outcomes, resource costs, and utility. 
  \end{block}
  
  The structure resembles a tree:
  \begin{itemize}
    \item Each internal node represents a decision based on a feature.
    \item Each branch represents the outcome of that decision.
    \item Each leaf node represents a class label.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Structure of Decision Trees}

  \begin{enumerate}
    \item \textbf{Root Node}
      \begin{itemize}
        \item The topmost node representing the entire dataset.
        \item It splits into sub-nodes based on a feature that best separates the data.
      \end{itemize}

    \item \textbf{Decision Nodes}
      \begin{itemize}
        \item Internal nodes where the dataset is split based on attribute tests.
        \item Corresponds to a feature, and branches represent the outcomes of these decisions.
        \item \textit{Example:} A decision node might test if a customer's age is greater than 30.
      \end{itemize}

    \item \textbf{Leaf Nodes}
      \begin{itemize}
        \item End points of the tree representing the final class labels.
        \item Once the path from the root to a leaf node is determined, the final outcome is reached.
        \item \textit{Example:} A leaf node indicating "Yes" for purchasing or "No" for not purchasing.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Examples}

  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item \textbf{Interpretability:} Easy to understand for stakeholders.
      \item \textbf{Non-Linear Relationships:} Can capture non-linear interactions that linear models cannot.
      \item \textbf{Overfitting:} Pruning is essential to avoid overfitting; balance between complexity and accuracy is crucial.
    \end{itemize}
  \end{block}

  \begin{block}{Example Structure of a Simple Decision Tree}
    \begin{lstlisting}[basicstyle=\tiny]
                      [Age > 30]
                      /      \
                   Yes        No
                    |          |
                [Income > 50K]  [Buy = No]
                   /    \
                Yes      No
                    |       |
                [Buy = Yes] [Buy = No]
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps}

  \begin{block}{Conclusion}
    Decision Trees serve as a foundational algorithm in supervised learning for classification tasks. By breaking down complex decision-making into simple, interpretable choices, they provide a clear and effective classification approach.
  \end{block}

  \begin{block}{Next Steps}
    The next slide will delve into the process of building decision trees, detailing crucial concepts like splitting criteria, entropy, and Gini impurity, which determine how decisions are made at each node.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Overview}
    \begin{block}{Overview of Decision Trees}
        Decision trees are a powerful supervised learning algorithm used for classification tasks. 
        They model decisions based on asking sequential questions about the features of the data, leading to a final classification.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Constructing a Decision Tree}
    \begin{enumerate}
        \item \textbf{Selection of Features}: Identify the relevant features based on the dataset.
        \item \textbf{Splitting Criteria}: Choosing how to split the data at each node in the tree.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Splitting Criteria}
    \begin{block}{Splitting Criteria}
        Two common methods for determining how to split the dataset are \textbf{Entropy} and \textbf{Gini Impurity}. 
        These measures help calculate the "impurity" or "information gain" of the dataset when a feature is selected for splitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Entropy}
    \begin{block}{Entropy}
        Entropy measures the unpredictability or disorder in the data and helps to determine the purity of a split:
        \begin{equation}
            \text{Entropy}(S) = -\sum_{i=1}^{C} p_i \log_2(p_i)
        \end{equation}
        Where \(p_i\) is the probability of class \(i\) in set \(S\) and \(C\) is the number of classes.
    \end{block}
    \begin{block}{Example}
        If we have a dataset with two classes (Yes and No), and the distribution is 70\% Yes and 30\% No:
        \begin{equation}
            \text{Entropy} \approx - (0.7 \log_2(0.7) + 0.3 \log_2(0.3)) \approx 0.88 
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Gini Impurity}
    \begin{block}{Gini Impurity}
        Gini Impurity quantifies how often a randomly chosen element would be incorrectly labeled:
        \begin{equation}
            Gini(S) = 1 - \sum_{i=1}^{C} (p_i)^2
        \end{equation}
    \end{block}
    \begin{block}{Example}
        Using the same dataset with a 70\% Yes and 30\% No distribution:
        \begin{equation}
            Gini = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 0.42
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Splitting Process}
    \begin{block}{Splitting Process}
        At each node, the feature that results in the \textbf{highest information gain} (from entropy) 
        or the \textbf{lowest Gini impurity} is selected for splitting the data.
    \end{block}
    \begin{itemize}
        \item \textbf{Information Gain}: The difference between the entropy of the parent node 
        and the weighted average of the entropies of the child nodes created by the split.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Key Points}
    \begin{itemize}
        \item Decision trees recursively split the dataset based on features that provide the most significant reduction in impurity.
        \item Understanding both Entropy and Gini Impurity is crucial as they guide the selection of features resulting in the most informative splits.
        \item The goal is to achieve a leaf node that is as pure as possible, ideally classifying all samples in that node into a single class.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Conclusion}
    By understanding these concepts, students will be equipped to build and interpret decision trees effectively, 
    setting the stage for analyzing their advantages in the subsequent slide.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Overview}
    \begin{itemize}
        \item Decision Trees are a popular supervised learning algorithm for both classification and regression tasks.
        \item They offer several key advantages:
        \begin{itemize}
            \item Interpretability
            \item Feature Importance
            \item Non-parametric Nature
            \item Handling of Missing Values
            \item Robust to Outliers
            \item Versatile Applications
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Detail}
    \begin{enumerate}
        \item \textbf{Interpretability}
        \begin{itemize}
            \item \textbf{Clear Visualization}: Easy to understand with flowchart-like structure.
            \item \textbf{Easy to Explain}: Rules derived can be communicated clearly.
        \end{itemize}
        
        \item \textbf{Feature Importance}
        \begin{itemize}
            \item Automatically assesses which features influence the target variable.
            \item Importance is determined based on uncertainty reduction during splits.
        \end{itemize}
        
        \item \textbf{Non-parametric Nature}
        \begin{itemize}
            \item Does not assume a specific distribution for data.
            \item Flexibility to model complex relationships.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Further Benefits}
    \begin{enumerate}[resume]
        \item \textbf{Handling of Missing Values}
        \begin{itemize}
            \item Effectively manages missing values without discarding data.
        \end{itemize}

        \item \textbf{Robust to Outliers}
        \begin{itemize}
            \item Splitting based on feature values provides resilience against outliers.
        \end{itemize}

        \item \textbf{Versatile Applications}
        \begin{itemize}
            \item Applicable to both classification and regression tasks.
            \item Examples: Classifying emails as spam and predicting house prices.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Decision Trees provide an interpretable model for classifications.
            \item Highlight the importance of features for better data understanding.
            \item Their non-parametric nature allows versatility across different applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet (Python)}
    Here’s a simple implementation of a Decision Tree using Python's \texttt{sklearn} library:
    
    \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample data
X = [[30, 58000], [25, 49000], [40, 52000], [35, 65000]]  # Features: [Age, Income]
y = [0, 0, 1, 1]  # Targets: 0 (No), 1 (Yes)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create and train model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Predictions
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy * 100}%')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Overview}
    \begin{itemize}
        \item Decision trees are widely used for classification due to their:
        \begin{itemize}
            \item Simplicity
            \item Interpretability
        \end{itemize}
        \item However, they have notable limitations that affect:
        \begin{itemize}
            \item Performance
            \item Generalization abilities
        \end{itemize}
        \item Two primary limitations:
        \begin{itemize}
            \item Overfitting
            \item Sensitivity to noise
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Overfitting}
    \begin{block}{Definition}
        Overfitting occurs when a model learns both the underlying patterns and the noise in the training data, leading to poor performance on unseen data.
    \end{block}
    \begin{itemize}
        \item \textbf{Illustration}:
        \begin{itemize}
            \item A deep tree fits every detail of the training dataset, including noise.
        \end{itemize}
        \item \textbf{Example}:
        \begin{itemize}
            \item In a medical dataset, a decision tree might overly classify based on anomalies rather than general patterns.
        \end{itemize}
        \item \textbf{Mitigation Strategies}:
        \begin{itemize}
            \item Pruning: Remove branches that contribute little.
            \item Set maximum depth of the tree.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Sensitivity to Noise}
    \begin{block}{Definition}
        Decision trees are highly sensitive to fluctuations in the training dataset, leading to varying tree structures.
    \end{block}
    \begin{itemize}
        \item \textbf{Illustration}:
        \begin{itemize}
            \item Mislabeled or noisy instances can drastically alter the generated tree.
        \end{itemize}
        \item \textbf{Example}:
        \begin{itemize}
            \item In a dataset with mislabeled cases, a decision tree can make incorrect splits, affecting prediction accuracy.
        \end{itemize}
        \item \textbf{Mitigation Strategies}:
        \begin{itemize}
            \item Use ensemble methods, such as Random Forests, to enhance robustness.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Key Points}
    \begin{itemize}
        \item Decision tree models can become too complex, affecting generalization.
        \item Sensitivity to training data can skew results due to minor inaccuracies.
        \item To improve performance, employ techniques such as:
        \begin{itemize}
            \item Pruning
            \item Using ensemble methods
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Code Example}
    \begin{block}{Key Formula for Depth Control}
        To limit overfitting, you can set parameters such as the maximum depth of the tree:
    \end{block}
    \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=5) # Limit tree depth to 5
    \end{lstlisting}
    \begin{itemize}
        \item This code snippet creates a decision tree classifier that grows to a maximum depth of 5, helping prevent overfitting.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to k-Nearest Neighbors}
    \begin{block}{What is k-Nearest Neighbors (k-NN)?}
        k-Nearest Neighbors (k-NN) is a simple, yet effective classification algorithm in supervised learning that categorizes a data point based on how its neighbors are classified. 
        It operates on the principle that similar data points are closely positioned in a feature space to make classification decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Functioning of k-NN}
    \begin{enumerate}
        \item \textbf{Instance-Based Learning:} k-NN does not learn a model explicitly. Instead, it stores all training instances and classifies data on-the-fly.
        \item \textbf{Classification Process:}
            \begin{itemize}
                \item Examines the 'k' closest training examples (neighbors).
                \item Each neighbor votes for its class.
                \item The class with the most votes is assigned to the new instance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in k-NN}
    \begin{enumerate}
        \item \textbf{Choosing the Parameter k:}
            \begin{itemize}
                \item 'k' specifies the number of nearest neighbors to consider.
                \item Small k can lead to overfitting; large k may result in underfitting.
            \end{itemize}
        \item \textbf{Distance Measurement:}
            \begin{itemize}
                \item \textbf{Euclidean Distance:}
                    \begin{equation}
                        d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}
                    \end{equation}
                \item \textbf{Manhattan Distance:}
                    \begin{equation}
                        d(p, q) = \sum_{i=1}^{n} |p_i - q_i|
                    \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Working of k-NN}
    \begin{block}{Understanding the k-NN Algorithm}
        The k-Nearest Neighbors (k-NN) algorithm is a simple yet powerful supervised learning technique used for classification (and sometimes regression). It operates on the principle that similar data points are located close to one another in the feature space.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Process of k-NN}
    \begin{enumerate}
        \item \textbf{Input the Training Data}
              \begin{itemize}
                  \item Start with a set of labeled training data points.
              \end{itemize}
        \item \textbf{Calculate Distances}
              \begin{itemize}
                  \item For a new query point, calculate its distance from all points in the training set.
              \end{itemize}
              \begin{block}{Distance Metrics}
                  \begin{itemize}
                      \item \textbf{Euclidean Distance:} 
                      \[
                      d = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
                      \]
                      \item \textbf{Manhattan Distance:} 
                      \[
                      d = \sum_{i=1}^{n} |x_i - y_i|
                      \]
                  \end{itemize}
              \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Choosing 'k' and Final Steps}
    \begin{enumerate}[resume]
        \item \textbf{Identify the k Nearest Neighbors}
              \begin{itemize}
                  \item Select the 'k' closest training examples.
              \end{itemize}
        \item \textbf{Vote for Class Labels}
              \begin{itemize}
                  \item The predicted class is determined by a majority vote among k-nearest neighbors.
              \end{itemize}
        \item \textbf{Final Classification}
              \begin{itemize}
                  \item The class with the most votes is assigned to the new data point.
              \end{itemize}
    \end{enumerate}
    
    \begin{block}{Choosing 'k'}
        \textbf{Small k:} Sensitive to noise (overfitting).\\
        \textbf{Large k:} Smoother boundaries (may include dissimilar points).\\
        \textbf{Best Practice:} Use cross-validation to determine optimal 'k'.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)

# Initialize classifier with k
k = 3
knn = KNeighborsClassifier(n_neighbors=k)

# Fit model
knn.fit(X_train, y_train)

# Make predictions
predictions = knn.predict(X_test)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of k-NN - Overview}
    The k-Nearest Neighbors (k-NN) algorithm is a popular supervised learning method used for classification and regression tasks. It is grounded in simplicity and can be highly effective with certain types of datasets. In this section, we will explore the key advantages of k-NN.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of k-NN - Simplicity and Intuition}
    \begin{itemize}
        \item \textbf{Easy to Understand:} Classifies a data point based on the class of its 'k' nearest neighbors in feature space.
        \item \textbf{No Training Phase:} Minimal training involved; the model stores the entire training dataset without learning parameters.
    \end{itemize}
    \begin{block}{Example}
        To classify a new data point, calculate its distance to all points in the training set, identify the closest 'k' neighbors, and assign the most common class among them.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of k-NN - Versatility and Effectiveness}
    \begin{itemize}
        \item \textbf{Works with Various Distance Metrics:} Adaptable to different datasets through metrics like Euclidean and Manhattan distances.
        
        \begin{itemize}
            \item \textbf{Euclidean Distance:} Straight-line distance in multi-dimensional space.
            \item \textbf{Manhattan Distance:} Grid-like path distance (sum of absolute differences).
        \end{itemize}
        
        \item \textbf{Multi-class Problem Handling:} Naturally accommodates multi-class classification with no extra configuration.

        \item \textbf{High Performance with Small Datasets:} Excels with small datasets and low dimensionality, leveraging local structures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of k-NN - Further Benefits}
    \begin{itemize}
        \item \textbf{No Assumptions About Data Distribution:} Non-parametric method, beneficial for real-world datasets that may not conform to parametric model assumptions.
        \item \textbf{Incremental Learning:} New data points can be added easily without retraining, making k-NN suitable for evolving datasets.
    \end{itemize}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Simplicity and Intuition
            \item Versatile with distance metrics
            \item Effective for small datasets
            \item Non-parametric nature
            \item Dynamic updates with new data
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of k-NN - Conclusion}
    The advantages of k-NN make it a valuable tool for data scientists and machine learning practitioners, particularly for tasks where interpretability and simplicity are prioritized. In the next slide, we will discuss the limitations of k-NN and scenarios where it may not perform as well.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-NN - Overview}
    \begin{block}{Overview}
        While k-Nearest Neighbors (k-NN) is a widely used and intuitive classification algorithm, it has notable limitations that can impact its effectiveness, particularly in large datasets or complex feature spaces.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-NN - Computational Inefficiency}
    \begin{itemize}
        \item \textbf{Computational Inefficiency}
        \begin{itemize}
            \item \textbf{Description:} The k-NN algorithm is computationally expensive as it requires calculating distances between the new instance and all training instances during both training and prediction.
            \item \textbf{Impact:} Time complexity for both training and querying is O(n \cdot d), where:
            \begin{itemize}
                \item $n$ is the number of instances in the dataset
                \item $d$ is the number of dimensions (features)
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \begin{block}{Example}
        For a dataset with 1,000 data points and 50 features, predicting the class for a new point requires calculating distances to all 1,000 points, requiring considerable computational resources.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-NN - Sensitivity to Irrelevant Features}
    \begin{itemize}
        \item \textbf{Sensitivity to Irrelevant Features}
        \begin{itemize}
            \item \textbf{Description:} k-NN employs distance metrics that can be influenced by irrelevant or redundant features.
            \item \textbf{Impact:} Inclusion of irrelevant features can introduce distance variance, increasing the likelihood of misclassification.
        \end{itemize}
    \end{itemize}
    \begin{block}{Example}
        Consider a dataset where one feature is height and another irrelevant feature is shoe color. Including shoe color can distort distance calculations and lead to poor results.
    \end{block}
    \begin{itemize}
        \item \textbf{Mitigation Strategies:}
        \begin{itemize}
            \item Feature Selection: Identify and remove irrelevant features through techniques like recursive feature elimination (RFE).
            \item Normalization: Standardizing feature values to minimize skewing effects of features with larger ranges.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-NN - Conclusion and Key Points}
    \begin{block}{Conclusion}
        Understanding these limitations is crucial for effectively applying k-NN in real-world scenarios. Attention to computational efficiency and feature relevance is essential for optimal performance.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Points to Remember:}
        \begin{itemize}
            \item k-NN is computationally intensive, especially with large datasets.
            \item It is sensitive to irrelevant features, which can skew results.
            \item Strategies exist to mitigate these limitations, including dimensionality reduction and careful feature selection.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Techniques}
    \begin{block}{Understanding Model Evaluation in Classification}
        Model evaluation is crucial to understand how well our classification models are performing. In this section, we will discuss three key evaluation techniques:
        \begin{itemize}
            \item Accuracy
            \item F1 Score
            \item ROC Curves
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Accuracy}
    \begin{block}{Definition}
        Accuracy is the ratio of correctly predicted instances to the total instances in the dataset.
    \end{block}
    
    \begin{block}{Formula}
        \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \end{equation}
        Where:
        \begin{itemize}
            \item TP (True Positives): Correctly predicted positive cases
            \item TN (True Negatives): Correctly predicted negative cases
            \item FP (False Positives): Incorrectly predicted positive cases
            \item FN (False Negatives): Incorrectly predicted negative cases
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Imagine a model predicting if an email is spam (positive) or not spam (negative):
        \begin{itemize}
            \item TP: 80, TN: 150, FP: 10, FN: 5
        \end{itemize}
        
        Calculating accuracy:
        \begin{equation}
            \text{Accuracy} = \frac{80 + 150}{80 + 150 + 10 + 5} = \frac{230}{245} \approx 0.94 \, (94\%)
        \end{equation}
    \end{block}
    
    \begin{block}{Key Point}
        Accuracy can be misleading in cases of imbalanced datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. F1 Score}
    \begin{block}{Definition}
        The F1 Score is the harmonic mean of precision and recall, providing a balance between the two.
    \end{block}

    \begin{block}{Formulas}
        \begin{itemize}
            \item Precision: \(\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\)
            \item Recall: \(\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\)
            \item F1 Score: 
            \begin{equation}
                \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Using the previous example:
        \begin{itemize}
            \item TP = 80, FP = 10, FN = 5
        \end{itemize}
        
        Calculating Precision:
        \begin{equation}
            \text{Precision} = \frac{80}{80 + 10} = 0.89
        \end{equation}
        
        Calculating Recall:
        \begin{equation}
            \text{Recall} = \frac{80}{80 + 5} = 0.94
        \end{equation}
        
        Calculating F1 Score:
        \begin{equation}
            \text{F1 Score} \approx 0.91 \, (91\%)
        \end{equation}
    \end{block}
    
    \begin{block}{Key Point}
        The F1 Score is valuable when false positives and false negatives carry different costs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. ROC Curves}
    \begin{block}{Definition}
        The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) at various threshold settings.
    \end{block}
    
    \begin{block}{Key Terms}
        \begin{itemize}
            \item True Positive Rate (TPR): Same as Recall.
            \item False Positive Rate (FPR): \(\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\)
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Varying the classification threshold for predicting spam emails plots TPR against FPR and visualizes the trade-off between sensitivity and specificity.
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item An ideal classifier has an ROC curve that hugs the top left corner of the plot.
            \item The area under the curve (AUC) represents the model performance: AUC of 1 indicates perfect classification; AUC of 0.5 indicates no ability to discriminate.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    In classification tasks, data scientists use various metrics to evaluate model performance. 
    \begin{itemize}
        \item Accuracy provides a general sense of performance.
        \item F1 Score balances precision and recall.
        \item ROC Curves visualize trade-offs between true and false positive rates.
    \end{itemize}
    Understanding these techniques will enable better model evaluation and selection based on problem requirements.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation}
    \begin{block}{Understanding Cross-Validation}
        Cross-validation is a robust statistical method used to evaluate the performance of machine learning models. It involves partitioning the dataset into several subsets, training the model on one subset, and validating it on another. This technique is essential for assessing how well a model generalizes to an independent dataset.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Main Objectives of Cross-Validation}
    \begin{enumerate}
        \item \textbf{Model Evaluation}: Gauges the effectiveness of a classification model by mimicking its behavior on unseen data.
        \item \textbf{Prevent Overfitting}: Minimizes the risk of creating a model that performs well on training data but poorly on new data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Cross-Validation}
    \begin{itemize}
        \item \textbf{Resilience Against Variability}: Provides a comprehensive assessment by averaging results over multiple iterations rather than relying on a single split.
        \item \textbf{Better Utilization of Data}: Permits the entire dataset to be used for training and validation, yielding a more reliable model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Methods of Cross-Validation}
    \begin{enumerate}
        \item \textbf{k-Fold Cross-Validation}:
            \begin{itemize}
                \item The dataset is divided into $k$ equally sized folds.
                \item The model is trained on $k-1$ folds and validated on the remaining fold, repeated $k$ times.
                \item Final performance is averaged over the $k$ iterations.
            \end{itemize}
        \item \textbf{Leave-One-Out Cross-Validation (LOOCV)}:
            \begin{itemize}
                \item Each training set is created by leaving out a single instance for validation. Best for small datasets but computationally expensive for large datasets.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example for k-Fold Cross-Validation}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
scores = cross_val_score(model, X, y, cv=5)  # where X is features and y is labels
print("Mean accuracy:", scores.mean())
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Overfitting Mitigation}: Helps in identifying model complexity and whether overfitting is occurring.
        \item \textbf{Hyperparameter Tuning}: Essential for ensuring selected parameters yield consistent performance across data splits.
        \item \textbf{Generalization Assessment}: Provides insight into model performance on unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    In summary, cross-validation is a crucial technique for validating a classification model’s effectiveness. It ensures that our model is reliable, generalizes well, and helps safeguard against the risk of overfitting, making it an essential step in the model development process.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Classification - Introduction}
    \begin{block}{Overview}
        Classification algorithms play a crucial role in various applications:
        \begin{itemize}
            \item Credit scoring
            \item Healthcare diagnostics
            \item Facial recognition
            \item Social services
        \end{itemize}
        Ethical considerations are vital when deploying these models. We will explore two key aspects:
        \begin{itemize}
            \item Algorithmic bias
            \item Data privacy
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Classification - Algorithmic Bias}
    \begin{block}{Definition}
        Algorithmic bias occurs when a model systematically produces unfair results for certain groups due to:
        \begin{itemize}
            \item Prejudiced training data
            \item Biased model design
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        A hiring algorithm trained on historical employee data may favor candidates from a particular demographic, perpetuating existing inequalities.
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Sources of Bias:
                \begin{itemize}
                    \item Biased training data
                    \item Implicit bias in feature selection
                \end{itemize}
            \item Consequences:
                \begin{itemize}
                    \item Discrimination against underrepresented groups
                    \item Erosion of trust in automated systems
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Classification - Data Privacy}
    \begin{block}{Definition}
        Data privacy involves protecting personal information to prevent unauthorized access and safeguard individual rights.
    \end{block}
    
    \begin{block}{Example}
        Health informatics models may use sensitive data, such as medical history, putting patient privacy at risk if disclosed.
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Importance of Consent: Collect and use data with informed consent.
            \item Data Anonymization: Techniques like k-anonymity help obscure identities.
            \item Secure Data Practices:
                \begin{itemize}
                    \item Use encryption and secure storage
                    \item Implement strict access controls
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Classification - Conclusion}
    Ethical considerations in classification are crucial for fair and respectful model development. Key takeaways:
    \begin{itemize}
        \item Addressing algorithmic bias is essential to prevent unfair treatment.
        \item Protecting data privacy is vital for maintaining individuals' rights.
        \item Implementing ethical guidelines enhances public trust and effectiveness.
    \end{itemize}
    
    By prioritizing these considerations, we can responsibly harness the power of classification.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Real-World Case Studies - Introduction}
  \begin{block}{Overview}
    \begin{itemize}
      \item **Decision Trees** and **k-Nearest Neighbors (k-NN)** are prominent supervised learning algorithms used in classification tasks.
      \item Both algorithms enable predictions based on input features learned from historical data.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study 1: Decision Trees in Healthcare}
  \begin{block}{Scenario}
    Predicting Patient Readmission
  \end{block}
  \begin{itemize}
    \item **Objective:** Analyze patient data to predict likelihood of readmission within 30 days of discharge.
    \item **Data Collected:** Age, previous admissions, treatment history, and health metrics.
  \end{itemize}
  
  \begin{block}{Implementation Steps}
    \begin{enumerate}
      \item Data Preparation: Clean and preprocess the data.
      \item Model Training: Utilize a Decision Tree classifier.
      \item Evaluation: Assess accuracy with stratified cross-validation.
    \end{enumerate}
  \end{block}
  
  \begin{block}{Results}
    \begin{itemize}
      \item Achieved an accuracy rate of 85\%.
      \item Provided insights into factors leading to readmissions.
    \end{itemize}
  \end{block}
  
  \begin{block}{Key Benefit}
    Easily interpretable model aiding clinicians in decision-making.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study 2: k-NN in Retail}
  \begin{block}{Scenario}
    Customer Segmentation for Marketing
  \end{block}
  \begin{itemize}
    \item **Objective:** Classify customers based on purchasing behavior.
    \item **Data Collected:** Purchase history, demographics, and engagement metrics.
  \end{itemize}
  
  \begin{block}{Implementation Steps}
    \begin{enumerate}
      \item Feature Selection: Normalize data for distance metrics.
      \item Model Training: Implement k-NN with \( k=5 \).
      \item Evaluation: Use metrics like precision and recall.
    \end{enumerate}
  \end{block}
  
  \begin{block}{Results}
    \begin{itemize}
      \item Effectively segmented customers, highlighting distinct purchasing patterns.
      \item Increased marketing campaign response by 20\% in targeted groups.
    \end{itemize}
  \end{block}
  
  \begin{block}{Key Benefit}
    Enables businesses to focus resources on high-value customers, enhancing ROI.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Conclusion}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item **Interpretability:** Decision Trees provide visually interpretable models useful for transparency.
      \item **Flexibility:** k-NN is versatile for different types of data, especially in high-dimensional spaces.
      \item **Practical Application:** These algorithms improve decision-making across sectors.
    \end{itemize}
  \end{block}
  
  \begin{block}{Conclusion}
    Understanding practical applications of algorithms like Decision Trees and k-NN
    bridges theoretical knowledge with real-world usage, emphasizing their role in
    data-driven decision-making across industries.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Overview}
    \begin{block}{Key Takeaways}
        \begin{enumerate}
            \item \textbf{What is Classification?} 
            \begin{itemize}
                \item A supervised learning technique for assigning labels to input data.
                \item Key algorithms: Decision Trees, k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), and Neural Networks.
            \end{itemize}
            
            \item \textbf{Performance Metrics:} 
            \begin{itemize}
                \item Accuracy, Precision, Recall (Sensitivity), F1 Score.
            \end{itemize}
            
            \item \textbf{Overfitting and Underfitting:} 
            \begin{itemize}
                \item Overfitting: Good performance on training data but poor on unseen data.
                \item Underfitting: Too simple to capture underlying patterns.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Performance Metrics}
    \begin{block}{Performance Metrics Example}
        \begin{itemize}
            \item Medical test scenario with 100 patients, 30 have the disease:
                \begin{itemize}
                    \item True Positives: 25, True Negatives: 60, False Positives: 10, False Negatives: 5
                \end{itemize}
            \item \textbf{Accuracy}:
            \begin{equation}
                \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}} = \frac{25 + 60}{100} = 0.85\text{ or } 85\%
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Classification}
    \begin{block}{Emerging Trends}
        \begin{enumerate}
            \item \textbf{Deep Learning:} 
            \begin{itemize}
                \item Neural networks, especially CNNs, have revolutionized tasks in image and speech recognition.
            \end{itemize}
            
            \item \textbf{Automated Machine Learning (AutoML):} 
            \begin{itemize}
                \item Automating algorithm selection and tuning. Frameworks like Google’s AutoML are simplifying model building.
            \end{itemize}
            
            \item \textbf{Interpretability:} 
            \begin{itemize}
                \item Importance of understanding decisions made by classifiers using methods like LIME and SHAP.
            \end{itemize}
            
            \item \textbf{Transfer Learning:} 
            \begin{itemize}
                \item Using pre-trained models to improve performance and save training time, especially with limited data.
            \end{itemize}
            
            \item \textbf{Ethics and Fairness:} 
            \begin{itemize}
                \item Critical need to ensure ethical use and fairness in classifications and mitigating bias.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}


\end{document}