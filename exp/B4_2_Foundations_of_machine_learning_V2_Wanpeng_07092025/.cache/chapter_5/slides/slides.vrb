\frametitle{Limitations of k-NN - Computational Inefficiency}
    \begin{itemize}
        \item \textbf{Computational Inefficiency}
        \begin{itemize}
            \item \textbf{Description:} The k-NN algorithm is computationally expensive as it requires calculating distances between the new instance and all training instances during both training and prediction.
            \item \textbf{Impact:} Time complexity for both training and querying is O(n \cdot d), where:
            \begin{itemize}
                \item $n$ is the number of instances in the dataset
                \item $d$ is the number of dimensions (features)
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \begin{block}{Example}
        For a dataset with 1,000 data points and 50 features, predicting the class for a new point requires calculating distances to all 1,000 points, requiring considerable computational resources.
    \end{block}
