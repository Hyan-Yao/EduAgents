\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Data Preprocessing Techniques]{Week 2: Data Preprocessing Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{Overview}
        Data preprocessing is a critical step in the machine learning pipeline that involves transforming raw data into a suitable format for modeling. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Enhances Data Quality}
        \begin{itemize}
            \item High-quality data leads to better model performance. 
            \item Techniques include removing duplicates and correcting mislabeled classes.
        \end{itemize}

        \item \textbf{Improves Model Accuracy}
        \begin{itemize}
            \item Properly preprocessed data results in models that generalize well. 
            \item Filling missing values can stabilize training outcomes.
        \end{itemize}

        \item \textbf{Facilitates Model Training}
        \begin{itemize}
            \item Certain algorithms have specific data requirements (e.g., scaling).
            \item Example: K-Nearest Neighbors (KNN) is sensitive to feature magnitudes.
        \end{itemize}

        \item \textbf{Reduces Computational Complexity}
        \begin{itemize}
            \item Preprocessing can significantly decrease dimensionality.
            \item Feature selection retains only relevant variables, enhancing efficiency.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques in Data Preprocessing}
    \begin{itemize}
        \item \textbf{Data Cleaning}: Identifying and correcting errors in the dataset.
        \item \textbf{Data Transformation}: Changing data format or values for analysis.
        \item \textbf{Data Reduction}: Reducing data volume while maintaining integrity (e.g., PCA).
        \item \textbf{Feature Engineering}: Creating new features from existing ones to enhance performance.
    \end{itemize}
    \begin{block}{Final Thoughts}
        Data preprocessing is a cornerstone of successful machine learning projects. The quality of input data directly influences model performance. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality}
    \begin{block}{What is Data Quality?}
        Data Quality refers to the condition of a dataset as it relates to:
        \begin{itemize}
            \item Accuracy
            \item Completeness
            \item Consistency
            \item Reliability
            \item Relevance
        \end{itemize}
        High-quality data is essential for producing reliable models and generating valid insights in data analysis and machine learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Characteristics of High-Quality Data}
    \begin{enumerate}
        \item \textbf{Accuracy}:
            \begin{itemize}
                \item Data values are correct and free from errors.
                \item Example: Verified temperature readings.
            \end{itemize}
        \item \textbf{Completeness}:
            \begin{itemize}
                \item Data should include all necessary information.
                \item Example: A customer database without missing contact details.
            \end{itemize}
        \item \textbf{Consistency}:
            \begin{itemize}
                \item Consistent data formats across datasets.
                \item Example: Uniform date representations.
            \end{itemize}
        \item \textbf{Reliability}:
            \begin{itemize}
                \item Data should be sourced reliably.
                \item Example: Validated sensors versus manually entered data.
            \end{itemize}
        \item \textbf{Relevance}:
            \begin{itemize}
                \item Data must be suitable for the analysis needs.
                \item Example: Social media metrics for engagement analysis.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Poor Data Quality on Model Performance}
    \begin{enumerate}
        \item \textbf{Decreased Model Accuracy}:
            \begin{itemize}
                \item Flawed inputs result in incorrect predictions.
                \item Example: Mislabeled images affecting classification accuracy.
            \end{itemize}
        \item \textbf{Increased Overfitting}:
            \begin{itemize}
                \item Models may learn noise instead of actual patterns.
                \item Example: Random fluctuations leading to misleading trends.
            \end{itemize}
        \item \textbf{Bias Introduction}:
            \begin{itemize}
                \item Models may generalize poorly with biased data.
                \item Example: Under-represented demographics affecting predictions.
            \end{itemize}
        \item \textbf{Ineffective Decision Making}:
            \begin{itemize}
                \item Decisions based on faulty data lead to poor outcomes.
                \item Example: Erroneous customer data affecting marketing strategies.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Introduction}
    \begin{block}{Introduction}
        Data cleaning is a crucial step in the data preprocessing phase that prepares our datasets for analysis or model training. High-quality data is essential for producing reliable outcomes, and cleaning helps rectify issues that can lead to poor model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Key Techniques}
    \begin{enumerate}
        \item Removing Duplicates
        \item Correcting Errors
        \item Addressing Formatting Issues
        \item Handling Outliers
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Removing Duplicates}
    \begin{itemize}
        \item \textbf{Description}: Duplicate records can skew results and lead to incorrect conclusions. Identifying and removing these duplicates ensures each observation is unique.
        \item \textbf{Example}: Suppose your dataset has duplicate entries for a particular customer. If "John Doe" appears three times, retaining only one entry will provide an accurate representation of your dataset.
        \item \textbf{Code Snippet (Pandas)}:
        \begin{lstlisting}[language=python]
df.drop_duplicates(inplace=True)
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Correcting Errors}
    \begin{itemize}
        \item \textbf{Description}: Data entry errors can occur due to human mistakes, system errors, or incorrect data integration.
        \item \textbf{Example}: If a date of birth is recorded as 2022 instead of 1992, this incorrect entry needs correction. Regular expressions can be helpful to identify and fix these types of inconsistencies.
        \item \textbf{Formula for Utilizing Regular Expressions (Python)}:
        \begin{lstlisting}[language=python]
df['DOB'] = df['DOB'].replace(r'2022', '1992', regex=True)
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Addressing Formatting Issues}
    \begin{itemize}
        \item \textbf{Description}: Consistency in data formatting is essential, especially when dealing with categorical variables, dates, or text fields.
        \item \textbf{Example}: "Yes" and "yes" should be standardized to the same format (e.g., all lowercase) to avoid mismatches during analysis.
        \item \textbf{Code Snippet (Pandas)}:
        \begin{lstlisting}[language=python]
df['Response'] = df['Response'].str.lower()
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Handling Outliers}
    \begin{itemize}
        \item \textbf{Description}: Outliers can distort statistical analyses and models. Identifying and addressing these values (either by removal or adjustment) is critical.
        \item \textbf{Example}: If you are analyzing household income and one entry is significantly higher than any others, you might choose to remove or cap this entry.
        \item \textbf{Visual Example}: Creating a boxplot can help visualize outliers:
        \begin{lstlisting}[language=python]
import seaborn as sns
sns.boxplot(data=df['Income'])
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Key Points & Conclusion}
    \begin{itemize}
        \item High-quality data directly affects model performance.
        \item Removing duplicates, correcting errors, and formatting issues are foundational steps in data cleansing.
        \item Always validate your cleaning process to ensure no relevant information is lost.
    \end{itemize}
    \begin{block}{Conclusion}
        Data cleaning techniques enhance the reliability of your dataset, ensuring accurate predictions and insights. Effective data cleaning leads to more informed decision-making based on accurate data insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Identifying Missing Values}
  \begin{block}{Overview}
    Missing values are common in datasets and can arise from various reasons, such as:
    \begin{itemize}
      \item Data collection errors
      \item Incomplete surveys
      \item System malfunctions
    \end{itemize}
    Identifying these missing values is crucial for data preprocessing as they can impact analyses and model accuracy.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Types and Implications of Missing Values}
  \begin{block}{Key Concepts}
    \begin{enumerate}
      \item \textbf{Types of Missing Values}:
        \begin{itemize}
          \item Missing Completely at Random (MCAR)
          \item Missing at Random (MAR)
          \item Missing Not at Random (MNAR)
        \end{itemize}
      \item \textbf{Implications of Missing Values}:
        \begin{itemize}
          \item Can introduce bias if not handled correctly
          \item Affects statistical analysis and model predictions
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Methods to Identify Missing Values}
  \begin{block}{Methods}
    \begin{enumerate}
      \item \textbf{Visual Inspection}
        \begin{itemize}
          \item Check data manually in small datasets.
        \end{itemize}
      \item \textbf{Descriptive Statistics}
        \begin{itemize}
          \item Summarize datasets and count missing values.
          \item \begin{lstlisting}[language=Python]
import pandas as pd
data = pd.read_csv('data.csv')
print(data.isnull().sum())
          \end{lstlisting}
        \end{itemize}
      \item \textbf{Data Visualization}
        \begin{itemize}
          \item Use heatmaps to visualize patterns.
          \item \begin{lstlisting}[language=Python]
import seaborn as sns
import matplotlib.pyplot as plt
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.show()
          \end{lstlisting}
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Methods to Identify Missing Values (cont.)}
  \begin{block}{Continued Methods}
    \begin{enumerate}
      \setcounter{enumi}{3}  % Continue numbering from previous frame
      \item \textbf{Data Profiling Libraries}
        \begin{itemize}
          \item Use libraries like \texttt{pandas\_profiling} or \texttt{sweetviz} to generate comprehensive reports.
          \item \begin{lstlisting}[language=Python]
from pandas_profiling import ProfileReport
profile = ProfileReport(data)
profile.to_file("output.html")
          \end{lstlisting}
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Next Steps}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Identifying missing values is crucial for data quality.
      \item Methods vary in suitability based on dataset size and complexity.
      \item Regular checks for missing values are essential in data preprocessing.
    \end{itemize}
  \end{block}
  \begin{block}{Next Steps}
    Discuss strategies for handling missing values (e.g., deletion, imputation) in the next session.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values}
    \begin{block}{Overview}
        In data preprocessing, handling missing values is crucial. There are several strategies to manage missing data effectively:
    \end{block}
    \begin{itemize}
        \item Deletion
        \item Imputation
        \item Interpolation
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Deletion}
    \begin{block}{Definition}
        Removing records with missing data.
    \end{block}
    \begin{itemize}
        \item \textbf{Types}:
        \begin{itemize}
            \item \textbf{Listwise Deletion}: Eliminates any row with a missing value.
            \item \textbf{Pairwise Deletion}: Excludes missing values only in pairwise comparisons.
        \end{itemize}
        \item \textbf{Key Point}: Useful when the proportion of missing values is small to maintain dataset integrity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Imputation}
    \begin{block}{Definition}
        Filling in missing values with estimated or calculated substitutes.
    \end{block}
    \begin{itemize}
        \item \textbf{Methods}:
        \begin{itemize}
            \item Mean/Median/Mode Imputation
            \item K-Nearest Neighbors (KNN) Imputation
            \item Regression Imputation
        \end{itemize}
        \item \textbf{Key Point}: Imputation retains dataset size and can lead to better model performance, but risks introducing bias.
    \end{itemize}
    \begin{block}{Illustration}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.impute import SimpleImputer

df = pd.DataFrame({
    'A': [1, 2, None, 4],
    'B': [None, 2, 3, 4]
})

imputer = SimpleImputer(strategy='mean')
df['A'] = imputer.fit_transform(df[['A']])
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Interpolation}
    \begin{block}{Definition}
        Estimating missing values based on existing values, especially for time-series data.
    \end{block}
    \begin{itemize}
        \item \textbf{Methods}:
        \begin{itemize}
            \item Linear Interpolation
            \item Spline Interpolation
        \end{itemize}
        \item \textbf{Key Point}: Effective for time-dependent data but demands careful approach to fit the underlying data trend.
    \end{itemize}
    \begin{block}{Illustration}
    \begin{lstlisting}[language=Python]
df['A'] = df['A'].interpolate(method='linear')
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Choosing the right strategy for handling missing values is critical and depends on:
    \end{block}
    \begin{itemize}
        \item Data characteristics
        \item Analysis objectives
    \end{itemize}
    \begin{itemize}
        \item \textbf{Summary Points}:
        \begin{itemize}
            \item Deletion: Straightforward but may cost valuable data.
            \item Imputation: Preserves data size but requires careful selection of methods.
            \item Interpolation: Best for continuous data, especially in time series but needs careful application.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques}
    \begin{block}{Introduction to Data Normalization}
        Normalization is a crucial preprocessing technique in machine learning that involves adjusting the scales of features or attributes in your data. It ensures that different features contribute equally to the analysis, improving the performance of models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Normalization}
    \begin{itemize}
        \item \textbf{Scale Sensitivity:} Algorithms like k-NN and SVM are sensitive to feature scales.
        \item \textbf{Improved Convergence:} Accelerates gradient descent convergence for faster training.
        \item \textbf{Enhanced Performance:} Better accuracy and generalization by minimizing bias from larger magnitudes.
        \item \textbf{Uniform Comparison:} Enables fair comparisons across attributes for meaningful distance calculations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Normalization Techniques}
    \begin{enumerate}
        \item \textbf{Min-Max Normalization}
        \begin{equation}
        X' = \frac{X - X_{min}}{X_{max} - X_{min}}
        \end{equation}
        
        \item \textbf{Z-Score Normalization (Standardization)}
        \begin{equation}
        Z = \frac{X - \mu}{\sigma}
        \end{equation}
        Where $\mu$ is the mean and $\sigma$ is the standard deviation of the dataset.
    \end{enumerate}
    
    \begin{block}{Example}
        For the dataset of heights: [150, 160, 170, 180, 190] using Min-Max Normalization:
        \begin{equation}
        170' = \frac{170 - 150}{190 - 150} = 0.5
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Normalization is essential for many machine learning algorithms.
        \item Different normalization techniques suit various data types and models.
        \item Always visualize the data before and after normalization to understand scalability impact.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Normalization is crucial in data preprocessing to effectively train machine learning models and avoid issues related to disparate feature scales. Further slides will delve into specific normalization techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Min-Max Normalization}
    \begin{block}{What is Min-Max Normalization?}
        Min-Max Normalization is a technique used to scale data into a specific range, typically between 0 and 1. 
        It transforms the features so that the minimum value of the dataset becomes 0 and the maximum value becomes 1. 
        This technique is particularly useful when the data does not follow a Gaussian distribution.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Min-Max Normalization?}
    \begin{itemize}
        \item \textbf{Preservation of Relationships:} Retains linear relationships between data points.
        \item \textbf{Improved Performance of Algorithms:} Benefits algorithms like neural networks, leading to faster convergence during training.
        \item \textbf{Consistent Data Range:} Ensures equal contribution of features to model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Formula}
    The mathematical formula for Min-Max Normalization is:
    \begin{equation}
        X' = \frac{(X - X_{min})}{(X_{max} - X_{min})}
    \end{equation}
    Where:
    \begin{itemize}
        \item \( X' \) = normalized value
        \item \( X \) = original value
        \item \( X_{min} \) = minimum value in the dataset
        \item \( X_{max} \) = maximum value in the dataset
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Min-Max Normalization}
    Consider a dataset with the following values: \([10, 20, 30, 40, 50]\).
    
    \begin{enumerate}
        \item Calculate the minimum and maximum:
        \begin{itemize}
            \item \( X_{min} = 10 \)
            \item \( X_{max} = 50 \)
        \end{itemize}
        
        \item Normalize the data:
        \begin{itemize}
            \item For \( X = 10 \): 
                \[
                X' = \frac{(10 - 10)}{(50 - 10)} = 0
                \]
            \item For \( X = 20 \): 
                \[
                X' = \frac{(20 - 10)}{(50 - 10)} = \frac{10}{40} = 0.25
                \]
            \item For \( X = 30 \): 
                \[
                X' = \frac{(30 - 10)}{(50 - 10)} = \frac{20}{40} = 0.5
                \]
            \item For \( X = 40 \): 
                \[
                X' = \frac{(40 - 10)}{(50 - 10)} = 0.75
                \]
            \item For \( X = 50 \): 
                \[
                X' = \frac{(50 - 10)}{(50 - 10)} = 1
                \]
        \end{itemize}
        
        \item The normalized dataset will be: \textbf{[0, 0.25, 0.5, 0.75, 1]}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Range:} Scales all values between 0 and 1.
        \item \textbf{Sensitivity:} Sensitive to outliers, which can affect scaling.
        \item \textbf{Applicability:} Useful for algorithms requiring normalized features, like neural networks and KNN.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example}
    Here is a simple Python code snippet implementing Min-Max Normalization using NumPy:
    
    \begin{lstlisting}[language=Python]
import numpy as np

def min_max_normalize(data):
    data_min = np.min(data)
    data_max = np.max(data)
    return (data - data_min) / (data_max - data_min)

# Example usage:
data = np.array([10, 20, 30, 40, 50])
normalized_data = min_max_normalize(data)
print(normalized_data)  # Output: [0.   0.25 0.5  0.75 1.  ]
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By understanding and applying Min-Max Normalization, you can ensure that your machine learning models perform more effectively, making it a crucial preprocessing step in data science.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Z-Score Normalization - Overview}
    \begin{itemize}
        \item Z-score normalization, also known as standardization.
        \item Centers and scales data to have a mean of 0 and standard deviation of 1.
        \item Important for machine learning algorithms sensitive to data scale.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Z-Score Normalization - Formula}
    The Z-score for a given value is calculated using the following formula:
    
    \begin{equation}
        Z = \frac{(X - \mu)}{\sigma}
    \end{equation}
    
    Where:
    \begin{itemize}
        \item \( Z \) = Z-score
        \item \( X \) = Original value
        \item \( \mu \) = Mean of the dataset
        \item \( \sigma \) = Standard deviation of the dataset
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Z-Score Normalization - Example and Use Cases}
    \textbf{Example:}
    \begin{enumerate}
        \item Calculate the Mean (\( \mu \)):
            \[
            \mu = \frac{70 + 80 + 90 + 100 + 110}{5} = 90
            \]
        \item Calculate the Standard Deviation (\( \sigma \)):
            \[
            \sigma = \sqrt{\frac{(70 - 90)^2 + (80 - 90)^2 + (90 - 90)^2 + (100 - 90)^2 + (110 - 90)^2}{5}} \approx 15.81
            \]
        \item Transform a score (e.g., 100):
            \[
            Z = \frac{(100 - 90)}{15.81} \approx 0.632
            \]
    \end{enumerate}

    \textbf{Use Cases:}
    \begin{itemize}
        \item Machine Learning: Algorithms requiring similar feature scales.
        \item Financial Models: Detecting anomalies in stock prices.
        \item Medical Studies: Standardizing measurements for comparison.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Log Transformation}
  \begin{block}{Introduction to Log Transformation}
    Log transformation is a powerful data preprocessing technique used to:
    \begin{itemize}
      \item Mitigate skewness of data distributions.
      \item Stabilize variance.
      \item Make patterns in data more evident.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Why Use Log Transformation?}
  \begin{itemize}
    \item \textbf{Handling Skewed Data:} 
      \begin{itemize}
        \item Many datasets exhibit right skewness. 
        \item Log transformation helps normalize these distributions.
      \end{itemize}
    \item \textbf{Reducing Variance:} Stabilizes variance across different levels of measurement.
    \item \textbf{Interpretability:} 
      \begin{itemize}
        \item Transformed values can be easier to interpret (e.g., growth rates).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applying Log Transformation}
  \begin{block}{When to Apply Log Transformation}
    \begin{itemize}
      \item Data that spans several orders of magnitude (e.g., income levels).
      \item Data with exponential growth patterns (e.g., viral infections).
    \end{itemize}
  \end{block}

  \begin{block}{Formula}
    The log transformation is applied using:
    \begin{equation}
      Y' = \log(Y + 1)
    \end{equation}
    \begin{itemize}
      \item \(Y\) is the original data, \(Y'\) is the transformed data.
      \item Adding 1 avoids taking the logarithm of zero, which is undefined.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example of Log Transformation}
  \begin{block}{Original Data}
    Income Levels (in Thousands):
    \begin{itemize}
      \item 10, 20, 30, 50, 100, 300
    \end{itemize}
  \end{block}

  \begin{block}{Log-Transformed Data}
    Calculation:
    \begin{itemize}
      \item \( \log(10) \approx 2.3 \)
      \item \( \log(20) \approx 3.0 \)
      \item \( \log(30) \approx 3.4 \)
      \item \( \log(50) \approx 3.9 \)
      \item \( \log(100) \approx 4.6 \)
      \item \( \log(300) \approx 5.7 \)
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Code Snippet}
  \begin{block}{Key Points}
    \begin{itemize}
      \item \textbf{Normality Assumption:} Log transformation helps meet this assumption.
      \item \textbf{Data Context:} Consider the impact of transformation on original interpretation.
      \item \textbf{Reversibility:} Original scale can be retrieved using the exponential function.
    \end{itemize}
  \end{block}

  \begin{block}{Code Snippet (Using Python's Pandas)}
    \begin{lstlisting}[language=Python]
import pandas as pd
import numpy as np

# Sample Data
data = pd.Series([10, 20, 30, 50, 100, 300])

# Log Transformation
log_transformed_data = data.apply(lambda x: np.log(x + 1))

print(log_transformed_data)
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Concluding Note}
  Log transformation is valuable when dealing with skewed distributions. By transforming the data, we can:
  \begin{itemize}
    \item Enhance validity of analyses.
    \item Build more robust predictive models.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Overview - Introduction}
    \begin{block}{Introduction}
        Data transformation is a crucial step in data preprocessing that improves model performance and interpretability. In this section, we will discuss two major techniques: \textbf{scaling} and \textbf{encoding categorical variables}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Overview - Scaling}
    \begin{block}{1. Scaling}
        Scaling adjusts the range of feature variables so they contribute equally to model training. Common scaling techniques include:
    \end{block}
    \begin{itemize}
        \item \textbf{Min-Max Scaling:}
        \begin{itemize}
            \item \textbf{Formula:}
            \[
            X' = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
            \]
            \item \textbf{Example:} Transforming age data (15, 20, 30) to the range [0, 1].
        \end{itemize}
        
        \item \textbf{Standardization (Z-score Normalization):}
        \begin{itemize}
            \item \textbf{Formula:}
            \[
            X' = \frac{X - \mu}{\sigma}
            \]
            where $\mu$ is the mean and $\sigma$ is the standard deviation.
            \item \textbf{Example:} Transforming scores of students to standard normal distribution.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Overview - Key Points on Scaling}
    \begin{block}{Key Points about Scaling}
        \begin{itemize}
            \item \textbf{Min-Max Scaling} preserves relationships between data points. Ideal for algorithms sensitive to distance metrics.
            \item \textbf{Standardization} helps with normally distributed data and is better for algorithms that assume normality, like linear regression.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Overview - Encoding Categorical Variables}
    \begin{block}{2. Encoding Categorical Variables}
        Machine learning algorithms require numerical input. Therefore, categorical variables must be encoded numerically using techniques such as:
    \end{block}
    \begin{itemize}
        \item \textbf{One-Hot Encoding:}
        \begin{itemize}
            \item Converts categorical variables into binary vectors.
            \item \textbf{Example:} For a "Color" feature with values {Red, Blue, Green}, it becomes:
            \begin{lstlisting}
            Red   -> [1, 0, 0]
            Blue  -> [0, 1, 0]
            Green -> [0, 0, 1]
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Label Encoding:}
        \begin{itemize}
            \item Assigns a unique integer to each category.
            \item \textbf{Example:} "Color" feature could be encoded as: Red=1, Blue=2, Green=3. This method is simple but may introduce ordinality.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Overview - Key Points on Encoding}
    \begin{block}{Key Points about Encoding}
        \begin{itemize}
            \item Use \textbf{One-Hot Encoding} to avoid introducing unintended order in categorical data.
            \item \textbf{Label Encoding} is useful for ordinal categories where rank matters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Overview - Conclusion}
    \begin{block}{Conclusion}
        Effective data transformation, including scaling and encoding, is essential for building robust models. Understanding these techniques will enhance our ability to preprocess data effectively, enabling better predictions in subsequent modeling stages.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Overview - Next Steps}
    \begin{block}{Next Up}
        Explore the basics of feature engineering and its relationship with data preprocessing!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering Basics}
    % Introduction to the concept
    \begin{block}{Introduction to Feature Engineering}
        Feature engineering is a crucial step in the data preprocessing phase of the machine learning pipeline. It involves the creation of new input features based on existing ones, using domain knowledge or data manipulation. The quality and relevance of features significantly affect the performance of predictive models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance and Key Steps}
    % Importance of Feature Engineering
    \begin{block}{Why is Feature Engineering Important?}
        \begin{itemize}
            \item \textbf{Enhances Model Performance:} Well-crafted features can lead to better model accuracy and generalization.
            \item \textbf{Reduces Overfitting:} Eliminating irrelevant or redundant features reduces complexity and aids generalization.
            \item \textbf{Improves Interpretability:} Relevant features make model decisions more understandable.
        \end{itemize}
    \end{block}
    
    % Key Steps in Feature Engineering
    \begin{block}{Key Steps in Feature Engineering}
        \begin{enumerate}
            \item \textbf{Feature Selection:} Identify important features (e.g., Recursive Feature Elimination).
            \item \textbf{Feature Creation:} Generate new features (e.g., Body Mass Index from height and weight).
            \item \textbf{Feature Transformation:} Modify features to reflect data patterns (e.g., log transformations).
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Feature Engineering}
    % Techniques Overview
    \begin{block}{Techniques for Feature Engineering}
        \begin{itemize}
            \item \textbf{Binning:} Grouping continuous variables into discrete categories (e.g., age categories).
            \item \textbf{Polynomial Features:} Adding interaction terms or exponentials (e.g., \( x_1^2, x_2^2, x_1 \times x_2 \)).
            \item \textbf{Text Feature Extraction:} Converting text data into numerical values using TF-IDF.
        \end{itemize}
    \end{block}
    
    % Key Points and Conclusion
    \begin{block}{Key Points}
        \begin{itemize}
            \item Quality over Quantity: Focus on meaningful features rather than sheer numbers.
            \item Iterative Process: Experimentation is key in improving model performance.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Feature engineering is an ongoing process vital for building effective predictive models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Encoding Categorical Variables - Introduction}
  \begin{block}{Introduction to Categorical Encoding}
    Categorical variables represent groups or categories, which can be:
    \begin{itemize}
        \item \textbf{Nominal}: No specific order (e.g., colors, names)
        \item \textbf{Ordinal}: With a specific order (e.g., ratings)
    \end{itemize}
    Categorical encoding is a vital preprocessing step for machine learning algorithms requiring numerical inputs.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Encoding Techniques - Label Encoding}
  \begin{block}{Label Encoding}
    \begin{itemize}
        \item \textbf{Definition}: Converts each category to a unique integer based on alphabetical order.
        \item \textbf{Use Case}: Ideal for ordinal data where the order matters (e.g., "Low", "Medium", "High").
    \end{itemize}
    \textbf{Example}:
    \renewcommand{\arraystretch}{1.5} % For better spacing in the table
    \begin{center}
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Category} & \textbf{Label Encoded} \\
        \hline
        Low & 0 \\
        Medium & 1 \\
        High & 2 \\
        \hline
      \end{tabular}
    \end{center}
    \textbf{Key Point}: Preserves ordinal relationships but can imply misleading distances between categories.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Encoding Techniques - One-Hot Encoding}
  \begin{block}{One-Hot Encoding}
      \begin{itemize}
          \item \textbf{Definition}: Creates a new binary column for each category, indicating presence (1) or absence (0).
          \item \textbf{Use Case}: Best for nominal data without inherent order (e.g., "Red", "Blue", "Green").
      \end{itemize}
      \textbf{Example}:
      \renewcommand{\arraystretch}{1.5} % For better spacing in the table
      \begin{center}
        \begin{tabular}{|c|c|}
          \hline
          \textbf{Color} & \textbf{One-Hot Encoded} \\
          \hline
          Red & [1, 0, 0] \\
          Blue & [0, 1, 0] \\
          Green & [0, 0, 1] \\
          \hline
        \end{tabular}
      \end{center}
      \textbf{Key Point}: Avoids ordinal implications of label encoding, suitable for nominal variables.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Important Considerations}
  \begin{block}{Considerations for Categorical Encoding}
    \begin{itemize}
        \item \textbf{Dimensionality}: One-hot encoding can greatly increase the number of features.
        \item \textbf{Algorithm Compatibility}: Some algorithms work better with label encoding, while others require one-hot encoded data.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Encoding Techniques - Code Snippets}
  \begin{block}{Python Code for Encoding}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Label Encoding
data = {'Size': ['Small', 'Medium', 'Large']}
df = pd.DataFrame(data)
label_encoder = LabelEncoder()
df['Size_Encoded'] = label_encoder.fit_transform(df['Size'])

# One-Hot Encoding
df_one_hot = pd.get_dummies(df, columns=['Size'], prefix='Size')
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Categorical Encoding}
  \begin{block}{Summary}
    \begin{itemize}
        \item \textbf{Label Encoding}: Useful for ordinal categories; be cautious of hierarchy assumptions.
        \item \textbf{One-Hot Encoding}: Best for nominal categories; prevents misinterpretation but may increase dimensionality.
    \end{itemize}
    Choose encoding techniques based on variable nature and model requirements to enhance performance.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Practical Examples of Data Preprocessing}
  
  \begin{block}{Introduction to Data Preprocessing}
    Data preprocessing is a crucial step in the data analysis workflow that prepares raw data for modeling. It transforms raw data into a clean and usable format to ensure better performance of machine learning models.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Steps in Data Preprocessing - Overview}
  
  We will demonstrate an example using a sample dataset of Iris flower species, which includes measurements of sepals and petals along with their respective species labels.
  
  \begin{enumerate}
    \item Data Cleaning
    \item Encoding Categorical Variables
    \item Feature Scaling
    \item Splitting the Dataset
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Step 1: Data Cleaning}
  
  \begin{block}{Data Cleaning Example}
    - Check for missing values and inconsistencies.
    - If there are nulls in the \texttt{sepal\_length} column, we can either:
      \begin{itemize}
        \item Remove rows with missing values
        \item Replace missing values with the mean or median of that column
      \end{itemize}
  \end{block}
  
  \begin{lstlisting}[language=Python]
import pandas as pd
iris = pd.read_csv('iris.csv')
iris.fillna(iris['sepal_length'].mean(), inplace=True)  # Filling missing values
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Step 2: Encoding Categorical Variables}
  
  \begin{block}{Encoding Example}
    - Convert the \texttt{species} column (categorical) into numerical values using One-Hot Encoding.
  \end{block}
  
  \begin{lstlisting}[language=Python]
iris_encoded = pd.get_dummies(iris, columns=['species'], drop_first=True)
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Step 3: Feature Scaling}
  
  \begin{block}{Scaling Example}
    - Standardize features by removing the mean and scaling to unit variance.
    - This is done using \texttt{StandardScaler} from \texttt{scikit-learn}.
  \end{block}
  
  \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
iris_scaled = scaler.fit_transform(iris_encoded.drop(columns=['species_versicolor', 'species_virginica']))  # Keeping one category
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Step 4: Splitting the Dataset}
  
  \begin{block}{Splitting Example}
    - Split the dataset into training and testing sets to evaluate model performance.
  \end{block}
  
  \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris_scaled, iris_encoded[['species_versicolor', 'species_virginica']], test_size=0.2, random_state=42)
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Conclusion}
  
  \begin{itemize}
    \item Importance of each step: Improves data quality, leading to more accurate predictions.
    \item Flexibility of techniques: Choose methods based on dataset characteristics and ML algorithms.
  \end{itemize}
  
  \begin{block}{Conclusion}
    By following these preprocessing steps, we ensure that our data is clean, well-structured, and ready for further analysis or model training, improving accuracy and reliability.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Overview}
    \begin{block}{Overview of Common Challenges}
        Data preprocessing is a critical step in the data science workflow,
        yet it is often fraught with challenges. Here are some common challenges 
        and strategies to overcome them:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Handling Missing Data}
    \begin{itemize}
        \item \textbf{Challenge}: Missing values can lead to biased model predictions.
        \item \textbf{Strategies}:
            \begin{itemize}
                \item \textbf{Imputation}: Replace missing values with mean, median, or mode.
                \item \textbf{Removal}: Eliminate records with missing data if they represent a 
                small percentage.
            \end{itemize}
        \item \textbf{Example}: Use the mean age of the available data to fill in missing entries.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Outliers and Feature Scaling}
    \begin{itemize}
        \item \textbf{Dealing with Outliers}:
            \begin{itemize}
                \item \textbf{Challenge}: Outliers can skew results and negatively impact model performance.
                \item \textbf{Strategies}:
                    \begin{itemize}
                        \item \textbf{Detection}: Use statistical methods (e.g., Z-score, IQR).
                        \item \textbf{Treatment}: Cap values or remove them from the dataset.
                    \end{itemize}
                \item \textbf{Illustration}: Utilize a box plot for visualization.
            \end{itemize}
        
        \item \textbf{Feature Scaling}:
            \begin{itemize}
                \item \textbf{Challenge}: Features on different scales confuse distance-based algorithms.
                \item \textbf{Strategies}:
                    \begin{itemize}
                        \item \textbf{Normalization}: Rescale features to a range of [0, 1].
                        \item \textbf{Standardization}: Center data around the mean with unit standard deviation.
                    \end{itemize}
                \item \textbf{Formulas}:
                    \begin{equation}
                        x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)}
                    \end{equation}
                    \begin{equation}
                        z = \frac{x - \mu}{\sigma}
                    \end{equation}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Encoding Categorical Variables and Data Imbalance}
    \begin{itemize}
        \item \textbf{Encoding Categorical Variables}:
            \begin{itemize}
                \item \textbf{Challenge}: Categorical data must be converted to numerical input.
                \item \textbf{Strategies}:
                    \begin{itemize}
                        \item \textbf{One-Hot Encoding}: Create binary columns for each category.
                        \item \textbf{Label Encoding}: Assign unique integers to each category.
                    \end{itemize}
                \item \textbf{Example}: For 'color' with values {Red, Blue, Green}, create three new columns.
            \end{itemize}

        \item \textbf{Data Imbalance}:
            \begin{itemize}
                \item \textbf{Challenge}: Imbalanced datasets can skew results towards the majority class.
                \item \textbf{Strategies}:
                    \begin{itemize}
                        \item \textbf{Resampling}: Use SMOTE or undersampling techniques.
                        \item \textbf{Cost-sensitive Learning}: Assign different costs to misclassifications.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Effective preprocessing is essential for model accuracy and reliability.
            \item Each challenge requires specific strategies tailored to the dataset.
            \item Understanding the underlying data is crucial for effective preprocessing and modeling.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Data Preprocessing in Action}
    \begin{block}{Introduction to Data Preprocessing}
        Data preprocessing is a critical step in the machine learning workflow. It involves transforming raw data into a clean dataset suitable for building models. Effective preprocessing ensures improved model performance, accuracy, and generalization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Overview: Predicting Housing Prices}
    \begin{itemize}
        \item \textbf{Objective:} To predict housing prices based on various factors such as size, location, and amenities.
        \item \textbf{Dataset:} A real estate dataset consisting of 1,000 entries with features such as the number of bedrooms, square footage, and neighborhood ratings.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Preprocessing Steps Applied}
    \begin{enumerate}
        \item \textbf{Handling Missing Values}
            \begin{itemize}
                \item Before: 20\% of entries had missing values in the ‘number of bathrooms’ column.
                \item Action: Filled using the median value of the column.
                \item Impact: Addressed potential bias, allowing the model to use all data points.
            \end{itemize}
        \item \textbf{Feature Encoding}
            \begin{itemize}
                \item Before: Categorical features were in text format.
                \item Action: One-Hot Encoding converted them into numerical format.
                \item Impact: Improved the model's ability to learn patterns.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Preprocessing Steps Applied (continued)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Scaling Numerical Features}
            \begin{itemize}
                \item Before: Features like ‘square footage’ ranged from 500 to 5,000.
                \item Action: Min-Max Scaling normalized the data.
                \item Formula: 
                \begin{equation}
                X' = \frac{X - X_{min}}{X_{max} - X_{min}}
                \end{equation}
                \item Impact: Ensured all features contributed equally to calculations.
            \end{itemize}
        \item \textbf{Removing Outliers}
            \begin{itemize}
                \item Before: Statistics showed several entries with unrealistically high prices (e.g., \$10 million).
                \item Action: Outliers were removed using the Interquartile Range (IQR) method.
                \item Impact: Led to a more reliable model not skewed by extreme values.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation}
    \begin{itemize}
        \item \textbf{Model Used:} Linear Regression
        \item \textbf{Performance Metrics:}
            \begin{itemize}
                \item Original Model (without preprocessing): RMSE = \$25,000
                \item After Preprocessing: RMSE = \$15,000
            \end{itemize}
        \item \textbf{Improvement:} 40\% increase in model accuracy, showcasing the importance of preprocessing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Data Quality is Critical:} Preprocessing significantly impacts outcomes in machine learning.
        \item \textbf{Systematic Approach:} Each step addresses specific issues in the data for better performance.
        \item \textbf{Investment in Preprocessing Pays Off:} 
            \begin{itemize}
                \item Always evaluate preprocessing impacts with performance metrics.
                \item Tailor techniques to the challenges presented by each dataset.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Takeaways - Part 1}
  
  \begin{enumerate}
      \item \textbf{Importance of Data Preprocessing}
        \begin{itemize}
            \item \textbf{Definition}: Preparing raw data for analysis to ensure high-quality outcomes in machine learning models.
            \item \textbf{Significance}: Clean and well-structured data directly impacts model performance, accuracy, and reliability.
        \end{itemize}
  
      \item \textbf{Key Preprocessing Techniques Discussed}
          \begin{itemize}
              \item \textbf{Data Cleaning}: Handling missing values, removing duplicates, and correcting inaccuracies.
              \item \textbf{Feature Scaling}: Normalizing features prevents some from dominating due to scale.
              \item \textbf{Encoding Categorical Variables}: Converts categorical data into numerical format for machine learning algorithms.
              \item \textbf{Feature Selection}: Selecting relevant features reduces overfitting and enhances interpretability.
          \end{itemize}
  \end{enumerate}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Takeaways - Part 2}
  
  \begin{enumerate}
      \setcounter{enumi}{3} % Continue numbering from previous frame
      \item \textbf{Summary of Impact on Model Performance}
          \begin{itemize}
              \item \textbf{Enhanced Accuracy}: Proper preprocessing leads to improvements in model accuracy and generalization.
              \item \textbf{Reduced Training Time}: Streamlined datasets with fewer irrelevant features require less computation.
              \item \textbf{Improved Interpretability}: Well-preprocessed datasets yield transparent and interpretable models for decision-making.
          \end{itemize}
  
      \item \textbf{Final Thoughts}
          \begin{itemize}
              \item \textbf{Continuous Process}: Data preprocessing should be continuous as new data is collected.
              \item \textbf{Iterative Refinement}: Process evolves with model performance feedback.
          \end{itemize}
  \end{enumerate}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Takeaway and Code Example}
  
  \begin{block}{Key Takeaway}
      Proficient data preprocessing is the backbone of any successful machine learning project—it sets the stage for effective model training and the generation of actionable insights.
  \end{block}

  \vspace{1cm}

  \begin{block}{Example Code Snippet for Normalization}
      \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Sample dataset
data = pd.DataFrame({'height': [150, 160, 175, 180], 'weight': [50, 60, 75, 80]})

# Initializing Min-Max Scaler
scaler = MinMaxScaler()

# Scaling features
scaled_data = scaler.fit_transform(data)
scaled_df = pd.DataFrame(scaled_data, columns=data.columns)

print(scaled_df)
      \end{lstlisting}
  \end{block}
  
\end{frame}


\end{document}