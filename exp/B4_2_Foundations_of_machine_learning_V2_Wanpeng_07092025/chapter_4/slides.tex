\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Supervised Learning - Regression]{Week 4: Supervised Learning - Regression}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning}
    \begin{block}{Overview of Supervised Learning}
        Supervised learning is a core machine learning paradigm where the model learns from labeled data. 
        Algorithms are trained using input-output pairs to enable predictions on new, unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Supervised Learning}
    \begin{itemize}
        \item \textbf{Prediction Accuracy:} 
        Algorithms can make precise predictions using historical labeled data.
        \item \textbf{Wide Applications:}
        Used in various fields, such as:
        \begin{itemize}
            \item Finance (credit scoring)
            \item Healthcare (disease prediction)
            \item Marketing (customer segmentation)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Focus on Regression Techniques}
    Regression is a type of supervised learning for continuous output variables. Key techniques include:
    \begin{enumerate}
        \item \textbf{Linear Regression:}
        \begin{equation}
            y = mx + b
        \end{equation}
        Example: Predicting house price based on size.
        
        \item \textbf{Polynomial Regression:}
        \begin{equation}
            y = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + b
        \end{equation}
        Example: Modeling projectile trajectories.

        \item \textbf{Ridge Regression:}
        Cost Function:
        \begin{equation}
            J(\theta) = \text{MSE} + \lambda \sum_{i=1}^{n} \theta_i^2
        \end{equation}
        
        \item \textbf{Lasso Regression:}
        Cost Function:
        \begin{equation}
            J(\theta) = \text{MSE} + \lambda \sum_{i=1}^{n} |\theta_i|
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item Supervised learning requires labeled data, which serves as a foundation for model training.
        \item Emphasis on regression techniques is crucial for predicting continuous outcomes.
        \item Understanding evaluation metrics (e.g., Mean Squared Error (MSE) and $R^2$) is vital for model assessment.
    \end{itemize}
    
    \begin{block}{Summary}
        Supervised learning is integral to machine learning, enabling accurate predictions based on input features.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet (Python)}
    \begin{lstlisting}[language=Python]
import numpy as np
from sklearn.linear_model import LinearRegression

# Sample data (features and target)
X = np.array([[1], [2], [3], [4], [5]])  # Feature: size 
y = np.array([150, 250, 350, 450, 550])    # Target: price

# Create and train the model
model = LinearRegression()
model.fit(X, y)

# Make a prediction
predicted_price = model.predict([[6]])  # Predict price for size 6
print(predicted_price)
    \end{lstlisting}
    This code snippet illustrates how to implement a linear regression model using Python's `scikit-learn` library.
\end{frame}

\begin{frame}[fragile]{Key Concepts of Supervised Learning - Overview}
    \begin{block}{Overview of Supervised Learning}
        Supervised learning is a category of machine learning where the model is trained using labeled data. 
        This involves providing the model with input data (features) and corresponding output data (labels) 
        so that it can learn to make predictions or classifications based on new, unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Concepts of Supervised Learning - Core Concepts}
    \begin{block}{Core Concepts}
        \begin{enumerate}
            \item Labeled Data
            \item Model Training
            \item Prediction
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Concepts of Supervised Learning - Labeled Data}
    \frametitle{Labeled Data}
    \begin{itemize}
        \item \textbf{Definition}: Labeled data consists of input-output pairs where each input feature (X) is 
        associated with a correct output label (Y).
        \item \textbf{Example}: In a dataset of house prices, features could be square footage, number of rooms, 
        and location, while the label would be the house price.
        \item \textbf{Importance}: Labeled data is essential for the training process, as it provides the 
        model with the necessary information to learn patterns and relationships.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Concepts of Supervised Learning - Model Training}
    \frametitle{Model Training}
    \begin{itemize}
        \item \textbf{Definition}: Model training is the process where the supervised learning algorithm 
        processes input features and aims to minimize the difference between its predictions and the actual 
        labels in the dataset.
        \item \textbf{Training Process}:
        \begin{itemize}
            \item \textbf{Initialization}: Start with a model that has initial parameters.
            \item \textbf{Loss Function}: Use a loss function to quantify the difference between predicted 
            values and actual labels.
            \begin{equation}
            MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
            \end{equation}
            where \(y_i\) is the actual value and \(\hat{y}_i\) is the predicted value.
            \item \textbf{Optimization}: Use optimization algorithms (like Gradient Descent) to adjust 
            parameters to minimize the loss.
        \end{itemize}
        \item \textbf{Example}: Training a linear regression model on the housing dataset to predict price 
        based on square footage.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Concepts of Supervised Learning - Prediction}
    \frametitle{Prediction}
    \begin{itemize}
        \item \textbf{Definition}: After training, the model is capable of making predictions on 
        new, unseen inputs based on the knowledge it has acquired during training.
        \item \textbf{Process}:
        \begin{itemize}
            \item Input new features.
            \item The trained model generates a prediction for the corresponding output.
        \end{itemize}
        \item \textbf{Example}: A new house with 1500 square feet can have its price predicted. If the 
        model outputs \$300,000 based on the learned relationship, this is the prediction for that house.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Concepts of Supervised Learning - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Supervised learning relies heavily on the quality and quantity of labeled data.
            \item The training process involves adjusting model parameters to best fit the training data.
            \item The ultimate goal is for the model to generalize well to new, unseen data, ensuring 
            accurate predictions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Types of Regression - Introduction}
\begin{block}{Introduction}
Regression analysis is a pivotal technique in supervised learning used to model and analyze the relationship between a dependent variable and one or more independent variables. It helps in predicting outcomes based on input data. In this presentation, we will delve into two fundamental types of regression: \textbf{Linear Regression} and \textbf{Logistic Regression}.
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Types of Regression - Linear Regression}
\begin{block}{1. Linear Regression}
\begin{itemize}
    \item \textbf{Concept:} Predicts a continuous dependent variable based on one or more independent variables by fitting a linear equation to observed data.
    \item \textbf{Equation:} The linear regression model can be represented as:
    \begin{equation}
    Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
    \end{equation}
    Where:
    \begin{itemize}
        \item \(Y\) = dependent variable
        \item \(X_i\) = independent variables
        \item \(\beta_i\) = coefficients (parameters to be learned)
        \item \(\epsilon\) = error term
    \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Types of Regression - Linear Regression Examples and Applications}
\begin{block}{Example}
Suppose we want to predict house prices based on square footage. The linear regression model might yield:
\begin{equation}
\text{Price} = 50000 + 300 \times \text{Square Footage}
\end{equation}
This means for every additional square foot, the price increases by \$300.
\end{block}

\begin{block}{Applications}
\begin{itemize}
    \item Predicting sales, prices, scores, and other continuous outcomes.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Types of Regression - Logistic Regression}
\begin{block}{2. Logistic Regression}
\begin{itemize}
    \item \textbf{Concept:} Used for binary classification problems where the outcome variable is categorical (e.g., yes/no, success/failure).
    \item \textbf{Equation:} The logistic regression model uses a logistic function (sigmoid) to constrain the output between 0 and 1:
    \begin{equation}
    P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + ... + \beta_nX_n)}}
    \end{equation}
    Where:
    \begin{itemize}
        \item \(P(Y=1 | X)\) = probability of the positive class
        \item \(e\) = base of the natural logarithm
    \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Types of Regression - Logistic Regression Examples and Applications}
\begin{block}{Example}
Suppose we want to determine if a student will pass (1) or fail (0) based on study hours. The logistic regression model may output:
\begin{equation}
P(\text{Pass}) = \frac{1}{1 + e^{-(2.5 + 0.8 \times \text{Study Hours})}}
\end{equation}
This predicts the likelihood of passing based on hours studied.
\end{block}

\begin{block}{Applications}
\begin{itemize}
    \item Medical diagnoses, marketing classifications, and spam detection systems.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Types of Regression - Key Points and Conclusion}
\begin{block}{Key Points to Emphasize}
\begin{itemize}
    \item \textbf{Linear Regression} is suited for continuous outcomes, while \textbf{Logistic Regression} is for binary outcomes.
    \item Understanding the difference in model equations helps in choosing the right approach for a given dataset.
    \item Visualizing results (e.g., scatter plots for linear regression, ROC curves for logistic regression testing) enhances interpretability.
\end{itemize}
\end{block}

\begin{block}{Conclusion}
Both linear and logistic regression are foundational techniques in machine learning, each with its specific use cases and methodologies. Understanding these types helps in developing appropriate predictive models and making informed analytical decisions.
\end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Linear Regression Fundamentals - Overview}
  \begin{block}{What is Linear Regression?}
    Linear regression is a statistical method used in supervised learning for predicting a continuous outcome variable based on one or more predictor variables. It establishes a linear relationship between the dependent variable and the independent variables.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Linear Regression Fundamentals - Equation}
  \begin{block}{The Linear Regression Equation}
    The basic formula for simple linear regression is expressed as:
    \begin{equation}
      y = \beta_0 + \beta_1 x + \epsilon 
    \end{equation}

    Where:
    \begin{itemize}
      \item \( y \) = dependent variable
      \item \( \beta_0 \) = y-intercept
      \item \( \beta_1 \) = slope of the line
      \item \( x \) = independent variable
      \item \( \epsilon \) = error term
    \end{itemize}
    
    For multiple linear regression:
    \begin{equation}
      y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon 
    \end{equation}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Linear Regression Fundamentals - Interpretation and Cost Function}
  \begin{block}{Interpretation of Coefficients}
    \begin{itemize}
      \item **Intercept (\( \beta_0 \))**: Expected value of \( y \) when all predictors are zero.
      \item **Slope (\( \beta_1, \beta_2, \ldots, \beta_n \))**: Change in \( y \) for a one-unit increase in the respective predictor variable.
    \end{itemize}
    \begin{exampleblock}{Example}
      If \( \beta_1 = 2 \), it means that for every unit increase in \( x \), the predicted outcome \( y \) will increase by 2 units.
    \end{exampleblock}
  \end{block}

  \begin{block}{Cost Function: Mean Squared Error (MSE)}
    The MSE is defined as:
    \begin{equation}
      MSE = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 
    \end{equation}
    Where:
    \begin{itemize}
      \item \( n \) = number of observations
      \item \( y_i \) = actual values
      \item \( \hat{y}_i \) = predicted values
    \end{itemize}
    Key points:
    \begin{itemize}
      \item A lower MSE indicates a better fit.
      \item The goal is to minimize this error.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Overview}
    \begin{itemize}
        \item In linear regression, certain key assumptions must hold true for the model to be valid.
        \item These assumptions help ensure the reliability of estimated coefficients and the generalizability of results.
        \item The main assumptions we will discuss are:
        \begin{enumerate}
            \item Linearity
            \item Independence
            \item Homoscedasticity
            \item Normality
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Key Assumptions}
    \begin{enumerate}
        \item \textbf{Linearity}
        \begin{itemize}
            \item \textbf{Definition:} The relationship between predictors and the outcome should be linear.
            \item \textbf{Equation:} 
            \begin{equation}
                Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \varepsilon
            \end{equation}
            \item \textbf{Example:} Each additional square foot of house size adds a consistent amount to the price.
        \end{itemize}

        \item \textbf{Independence}
        \begin{itemize}
            \item \textbf{Definition:} Residuals should be independent of each other.
            \item \textbf{Example:} If house prices influence neighboring house prices, this assumption may be violated.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Homoscedasticity}
        \begin{itemize}
            \item \textbf{Definition:} Variance of residuals should remain constant across levels of independent variables.
            \item \textbf{Example:} In income predictions, if high-income individuals display greater variance, this assumption is violated.
            \item \textbf{Visual Aid:} Include a residual plot showing homoscedasticity vs. heteroscedasticity.
        \end{itemize}

        \item \textbf{Normality}
        \begin{itemize}
            \item \textbf{Definition:} Residuals should follow an approximately normal distribution.
            \item \textbf{Example:} Use a histogram or Q-Q plot of residuals to check this assumption.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Linear Regression Models - Introduction}
    % Introduction to performance evaluation in linear regression
    Evaluating the performance of linear regression models is essential to understand how well the model predicts outcomes. In this slide, we will discuss three key metrics:
    \begin{itemize}
        \item R-squared
        \item Mean Squared Error (MSE)
        \item Adjusted R-squared
    \end{itemize}
    Each of these metrics provides valuable insights into the quality of your regression model.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Linear Regression Models - R-squared}
    % Explanation of R-squared
    \textbf{1. R-squared (R²)} \\
    \begin{itemize}
        \item \textbf{Definition}: R-squared measures the proportion of variance in the dependent variable that can be predicted from the independent variables. It ranges from 0 to 1.
        \item \textbf{Interpretation}: 
        \begin{itemize}
            \item \textbf{0}: No explanatory power; the model does not explain any variability.
            \item \textbf{1}: Perfect fit; the model explains all variability.
        \end{itemize}
        \item \textbf{Formula}:
            \begin{equation}
                R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
            \end{equation}
            where:
            \begin{itemize}
                \item \(SS_{res}\) = residual sum of squares
                \item \(SS_{tot}\) = total sum of squares
            \end{itemize}
        \item \textbf{Example}: An R² value of 0.85 indicates that 85\% of the variability can be explained by the independent variables in the model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Linear Regression Models - MSE and Adjusted R-squared}
    % Explanation of MSE and Adjusted R-squared
    \textbf{2. Mean Squared Error (MSE)} \\
    \begin{itemize}
        \item \textbf{Definition}: MSE measures the average of the squares of the errors between predicted and observed values.
        \item \textbf{Interpretation}: Lower values indicate a better fit.
        \item \textbf{Formula}:
            \begin{equation}
                MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
            where:
            \begin{itemize}
                \item \(n\) = number of observations
                \item \(y_i\) = observed value
                \item \(\hat{y}_i\) = predicted value
            \end{itemize}
        \item \textbf{Example}: An MSE of 4 indicates that on average, the squared difference is 4.
    \end{itemize}
    
    \textbf{3. Adjusted R-squared} \\
    \begin{itemize}
        \item \textbf{Definition}: Adjusted R-squared adjusts R-squared for the number of predictors in the model.
        \item \textbf{Interpretation}: A decrease in Adjusted R² with added variable indicates insignificance.
        \item \textbf{Formula}:
            \begin{equation}
                \text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \frac{n - 1}{n - p - 1}
            \end{equation}
            where:
            \begin{itemize}
                \item \(n\) = number of observations
                \item \(p\) = number of predictors
            \end{itemize}
        \item \textbf{Example}: An Adjusted R² of 0.78 suggests that 78\% of the variance is explained after adjusting for predictors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple vs Multiple Linear Regression - Concepts}
  \begin{block}{Simple Linear Regression}
    \begin{itemize}
      \item Definition: A statistical method that models the relationship between two variables with one independent and one dependent variable.
      \item Formula: 
      \begin{equation}
        Y = \beta_0 + \beta_1X + \epsilon
      \end{equation}
      \begin{itemize}
        \item \( Y \) = dependent variable
        \item \( X \) = independent variable
        \item \( \beta_0 \) = y-intercept
        \item \( \beta_1 \) = slope of the line
        \item \( \epsilon \) = error term
      \end{itemize}
      \item \textbf{Use Case:} Predict a person's weight (Y) based on their height (X).
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple vs Multiple Linear Regression - Concepts}
  \begin{block}{Multiple Linear Regression}
    \begin{itemize}
      \item Definition: An extension of simple linear regression which models the relationship between one dependent variable and two or more independent variables.
      \item Formula: 
      \begin{equation}
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_nX_n + \epsilon
      \end{equation}
      \begin{itemize}
        \item \( Y \) = dependent variable
        \item \( X_1, X_2, \ldots, X_n \) = independent variables
        \item \( \beta_0, \beta_1, \ldots, \beta_n \) = coefficients
        \item \( \epsilon \) = error term
      \end{itemize}
      \item \textbf{Use Case:} Predict a person's weight (Y) based on height (X1), age (X2), and diet (X3).
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple vs Multiple Linear Regression - Examples}
  \begin{block}{Examples}
    \begin{itemize}
      \item \textbf{Simple Linear Regression Example:}
      \begin{itemize}
        \item Dataset of Heights and Weights:
        \begin{itemize}
          \item Heights (cm): [150, 160, 170, 180]
          \item Weights (kg): [50, 60, 70, 80]
        \end{itemize}
        \item Fit a line to predict weight based on height.
      \end{itemize}

      \item \textbf{Multiple Linear Regression Example:}
      \begin{itemize}
        \item Dataset including Height, Age, and Diet:
        \begin{itemize}
          \item Heights: [150, 160, 170, 180]
          \item Ages: [20, 25, 30, 35]
          \item Diet Quality (1-10): [5, 6, 8, 7]
        \end{itemize}
        \item Fit a model to predict weight based on all three variables.
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple vs Multiple Linear Regression - Key Points}
  \begin{itemize}
    \item \textbf{Simplicity vs Complexity:} 
      \begin{itemize}
        \item Simple linear regression is easier to interpret and visualize.
        \item Multiple linear regression captures more complex relationships.
      \end{itemize}
    \item \textbf{Applicability:} 
      \begin{itemize}
        \item Use simple linear regression for straightforward predictions.
        \item Use multiple linear regression for problems needing multiple variables.
      \end{itemize}
    \item \textbf{Assumptions:} 
      \begin{itemize}
        \item Both methods assume linearity, independence, and homoscedasticity of residuals.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple vs Multiple Linear Regression - Example Code}
  \begin{block}{Example Code in Python}
    \begin{lstlisting}[language=Python]
# Simple Linear Regression
from sklearn.linear_model import LinearRegression
import numpy as np

# Simple Data
X_simple = np.array([[150], [160], [170], [180]])  # Height
y_simple = np.array([50, 60, 70, 80])              # Weight
model_simple = LinearRegression().fit(X_simple, y_simple)

# Multiple Linear Regression
X_multiple = np.array([[150, 20, 5], 
                       [160, 25, 6], 
                       [170, 30, 8], 
                       [180, 35, 7]])  # Height, Age, Diet
y_multiple = np.array([50, 60, 70, 80])            # Weight
model_multiple = LinearRegression().fit(X_multiple, y_multiple)
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction to Logistic Regression - Overview}
    \begin{block}{Concept Overview}
        Logistic Regression is a statistical method for predicting binary classes. It estimates the probability that a given input belongs to a certain category, suitable for outcomes such as yes/no, true/false, or success/failure.
    \end{block}
    
    \begin{block}{Why Use Logistic Regression?}
        \begin{itemize}
            \item \textbf{Classification Tasks:} Ideal for binary outcomes (e.g., spam detection).
            \item \textbf{Probabilistic Framework:} Outputs probabilities providing model confidence.
            \item \textbf{Simplicity and Interpretability:} Easy to implement; coefficients represent change in log-odds for a one-unit increase in predictors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction to Logistic Regression - Mathematical Background}
    \begin{block}{Mathematical Formulation}
        Logistic regression uses the logistic function to model a binary dependent variable.
    \end{block}
    
    \begin{equation}
        P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
    \end{equation}
    
    where:
    \begin{itemize}
        \item $P(Y=1 | X)$ = probability of success given the predictors
        \item $e$ = Euler's number (approx. 2.718)
        \item $\beta_0$ = intercept
        \item $\beta_1, \beta_2, \ldots, \beta_n$ = coefficients of predictors $X_1, X_2, \ldots, X_n$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Logistic Regression - Example and Code}
    \begin{block}{Interpretation of Coefficients}
        Each coefficient ($\beta_i$) indicates how the log-odds of the outcome change with a one-unit increase in the associated predictor.
    \end{block}

    \begin{block}{Example}
        Consider a medical study predicting whether a patient will develop diabetes (1) or not (0) based on age and BMI. A logistic model may use coefficients such as:
        \begin{itemize}
            \item $\beta_0 = -6$
            \item $\beta_1 = 0.05$ (for age)
            \item $\beta_2 = 0.1$ (for BMI)
        \end{itemize}
        This would result in changes in the odds of developing diabetes based on these factors.
    \end{block}

    \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression
import numpy as np

# Sample data: e.g., age and BMI
X = np.array([[25, 22], [30, 24], [45, 26]])  # Features: [Age, BMI]
y = np.array([0, 1, 1])  # Target: [No Diabetes, Diabetes, Diabetes]

# Create a logistic regression model
model = LogisticRegression()
model.fit(X, y)

# Predict the probability of diabetes for a new patient
new_patient = np.array([[35, 23]])  # Age: 35, BMI: 23
probability = model.predict_proba(new_patient)[:, 1]  # Probability of diabetes
print(f"Probability of Diabetes: {probability[0]}")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function and Odds Ratio - Part 1}
    
    \begin{block}{Concept of the Logistic Function}
        \begin{itemize}
            \item \textbf{Definition}: 
            \[
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
            \]
            \item \textbf{Characteristics}:
            \begin{itemize}
                \item \textbf{S-Shaped Curve}: Graph illustrates the transition between two classes.
                \item \textbf{Interpretation}: Captures gradual changes in predicted probabilities based on predictors.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Illustration}
        Consider a simple logistic curve where:
        \begin{itemize}
            \item x-axis: predictor variable (e.g., hours studied)
            \item y-axis: probability of passing (e.g., passing an exam)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function and Odds Ratio - Part 2}
    
    \begin{block}{Understanding Odds and Odds Ratio}
        \begin{itemize}
            \item \textbf{Odds}:
            \[
            Odds = \frac{P(Y=1)}{P(Y=0)} = \frac{P(Y=1)}{1 - P(Y=1)}
            \]
            \item \textbf{Example}:
            \[
            \text{If } P(Y=1) = 0.8, \quad Odds = \frac{0.8}{0.2} = 4
            \]
            \item \textbf{Odds Ratio (OR)}:
            \[
            OR = \frac{Odds_{Group1}}{Odds_{Group2}}
            \]
            \item \textbf{Interpretation}: 
            - OR $>$ 1: Increased odds for Group 1.
            - OR $<$ 1: Decreased odds for Group 1.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        \[
        OR = \frac{6}{3} = 2 \quad \text{(Group 1 is twice as likely to experience the event)}
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function and Odds Ratio - Part 3}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Transforms linear predictors into probabilities for binary classification.
            \item S-shaped curve represents probability distribution effectively.
            \item Understanding odds and odds ratio is essential in various fields.
        \end{itemize}
    \end{block}
    
    \begin{block}{Additional Notes}
        \begin{itemize}
            \item \textbf{Visual Representation}: Consider including diagrams and graphs.
            \item \textbf{Practical Applications}: Widely used in epidemiology, marketing, and social sciences.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for Logistic Regression - Part 1}
    \begin{block}{Understanding Model Evaluation Metrics}
        Evaluating a logistic regression model requires several key metrics to assess its performance in predicting binary outcomes.
    \end{block}

    \begin{itemize}
        \item \textbf{Accuracy}
        \item \textbf{Precision}
        \item \textbf{Recall (Sensitivity)}
        \item \textbf{F1 Score}
        \item \textbf{ROC Curve (Receiver Operating Characteristic Curve)}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for Logistic Regression - Part 2}
    
    \begin{block}{Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted instances to total instances.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total instances}}
            \end{equation}
            \item \textbf{Example}: 80 out of 100 instances predicted correctly results in an accuracy of \( 0.80 \) or 80\%.
        \end{itemize}
    \end{block}

    \begin{block}{Precision}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted positive observations to total predicted positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Example}: 50 true positives and 10 false positives yields a precision of \( \frac{50}{60} \approx 0.83 \) or 83\%.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for Logistic Regression - Part 3}
    
    \begin{block}{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of correctly predicted positive observations to all actual positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Example}: 50 true positives and 20 false negatives gives a recall of \( \frac{50}{70} \approx 0.71 \) or 71\%.
        \end{itemize}
    \end{block}

    \begin{block}{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: Harmonic mean of precision and recall.
            \item \textbf{Formula}:
            \begin{equation}
                \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Importance}: Useful for comparing models, especially with imbalanced classes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for Logistic Regression - Part 4}

    \begin{block}{ROC Curve}
        \begin{itemize}
            \item \textbf{Definition}: Graphical representation of model's true positive rate versus false positive rate.
            \item \textbf{Key Concept}: Area under the ROC curve (AUC) indicates model performance:
            \begin{itemize}
                \item AUC = 1 implies perfect prediction.
                \item AUC = 0.5 implies no discrimination.
            \end{itemize}
            \item The curve bowing towards the top left corner indicates better model performance.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Use different metrics for nuanced evaluation, especially in imbalanced datasets.
            \item ROC Curve and AUC provide a visual and quantifiable performance measure.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation for Logistic Regression - Part 5}

    \begin{block}{Practical Applications}
        Implement these metrics using libraries such as \texttt{sklearn} in Python to assess model performance post-training.
        \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)
        \end{lstlisting}
    \end{block}

    \begin{block}{Conclusion}
        By understanding and applying these evaluation metrics, you can enhance the assessment of logistic regression models for practical applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Understanding Class Imbalance}
    \begin{block}{Definition}
        In supervised learning, class imbalance occurs when one class is significantly underrepresented compared to others. 
    \end{block}
    \begin{itemize}
        \item Example: In fraud detection, legitimate transactions may account for 98\% of the data, leaving only 2\% for fraudulent transactions.
        \item This imbalance can lead to biased models favoring the majority class.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Importance}
    \begin{itemize}
        \item \textbf{Model Performance:} 
        Models trained on imbalanced datasets often exhibit high accuracy but low predictive power for minority classes.
        
        \item \textbf{Real-World Impact:} 
        Misclassifying rare events can have serious consequences, such as failing to detect fraud or malfunctioning machinery.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Techniques}
    \begin{block}{Resampling Methods}
        \begin{itemize}
            \item \textbf{Oversampling:}
            \begin{itemize}
                \item Increase instances in the minority class by duplicating existing examples or generating synthetic ones (e.g., SMOTE).
                \item Example: Create 900 synthetic positive cases to balance 100 positives against 1,000 negatives.
            \end{itemize}
            
            \item \textbf{Undersampling:}
            \begin{itemize}
                \item Reduce majority class instances to achieve balance.
                \item Example: Randomly select 100 negative cases to match the positive case count.
            \end{itemize}
            
            \item \textbf{Combined Approach:} 
            Use both oversampling and undersampling to improve class balance while retaining enough data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Penalization Techniques}
    \begin{block}{Overview}
        Penalization alters the model's learning process to give more weight to misclassifications of the minority class.
    \end{block}
    \begin{itemize}
        \item \textbf{Class Weights:}
        \begin{itemize}
            \item Assign higher weight to the minority class during model training.
            \item Example in Logistic Regression:
            \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight={0: 1, 1: 10})  # Weighting minority class more heavily
model.fit(X_train, y_train)
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Custom Loss Functions:} 
        Define a custom loss function that imposes a higher penalty on false negatives of the minority class.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Class Imbalance - Key Points}
    \begin{itemize}
        \item Always evaluate the effectiveness of resampling and penalization using metrics like precision, recall, and the ROC curve, rather than accuracy alone.
        \item Choosing the right method (oversampling, undersampling, or penalization) depends on data nature and application context.
    \end{itemize}
    \begin{block}{Summary}
        Handling class imbalance is crucial for robust logistic regression models. Combining resampling techniques and penalization approaches improves model accuracy for minority class predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering for Regression Models - Introduction}
    \begin{block}{Introduction to Feature Engineering}
        Feature engineering is a critical step in the machine learning process, especially for regression models. The quality and relevance of input variables (features) significantly influence model performance. 
        It involves creating new features or transforming existing ones to enhance predictive accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering for Regression Models - Importance of Feature Selection}
    \begin{block}{Importance of Feature Selection}
        Feature selection is the process of identifying and selecting the most relevant features for model building. It helps in:
        \begin{itemize}
            \item \textbf{Reducing Overfitting:} Limits model complexity by reducing features, preventing noise learning.
            \item \textbf{Improving Accuracy:} Relevant features boost model generalization for unseen data.
            \item \textbf{Decreasing Computational Cost:} Fewer features mean simpler models and less resource consumption.
        \end{itemize}
        \textbf{Example:} In predicting house prices, relevant features include square footage and location, while irrelevant ones can be discarded.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering for Regression Models - Techniques}
    \begin{block}{Key Techniques for Feature Engineering}
        \begin{enumerate}
            \item \textbf{Creating New Features:}
                \begin{itemize}
                    \item Polynomial Features: Capture non-linear relationships.
                    \item Interaction Terms: Features combining multiple variables.
                \end{itemize}
                \begin{lstlisting}[language=Python]
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, interaction_only=True)
new_features = poly.fit_transform(existing_features)
                \end{lstlisting}

            \item \textbf{Encoding Categorical Variables:} Transforming categories into numerical formats (e.g., One-Hot Encoding).
            \item \textbf{Normalization/Standardization:} Scaling numerical variables.
                \begin{equation}
                X' = \frac{X - \min(X)}{\max(X) - \min(X)} \quad \text{(Normalization)}
                \end{equation}
                \begin{equation}
                X' = \frac{X - \mu}{\sigma} \quad \text{(Standardization)}
                \end{equation}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering for Regression Models - Evaluation of Importance}
    \begin{block}{Evaluation of Feature Importance}
        Utilizing techniques like Lasso Regression or tree-based models (e.g., Random Forests) allows for evaluation and ranking of features based on importance.
        \begin{itemize}
            \item \textbf{Example:} Lasso's coefficient shrinkage helps select features automatically by penalizing less important ones.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering for Regression Models - Summary}
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item Feature engineering enhances the predictive performance of regression models.
            \item Effective feature selection reduces overfitting and improves accuracy.
            \item Creating and transforming features captures complex relationships.
            \item Proper encoding and scaling are crucial preprocessing steps.
        \end{itemize}
    \end{block}
    By investing in thoughtful feature engineering, data scientists can significantly boost regression model effectiveness, turning raw data into powerful predictors.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Considerations}
    \begin{block}{Introduction to Data Preprocessing}
        Data preprocessing is a crucial step in the machine learning pipeline, particularly for regression tasks. It involves transforming raw data into a suitable format for analysis or modeling. Proper preprocessing can significantly enhance the performance and accuracy of regression models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Preprocessing Techniques - Scaling Features}
    \begin{enumerate}
        \item \textbf{Scaling Features}
            \begin{itemize}
                \item \textbf{Definition}: Adjusts the range of feature values to ensure equal contribution in distance calculations.
                \item \textbf{Common Methods}:
                    \begin{itemize}
                        \item \textbf{Min-Max Scaling}: Rescales features to a range of [0, 1].
                        \begin{equation}
                            X_{\text{scaled}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
                        \end{equation}
                        \item \textbf{Standardization}: Centers data around the mean with a standard deviation of 1.
                        \begin{equation}
                            X_{\text{scaled}} = \frac{X - \mu}{\sigma}
                        \end{equation}
                    \end{itemize}
                \item \textbf{Example}: Suppose a feature 'age' ranges from 0 to 100; scaling it to a [0, 1] range may help prevent models from bias based on larger numeric ranges.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Preprocessing Techniques - Encoding and Missing Values}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Encoding Categorical Variables}
            \begin{itemize}
                \item \textbf{Definition}: Converts categorical data into numerical format for regression algorithms.
                \item \textbf{Popular Methods}:
                    \begin{itemize}
                        \item \textbf{One-Hot Encoding}: Creates binary columns for each category.
                        \item \textbf{Label Encoding}: Assigns unique integers to each category.
                    \end{itemize}
                \item \textbf{Example}: The 'color' feature could transform 'red', 'blue', 'green' into three binary columns representing each.
            \end{itemize}

        \item \textbf{Handling Missing Values}
            \begin{itemize}
                \item \textbf{Definition}: Addresses gaps in the dataset where certain values are absent.
                \item \textbf{Common Strategies}:
                    \begin{itemize}
                        \item \textbf{Removal}: Exclude rows with missing values.
                        \item \textbf{Imputation}: Fill in missing values, e.g.:
                            \begin{itemize}
                                \item Mean/Median/Mode substitution
                                \item Predictive imputation (using a regression model)
                            \end{itemize}
                    \end{itemize}
                \item \textbf{Example}: Replacing missing values in 'size' of houses with mean sizes from the dataset.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Code Snippet Example}
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item Scaling, Encoding, and Handling Missing Values are crucial for optimal performance in regression models.
            \item Effective preprocessing improves model accuracy and manages biases in predictions.
            \item Evaluate the impact of preprocessing techniques on your dataset and regression model.
        \end{itemize}
    \end{block}

    \begin{block}{Code Snippet Example}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
import pandas as pd

# Example DataFrame
data = pd.DataFrame({
    'age': [25, 30, None, 22],
    'color': ['red', 'blue', 'green', 'blue']
})

# Impute missing 'age'
imputer = SimpleImputer(strategy='mean')
data['age'] = imputer.fit_transform(data[['age']])

# Scale 'age'
scaler = MinMaxScaler()
data['age'] = scaler.fit_transform(data[['age']])

# One-Hot encode 'color'
data = pd.get_dummies(data, columns=['color'], drop_first=True)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Regression Analysis - Introduction}
    \begin{itemize}
        \item Regression analysis is a powerful tool in supervised learning.
        \item It predicts outcomes based on input variables.
        \item Important to manage its use to avoid:
        \begin{itemize}
            \item Data privacy issues
            \item Algorithmic bias
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Regression Analysis - Data Privacy}
    \begin{block}{Definition}
        Data privacy refers to the ethical and legal standards of personal information management.
    \end{block}
    \begin{itemize}
        \item **Implications in Regression:**
            \begin{itemize}
                \item Sensitive Data: May include health status, financial information.
                \item Anonymization: Vital to anonymize datasets, but risks remain.
            \end{itemize}
        \item **Example:**
            \begin{itemize}
                \item A healthcare provider using patient data must anonymize names and locations to prevent privacy violations.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Regression Analysis - Algorithmic Bias}
    \begin{block}{Definition}
        Algorithmic bias occurs when models generate unfair discrimination against certain groups.
    \end{block}
    \begin{itemize}
        \item **Implications in Regression:**
            \begin{itemize}
                \item Training Data Issues: Biased historical data can perpetuate societal biases.
                \item Outcome Discrimination: Can lead to unfair treatment in areas like credit scoring and hiring.
            \end{itemize}
        \item **Example:**
            \begin{itemize}
                \item A credit scoring model may perform poorly for minority groups due to underrepresented training data.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Regression Analysis - Key Points and Conclusion}
    \begin{itemize}
        \item **Key Points to Emphasize:**
            \begin{itemize}
                \item Transparency: Clarify data sources and processing methods.
                \item Fairness Measures: Evaluate model performance across demographic groups.
                \item Ongoing Monitoring: Track models post-deployment for emerging biases.
            \end{itemize}
        \item **Conclusion:**
            \begin{itemize}
                \item Addressing ethical considerations enhances model reliability and fairness.
                \item Next, we will explore practical applications of regression models with these ethical considerations in mind.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Regression Analysis - Final Note}
    \begin{block}{Important Reminder}
        \begin{itemize}
            \item Always ask: "How does this model impact the individuals represented in the data?"
            \item Incorporate bias detection methods and data protection proactively in regression projects.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Practical Applications and Case Studies}
  \begin{block}{Understanding Regression Models in Real-World Contexts}
    Regression analysis is a powerful statistical method used to model relationships between variables. In supervised learning, regression is employed to predict a continuous outcome based on one or more predictor variables. Let's explore its applications across various domains.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{1. Healthcare Applications}
  \begin{itemize}
    \item \textbf{Predicting Patient Outcomes}: 
      Regression models can estimate the likelihood of recovery based on factors such as age, medical history, and treatment plan.
      \begin{itemize}
        \item \textit{Example}: A hospital employs regression to discern how preoperative risk factors (e.g., hypertension, diabetes) influence recovery time post-surgery.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{2. Finance Applications}
  \begin{itemize}
    \item \textbf{Risk Assessment \& Credit Scoring}:
      Financial institutions use regression to assess credit risk and predict borrower defaults based on historical data.
      \begin{itemize}
        \item \textit{Example}: A bank applies multiple linear regression to evaluate factors like income, age, and credit history for determining loan approval.
      \end{itemize}
    \item \textbf{Stock Market Predictions}:
      Regression models forecast stock prices through historical data and economic indicators.
      \begin{itemize}
        \item \textit{Example}: A financial analyst predicts stock prices based on economic indicators such as GDP growth and interest rates.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3. Social Media Analytics}
  \begin{itemize}
    \item \textbf{User Engagement Prediction}:
      Companies leverage regression analysis to predict engagement levels based on post metrics like timing and content type.
      \begin{itemize}
        \item \textit{Example}: A digital marketing team identifies impactful elements (e.g., image quality, post length) that influence user interactions.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Versatility}: Regression applies to diverse research and industrial fields, showcasing its predictive modeling effectiveness.
    \item \textbf{Impact of Data Quality}: Accurate predictions depend significantly on the dataset's quality, necessitating careful data integrity and relevance checks.
    \item \textbf{Ethical Implications}: Ethical considerations, such as privacy and bias, must inform the development of regression models, especially in sensitive sectors like healthcare and finance.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Formulas and Code Snippet}
  \begin{block}{Simple Linear Regression Formula}
    \begin{equation}
      Y = \beta_0 + \beta_1 X_1 + \epsilon
    \end{equation}
    Where:
    \begin{itemize}
      \item \(Y\) = dependent variable (outcome)
      \item \(X_1\) = independent variable (predictor)
      \item \(\beta_0\) = y-intercept
      \item \(\beta_1\) = slope of the regression line
      \item \(\epsilon\) = error term
    \end{itemize}
  \end{block}

  \begin{block}{Python Code Example for Linear Regression}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load data
data = pd.read_csv('healthcare_data.csv')
X = data[['age', 'blood_pressure', 'cholesterol']]
y = data['recovery_time']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
predictions = model.predict(X_test)
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  By integrating regression analysis into various sectors, organizations can enhance decision-making, improve outcomes, and derive valuable insights from their data.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Wrap-up and Key Takeaways - Overview}
  \begin{block}{Overview of Supervised Learning and Regression}
    In our exploration of supervised learning, particularly regression, we've discovered how this powerful technique enables prediction of continuous outcomes based on input features. 
    Regression analysis establishes relationships between variables, allowing us to understand trends and make informed decisions.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Wrap-up and Key Takeaways - Key Concepts}
  \begin{enumerate}
    \item \textbf{Definition of Regression}:
      \begin{itemize}
        \item A statistical method to model relationships between a dependent variable and one or more independent variables.
        \item Common types: Linear Regression, Polynomial Regression, Ridge Regression, Lasso Regression.
      \end{itemize}
    
    \item \textbf{Importance of Regression in Supervised Learning}:
      \begin{itemize}
        \item Enables predictive modeling to inform future decisions based on historical data.
        \item Applicable in various domains like healthcare, finance, and social media analytics.
      \end{itemize}
  
    \item \textbf{Linear Regression Basics}:
      \begin{equation}
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
      \end{equation}
      \begin{itemize}
        \item \(Y\): Dependent variable, 
        \item \(\beta_0\): Intercept, 
        \item \(\beta_i\): Coefficients,
        \item \(X_i\): Independent variables, 
        \item \(\epsilon\): Error term.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Wrap-up and Key Takeaways - Evaluation and Impact}
  \begin{enumerate}[resume]
    \item \textbf{Evaluation Metrics}:
      \begin{itemize}
        \item Model performance is assessed using:
          \begin{itemize}
            \item Mean Absolute Error (MAE),
            \item Mean Squared Error (MSE),
            \item R-squared, indicating the proportion of variance explained by the model.
          \end{itemize}
      \end{itemize}
  
    \item \textbf{Practical Application Examples}:
      \begin{itemize}
        \item In healthcare: Predict health outcomes from risk factors for patient management.
        \item In finance: Data-driven investment decisions regarding stocks or risk assessment.
      \end{itemize}
  \end{enumerate}

  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item Versatility of Regression: A foundational method in supervised learning across multiple industries.
      \item Data-Driven Decision Making: Leverages historical data for better strategic insights.
      \item Real-World Impact: Contributes to efficient problem-solving, improving outcomes in critical sectors.
    \end{itemize}
  \end{block}
\end{frame}


\end{document}