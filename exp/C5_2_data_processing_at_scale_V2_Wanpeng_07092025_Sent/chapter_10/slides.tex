\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Chapter 10: Streaming Data Processing]{Chapter 10: Streaming Data Processing}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Streaming Data Processing}
    \begin{block}{Overview}
        In today's rapidly evolving digital landscape, real-time data processing has become a crucial element for businesses and organizations. Streaming data processing enables the analysis and action on data that are transmitted continuously, providing insights as events occur. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Real-Time Data Processing}
    \begin{enumerate}
        \item \textbf{Immediate Insights:}
        \begin{itemize}
            \item Allows organizations to derive insights as data is created, enabling quick reactions to changing conditions.
            \item \textit{Example:} A financial trading platform uses streaming data to track market fluctuations and execute instant trades.
        \end{itemize}
        
        \item \textbf{Enhanced Decision Making:}
        \begin{itemize}
            \item Presents relevant data instantly for timely decisions, critical in finance, e-commerce, and healthcare.
            \item \textit{Example:} Hospitals monitor patient vital signs in real time for immediate medical intervention.
        \end{itemize}
        
        \item \textbf{Scalability:}
        \begin{itemize}
            \item Efficiently handles the increasing volume and velocity of data from IoT devices and social media.
            \item \textit{Illustration:} A smart city implementation uses sensors generating continuous data needing real-time analysis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Real-Time Data Processing (Continued)}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Improved Customer Experiences:}
        \begin{itemize}
            \item Personalizes user experiences by providing real-time recommendations and offers.
            \item \textit{Example:} Online retailers suggest products dynamically based on real-time user actions.
        \end{itemize}
        
        \item \textbf{Data-driven Innovation:}
        \begin{itemize}
            \item Organizations gain competitive advantages through real-time insights leading to new products and services.
            \item \textit{Example:} Ride-sharing apps optimize routes and predict demand using real-time location data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion & Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Streaming data is characterized by high velocity, significant volume, and variability.
            \item Real-time data processing is a necessity for thriving in a data-centric economy.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Streaming data processing serves as the backbone of modern data intelligence, enabling organizations to harness the power of real-time data for better decision-making, enhanced customer service, and innovative operational strategies.
        As we move forward in this chapter, we will delve deeper into the specific characteristics of streaming data and the technologies that facilitate effective processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding Streaming Data - Definition}
    \begin{block}{Definition of Streaming Data}
        \textbf{Streaming Data} refers to data that is continuously generated by various sources at a high velocity, necessitating real-time processing. Unlike traditional data processing, where data is collected in batches, streaming data allows organizations to analyze information as it is produced, facilitating timely decision-making and insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding Streaming Data - Key Characteristics}
    \begin{block}{Key Characteristics of Streaming Data}
        \begin{enumerate}
            \item \textbf{High Velocity}
                \begin{itemize}
                    \item \textit{Explanation}: Created and transmitted at a rapid rate; requires immediate processing to remain relevant.
                    \item \textit{Example}: Financial market data, where stock prices fluctuate multiple times per second.
                \end{itemize}
                
            \item \textbf{High Volume}
                \begin{itemize}
                    \item \textit{Explanation}: Encompasses massive amounts of information generated from multiple sources simultaneously.
                    \item \textit{Example}: Social media platforms producing millions of tweets and posts every minute.
                \end{itemize}
                
            \item \textbf{Variability}
                \begin{itemize}
                    \item \textit{Explanation}: Can be diverse, coming from various sources with differing formats, including structured, semi-structured, and unstructured data.
                    \item \textit{Example}: Sensor data from IoT devices with varied formats and intervals.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding Streaming Data - Illustrative Example}
    \begin{block}{Example Scenario: Smart City Environment}
        \begin{itemize}
            \item Sensors monitor traffic flow, generating data on vehicle count every second.
            \item Cameras analyze video feeds for congestion, producing multiple frames per second.
            \item Social media platforms provide real-time posts regarding traffic conditions.
        \end{itemize}
        \vspace{0.5cm}
        This context exemplifies:
        \begin{itemize}
            \item High velocity: Data generated every second.
            \item High volume: Multiple sources contributing simultaneously.
            \item Variability: Different data formats being produced.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Streaming vs. Batch Processing - Introduction}
    In the realm of data processing, \textbf{streaming} and \textbf{batch} processing represent two distinct paradigms. Understanding these concepts is critical for choosing the appropriate method for handling data based on specific use cases and requirements.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Streaming Data Processing}
    \begin{itemize}
        \item \textbf{Definition}: Streaming data processing refers to the continuous input, processing, and output of data streams in real time.
        \item \textbf{Characteristics}:
        \begin{itemize}
            \item \textbf{Latency}: Low, as it processes data immediately upon arrival.
            \item \textbf{Data Volume}: Can handle high-velocity data from sources like IoT devices and social media feeds.
            \item \textbf{Dynamics}: Adapts to variations in data flow.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Use Cases}
        \begin{itemize}
            \item Real-Time Analytics (e.g., social media sentiment monitoring)
            \item Fraud Detection (e.g., real-time anomaly identification)
            \item IoT Applications (e.g., processing sensor data)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Batch Processing}
    \begin{itemize}
        \item \textbf{Definition}: Batch processing involves collecting data over a period and processing it as a single unit or batch.
        \item \textbf{Characteristics}:
        \begin{itemize}
            \item \textbf{Latency}: Higher, as it processes data after a defined period.
            \item \textbf{Data Volume}: Suitable for large volumes of data collected over time.
            \item \textbf{Structure}: Typically uses more structured data with predefined schemas.
        \end{itemize}
    \end{itemize}

    \begin{block}{Use Cases}
        \begin{itemize}
            \item Data Warehousing (e.g., daily sales data compilation)
            \item ETL Processes (e.g., data transformation for business intelligence)
            \item Log Processing (e.g., daily analysis of server logs)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Comparisons: Streaming vs. Batch}
    \begin{center}
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Feature} & \textbf{Streaming Data Processing} & \textbf{Batch Processing} \\
            \hline
            Latency & Low & High \\
            \hline
            Data Volume & High velocity, real-time & Large volumes collected over time \\
            \hline
            Complexity & Requires sophisticated infrastructure & Simpler architecture \\
            \hline
            Data Handling & Continuous flow & Discrete sets \\
            \hline
            Scalability & Highly scalable with demand & Requires significant resources for large batches \\
            \hline
            Best For & Real-time insights and actions & Historical data analysis and trends \\
            \hline
        \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    Choosing between streaming and batch processing depends on the specific needs of the application in question. 
    \begin{itemize}
        \item Streaming is ideal for real-time insights.
        \item Batch processing is better suited for comprehensive analysis of historical data.
        \item Stay aware of system architecture when choosing a processing paradigm.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visual Diagram}
    \begin{center}
        \texttt{ 
                  +--------------------+ \\
                  |                    | \\
                  |   Data Sources     | \\
                  |                    | \\
                  +------|-------------+ \\
                         | \\
              +----------+----------+ \\
              |                     | \\
        +-----v-----+       +-------v-------+ \\
        | Streaming  |       |   Batch       | \\
        | Processing |       |   Processing   | \\
        +-----------+        +---------------+ \\
              |                       | \\
              |                       | \\
      +-------v-------+         +-----v-----+ \\
      | Real-Time     |         | Scheduled | \\
      | Insights      |         | Analyses  | \\
      +---------------+         +-----------+ \\
        }
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Technologies in Streaming Data Processing}
    Streaming data processing is essential in today's data-centric applications, where information is generated continuously and must be processed in real-time. This slide introduces three key technologies widely used in streaming data architectures: 
    \begin{itemize}
        \item \textbf{Apache Kafka}
        \item \textbf{Apache Flink}
        \item \textbf{Apache Spark Streaming}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Apache Kafka}
    \begin{block}{Overview}
        Kafka is a distributed messaging system designed for high-throughput, fault-tolerant data streaming, operating on a publish-subscribe model.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Scalability: Handles large volumes of data across clusters.
            \item Durability: Messages are persisted on disk to ensure no data loss.
            \item Real-time Data Processing: Enables the building of real-time applications.
        \end{itemize}
    \end{itemize}

    \begin{block}{Example Use Case}
        In an e-commerce application, Kafka can stream user actions (e.g., product views, clicks) to analytics engines for live tracking of user behavior.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Apache Flink}
    \begin{block}{Overview}
        Flink is a stream processing framework for event-driven applications, offering low-latency processing and consistent state management.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Event Time Processing: Manages out-of-order event processing.
            \item Stateful Computations: Allows complex event processing with state management.
            \item Fault Tolerance: Implements exactly-once processing through snapshots.
        \end{itemize}
    \end{itemize}

    \begin{block}{Example Use Case}
        In fraud detection, Flink analyzes real-time transactions to identify patterns indicative of fraudulent behavior, allowing for immediate action.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Apache Spark Streaming}
    \begin{block}{Overview}
        Spark Streaming processes data streams in micro-batches and builds upon the capabilities of Apache Spark's Batch processing.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Unified Framework: Combines batch and stream processing.
            \item High-level APIs: Simplifies tasks using RDDs (Resilient Distributed Datasets) and DataFrames.
            \item Integration: Works with MLlib for machine learning applications.
        \end{itemize}
    \end{itemize}

    \begin{block}{Example Use Case}
        In live sports analytics, Spark Streaming can process real-time statistics, providing insights into player performance during a game.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Kafka:} Primarily handles high-volume messaging.
        \item \textbf{Flink:} Excels in low-latency processing with rich state management.
        \item \textbf{Spark Streaming:} Offers powerful analytics capabilities while bridging batch and streaming tasks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Each of these technologies plays a crucial role in the streaming data processing landscape. Selecting the right tool depends on:
    \begin{itemize}
        \item Specific use cases
        \item Data volume
        \item Processing requirements
        \item Desired architecture
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Kafka Overview}
    \begin{block}{Introduction}
        Apache Kafka is a distributed streaming platform designed for high-throughput, fault-tolerant, real-time data streaming. 
        It handles large volumes of data by allowing producers to send messages to one or more consumers.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Kafka Architecture}
    \begin{enumerate}
        \item \textbf{Topics:} Channels for sending and receiving records; partitioned for scalability.
        \item \textbf{Producers:} Applications that publish data to Kafka topics; can send data in batches.
        \item \textbf{Consumers:} Applications that process streams of records from topics.
        \item \textbf{Brokers:} Kafka servers that store data; a cluster consists of multiple brokers for availability.
        \item \textbf{Zookeeper:} Manages Kafka brokers for leader election and configuration (some functions integrated into Kafka).
    \end{enumerate}
    \begin{block}{Illustration}
        \includegraphics[width=\linewidth]{https://kafka.apache.org/documentation/#architecture}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features and Use Cases}
    \begin{itemize}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Scalability: Horizontal scaling by adding brokers.
            \item Durability: Messages persisted on disk and replicated for fault tolerance.
            \item Performance: Handles millions of messages per second with low latency.
        \end{itemize}
        \item \textbf{Use Cases:}
        \begin{itemize}
            \item Real-time analytics for applications like financial services.
            \item Log aggregation from services across distributed systems.
            \item Data integration across heterogeneous systems.
            \item Stream processing with frameworks like Apache Flink or Spark Streaming.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    Below is an example of a simple Java Kafka Producer:
    \begin{lstlisting}[language=Java]
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        producer.send(new ProducerRecord<>("my-topic", "key", "value"));
        producer.close();
    }
}
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Apache Kafka is built for high-throughput and low-latency streaming.
        \item It employs a pub/sub model for independent producer and consumer operations.
        \item Core components include topics, producers, consumers, brokers, and Zookeeper.
        \item Vital for various real-time data processing applications.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Stream Processing Frameworks}
    \begin{block}{Introduction}
        In the age of big data, stream processing frameworks are vital for handling and analyzing continuous streams of data in real-time. Unlike traditional batch processing, stream processing allows for responsive, real-time data operations, making it essential for applications such as:
        \begin{itemize}
            \item Real-time analytics
            \item Fraud detection
            \item Monitoring
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Popular Stream Processing Frameworks}
    \begin{enumerate}
        \item \textbf{Apache Flink}
        \begin{itemize}
            \item Powerful for distributed stream processing with stateful computation.
            \item Features:
            \begin{itemize}
                \item Event time processing and state management
                \item Built-in fault tolerance via distributed snapshots
                \item Scalable and high throughput
            \end{itemize}
            \item \textbf{Example Use Case:} Real-time analytics for financial transactions
        \end{itemize}
        
        \item \textbf{Apache Spark Streaming}
        \begin{itemize}
            \item An extension of Apache Spark for processing real-time data streams.
            \item Features:
            \begin{itemize}
                \item Micro-batch processing model
                \item Integrates with existing Spark batch processing
                \item Supports windowing and stateful processing
            \end{itemize}
            \item \textbf{Example Use Case:} Processing live logs from web applications
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Popular Stream Processing Frameworks (Cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Apache Kafka Streams}
        \begin{itemize}
            \item A client library for processing data stored in Kafka.
            \item Features:
            \begin{itemize}
                \item Simple programming model
                \item Strong scalability and fault tolerance
                \item Interactive queries for real-time analytics
            \end{itemize}
            \item \textbf{Example Use Case:} Real-time monitoring and alerting for sensor data
        \end{itemize}

        \item \textbf{Google Cloud Dataflow}
        \begin{itemize}
            \item A fully-managed service for stream and batch processing.
            \item Features:
            \begin{itemize}
                \item Unified stream and batch data processing
                \item Supports event time and processing time
                \item Handles dynamic work rebalancing
            \end{itemize}
            \item \textbf{Example Use Case:} ETL processes in a cloud-based analytics platform
        \end{itemize}

        \item \textbf{Apache Storm}
        \begin{itemize}
            \item A distributed real-time computation system.
            \item Features:
            \begin{itemize}
                \item Processes unbounded streams of data in real-time
                \item Rich support for complex event processing
                \item Fault-tolerant and scalable
            \end{itemize}
            \item \textbf{Example Use Case:} Real-time sentiment analysis on social media data
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Real-time Processing:} Stream processing frameworks handle continuous data flows, enabling immediate insights.
        \item \textbf{Fault Tolerance:} Most frameworks ensure data integrity and processing continuity in case of failures.
        \item \textbf{Scalability:} Frameworks like Flink and Spark Streaming are designed to scale horizontally to accommodate data growth.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Each stream processing framework has unique strengths suited for different applications. Understanding these frameworks allows organizations to select the right tool for their specific real-time data processing needs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example}
    \begin{lstlisting}[language=Java]
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
DataStream<String> text = env.socketTextStream("localhost", 9999);
DataStream<Tuple2<String, Integer>> counts = text.flatMap(new Tokenizer())
                                                  .keyBy(0)
                                                  .sum(1);
counts.print();
env.execute("WordCount");
    \end{lstlisting}
    \begin{block}{Description}
        This snippet illustrates the simplicity of a Flink application for counting words from a real-time socket input.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Event-Driven Architectures}
    % Understanding Event-Driven Architectures for Real-Time Data Processing
    \begin{block}{Definition of EDA}
        An architectural pattern where the system responds to events, which are changes in state or actions taken by users or systems.
        Events can be triggered by user actions, system states, or messages from other services.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of EDA}
    \begin{itemize}
        \item \textbf{Events}: The fundamental unit of communication in EDA, representing a meaningful change.
        \item \textbf{Event Producers}: Components that generate events (e.g., web applications, IoT devices).
        \item \textbf{Event Consumers}: Components that process or respond to events (e.g., microservices, data analytics engines).
        \item \textbf{Event Brokers}: Middleware that transmits events between producers and consumers, ensuring loose coupling.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-Time Data Processing}
    % Applications of EDA in various domains
    EDA enables systems to react immediately to new data, making it ideal for applications requiring speed and agility, such as:
    \begin{itemize}
        \item Financial market trading platforms
        \item Real-time fraud detection systems
        \item IoT scenarios, such as monitoring smart devices
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration: Order Processing System}
    \begin{itemize}
        \item \textbf{Event Producer}: E-commerce website that generates an “Order Placed” event when a user completes a purchase.
        \item \textbf{Event Broker}: Kafka or RabbitMQ, manages and distributes the events to various services.
        \item \textbf{Event Consumers}: Microservices like inventory management (updates stock levels) and notification service (sends confirmation emails).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of EDA}
    \begin{itemize}
        \item \textbf{Scalability}: Easily scales components independently to handle varying data loads.
        \item \textbf{Resilience}: If one part fails, others can continue to function, improving the system's reliability.
        \item \textbf{Flexibility}: New functionalities can be added without restructuring the entire system.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=json]
    {
      "event": "OrderPlaced",
      "data": {
        "orderId": "12345",
        "userId": "67890",
        "totalAmount": 99.99
      }
    }
    \end{lstlisting}
    \begin{block}{Note}
        This represents an order placed event containing relevant data for processing by various consumers.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Event-driven systems improve responsiveness and user experience.
        \item They enable better resource utilization compared to traditional request-based systems.
        \item EDA is an essential component of modern software architecture, particularly in large-scale and distributed systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Utilizing event-driven architectures is crucial for real-time data processing, leading to faster, more efficient applications capable of meeting the demands of today’s data-driven environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion Techniques - Overview}
    \begin{block}{Data Ingestion Definition}
        Data ingestion is the process of capturing and transporting data from various sources to a target system for storage and analysis. 
    \end{block}
    \begin{block}{Importance}
        In the context of streaming data, effective ingestion techniques are crucial for real-time processing and analytics.
    \end{block}
    \begin{block}{Focus}
        Here we will explore common techniques for streaming data ingestion.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion Techniques - Key Techniques}
    \begin{enumerate}
        \item \textbf{File-Based Ingestion}
        \begin{itemize}
            \item Description: Reading data from files stored in object storage as they become available.
            \item Example: Daily log file processed in real-time.
        \end{itemize}
        \item \textbf{Message Queues}
        \begin{itemize}
            \item Description: Tools like Apache Kafka and AWS Kinesis provide a publish-subscribe mechanism.
            \item Example: Sensor data sent every second to a Kafka topic.
        \end{itemize}
        \item \textbf{Stream Processing Frameworks}
        \begin{itemize}
            \item Description: Frameworks that ingest and process streams simultaneously.
            \item Example: Real-time transaction analysis for fraud detection.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion Techniques - API and CDC}
    \begin{enumerate}[resume]
        \item \textbf{API-Based Ingestion}
        \begin{itemize}
            \item Description: Fetching real-time data through APIs during specific events.
            \item Example: Streaming weather data through an API.
        \end{itemize}
        \item \textbf{Database Change Data Capture (CDC)}
        \begin{itemize}
            \item Description: Captures and streams changes in a database in real-time.
            \item Example: Streaming inventory changes for real-time analytics.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Scalability}: Must scale with data volume and velocity.
            \item \textbf{Latency}: Low latency is crucial for real-time applications.
            \item \textbf{Fault Tolerance}: Systems should handle failures gracefully without data loss.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Choosing the right data ingestion technique is vital for ensuring efficient streaming data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration and Code Snippet}
    \begin{block}{Example Illustration}
        - Sequence diagram illustrating a typical data ingestion pipeline:
        \begin{itemize}
            \item Sources: Sensors, APIs
            \item Messaging: Feeding into message queues
            \item Processing: Consumed by processing applications
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers='localhost:9092')

# Send data to Kafka topic
producer.send('topic_name', b'new data message')
producer.flush()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Processing Techniques in Streaming}
    Key processing techniques include:
    \begin{enumerate}
        \item Transformations
        \item Aggregations
        \item Windowing
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformations}
    \begin{block}{Definition}
        Transformations are operations applied to streaming data to convert, filter, or enrich the data stream. They can be stateless (like mapping) or stateful (like grouping).
    \end{block}
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Map Transformation}: Applies a function to every element in the stream.
                \begin{equation}
                \text{Fahrenheit} = \left(\text{Celsius} \times \frac{9}{5}\right) + 32
                \end{equation}
            \item \textbf{Filter Transformation}: Removes data points not meeting specific criteria, such as filtering out records of people below a certain age.
        \end{itemize}
    \end{block}
    \begin{block}{Key Point}
        Transformations enable the preparation and cleaning of data for subsequent analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Aggregations and Windowing}
    \begin{block}{Aggregations}
        \begin{itemize}
            \item \textbf{Definition}: Summarize multiple data points into a single data point through mathematical operations.
            \item \textbf{Examples}:
                \begin{itemize}
                    \item \textbf{Count Aggregation}: Counting transactions in a given period.
                    \item \textbf{Average Calculation}: Calculating average temperature over a sliding window.
                \end{itemize}
            \item \textbf{Key Point}: Critical for insights and trends in data streams.
        \end{itemize}
    \end{block}

    \begin{block}{Windowing}
        \begin{itemize}
            \item \textbf{Definition}: Divides the stream into manageable chunks for processing.
            \item \textbf{Types of Windows}:
                \begin{itemize}
                    \item \textbf{Tumbling Window}: Fixed-size, non-overlapping (e.g., processing data every 5 minutes).
                    \item \textbf{Sliding Window}: Overlapping window (e.g., aggregating data every minute, retaining data from the last 10 minutes).
                \end{itemize}
            \item \textbf{Key Point}: Essential for handling continuous data and deriving insights from finite data segments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-Time Analytics Applications}
    \begin{block}{Overview}
        Real-time analytics allows organizations to process and analyze data as it is generated, leading to timely insights and actions. By leveraging streaming data, businesses can operate more efficiently, enhance customer experiences, and mitigate risks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Use Cases - Part 1}
    \begin{enumerate}
        \item \textbf{Fraud Detection}
            \begin{itemize}
                \item \textbf{Concept:} Detecting fraudulent transactions as they happen to minimize losses.
                \item \textbf{How it Works:} Streaming algorithms analyze transactions in real-time against established patterns of legitimate behavior.
                \item \textbf{Example:} A banking system flags a transaction for further investigation or freezes the account if an unusual pattern is detected.
            \end{itemize}

        \item \textbf{IoT Sensor Data Analysis}
            \begin{itemize}
                \item \textbf{Concept:} Monitoring data from IoT devices for predictive maintenance and operational optimization.
                \item \textbf{How it Works:} Real-time anomaly detection from multiple sensor data streams to avert equipment failure.
                \item \textbf{Example:} In a smart factory, overheating sensors prompt preventative actions to avoid breakdowns.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Use Cases - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Social Media Monitoring}
            \begin{itemize}
                \item \textbf{Concept:} Analyzing social media feeds in real-time to gauge public sentiment and emerging trends.
                \item \textbf{How it Works:} Processing social media data to identify sentiment shifts and trending topics.
                \item \textbf{Example:} Brands can monitor Twitter for product feedback and respond quickly to customer reactions.
            \end{itemize}

        \item \textbf{Stock Market Analysis}
            \begin{itemize}
                \item \textbf{Concept:} Real-time processing of stock market data for informed trading decisions.
                \item \textbf{How it Works:} Algorithms react to market fluctuations immediately after they occur.
                \item \textbf{Example:} Trading firms execute trades based on milliseconds of insights to gain competitive advantages.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Timeliness:} Empower businesses to act instantly on insights.
            \item \textbf{Scalability:} Adaptable streaming architectures for increasing data flows.
            \item \textbf{Adaptability:} Continuous refinement of analytics based on real-time feedback.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Real-time analytics powered by streaming data transforms industries by enabling immediate insights and actions. The potential applications will expand, offering competitive advantages in fast-changing environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Streaming Data Processing - Overview}
    \begin{block}{Introduction}
        Streaming data processing involves handling data in real-time, presenting unique challenges that must be addressed to build efficient and reliable systems. Key challenges include:
    \end{block}
    \begin{itemize}
        \item Latency
        \item Fault Tolerance
        \item Data Consistency
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Streaming Data Processing - Latency}
    \begin{block}{Definition}
        Latency is the delay between data generation and processing.
    \end{block}
    \begin{itemize}
        \item \textbf{Types of Latency:}
        \begin{itemize}
            \item Network Latency: Time taken for data to travel over the network.
            \item Processing Latency: Time taken by the processing engine to analyze the data.
        \end{itemize}
        \item \textbf{Example:} In online fraud detection, low latency is crucial for real-time processing of transactions.
        \item \textbf{Key Point:} Minimizing latency is essential; strategies include optimizing protocols and using in-memory processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Streaming Data Processing - Fault Tolerance}
    \begin{block}{Definition}
        Fault tolerance is the ability to continue operation despite failures.
    \end{block}
    \begin{itemize}
        \item \textbf{Importance:} Streaming systems must handle errors gracefully to avoid data loss and maintain processing.
        \item \textbf{Example:} In a video streaming platform, a server failure must not disrupt user experience or result in data loss.
        \item \textbf{Key Point:} Effective fault tolerance mechanisms, such as checkpointing and data replication, are crucial.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Streaming Data Processing - Data Consistency}
    \begin{block}{Definition}
        Data consistency ensures that data remains accurate and reliable across distributed systems.
    \end{block}
    \begin{itemize}
        \item \textbf{Challenges:} Achieving strong consistency in a real-time environment is difficult due to the speed of data inflow.
        \item \textbf{Example:} In stock trading applications, inconsistent processing of stock price updates can lead to incorrect trades.
        \item \textbf{Key Point:} Implementing consistency models, like eventual consistency or strong consistency, is vital for reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Streaming Data Processing - Summary and Conclusion}
    \begin{block}{Summary}
        \begin{itemize}
            \item \textbf{Latency:} Key for real-time applications; requires optimization techniques.
            \item \textbf{Fault Tolerance:} Essential for maintaining operations during failures; uses methods like checkpointing.
            \item \textbf{Data Consistency:} Critical for reliability; needs appropriate consistency models in distributed systems.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Addressing these challenges is essential for developing robust streaming data processing systems. In the next section, we will explore strategies to ensure data quality in streaming systems.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Data Quality in Streaming Systems}
    \begin{block}{Introduction}
        Data quality is critical in streaming data processing, where large volumes of continuous data are generated in real-time. 
        Ensuring the accuracy, consistency, and reliability of data is essential to make sound decisions based on this data. 
        This slide outlines strategies to maintain data quality and integrity while dealing with streaming data.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Strategies for Ensuring Data Quality}
    \begin{enumerate}
        \item \textbf{Data Validation}
        \item \textbf{Schema Enforcement}
        \item \textbf{Deduplication}
        \item \textbf{Monitoring and Alerting}
        \item \textbf{Data Enrichment}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Validation}
    \begin{itemize}
        \item \textbf{Definition:} Process of ensuring data is accurate and useful by checking for errors during data ingestion.
        \item \textbf{Example:} Implementing checks that ensure values fall within expected ranges.
    \end{itemize}
    \begin{lstlisting}[language=Python]
# Example in Python for validating incoming temperature data
def validate_temperature(data):
    return 0 <= data <= 100  # Assuming valid range is between 0 to 100
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Schema Enforcement}
    \begin{itemize}
        \item \textbf{Definition:} Defining and enforcing a data schema that specifies the format and structure of incoming data streams.
        \item \textbf{Example:} Use of data formats like Avro or JSON schema to validate data fields.
    \end{itemize}
    \begin{lstlisting}[language=json]
{
    "type": "record",
    "name": "WeatherData",
    "fields": [
        {"name": "temperature", "type": "float"},
        {"name": "humidity", "type": "float"},
        {"name": "timestamp", "type": "string"}
    ]
}
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deduplication and Monitoring}
    \begin{itemize}
        \item \textbf{Deduplication:}
        \begin{itemize}
            \item Eliminating duplicate records that can skew analysis and insights.
            \item \textbf{Example:} Use an identifier to filter out duplicate transactions.
        \end{itemize}
        \begin{lstlisting}[language=Python]
# Example snippet for deduplication using a set
seen_ids = set()
unique_data = []
for record in incoming_stream:
    if record.id not in seen_ids:
        unique_data.append(record)
        seen_ids.add(record.id)
        \end{lstlisting}
        
        \item \textbf{Monitoring and Alerting:}
        \begin{itemize}
            \item Continuously monitoring data quality metrics.
            \item \textbf{Example:} Setting up alerts for latency or quality drops.
        \end{itemize}
        \begin{lstlisting}[language=yaml]
# Example pseudo-configuration for monitoring alert
alerts:
    - metric: latency
      threshold: 200ms
      action: send_alert
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Enrichment and Key Points}
    \begin{itemize}
        \item \textbf{Data Enrichment:}
        \begin{itemize}
            \item Enhancing data quality by adding additional context.
            \item \textbf{Example:} Enriching user interaction data with demographics.
        \end{itemize}
        \begin{lstlisting}[language=sql]
SELECT user_id, interaction_data, demographics 
FROM interactions JOIN demographics ON interactions.user_id = demographics.user_id
        \end{lstlisting}
    \end{itemize}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Proactive Approach
            \item Continuous Improvement
            \item Collaboration with Stakeholders
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Maintaining data quality in streaming systems is challenging but essential. 
    Employing robust strategies such as validation, schema enforcement, deduplication, monitoring, and enrichment ensures the integrity and value of streaming data leading to more accurate insights and better decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Streaming Data Processing}
    \begin{block}{Overview}
        Streaming data processing is evolving rapidly to meet the demands of real-time analysis and immediate decision-making. 
        Two major trends shaping the future are \textbf{Edge Computing} and \textbf{Real-Time Machine Learning}. 
        Understanding these trends is critical for leveraging streaming data effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends - Edge Computing}
    \begin{block}{Definition}
        Edge Computing refers to processing data near the source of generation rather than relying on a centralized data center. 
        This reduces latency, saves bandwidth, and increases responsiveness.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Proximity}: Data is processed closer to where it is generated, which is crucial for time-sensitive applications.
        \item \textbf{Reduced Latency}: Minimizing the distance data must travel allows for responses in milliseconds.
        \item \textbf{Bandwidth Efficiency}: Only essential data is sent to the cloud, drastically reducing the amount of data transferred.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends - Edge Computing Example}
    \begin{block}{Example: Autonomous Vehicles}
        Vehicles generate massive streams of data from sensors. 
        Processing data at the edge allows for quick decision-making, crucial for safety and efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends - Real-Time Machine Learning}
    \begin{block}{Definition}
        Real-Time Machine Learning combines streaming data with machine learning algorithms to make instant predictions and automations.
    \end{block}

    \begin{itemize}
        \item \textbf{Adaptive Models}: Models can be updated continuously as new data arrives.
        \item \textbf{Immediate Decision Making}: Businesses can respond to user actions instantly.
        \item \textbf{Use Cases}: Fraud detection, personalized recommendations, and dynamic pricing models.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends - Real-Time Machine Learning Example}
    \begin{block}{Example: E-commerce}
        An online retail site can analyze user behavior in real-time, adjusting recommendations and promotions based on current engagement.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Important Takeaways}
    \begin{block}{Conclusion}
        The convergence of edge computing and real-time machine learning will revolutionize how organizations process and utilize streaming data.
    \end{block}

    \begin{itemize}
        \item Edge computing speeds up processing times and cuts bandwidth costs.
        \item Real-time machine learning allows for adaptive, data-driven insights on-the-fly.
        \item Businesses must evolve with these technologies to stay competitive and provide enhanced services.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formulas/Diagrams}
    \begin{block}{Data Flow in Edge Computing}
        \begin{equation}
            \text{[Data Source]} \rightarrow \text{[Edge Device]} \rightarrow \text{[Cloud Processing (if necessary)]}
        \end{equation}
    \end{block}

    \begin{block}{Real-Time ML Prediction Cycle}
        \begin{equation}
            \text{Data Ingestion} \rightarrow \text{Feature Extraction} \rightarrow \text{Prediction} \rightarrow \text{Feedback Loop} \rightarrow \text{Model Update}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies on Streaming Data Processing - Introduction}
    \begin{block}{Introduction to Streaming Data Processing}
        Streaming Data Processing involves the continuous ingestion, processing, and analysis of data in real-time. It enables organizations to respond to events as they happen, thereby gaining insights and driving timely decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Streaming Data Processing}
    \begin{itemize}
        \item \textbf{Real-Time Analytics:} The ability to analyze and derive insights from data instantly as it streams in.
        \item \textbf{Event-Driven Architecture:} A design paradigm where events are processed as they occur, allowing applications to respond dynamically.
        \item \textbf{Scalability and Fault Tolerance:} Systems designed to handle increasing loads while ensuring reliability, even in the face of failures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Example 1: Financial Services - Fraud Detection}
    \begin{itemize}
        \item \textbf{Overview:} A large bank implemented a streaming analytics solution to detect fraudulent activities in real-time.
        \item \textbf{Implementation:} Using Apache Kafka and Apache Flink, the bank captures transaction data streams and applies machine learning models to assess risks.
        \item \textbf{Outcome:} Suspicious transactions are flagged instantly, reducing fraud losses by 30\%.
        \item \textbf{Key Point:} Real-time processing of data allows for immediate corrective action, enhancing security.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Example 2: E-Commerce - Personalized Recommendations}
    \begin{itemize}
        \item \textbf{Overview:} An e-commerce platform deployed a streaming engine to deliver personalized shopping experiences to users.
        \item \textbf{Implementation:} Utilizing Amazon Kinesis, the platform analyzes user clickstream data to tailor recommendations dynamically.
        \item \textbf{Outcome:} Increased conversion rates by 20\% due to more relevant product suggestions delivered in real-time.
        \item \textbf{Key Point:} Streaming data helps businesses enhance customer engagement and satisfaction.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Example 3: Social Media - Sentiment Analysis}
    \begin{itemize}
        \item \textbf{Overview:} A major social media platform uses streaming data processing to analyze user sentiment around trending topics.
        \item \textbf{Implementation:} Leveraging Spark Streaming, they gather and process tweets in real-time to gauge public opinion.
        \item \textbf{Outcome:} Enhanced content moderation and user experience by responding promptly to negative sentiments.
        \item \textbf{Key Point:} Continuous analysis of user interactions provides valuable insights for brand management.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        These case studies illustrate the significant impact of streaming data processing across industries. 
        By implementing real-time analytics, organizations can react swiftly to insights gained from their data, positioning themselves competitively in an increasingly data-driven world.
    \end{block}
    \begin{itemize}
        \item Streaming Data Processing allows for immediate insights and responses, transforming decision-making processes.
        \item Successful implementations across different sectors showcase the versatility and necessity of adopting streaming approaches.
        \item Understanding the architecture and tools involved is crucial for leveraging the potential of real-time data analytics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    Prepare for the upcoming capstone project by considering how you could apply streaming solutions to real-world data challenges. 
    Think about potential data sources and analysis goals that align with the patterns discussed in the case studies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Capstone Project Overview}
    \begin{block}{Overview of Real-Time Data Processing Challenges}
        In this capstone project, students will explore real-time data processing within streaming data systems. 
        The goal is to identify various challenges and develop practical solutions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Streaming Data Processing}
    \begin{itemize}
        \item \textbf{Definition of Streaming Data:} 
        Continuously generated data from multiple sources, processed in real-time.
        
        \item \textbf{Characteristics of Streaming Data:}
        \begin{itemize}
            \item \textbf{Velocity:} High-speed data generation must be processed as it arrives.
            \item \textbf{Variety:} Data exists in diverse formats (text, audio, video, etc.).
            \item \textbf{Volume:} Large data quantities require efficient storage and handling methods.
        \end{itemize}
        
        \item \textbf{Real-Time Data Processing Frameworks:} 
        Common frameworks like Apache Kafka, Apache Flink, and Apache Spark Streaming cater to different needs. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Objectives and Scenario}
    \begin{block}{Project Objectives}
        \begin{itemize}
            \item \textbf{Identify Real-Time Challenges:} 
            Examples include data latency, handling out-of-order data, and ensuring system scalability.
            
            \item \textbf{Develop Solutions:} 
            Propose strategies using frameworks, algorithms, and architectures that improve performance and reliability.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Scenario}
        \textbf{Scenario:} An IoT-enabled smart city collecting data from various sources.
        \begin{itemize}
            \item \textbf{Challenges:} Real-time video processing for incident detection and social media analysis for traffic conditions.
            \item \textbf{Solutions:} A low-latency processing pipeline with Apache Kafka and analytics through Apache Flink.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Importance of Stream Processing:} 
        Enables timely decision-making in various domains, including finance and healthcare.

        \item \textbf{Interdisciplinary Collaboration:} 
        The project integrates knowledge from data engineering and real-world applications.
        
        \item \textbf{Illustrative Framework Example:}
        \begin{enumerate}
            \item Data Sources: IoT Devices, Social Media
            \item Stream Processing Engine: Apache Kafka
            \item Analytics Engine: Apache Spark Streaming
            \item Output: Dashboards, alerts, and reports.
        \end{enumerate}
    \end{itemize}
    
    \begin{block}{Conclusion}
        The capstone project synthesizes knowledge on streaming data processing while providing hands-on experience with real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Key Takeaways - Part 1}
    \begin{block}{Understanding Streaming Data Processing}
        \begin{enumerate}
            \item \textbf{Definition and Importance:} 
                Streaming data processing involves the continuous input, processing, and output of data streams in real-time, enabling organizations to gain immediate insights.
            \item \textbf{Key Characteristics:}
                \begin{itemize}
                    \item \textbf{Low Latency:} Processing occurs in real-time, allowing for rapid responses to data events.
                    \item \textbf{Scalability:} Capable of handling data volumes from small bursts to massive streams.
                    \item \textbf{Fault Tolerance:} Systems can recover from failures without losing data.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Key Takeaways - Part 2}
    \begin{block}{Significance of Streaming Data Processing}
        \begin{itemize}
            \item \textbf{Enhanced Decision-Making:} Helps businesses react to trends and anomalies in real-time. 
            \item \textbf{Real-World Applications:}
                \begin{itemize}
                    \item \textbf{Financial Markets:} Real-time analysis of tick data for trading opportunities.
                    \item \textbf{Healthcare Monitoring:} Alerts healthcare providers to critical changes in patient vitals.
                    \item \textbf{Social Media Analytics:} Processes user interactions in real-time to identify trends.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Key Takeaways - Part 3}
    \begin{block}{Challenges and Key Takeaways}
        \begin{itemize}
            \item \textbf{Challenges:}
                \begin{itemize}
                    \item Complexity of implementation due to data volume, velocity, and variety.
                    \item Data quality management is crucial for accurate analytics.
                \end{itemize}
            \item \textbf{Key Takeaway Points:}
                \begin{itemize}
                    \item Integrate with existing batch systems for maximum efficiency.
                    \item Familiarize with tools like Apache Kafka, Flink, or Spark Streaming for specific streaming data needs.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Practical Example}
        Here’s a simple Python example using Apache Kafka for streaming messages:
        \begin{lstlisting}[language=Python]
from kafka import KafkaConsumer

# Create a Kafka consumer
consumer = KafkaConsumer('topic_name',
                         group_id='my_group',
                         bootstrap_servers='localhost:9092')

# Listen for messages
for message in consumer:
    print(f"Received message: {message.value.decode('utf-8')}")
        \end{lstlisting}
    \end{block}

    \begin{block}{Final Thoughts}
        Streaming data processing is transformational, reshaping how businesses operate in a data-driven world. Embrace its principles for a competitive advantage.
    \end{block}
\end{frame}


\end{document}