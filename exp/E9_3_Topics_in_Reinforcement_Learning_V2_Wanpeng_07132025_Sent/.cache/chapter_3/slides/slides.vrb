\frametitle{Value Iteration - Algorithm Steps}
    \begin{enumerate}
        \item \textbf{Initialization:}
            \begin{itemize}
                \item Start with arbitrary values for all states, often initializing \( V(s) = 0 \).
            \end{itemize}

        \item \textbf{Update Step:}
            \begin{equation}
                V_{new}(s) = \max_a \left( \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V(s')] \right)
            \end{equation}
            \begin{itemize}
                \item \( P(s' | s, a) \): Transition probability from state \( s \) to \( s' \) after action \( a \).
                \item \( R(s, a, s') \): Immediate reward after transitioning.
                \item \( \gamma \): Discount factor (0 â‰¤ \( \gamma \) < 1).
            \end{itemize}

        \item \textbf{Convergence Check:}
            \begin{itemize}
                \item Repeat updates until convergence (when the change is less than a threshold \( \epsilon \)).
            \end{itemize}

        \item \textbf{Extract Policy:}
            \begin{equation}
                \pi^*(s) = \arg\max_a \left( \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V(s')] \right)
            \end{equation}
    \end{enumerate}
