\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 8: Exploration vs. Exploitation]{Week 8: Exploration vs. Exploitation}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Exploration and Exploitation}
    \begin{itemize}
        \item In Reinforcement Learning (RL), agents balance between two strategies: 
        \textbf{exploration} and \textbf{exploitation}.
        \item Understanding this balance is crucial for effective RL algorithm design.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Exploration:}
            \begin{itemize}
                \item Trying out new actions to gather information about the environment.
                \item Goal: Discover potential rewards not previously identified.
                \item \textit{Example:} In a multi-armed bandit problem, an agent tries each lever to identify the highest average reward.
            \end{itemize}

        \item \textbf{Exploitation:}
            \begin{itemize}
                \item Using current knowledge to maximize reward.
                \item The agent selects actions believed to yield the highest expected reward based on past experiences.
                \item \textit{Example:} If an agent knows lever A provides a reward of 5, it will repeatedly choose lever A over less-known options.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Exploration-Exploitation Trade-off}
    \begin{itemize}
        \item The exploration-exploitation dilemma: decide whether to explore new actions or exploit known high-reward actions.
        \item Balancing these strategies is essential to optimize long-term rewards.
    \end{itemize}
    \begin{block}{Key Point}
        Too much exploration can lead to missed opportunities, while too much exploitation can prevent discovering better rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies for Balancing Exploration and Exploitation}
    \begin{itemize}
        \item \textbf{Epsilon-Greedy Strategy:}
            \begin{itemize}
                \item Chooses the best-known action with probability $(1 - \epsilon)$ and explores randomly with probability $\epsilon$.
                \item Formula: 
                \[
                \text{Action} = 
                \begin{cases} 
                \text{argmax}_a Q(a) & \text{with probability } (1 - \epsilon) \\
                \text{random action} & \text{with probability } \epsilon 
                \end{cases}
                \]
            \end{itemize}

        \item \textbf{Softmax Selection:}
            \begin{itemize}
                \item Actions are selected based on their estimated values, allowing less valuable actions to be chosen, but less frequently.
                \item Formula:
                \[
                P(a) = \frac{e^{Q(a)/\tau}}{\sum_{a'} e^{Q(a')/\tau}}
                \]
                (where $\tau$ is a temperature parameter controlling the level of exploration).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies for Balancing Exploration and Exploitation (cont.)}
    \begin{itemize}
        \item \textbf{Upper Confidence Bound (UCB):}
            \begin{itemize}
                \item Select actions based on both their estimated value and the uncertainty (variance) associated with each action.
                \item Encourages exploration of actions with high uncertainty.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Exploration and exploitation are central to Reinforcement Learning, affecting learning efficiency and performance.
        \item Balancing these strategies is essential for developing robust RL algorithms capable of optimal learning in dynamic environments.
    \end{itemize}
    \begin{block}{Remember}
        Effective RL design involves not just action selection, but also knowledge gathering for improved decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Defining Exploration and Exploitation - Key Concepts}
    \begin{itemize}
        \item \textbf{Exploration}: 
        \begin{itemize}
            \item The process of gathering information to discover better strategies or actions.
            \item Involves taking risks by trying new actions, potentially with no immediate reward.
            \item \textbf{Example}: Trying new cuisines at restaurants to find a favorite dish.
        \end{itemize}
        
        \item \textbf{Exploitation}:
        \begin{itemize}
            \item Utilizing known information to maximize immediate rewards.
            \item Can prevent discovering potentially better options.
            \item \textbf{Example}: Repeatedly eating a favorite dish rather than exploring new flavors.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation in Reinforcement Learning}
    In Reinforcement Learning (RL):
    \begin{itemize}
        \item Agents must allocate resources between exploration and exploitation.
        \item \textbf{Crucial for Effective Learning}: Striking the right balance is essential to achieve optimal performance.
    \end{itemize}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Trade-Off Dilemma}: Too much exploration wastes time on suboptimal actions, while too much exploitation limits learning.
            \item \textbf{Dynamic Decision Making}: The optimal strategy varies over time depending on the agent's knowledge.
            \item \textbf{Concrete Difficulty}: Determining the right balance often depends on trial and error tailored to specific problems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas Representing the Trade-Off}
    A common method to represent the exploration-exploitation trade-off is using parameters in algorithms like epsilon-greedy:
    
    \begin{itemize}
        \item \textbf{Exploration Rate ($\epsilon$)}: The probability of choosing a random action.
        \begin{equation}
            P(\text{exploration}) = \epsilon 
        \end{equation}
        \begin{equation}
            P(\text{exploitation}) = 1 - \epsilon 
        \end{equation}
        \item \textbf{Adaptive Methods}: Employ decaying epsilon strategies where $\epsilon$ decreases over time as more information is gained.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Exploration-Exploitation Trade-Off - Introduction}
    \begin{block}{Overview}
        The exploration-exploitation trade-off is a critical concept in Reinforcement Learning (RL).
        Balancing between:
        \begin{itemize}
            \item \textbf{Exploration}: Gathering new information.
            \item \textbf{Exploitation}: Utilizing known information to maximize rewards.
        \end{itemize}
        This balance is essential for achieving optimal policies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Exploration-Exploitation Trade-Off - Key Concepts}
    \begin{itemize}
        \item \textbf{Exploration}:
        \begin{itemize}
            \item Trying new actions to gain information about the environment.
            \item \textit{Example}: In a multi-armed bandit scenario, pulling a less-frequented lever may reveal higher rewards.
        \end{itemize}
        
        \item \textbf{Exploitation}:
        \begin{itemize}
            \item Selecting actions based on known information to maximize immediate rewards.
            \item \textit{Example}: Continuously pulling the lever with the highest historical reward.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Exploration-Exploitation Trade-Off - Mathematical Representation}
    \begin{block}{Trade-Off Importance}
        Achieving optimal performance in RL requires balancing exploration and exploitation:
        \begin{itemize}
            \item \textbf{Too Much Exploration}: Leads to suboptimal performance.
            \item \textbf{Too Much Exploitation}: Risks overlooking better strategies.
        \end{itemize}
    \end{block}

    \begin{equation}
    R = \alpha \cdot E + (1 - \alpha) \cdot X
    \end{equation}
    Where:
    \begin{itemize}
        \item \( R \): Total reward.
        \item \( E \): Reward from exploration (new actions).
        \item \( X \): Reward from exploitation (known actions).
        \item \( \alpha \): A parameter (0 ≤ \( \alpha \) ≤ 1) controlling the trade-off.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Exploration-Exploitation Trade-Off - Strategies for Balancing}
    \begin{enumerate}
        \item \textbf{Epsilon-Greedy}:
        \begin{itemize}
            \item With probability \( \epsilon \), choose a random action (exploration).
            \item With probability \( 1 - \epsilon \), choose the best-known action (exploitation).
        \end{itemize}
        
        \item \textbf{Softmax Action Selection}:
        \begin{itemize}
            \item Assign probabilities based on estimated values of each action.
        \end{itemize}
        
        \item \textbf{Upper Confidence Bound (UCB)}:
        \begin{itemize}
            \item Select actions based on both potential rewards and uncertainty.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Exploration-Exploitation Trade-Off - Conclusion}
    \begin{block}{Key Points}
        - Understand definitions and key implications of exploration and exploitation.
        - Recognize the impacts of imbalanced approaches.
        - Familiarize with strategies to effectively manage this trade-off.
    \end{block}
    
    By mastering the exploration-exploitation trade-off, students enhance their understanding of RL, promoting more effective learning algorithms and decision-making strategies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies for Exploration - Overview}
    \begin{itemize}
        \item Exploration is crucial in reinforcement learning (RL) for gathering information about the environment.
        \item Three key strategies for exploration:
        \begin{itemize}
            \item Epsilon-Greedy
            \item Softmax Actions
            \item Upper Confidence Bound (UCB)
        \end{itemize}
        \item Each strategy has unique characteristics and applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Epsilon-Greedy Strategy}
    \begin{block}{Concept}
        \begin{itemize}
            \item Balances exploration and exploitation.
            \item Random action with probability $\epsilon$, best-known action with probability $(1 - \epsilon)$.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula}
        If $\epsilon = 0.1$:
        \begin{itemize}
            \item 10\% likelihood of selecting a random action.
            \item 90\% likelihood of selecting the action with the highest estimated value.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        In a multi-armed bandit problem with 5 slot machines:
        \begin{itemize}
            \item Select one randomly (exploration) 10\% of the time.
            \item Play the machine giving the highest payout 90\% of the time (exploitation).
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Simple and effective.
            \item Performance depends on choice of $\epsilon$.
            \item $\epsilon$ can decay over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Softmax Actions}
    \begin{block}{Concept}
        \begin{itemize}
            \item Probabilistic action selection based on estimated values.
            \item Higher values mean higher likelihood of choosing that action.
        \end{itemize}
    \end{block}

    \begin{block}{Formula}
        The probability of selecting action $a$ is given by:
        \begin{equation}
            P(a) = \frac{e^{Q(a) / \tau}}{\sum_{b} e^{Q(b) / \tau}}
        \end{equation}
        Where $Q(a)$ is the estimated action value, and $\tau$ controls exploration.
    \end{block}

    \begin{block}{Example}
        \begin{itemize}
            \item Action A with $Q(A) = 5$, Action B with $Q(B) = 3$, $\tau = 1$:
            \item Action A will be chosen more often than B, but B still has a chance of being selected.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Allows for nuanced exploration.
            \item Temperature parameter $\tau$ can be tuned to balance exploration and exploitation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Upper Confidence Bound (UCB)}
    \begin{block}{Concept}
        \begin{itemize}
            \item Balances exploration and exploitation by considering uncertainty.
            \item Selects actions that maximize an upper confidence bound based on action trials.
        \end{itemize}
    \end{block}

    \begin{block}{Formula}
        UCB selection is defined as:
        \begin{equation}
            a_t = \arg\max_a \left( Q_t(a) + c \sqrt{\frac{\ln(t)}{N_t(a)}} \right)
        \end{equation}
        where $t$ is the current time step, $N_t(a)$ is the number of times action $a$ has been selected, and $c$ is a balancing hyperparameter.
    \end{block}

    \begin{block}{Example}
        \begin{itemize}
            \item If action A has been tried 10 times, B 5 times, UCB will favor B for exploration.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Encourages selection of less-explored actions without a fixed rate like $\epsilon$.
            \item Useful for assessing uncertainty in action values.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Understanding exploration strategies is essential for effective reinforcement learning.
        \item Each strategy has advantages and can be applied based on context and goals.
        \item Proper implementation can significantly enhance the performance and adaptability of RL algorithms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Strategies for Exploitation}
  \begin{block}{Key Concepts}
    This slide gives a detailed look at how RL algorithms exploit known strategies, focusing on value functions and policy derivation.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Definition of Exploitation}
  \begin{itemize}
    \item Exploitation in Reinforcement Learning (RL) leverages known information to maximize rewards.
    \item Focuses on using learned value functions or policies to make decisions.
    \item Contrast with exploration: while exploration gathers information, exploitation utilizes existing knowledge.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Value Functions}
  \begin{itemize}
    \item Value functions estimate how good a state or action is in terms of expected future rewards.
    \item Two primary types are:
      \begin{enumerate}
        \item \textbf{State Value Function} \( V(s) \):
        \begin{equation}
          V(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
        \end{equation}
        \item \textbf{Action Value Function} \( Q(s, a) \):
        \begin{equation}
          Q(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]
        \end{equation}
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Policy Derivation}
  \begin{itemize}
    \item Policies dictate actions at each state:
      \begin{itemize}
        \item \textbf{Deterministic Policy}: \( \pi(s) = a \)
        \item \textbf{Stochastic Policy}: \( \pi(a|s) = P(A_t = a \mid S_t = s) \)
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Exploitation Strategies}
  \begin{itemize}
    \item \textbf{Greedy Policy}:
    \begin{itemize}
      \item Maximizes immediate rewards using \( V(s) \) or \( Q(s, a) \):
        \begin{equation}
          \pi^*(s) = \arg\max_a Q(s,a)
        \end{equation}
    \end{itemize}
    \item \textbf{ε-Greedy Policy}:
    \begin{itemize}
      \item Primarily an exploration strategy, but can exploit by selecting the best-known action \( (1 - \epsilon) \) of the time.
    \end{itemize}
    \item \textbf{Value Iteration}:
    \begin{equation}
      V_{k+1}(s) = \max_a \sum_{s'} P(s' \mid s, a) [R(s, a, s') + \gamma V_k(s')]
    \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Importance of Exploitation in RL}
  \begin{itemize}
    \item **Maximizing Rewards**: Efficient action selection leads to higher expected returns.
    \item **Policy Improvement**: Enhances policies by focusing on high-performing actions.
    \item **Balancing Exploration and Exploitation**: Key for effective learning over time.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary}
  Understanding exploitation strategies enhances RL algorithm efficiency. Emphasizing known rewards and optimal actions allows reinforcement agents to operate more effectively, especially in familiar environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration Techniques - Introduction}
    \begin{block}{Introduction to Exploration in Reinforcement Learning}
        In Reinforcement Learning (RL), exploration refers to strategies employed to discover new information about the environment, which ultimately enhances the performance of an algorithm. 
    \end{block}
    \begin{itemize}
        \item Effective exploration is essential for balancing exploitation of known actions and discovering potentially better actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration Techniques - Key Techniques}
    \begin{enumerate}
        \item \textbf{Random Actions}
            \begin{itemize}
                \item \textbf{Description:} Choosing actions randomly with a certain probability. 
                \item \textbf{Example:} In a grid-world scenario, an agent may move randomly on 10\% of actions.
                \item \textbf{Formula:}
                    \begin{equation}
                    P(a) = 
                    \begin{cases} 
                    \epsilon & \text{if action } a \text{ is random} \\
                    1 - \epsilon & \text{if action } a \text{ is selected from the policy}
                    \end{cases}
                    \end{equation}
            \end{itemize}

        \item \textbf{Optimistic Initialization}
            \begin{itemize}
                \item \textbf{Description:} Setting initial value estimates to a high level to encourage exploration.
                \item \textbf{Example:} All Q-values in a bandit problem are initialized to 10.
                \item \textbf{Key Point:} This method enhances early-stage exploration as the agent seeks undervalued actions.
            \end{itemize}

        \item \textbf{Boltzmann Exploration}
            \begin{itemize}
                \item \textbf{Description:} Probabilistic action selection based on the Boltzmann distribution.
                \item \textbf{Formula:}
                    \begin{equation}
                    P(a) = \frac{e^{Q(a)/T}}{\sum_{b} e^{Q(b)/T}}
                    \end{equation}
                    where \( Q(a) \) is the action-value estimate and \( T \) is the temperature parameter.
                \item \textbf{Example:} Higher Q-value actions are preferred, but lower ones still have a chance of being selected.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration Techniques - Summary and Takeaways}
    \begin{block}{Summary of Exploration Techniques}
        \begin{itemize}
            \item \textbf{Random Actions:} Introduce variability for better exploration.
            \item \textbf{Optimistic Initialization:} Drives exploration by setting high initial values.
            \item \textbf{Boltzmann Exploration:} Balances exploration and exploitation based on a probability distribution.
        \end{itemize}
    \end{block}
    \begin{block}{Important Takeaways}
        \begin{itemize}
            \item Each technique has strengths and weaknesses based on environmental needs.
            \item Balancing exploration and exploitation improves RL performance, preventing local optima.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Balancing Techniques}
    % Overview of balancing techniques in RL
    In reinforcement learning, balancing exploration and exploitation is crucial for optimal performance. 
    This slide examines two popular methods:
    \begin{itemize}
        \item Decaying Epsilon Strategies
        \item Bayesian Approaches
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Decaying Epsilon Strategies}
    % Details on decaying epsilon strategies
    \begin{block}{Concept}
        Epsilon-greedy strategy starts by exploring random actions with probability \( \epsilon \) and exploits the best-known action with probability \( 1 - \epsilon \). As training progresses, \( \epsilon \) decays, reducing exploration.
    \end{block}

    \begin{block}{Implementation}
        \begin{itemize}
            \item Initial \( \epsilon \) (e.g., 1.0) provides equal chances to explore all actions.
            \item Gradually decrease \( \epsilon \) using a decay factor \( \gamma \) until it reaches a minimum threshold.
        \end{itemize}
    \end{block}

    \begin{equation}
        \epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_0 \cdot \gamma^t)
    \end{equation}
    
    where:
    \begin{itemize}
        \item \( \epsilon_t \): value at time step \( t \)
        \item \( \epsilon_0 \): initial exploration rate
        \item \( \epsilon_{\text{min}} \): minimum exploration rate
        \item \( \gamma \): decay factor (0 < \( \gamma \) < 1)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Decaying Epsilon Strategies (Continued)}
    % Example and summary of decaying epsilon strategies
    \begin{block}{Example}
        If \( \epsilon_0 = 1.0 \), \( \epsilon_{\text{min}} = 0.1 \), and \( \gamma = 0.99 \):
        \begin{equation}
            \epsilon_{1000} = \max(0.1, 1.0 \cdot 0.99^{1000})
        \end{equation}
        This allows for focused exploitation while retaining some exploration.
    \end{block}

    \begin{block}{Conclusion}
        Decaying epsilon strategies effectively reduce exploration over time, enabling the agent to focus on exploitation as it gains knowledge about the environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Bayesian Approaches}
    % Overview of Bayesian approaches in RL
    \begin{block}{Concept}
        Bayesian approaches utilize uncertainty in action value estimates through probability distributions instead of point estimates, facilitating informed exploration strategies.
    \end{block}

    \begin{block}{Implementation}
        \begin{itemize}
            \item Update beliefs about action values with new data (rewards).
            \item Select actions based on maximizing expected rewards from distributions.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example: Thompson Sampling}
        Each action \( a \) has a distribution representing its value. At each step, sample from each action's distribution and select the action with the highest sampled value.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Bayesian Approaches (Continued)}
    % Advantages of Bayesian approaches
    \begin{block}{Advantages}
        \begin{itemize}
            \item Naturally balances exploration and exploitation by selecting actions based on their probability of yielding the highest reward.
            \item Adapts to the information available, favoring less-known actions that might yield better rewards.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Integrating decaying epsilon strategies and Bayesian approaches allows RL agents to optimize their performance by effectively navigating the trade-off between exploring new possibilities and exploiting known successful actions.
    \end{block}
\end{frame}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}[fragile]
  \frametitle{Understanding Reward Structures in Reinforcement Learning (RL)}
  \begin{block}{Key Concept}
    In Reinforcement Learning, the reward structure plays a crucial role in steering agents' learning from environmental interactions. It significantly affects the exploration-exploitation trade-off.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{1. What is a Reward Structure?}
  \begin{itemize}
    \item \textbf{Definition}: A reward structure establishes the rules for how rewards are assigned to specific actions in a given state.
    \item \textbf{Types of Rewards}:
      \begin{itemize}
        \item \textbf{Immediate Rewards}: Received right after taking an action.
        \item \textbf{Delayed Rewards}: Received after a series of actions, complicating the learning process.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{2. Influence of Reward Structures on Exploration vs. Exploitation}
  \begin{itemize}
    \item \textbf{Exploration}: Taking uncertain actions for potentially higher long-term rewards.
    \item \textbf{Exploitation}: Choosing actions known to yield high rewards based on prior learning.
  \end{itemize}
  
  \begin{block}{Reward Structures Can:}
    \begin{itemize}
      \item \textbf{Promote Exploration}: Reward structures with high variability encourage uncertainty.
      \item \textbf{Encourage Exploitation}: Consistent rewards for specific actions reinforce them.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3. Examples of Reward Structures}
  \begin{itemize}
    \item \textbf{Binary Rewards}: Simple and can lead to fast learning but often bias towards exploitation.
    \item \textbf{Shaped Rewards}: Intermediate rewards facilitate exploration, e.g., small rewards guiding maze-solving tasks.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{4. Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Tuning Reward Functions}: Important for learning efficiency; poorly designed structures may yield suboptimal policies.
    \item \textbf{Trade-off Considerations}:
      \begin{itemize}
        \item Too much emphasis on exploration may waste resources.
        \item Too much emphasis on exploitation can miss better long-term opportunities.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{5. Final Thoughts}
  \begin{itemize}
    \item \textbf{Adaptive Reward Structures}: Dynamic structures help maintain agent engagement and beneficial exploration.
    \item \textbf{Conclusion}: Thoughtful design of reward structures is key to guiding decision-making and enhancing RL performance.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Code Snippet Example}
  \begin{lstlisting}[language=Python]
def calculate_reward(action, success):
    if success:
        return 10  # Reward for a successful action
    else:
        return -1  # Small penalty for failure
  \end{lstlisting}
  \begin{block}{Explanation}
    This function demonstrates a simple reward mechanism where successful actions yield strong rewards, while failures incur minor penalties, shaping the agent's strategy.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Next Slide: Impact on Learning and Performance}
  This will explore how the reward design in RL systems impacts the overall learning efficacy and performance outcomes for agents, connecting back to exploration and exploitation.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact on Learning and Performance}
    \begin{block}{Exploration vs. Exploitation in Reinforcement Learning (RL)}
    Agents in RL face the exploration-exploitation trade-off, a crucial factor influencing their learning efficacy and performance outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definitions of Exploration and Exploitation}
    \begin{itemize}
        \item \textbf{Exploration:}  
        The process of trying new actions to discover potential rewards and gather information about the environment.
        
        \item \textbf{Exploitation:}  
        Utilizing current knowledge to maximize rewards by selecting the best-known actions based on past experiences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Trade-Off}
    \begin{itemize}
        \item \textbf{High Exploration:}    
        Agents may fail to accumulate rewards by spending excessive time exploring.
        
        \item \textbf{High Exploitation:}
        Focusing solely on known high-reward actions risks getting stuck in local maxima and missing better strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact on Learning Efficacy}
    \begin{enumerate}
        \item \textbf{Learning Rate:}
        \begin{itemize}
            \item Excessive exploration leads to high variance and slow convergence.
            \item Inadequate exploration causes overfitting and can converge prematurely to suboptimal solutions.
        \end{itemize}
        
        \item \textbf{Sample Efficiency:} 
        Balanced exploration-exploitation improves sample efficiency, minimizing interactions needed for effective learning.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Outcomes}
    \begin{itemize}
        \item \textbf{Long-Term Rewards:} 
        Effective exploration helps agents find new actions that yield higher long-term rewards.
        
        \item \textbf{Robustness:} 
        A balanced approach fosters robust performance across diverse situations, enhancing environmental understanding.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Example: Multi-Armed Bandit Problem}
    \begin{block}{Scenario Description}
        An agent faces multiple slot machines, each with different payout rates:
    \end{block}
    \begin{itemize}
        \item \textbf{Exploratory Phase:} The agent tries different machines to gather payout information (exploration).
        
        \item \textbf{Exploitation Phase:} The agent selects the machine with historically the highest payout to maximize rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Balancing Exploration and Exploitation}
    \begin{block}{Common Strategy: $\epsilon$-greedy}
    \begin{equation}
    a_t = 
    \begin{cases}
    \text{random action}, & \text{with probability } \epsilon \\
    \text{argmax}_a Q(s_t, a), & \text{with probability } 1 - \epsilon
    \end{cases}
    \end{equation}
    \end{block}
    Where:
    \begin{itemize}
        \item $a_t$: action chosen at time $t$.
        \item $s_t$: current state.
        \item $Q(s_t, a)$: estimated expected reward of action $a$ in state $s_t$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding and applying the exploration-exploitation trade-off is essential for enhancing the learning efficacy and performance of RL agents. 
    The application of appropriate strategies enables informed decisions for better outcomes in dynamic environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Introduction}
    \begin{block}{Exploration and Exploitation}
        In reinforcement learning, **exploration** refers to trying new actions to discover their rewards, while **exploitation** involves using known actions that have yielded high rewards previously. 
    \end{block}
    \begin{block}{Importance}
        The balance between exploration and exploitation is crucial for optimizing agent performance and achieving superior outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - AlphaGo}
    \begin{block}{Case Study 1: Google DeepMind's AlphaGo}
        \begin{itemize}
            \item \textbf{Overview}: First program to defeat a professional human player in Go, utilizing a blend of strategies.
            \item \textbf{Implementation}:
                \begin{itemize}
                    \item \textbf{Exploration}: Employed Monte Carlo Tree Search to evaluate numerous potential moves and outcomes.
                    \item \textbf{Exploitation}: Used a database of historical games from expert players to apply proven strategies.
                \end{itemize}
            \item \textbf{Outcome}: AlphaGo not only won against top players but also introduced innovative strategies that transformed gameplay.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Recommendation Systems}
    \begin{block}{Case Study 2: E-commerce Recommendation Systems}
        \begin{itemize}
            \item \textbf{Overview}: Platforms like Amazon utilize algorithms to recommend products effectively.
            \item \textbf{Implementation}:
                \begin{itemize}
                    \item \textbf{Exploration}: Randomly display new products to test interest and collect user feedback.
                    \item \textbf{Exploitation}: Analyze past purchasing data to recommend successful products to similar users.
                \end{itemize}
            \item \textbf{Outcome}: Increased sales and improved customer satisfaction by introducing users to products they might not have found otherwise.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        1. Dynamic strategies lead to new insights while exploiting profitable actions. 
        2. Adaptive learning based on feedback is essential.
        3. This framework is applicable across various industries, enhancing technology and user experiences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Formula and Conclusion}
    \begin{block}{Optimal Strategy Formula}
        \begin{equation}
            \text{Optimal Strategy} = \alpha(\text{Exploration}) + (1 - \alpha)(\text{Exploitation})
        \end{equation}
        Where $\alpha$ is the exploration factor, adjusted based on agent confidence and knowledge.
    \end{block}
    
    \begin{block}{Conclusion}
        These case studies demonstrate that balancing exploration and exploitation is fundamental for success across various applications, showcasing their practical implications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Exploration vs. Exploitation in Reinforcement Learning}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Exploration and Exploitation}: 
                \begin{itemize}
                    \item \textbf{Exploration}: Trying new actions to discover their potential benefits.
                    \item \textbf{Exploitation}: Leveraging known actions that yield the highest rewards based on past experiences.
                \end{itemize}
            \item \textbf{The Exploration-Exploitation Trade-off}: 
                The fundamental dilemma in reinforcement learning where an agent must choose between exploring new strategies for better long-term rewards and exploiting strategies that currently provide the highest reward.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Balanced Management}
    \begin{block}{Why Balance is Crucial}
        \begin{itemize}
            \item Over-reliance on exploration may lead to suboptimal actions and wasted resources on unpromising strategies.
            \item Sole focus on exploitation can cause stagnation, where the agent fails to discover better long-term solutions.
        \end{itemize}
    \end{block}
    
    \begin{block}{Dynamic Contexts}
        In real-world applications, environments are not static. Balancing exploration and exploitation allows for adaptability as conditions change.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples Demonstrating the Trade-off}
    \begin{enumerate}
        \item \textbf{Example 1: Epsilon-Greedy Strategy}
            \begin{itemize}
                \item An agent explores with a small probability $\epsilon$ and exploits the best-known action otherwise.
                \item For instance, if $\epsilon = 0.1$, the agent explores 10\% of the time and exploits 90\% of the time.
            \end{itemize}

        \item \textbf{Example 2: Adaptive Learning Rates}
            \begin{itemize}
                \item Adjusting learning rates over time helps balance exploration and exploitation.
                \item Starting with a higher learning rate enables wider exploration, followed by a decrease to focus on consolidation as the model learns more about the environment.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions \& Discussion}
    % Open floor for questions and discussion on chapter themes and implications for RL applications.
    Open floor for questions and discussion on the chapter themes and their implications for Reinforcement Learning (RL) applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation Overview}
    % Overview of concepts
    \begin{itemize}
        \item \textbf{Exploration}: Trying new actions to gather information about the environment.
        \item \textbf{Exploitation}: Choosing actions to maximize immediate rewards based on existing knowledge.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Questions to Consider}
    % Key questions on exploration vs. exploitation
    \begin{enumerate}
        \item \textbf{When should an agent explore?}
        \begin{itemize}
            \item Crucial at the beginning with limited information, shifting toward exploitation as knowledge increases.
        \end{itemize}
        
        \item \textbf{How can we balance exploration and exploitation?}
        \begin{itemize}
            \item Techniques like the $\epsilon$-greedy strategy maintain balance, exploring with probability $\epsilon$ and exploiting with $(1 - \epsilon)$.
        \end{itemize}
        
        \item \textbf{What are the implications for different RL algorithms?}
        \begin{itemize}
            \item Algorithms like Q-learning and SARSA manage exploration/exploitation differently, impacting algorithm selection.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    % Robot exploring a maze
    Consider a robot exploring a maze:
    \begin{itemize}
        \item If it only exploits known pathways, it may miss new routes or shortcuts.
        \item If it only explores, it may not find the most efficient way to the exit.
        \item A balanced strategy allows exploration of new paths while utilizing known shortcuts.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Prompts}
    % Discussion cultivates deeper understanding
    Share your experiences regarding the exploration-exploitation trade-off:
    \begin{itemize}
        \item Discuss examples from projects where you considered this trade-off.
        \item What strategies have you found effective in balancing exploration and exploitation?
        \item How does this trade-off apply to your field or interests?
    \end{itemize}
\end{frame}


\end{document}