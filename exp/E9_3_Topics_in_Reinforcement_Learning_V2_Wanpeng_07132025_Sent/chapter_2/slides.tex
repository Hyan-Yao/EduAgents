\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Overview}

    A \textbf{Markov Decision Process (MDP)} is a mathematical framework used for modeling decision-making in scenarios where outcomes are partly random and partly under the control of a decision-maker. 
    MDPs are widely utilized in fields such as:
    
    \begin{itemize}
        \item Reinforcement Learning
        \item Robotics
        \item Operations Research
    \end{itemize}

    They provide a structured manner to represent and solve complex problems involving sequential decisions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics of MDPs}

    \begin{enumerate}
        \item \textbf{States (S)}: Finite set of all possible configurations of the environment.
        \item \textbf{Actions (A)}: Set of actions available to the decision-maker.
        \item \textbf{Transition Model (P)}: Probability of moving from one state to another based on an action: 
            \begin{equation}
                P(s' | s, a)
            \end{equation}
        \item \textbf{Rewards (R)}: Scalar feedback received after taking an action in a given state:
            \begin{equation}
                R(s, a, s')
            \end{equation}
        \item \textbf{Policy ($\pi$)}: Strategy that defines the action taken in each state.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use MDPs?}

    \begin{itemize}
        \item \textbf{Structured Decision-Making}: Clear way to make decisions in complex environments.
        \item \textbf{Optimal Solutions}: Formulate and solve to find the optimal policy maximizing expected cumulative rewards.
        \item \textbf{Foundation of Reinforcement Learning}: Serve as the backbone for many reinforcement learning algorithms.
    \end{itemize}

    \textbf{Example: Robot Navigation:}
    \begin{itemize}
        \item \textbf{States (S)}: Positions on a grid (e.g., cells in a 5x5 room).
        \item \textbf{Actions (A)}: Move Up, Down, Left, Right.
        \item \textbf{Transition Model (P)}: Moving right from (2,3) may lead to (2,4) with a probability of 0.8.
        \item \textbf{Rewards (R)}: Reaching (4,4) may yield +10; hitting a wall gives -1.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Markov Decision Processes (MDPs)}
    \begin{itemize}
        \item Overview of key components:
        \begin{itemize}
            \item States (S)
            \item Actions (A)
            \item Rewards (R)
            \item Transition Function (T)
            \item Policy (\(\pi\))
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. States (S)}
    \begin{block}{Definition}
        A state represents a specific situation in which an agent can find itself within the environment.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} In a chess game, each arrangement of pieces on the board is a unique state.
        \item \textbf{Key Point:} The collection of all possible states forms the state space (S), which is crucial for decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Actions (A)}
    \begin{block}{Definition}
        Actions are the choices available to an agent at a given state.
    \end{block}
    \begin{itemize}
        \item The set of all actions is called the action space (A).
        \item \textbf{Example:} In chess, possible actions include moving a pawn or castling.
        \item \textbf{Key Point:} The agent selects actions that influence its transition to subsequent states.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Rewards (R)}
    \begin{block}{Definition}
        A reward is a numerical value received for taking an action in a state.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} Winning a piece yields +10 points, while losing one incurs -10 points.
        \item \textbf{Formula:} The reward function:
        \begin{equation}
            R(s, a, s')
        \end{equation}
        where \(s\) is the current state, \(a\) is the action taken, and \(s'\) is the next state.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Transition Function (T)}
    \begin{block}{Definition}
        Determines the probability of moving from one state to another given an action.
    \end{block}
    \begin{itemize}
        \item Represented as:
        \begin{equation}
            T(s, a, s') = P(s' | s, a)
        \end{equation}
        \item \textbf{Example:} Rolling a 3 in a dice game transitions the player from state 1 to state 4 with certainty.
        \item \textbf{Key Point:} This captures the uncertainty in the environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Policy (\(\pi\))}
    \begin{block}{Definition}
        A policy defines the agent's behavior at any state, specifying the action to be taken.
    \end{block}
    \begin{itemize}
        \item Can be deterministic (one action per state) or stochastic (probabilities for each action).
        \item \textbf{Example:} A navigation policy might direct the agent to move "right" if in state A and "left" if in state B.
        \item \textbf{Key Point:} The optimal policy maximizes expected rewards over time, guiding the agent's actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Components}
    \begin{itemize}
        \item \textbf{States (S)}: Situations in the environment.
        \item \textbf{Actions (A)}: Choices available to the agent.
        \item \textbf{Rewards (R)}: Feedback received from actions.
        \item \textbf{Transitions (T)}: Probabilities governing state changes.
        \item \textbf{Policies (\(\pi\))}: Strategies for action selection.
    \end{itemize}
    \vspace{10pt}
    \begin{block}{Conclusion}
        Integrating these components models decision-making in various environments using MDPs, foundational for reinforcement learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States and Actions - Overview}
    \begin{block}{Definition}
        States and actions form the foundation of decision-making in Markov Decision Processes (MDPs).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States in MDPs}
    \begin{itemize}
        \item \textbf{Definition:} A state represents a specific configuration of an environment.
        \item \textbf{Key Characteristics:}
            \begin{itemize}
                \item Discrete or Continuous
                \item Memoryless due to the Markov property
            \end{itemize}
        \item \textbf{Example:} In a grid world, states can be represented as coordinates (x, y).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Actions in MDPs}
    \begin{itemize}
        \item \textbf{Definition:} An action is a decision made by an agent to transition between states.
        \item \textbf{Key Characteristics:}
            \begin{itemize}
                \item Action Space: Finite or Infinite
                \item Deterministic or Stochastic outcomes
            \end{itemize}
        \item \textbf{Example:} In a grid world, moving up can result in moving from (2, 3) to (2, 4) or (2, 2).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Role of States and Actions in Decision-Making}
    \begin{itemize}
        \item States and actions are crucial for decision-making processes in MDPs:
        \begin{itemize}
            \item Form the core of decision-making.
            \item Help in policy creation, defining actions based on current states.
            \item Linked to transition probabilities dictating state changes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{enumerate}
        \item \textbf{States:}
            \begin{itemize}
                \item Define the current environment scenario.
            \end{itemize}
        \item \textbf{Actions:}
            \begin{itemize}
                \item Determine transitions between states.
            \end{itemize}
        \item \textbf{Importance in MDPs:}
            \begin{itemize}
                \item Integral for decision-making strategies.
                \item Allow agents to maximize long-term goals.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Diagrams}
    \begin{itemize}
        \item \textbf{Transition Probability:} 
        \begin{equation}
            P(s'|s,a)
        \end{equation}
        \item \textbf{Policy:} 
        \begin{equation}
            \pi(a|s)
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Overview}
    \begin{block}{Concept Overview}
        In a Markov Decision Process (MDP), rewards provide feedback to the agent based on its actions within an environment. 
    \end{block}
    \begin{itemize}
        \item Rewards are numerical values indicating immediate benefits from states and actions.
        \item They guide agent behavior and influence decision-making and learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards - Definition and Importance}
    \begin{enumerate}
        \item \textbf{Definition:} A reward is a scalar value received after executing an action in a state, denoted as \( R(s, a) \).
        
        \item \textbf{Importance:}
        \begin{itemize}
            \item Rewards help evaluate action effectiveness.
            \item They shape exploration and exploitation strategies.
            \item Optimizing reward accumulation is the primary objective in MDPs.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Rewards in Learning}
    \begin{enumerate}
        \item \textbf{Behavioral Guidance:} Immediate rewards inform the agent on beneficial vs. detrimental actions.
        \item \textbf{Long-term Strategy:} Agents maximize cumulative reward over time, considering future rewards.
        \item \textbf{Learning from Rewards:} Update action value estimations through interactions.
        \begin{equation}
            R_{\text{total}}(s) = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)
        \end{equation}
        \begin{itemize}
            \item where \( \gamma \) is the discount factor (0 ≤ \( \gamma \) < 1).
        \end{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Rewards define success.
            \item Types of rewards: Positive and Negative (Penalties).
            \item Reward shaping can enhance learning outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transitions}
    \begin{block}{Understanding Transition Probabilities}
        Transition probabilities are fundamental in Markov Decision Processes (MDPs) as they describe how an agent moves between states based on actions taken.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Concept of Transition Probabilities}
    \begin{itemize}
        \item \textbf{Definition}: The transition probability \( P(s' | s, a) \) indicates the likelihood of ending up in state \( s' \) after taking action \( a \) in state \( s \).
        \item \textbf{Markov Property}: Future states depend solely on the current state and action, not on previous events.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Dynamics}
    \begin{itemize}
        \item The dynamics illustrate how actions affect state transitions. 
        \item \textbf{Example}: In a grid world with states \( S = \{s_1, s_2, s_3\} \):
        \begin{itemize}
            \item If the agent is in state \( s_1 \) and takes action "move right":
            \begin{itemize}
                \item 70\% chance to move to \( s_2 \)
                \item 10\% chance to move to \( s_3 \)
                \item 20\% chance to stay in \( s_1 \)
            \end{itemize}
        \end{itemize}
        \begin{equation}
            P(s_2 | s_1, \text{move right}) = 0.7, \quad P(s_3 | s_1, \text{move right}) = 0.1, \quad P(s_1 | s_1, \text{move right}) = 0.2
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Transition Probabilities}
    \begin{itemize}
        \item \textbf{Decision Making}: They help determine the consequences of actions and influence decision-making processes.
        \item \textbf{Expected Outcomes}: Aid agents in calculating expected rewards, crucial for developing optimal policies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Transition probabilities capture the stochastic nature of moving between states.
        \item All possible state transitions should be represented in a transition model.
        \item Agents can adapt their strategies by updating transition probabilities based on experiences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Transition Probability Matrix}
    \begin{block}{Transition Probability Matrix}
        For an MDP with three states and two actions, the transition probabilities can be organized as:
        \begin{equation}
            P = \begin{bmatrix}
            \text{Action 1} & \text{Action 2} \\
            P(s' | s_1, \text{A1}) & P(s' | s_1, \text{A2}) \\
            P(s' | s_2, \text{A1}) & P(s' | s_2, \text{A2}) \\
            P(s' | s_3, \text{A1}) & P(s' | s_3, \text{A2}) 
            \end{bmatrix}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding transitions in MDPs is crucial for developing intelligent agents that make effective decisions under uncertainty. By mastering transition probabilities, students can gain insight into decision-making and policy optimization in reinforcement learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies - Overview}
    \begin{block}{Definition of Policies}
        \begin{itemize}
            \item \textbf{Policy} ($\pi$): A strategy mapping the state space $S$ to the action space $A$.
            \item Formally, $\pi: S \rightarrow A$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies - Role and Types}
    \begin{block}{Role of Policies}
        \begin{itemize}
            \item Determining agent actions based on the current state.
            \item Instructing the agent on which actions to take.
        \end{itemize}
    \end{block}

    \begin{block}{Types of Policies}
        \begin{itemize}
            \item \textbf{Deterministic}: 
            \begin{itemize}
                \item A specific action for each state. 
                \item Example: $\pi(s) = a$.
            \end{itemize}
            \item \textbf{Stochastic}:
            \begin{itemize}
                \item Actions chosen based on probabilities.
                \item Example: $\pi(a|s)$ indicates probability distribution over actions.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Policies in MDPs}
    \begin{block}{Key Aspects of Policies}
        \begin{itemize}
            \item Fundamental role in decision-making in Markov Decision Processes (MDPs).
            \item Direct influence on expected outcomes and rewards.
            \item Evaluating policies based on expected cumulative rewards.
            \item The goal is often to find the optimal policy $\pi^*$.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        \begin{itemize}
            \item Policies bridge the agent's observed state and potential actions.
            \item Effective decision-making in uncertain environments relies on policies.
            \item Utilized in various applications like robotics, finance, and game playing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Policies}
    \begin{block}{Example in a Grid Environment}
        \begin{itemize}
            \item \textbf{Deterministic Policy}: 
            \begin{itemize}
                \item $\pi((2, 3)) = \text{left}$ leads to (2, 2).
            \end{itemize}
            \item \textbf{Stochastic Policy}: 
            \begin{itemize}
                \item 50\% chance to move left to (2, 2), 30\% chance to move right to (2, 4).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Property - Definition}
    \begin{block}{Definition}
        The \textbf{Markov Property} asserts that the future state of a stochastic process depends only on the current state, not on the sequence of events (states) that preceded it. Mathematically, this is expressed as:
    \end{block}
    \begin{equation}
        P(S_{t+1} | S_t, S_{t-1}, \ldots, S_0) = P(S_{t+1} | S_t)
    \end{equation}
    Where:
    \begin{itemize}
        \item \( S_t \) is the current state at time \( t \),
        \item \( S_{t+1} \) is the next state.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Property - Significance in MDPs}
    \begin{block}{Importance}
        The Markov Property plays a critical role in simplifying decision-making in Markov Decision Processes (MDPs) through the following ways:
    \end{block}
    \begin{itemize}
        \item \textbf{Simplicity in Modeling}: Reduces complexity by allowing agents to consider only the present state while making decisions.
        \item \textbf{Memoryless Property}: Agents do not need to track the entire history of states, leading to efficient calculations.
        \item \textbf{Future Independence}: Current state provides all information needed to predict future states, easing policy evaluations and updates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Property - Example}
    Consider a weather forecasting system where the state represents weather conditions:
    \begin{itemize}
        \item \textbf{States}: \{Sunny, Rainy, Cloudy\}
    \end{itemize}
    Suppose today is cloudy:
    \begin{itemize}
        \item Transition Probabilities:
        \begin{equation}
            P(S_{t+1} = \text{Sunny} | S_t = \text{Cloudy}) = 0.4
        \end{equation}
        \begin{equation}
            P(S_{t+1} = \text{Rainy} | S_t = \text{Cloudy}) = 0.3
        \end{equation}
        \begin{equation}
            P(S_{t+1} = \text{Cloudy} | S_t = \text{Cloudy}) = 0.3
        \end{equation}
    \end{itemize}
    This illustrates how tomorrow's weather (future state) is determined solely by today's weather (current state).
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Introduction}
    \begin{block}{Understanding Value Functions}
        In the context of Markov Decision Processes (MDPs), value functions are important tools used to evaluate how desirable different states are when following a specific policy. A value function estimates the expected return from each state, guiding decision-making for agents.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Key Concepts}
    \begin{enumerate}
        \item \textbf{Expected Return}:
        \begin{itemize}
            \item From state \(s\), under policy \(\pi\), the expected return is denoted as \(V^\pi(s)\).
            \item It is the total reward an agent expects to accumulate starting from state \(s\) and following policy \(\pi\):
            \[
            V^\pi(s) = \mathbb{E}_{\pi}\left[ G_t \mid S_t = s \right]
            \]
            where 
            \[
            G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
            \]
            and \(\gamma\) (0 ≤ \(\gamma\) < 1) is the discount factor.
        \end{itemize}
        
        \item \textbf{Policy}:
        \begin{itemize}
            \item A policy \(\pi\) is a mapping from states to actions, establishing the strategy the agent uses for decision-making.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Example}
    \begin{block}{Example: Simple Grid World}
        Consider a grid world where an agent can move in four directions and receives rewards based on its position:
        \begin{itemize}
            \item +10 reward for reaching a goal state
            \item -1 for each step taken
        \end{itemize}
        
        \textbf{Calculating Value Function}:
        
        \begin{itemize}
            \item If the agent starts in state \(s_1\) and follows policy \(\pi\):
            \begin{itemize}
                \item 30\% chance to move toward the goal
                \item 40\% chance to bump into a wall (returns to \(s_1\))
                \item 30\% chance to take a detour
            \end{itemize}
            \item The value function for \(s_1\) is:
            \[
            V^\pi(s_1) = 0.3 \times \left( 10 + \gamma V^\pi(s_{\text{goal}}) \right) + 0.4 \times \left( -1 + \gamma V^\pi(s_1) \right) + 0.3 \times \left( -1 + \gamma V^\pi(s_{\text{detour}}) \right)
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Summary}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Value functions are crucial for evaluating states in reinforcement learning.
            \item They quantify potential long-term benefits of being in specific states.
            \item Proper determination enables better decision-making in uncertain environments.
            \item Future rewards are discounted, reflecting their lesser importance compared to immediate rewards.
        \end{itemize}
    \end{block}

    \begin{block}{Final Thoughts}
        Understanding value functions is foundational in reinforcement learning and MDPs. They help agents develop strategies that maximize future rewards, leading to more effective decision-making models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Overview}
    \begin{itemize}
        \item The Bellman equations are foundational in Markov Decision Processes (MDPs) and reinforcement learning.
        \item They represent recursive relationships among the values of states or state-action pairs.
        \item Allow computation of the value function, which estimates the value of being in a given state.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions Recap}
    \begin{block}{Value Function \( V(s) \)}
        Represents the expected return (cumulative future reward) starting from state \( s \) under a policy \( \pi \):
        \[
            V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
        \]
        where \( G_t \) is the return from time \( t \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equation for State Value Function}
    The Bellman equation for the state value function under policy \( \pi \):
    \[
        V^\pi(s) = \sum_{a \in A} \pi(a | s) \sum_{s'} P(s' | s, a) \left( R(s, a, s') + \gamma V^\pi(s') \right)
    \]

    \textbf{Where:}
    \begin{itemize}
        \item \( A \): Set of all possible actions.
        \item \( \pi(a | s) \): Probability of taking action \( a \) in state \( s \).
        \item \( P(s' | s, a) \): Transition probability to state \( s' \).
        \item \( R(s, a, s') \): Expected immediate reward after the transition.
        \item \( \gamma \): Discount factor (0 ≤ \( \gamma \) < 1).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equation for Q-Value Function}
    The Q-value function represents the expected return for taking action \( a \) in state \( s \):
    \[
        Q^\pi(s, a) = \sum_{s'} P(s' | s, a) \left( R(s, a, s') + \gamma V^\pi(s') \right)
    \]

    \textbf{Key Understanding:}
    \begin{itemize}
        \item The Q-value indicates how good a specific action is in state \( s \) while following policy \( \pi \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Bellman Equations in Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Value Iteration:} Essential for updating value functions iteratively using the Bellman equations.
        \item \textbf{Policy Improvement:} Basis for evaluating and refining policies by leveraging estimated values of states or actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaway}
    The Bellman equations provide the mathematical framework for reinforcement learning, making it integral to understand how agents learn in uncertain environments.

    \begin{block}{Key Takeaway}
        The Bellman equations encapsulate the core principles of dynamic programming and reinforcement learning, enabling optimal policy derivation via recursive relationships in value estimation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimal Policies - Overview}
    \begin{block}{Definition}
        An **optimal policy** is a strategy defining the best action in each state to maximize the expected cumulative reward over time.
    \end{block}
    \begin{itemize}
        \item Results in the highest possible return from any initial state.
        \item Key concept in Markov Decision Processes (MDPs).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions and Optimal Policies}
    Value functions help derive optimal policies by estimating the quality of each state.
    
    \begin{itemize}
        \item **State Value Function (V)**: 
        \begin{equation}
            V^*(s) = \max_\pi \mathbb{E}_\pi \left[ R_t \,|\, S_t = s \right]
        \end{equation}
        
        \item **Action Value Function (Q)**: 
        \begin{equation}
            Q^*(s, a) = \mathbb{E} \left[ R_t \,|\, S_t = s, A_t = a \right]
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations and Optimal Policies}
    The connection between value functions and optimal policies is captured by the **Bellman Equations**:
    
    \begin{block}{Bellman Optimality Equation for State Values}
        \begin{equation}
            V^*(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right)
        \end{equation}
    \end{block}

    \begin{block}{Deriving the Optimal Policy}
        The optimal policy \( \pi^* \) is derived as:
        \begin{equation}
            \pi^*(s) = \arg\max_a Q^*(s, a)
        \end{equation}
        \begin{equation}
            Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s,a) V^*(s')
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Simple Grid World}
    Consider a simple grid world:
    \begin{itemize}
        \item States represent positions.
        \item Actions are movements (up, down, left, right).
        \item Rewards defined as:
        \begin{itemize}
            \item Falling off a cliff: -100 points.
            \item Reaching a goal state: +100 points.
        \end{itemize}
    \end{itemize}

    To find the optimal policy:
    \begin{enumerate}
        \item Initialize all values to zero or arbitrary values.
        \item Compute \( V^*(s) \) iteratively using the Bellman update until convergence.
        \item Derive \( \pi^*(s) \) based on \( Q^*(s, a) \).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item **Optimal Policy** maximizes expected cumulative rewards.
        \item Value functions are essential for finding optimal policies.
        \item The relationship between state and action value functions is critical in MDP analysis.
        \item Bellman equations systematically calculate value functions and derive optimal strategies.
    \end{itemize}

    \begin{block}{Conclusion}
        Understanding and applying value functions along with Bellman equations allows for effective derivation of optimal policies in decision-making processes modeled as MDPs. This foundation is crucial for learning algorithms like Value Iteration and Policy Iteration.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithms for MDPs - Overview}
    \begin{itemize}
        \item MDPs model decision-making with random and controllable outcomes.
        \item Key algorithms to solve MDPs:
        \begin{itemize}
            \item Value Iteration
            \item Policy Iteration
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithms for MDPs - Value Iteration}
    \begin{block}{Value Iteration}
        \begin{enumerate}
            \item \textbf{Initialization:} Start with an arbitrary value function \( V_0(s) \) for all states \( s \).
            \item \textbf{Value Update:}
            \begin{equation}
            V_{k+1}(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a)V_k(s') \right)
            \end{equation}
            \begin{itemize}
                \item \( R(s, a) \): immediate reward for action \( a \) in state \( s \)
                \item \( \gamma \): discount factor (0 ≤ \( \gamma \) < 1)
                \item \( P(s'|s, a) \): state transition probability
            \end{itemize}
            \item \textbf{Convergence Check:} Repeat until values converge.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithms for MDPs - Policy Iteration}
    \begin{block}{Policy Iteration}
        \begin{enumerate}
            \item \textbf{Policy Initialization:} Start with an arbitrary policy \( \pi_0 \).
            \item \textbf{Policy Evaluation:}
            \begin{equation}
            V^{\pi}(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V^{\pi}(s')
            \end{equation}
            \item \textbf{Policy Improvement:}
            \begin{equation}
            \pi_{k+1}(s) = \arg\max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a)V_k(s') \right)
            \end{equation}
            \item \textbf{Convergence Check:} Repeat until the policy no longer changes.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of MDPs - Overview}
    Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making situations where outcomes are partly random and partly under the control of a decision-maker. They are particularly useful in scenarios where future states depend only on the current state and the action taken.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Fields Utilizing MDPs}
    \begin{itemize}
        \item \textbf{Robotics}
        \begin{itemize}
            \item Path Planning: Enables robots to navigate complex environments efficiently.
            \item Autonomous Navigation: Facilitates decision-making in self-driving cars.
        \end{itemize}
        
        \item \textbf{Finance}
        \begin{itemize}
            \item Portfolio Management: Aids in investment decisions for maximizing returns with minimal risk.
            \item Insurance Pricing: Helps set premiums based on potential future claims.
        \end{itemize}
        
        \item \textbf{Healthcare}
        \begin{itemize}
            \item Clinical Decision Support: Assists in medical diagnosis and treatment planning.
            \item Treatment Optimization: Determines optimal medication regimens for chronic diseases.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of MDP Implementation}
    \begin{itemize}
        \item \textbf{Robotics:} A robot's states represent various locations in a room, and actions correspond to movement directions, using a policy to choose the best action.
        
        \item \textbf{Finance:} An investment scenario involves states representing market conditions (bullish, bearish, stable) and actions as 'invest', 'sell', or 'hold', maximizing expected profits over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Decision-making Under Uncertainty:} MDPs offer structured decision-making in uncertain environments.
        \item \textbf{Dynamic Programming:} Solutions often involve techniques like Value Iteration and Policy Iteration.
        \item \textbf{Wide Applicability:} MDPs are significant in solving real-world problems across various fields.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Markov Decision Processes are powerful tools that help model and solve decision-making problems across diverse domains. Understanding how to formulate problems as MDPs and devise optimal policies is crucial for success in fields such as robotics, finance, and healthcare.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - Introduction}
    \begin{block}{Introduction}
        Markov Decision Processes (MDPs) are effective modeling tools for decision-making under uncertainty. However, several challenges complicate their practical application, notably:
    \end{block}
    \begin{itemize}
        \item Curse of Dimensionality
        \item Scalability Issues
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - Curse of Dimensionality}
    \begin{block}{Curse of Dimensionality}
        The curse of dimensionality refers to the exponential growth of the state space as the number of features increases. Each unique state requires a separate value function, making computation increasingly challenging.
    \end{block}
    
    \begin{example}
        Consider a robot in a 5x5 grid having 25 states. In 3D (adding height), there are 125 states. Adding velocity and angle increases states exponentially. 
    \end{example}

    \begin{block}{Key Point}
        \begin{itemize}
            \item Exponential state growth demands substantial computational resources, complicating optimal policy discovery.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - Scalability Issues}
    \begin{block}{Scalability Issues}
        Scalability denotes an algorithm's ability to manage increasing complexity and data. In MDPs, as state and action spaces expand, computation time for optimal policy can become infeasible.
    \end{block}
    
    \begin{example}
        In financial trading, MDPs can encompass thousands of actions (e.g., buy, sell, hold). The growth of assets and time periods can lead to an overwhelming state-action space.
    \end{example}

    \begin{block}{Key Point}
        \begin{itemize}
            \item More efficient algorithms, like reinforcement learning methods or function approximation, are necessary for managing larger MDPs.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Takeaways}
    \begin{itemize}
        \item \textbf{Understanding MDPs}: 
        \begin{itemize}
            \item MDPs provide a framework for modeling decision-making in uncertain conditions.
            \item Defined by states $S$, actions $A$, transition probabilities $P$, rewards $R$, and discount factor $\gamma$.
        \end{itemize}
        
        \item \textbf{Applications}:
        \begin{itemize}
            \item MDPs are utilized in robotics, operations research, economics, and artificial intelligence for optimal decision-making.
        \end{itemize}
        
        \item \textbf{Challenges}:
        \begin{itemize}
            \item \textbf{Curse of Dimensionality}: Exponential growth in computational needs with state/action increases.
            \item \textbf{Scalability}: Solutions for simpler MDPs may not extend to complex scenarios.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Future Directions}
    \begin{itemize}
        \item \textbf{Approximate Dynamic Programming (ADP)}:
        \begin{itemize}
            \item Techniques for approximating value functions in large MDPs for computational efficiency.
        \end{itemize}

        \item \textbf{Hierarchical Reinforcement Learning}:
        \begin{itemize}
            \item Decomposing tasks into simpler sub-tasks to tackle scalability.
            \item Example: High-level commands (like "explore") can break down into low-level actions (like "turn left").
        \end{itemize}

        \item \textbf{Integration with Deep Learning}:
        \begin{itemize}
            \item Enhancing real-time decision-making and learning efficiency.
            \item Example: AlphaGo's use of deep reinforcement learning highlights this merger.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Concluding Remarks}
    \begin{itemize}
        \item Understanding MDPs enables development of scalable solutions in real-world applications.
        \item Future research should focus on:
        \begin{itemize}
            \item Combining computational methods with human feedback.
            \item Exploring model-free learning and interactive MDPs for adaptability.
        \end{itemize}
        \item Advances in these areas are anticipated to improve decision-making processes across various domains.
    \end{itemize}
\end{frame}


\end{document}