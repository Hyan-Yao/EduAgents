\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Deep Reinforcement Learning]{Week 5: Deep Reinforcement Learning}
\author[Your Name]{Your Name}
\institute[Your Institute]{
  Your Institute\\
  \vspace{0.3cm}
  Email: youremail@institute.edu\\
  Website: www.institute.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Deep Reinforcement Learning (DRL)?}
    \begin{itemize}
        \item DRL combines **Reinforcement Learning (RL)** and **Deep Learning**:
            \begin{itemize}
                \item \textbf{Reinforcement Learning}: Agent learns to make decisions to maximize cumulative rewards over time.
                \item \textbf{Deep Learning}: Utilizes deep neural networks to extract patterns from data.
            \end{itemize}
        \item DRL leverages deep learning to manage high-dimensional state spaces, suitable for complex environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of DRL in AI Applications}
    \begin{enumerate}
        \item \textbf{Real-World Applications}:
            \begin{itemize}
                \item **Gaming**: Human-level performance in games like Go (AlphaGo).
                \item **Robotics**: Robots learn tasks through trial and error (manipulation, navigation).
                \item **Autonomous Vehicles**: Learn real-time navigation and obstacle avoidance.
            \end{itemize}
        \item \textbf{Generalization}: Adapt learned strategies to new states (e.g., smart assistants adapting to user preferences).
        \item \textbf{Complex Problem Solving}: Handles large action spaces effectively (e.g., algorithmic trading, public safety).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of DRL}
    \begin{itemize}
        \item \textbf{Agent}: The decision maker (robot, software).
        \item \textbf{Environment}: The system interacting with the agent, providing observations and rewards.
        \item \textbf{State}: Current situation representation derived from the environment.
        \item \textbf{Action}: Choices made by the agent that affect the state.
        \item \textbf{Reward}: Feedback signal indicating gains or losses from actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of DRL Workflow}
    \begin{enumerate}
        \item \textbf{Initialization}: Starts in an initial state.
        \item \textbf{Action Selection}: Selects an action based on its policy.
        \item \textbf{State Transition}: Action leads to a new state.
        \item \textbf{Reward Reception}: Receives reward based on the action taken.
        \item \textbf{Learning}: Updates knowledge based on the reward and state transition.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item DRL integrates deep learning with reinforcement learning principles.
            \item Its importance is highlighted through various practical applications.
            \item Key components include agents, environments, states, actions, and rewards.
        \end{itemize}
    \end{block}

    \textbf{Optional Code Snippet:}
    \begin{lstlisting}[language=Python]
while not converged:
    state = environment.reset()
    done = False
    while not done:
        action = agent.select_action(state)
        next_state, reward, done = environment.step(action)
        agent.learn(state, action, reward, next_state)
        state = next_state
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 1}
    \frametitle{Week 5: Deep Reinforcement Learning}
    
    \begin{block}{Foundational Understanding}
        \begin{itemize}
            \item Overview of Deep Reinforcement Learning (DRL)
            \item Key Terms:
                \begin{itemize}
                    \item \textbf{Agent:} The learner or decision-maker.
                    \item \textbf{Environment:} The setting in which the agent operates.
                    \item \textbf{State (s):} Current situation of the agent.
                    \item \textbf{Action (a):} Choice made by the agent affecting the state.
                    \item \textbf{Reward (r):} Feedback from the environment.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 2}
    \frametitle{Week 5: Deep Reinforcement Learning}
    
    \begin{block}{Algorithm Implementation}
        \begin{itemize}
            \item Familiarity with Common DRL Algorithms:
                \begin{itemize}
                    \item \textbf{Deep Q-Networks (DQN):} 
                        \begin{itemize}
                            \item Uses a neural network to approximate Q-values.
                        \end{itemize}
                    \item \textbf{Policy Gradients:} 
                        \begin{itemize}
                            \item Optimizes the policy directly.
                        \end{itemize}
                    \item \textbf{Actor-Critic Methods:} 
                        \begin{itemize}
                            \item Combines value-based and policy-based approaches using separate networks.
                        \end{itemize}
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Part 3}
    \frametitle{Week 5: Deep Reinforcement Learning}
    
    \begin{block}{Problem Solving and Ethical Considerations}
        \begin{itemize}
            \item Real-World Applications:
                \begin{itemize}
                    \item Game Playing (e.g., Chess, Go)
                    \item Robotics (e.g., learning to walk)
                    \item Finance (e.g., trading strategies)
                \end{itemize}
            \item Understanding Ethical Implications:
                \begin{itemize}
                    \item Bias in Algorithms
                    \item Accountability for Autonomous Agent Actions
                    \item Impact on Employment
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Conclusion}
    \frametitle{Week 5: Deep Reinforcement Learning}
    
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item DRL combines reinforcement and deep learning techniques.
            \item Familiarity with DRL algorithms is essential for practical implementation.
            \item DRL application across various domains opens innovative problem-solving avenues.
            \item Ethical considerations are crucial in the design and deployment of DRL systems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Reinforcement Learning?}
    \begin{block}{Definition}
        Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. The agent receives feedback in the form of rewards or penalties based on its actions, guiding it towards optimal behavior over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminology in Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Agent:} The learner or decision-maker (e.g., a robot, software).
        \item \textbf{Environment:} Everything the agent interacts with (e.g., a game, robot workspace).
        \item \textbf{State (s):} A representation of the current situation of the agent in the environment.
        \item \textbf{Action (a):} A decision made by the agent that influences the environment (e.g., moving left or right).
        \item \textbf{Reward (r):} Feedback received after taking an action; indicates success or failure (e.g., +1 for winning, -1 for losing).
        \item \textbf{Policy ($\pi$):} A strategy that the agent employs to decide which action to take in a given state.
        \item \textbf{Value Function (V):} A prediction of future rewards, helping the agent assess the desirability of states.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other Learning Paradigms}
    \begin{block}{How Reinforcement Learning Differs}
        \begin{itemize}
            \item \textbf{Supervised Learning:}
                \begin{itemize}
                    \item Uses labeled data with known outputs.
                    \item Goal: Learn a mapping from inputs to outputs (e.g., classifying images).
                    \item Example: Training a model to recognize cats in pictures based on provided labeled examples.
                \end{itemize}

            \item \textbf{Unsupervised Learning:}
                \begin{itemize}
                    \item Uses unlabeled data to uncover patterns.
                    \item Goal: Discover inherent patterns or groupings (e.g., clustering).
                    \item Example: Segmenting customers into groups based on behavior without predefined labels.
                \end{itemize}

            \item \textbf{Reinforcement Learning:}
                \begin{itemize}
                    \item Learns through trial and error with rewards/punishments.
                    \item Goal: Maximize cumulative reward over time rather than predicting a fixed output.
                    \item Example: A game-playing AI learns strategies by playing millions of games.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Networks in Reinforcement Learning - Introduction}
    \begin{block}{What is RL?}
        Reinforcement Learning (RL) involves teaching agents to make decisions by interacting with an environment.
    \end{block}
    \begin{block}{Challenge}
        When state and action spaces are high-dimensional (e.g., images, videos), traditional RL methods struggle.
    \end{block}
    \begin{block}{Solution}
        Neural Networks (NNs) serve as powerful function approximators to tackle the complexity of high-dimensional inputs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Networks in Reinforcement Learning - Key Concepts}
    \begin{enumerate}
        \item \textbf{Function Approximation:}
        \begin{itemize}
            \item NNs approximate value functions or policies, mapping states to expected future rewards.
        \end{itemize}
        
        \item \textbf{High-Dimensional Inputs:}
        \begin{itemize}
            \item Traditional RL uses simple feature representations for low-dimensional states. 
            \item NNs automatically extract features from complex data, enhancing perception.
        \end{itemize}
        
        \item \textbf{Representation Learning:}
        \begin{itemize}
            \item NNs learn meaningful representations by abstracting features at varying complexity levels.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Using CNNs in RL}
    \begin{itemize}
        \item \textbf{Convolutional Neural Networks (CNNs)} are commonly used when dealing with visual inputs, such as playing games.
        \item \textbf{Illustration:} In a game like Breakout, original pixel data is fed into a CNN:
        \begin{itemize}
            \item The CNN processes the image, identifying the ball and paddle.
            \item This allows the agent to predict the best action (e.g., move left or right).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation}
    \begin{itemize}
        \item The Q-value approximation using a neural network:
        \begin{equation}
            Q(s, a; \theta) \quad \text{(where } \theta \text{ are the parameters of the NN )}
        \end{equation}
        
        \item The loss function for training:
        \begin{equation}
            L(\theta) = \mathbb{E}\left[(r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))^2\right]
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Scalability:} NNs enable RL to scale to larger and more complex problems.
            \item \textbf{Generalization:} They improve an agent's ability to generalize from diverse experiences.
            \item \textbf{Flexibility:} NNs can be adapted for various RL tasks, from gaming to robotics.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Neural networks play a crucial role in enhancing the capabilities of RL models, particularly in environments where traditional methods falter. They allow for the design of intelligent agents that can learn and adapt effectively. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Overview}
    Deep Q-Networks (DQNs) are a groundbreaking advancement in 
    reinforcement learning (RL) that leverage deep learning to manage 
    high-dimensional input spaces. Based on the Q-learning algorithm, 
    DQNs use neural networks to approximate the Q-value function, 
    enabling agents to make decisions based on experience.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Key Concepts}
    \begin{enumerate}
        \item \textbf{Q-Learning Refresher:}
        \begin{block}{Q-value Definition}
            Represents the expected future rewards for selecting an action \( a \) in state \( s \):
            \begin{equation}
                Q(s, a) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R_t | s, a\right]
            \end{equation}
        \end{block}
        
        \item \textbf{Neural Network Integration:}
        \begin{itemize}
            \item DQNs generalize Q-values from states using neural networks, making it feasible to learn from raw sensory input (e.g., images).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Architecture}
    \begin{itemize}
        \item \textbf{Input Layer:} Takes state representations (e.g., video game frames).
        \item \textbf{Hidden Layers:} Comprises dense or convolutional layers to extract meaningful features.
        \item \textbf{Output Layer:} Represents Q-values for each possible action, aiding action selection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Training Dynamics}
    \begin{itemize}
        \item \textbf{Experience Replay:} Uses a replay buffer of past transitions (state, action, reward, next state) to stabilize learning.
        \item \textbf{Target Network:} Employs a separate target network to provide stable Q-value estimates during training.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Example in Action}
    Consider an agent trained to play a video game:
    \begin{itemize}
        \item The DQN processes frames (pixel data) and identifies obstacles & rewards.
        \item Agent's experiences modify Q-values, leading to better action choices over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Q-value Update}
    The Q-value update is defined as:
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[\text{target} - Q(s, a)\right]
    \end{equation}
    where the target is defined as:
    \begin{equation}
        \text{target} = R + \gamma \max_{a'} Q(s', a')
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Conclusion}
    Deep Q-Networks bridge reinforcement learning and deep learning, enabling agents to perform efficiently in complex environments.
    \begin{itemize}
        \item Understanding DQNs is crucial for advancements in robotics, gaming, and autonomous systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Deep Q-Networks (DQNs) - Overview}
    \begin{itemize}
        \item Deep Q-Networks (DQNs) integrate deep learning into traditional Q-learning.
        \item Training involves key components:
        \begin{itemize}
            \item Experience Replay
            \item Target Network
            \item Loss Function
        \end{itemize}
        \item Aim: To learn optimal actions in various states of a given environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Deep Q-Networks (DQNs) - Experience Replay}
    \begin{block}{Experience Replay}
        \begin{itemize}
            \item \textbf{Concept:} Stores past experiences in a replay buffer to sample randomly for training.
            \item \textbf{Reason:} Maximizes usage of past experiences and stabilizes learning.
            \item \textbf{Example:} Instead of using the last 32 experiences, sampling from a buffer of 1000 experiences allows for varied learning.
        \end{itemize}
    \end{block}
    
    \begin{equation}
        \text{Experience} = (s_t, a_t, r_t, s_{t+1})
    \end{equation}
    \begin{itemize}
        \item Where \( s_t \): state; \( a_t \): action; \( r_t \): reward; \( s_{t+1} \): new state.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Deep Q-Networks (DQNs) - Target Network and Loss Function}
    \begin{block}{Target Network}
        \begin{itemize}
            \item \textbf{Concept:} A separate network computes target Q-values; updated less frequently.
            \item \textbf{Reason:} Provides stability, reducing divergence risk.
            \item \textbf{Example:} Main network updates every iteration; target network updates every 200 iterations.
        \end{itemize}
        
        \begin{equation}
            y = r + \gamma \max_{a'} Q_{\text{target}}(s_{t+1}, a')
        \end{equation}
    \end{block}
    
    \begin{block}{Loss Function}
        \begin{itemize}
            \item \textbf{Concept:} Measures difference between predicted Q-value and target Q-value.
            \item \textbf{Formula:}
            \end{itemize}
            
            \begin{equation}
                L(\theta) = \frac{1}{N} \sum_{i=1}^{N} (y_i - Q(s_i, a_i; \theta))^2
            \end{equation}
            
            \begin{itemize}
                \item Components: \(y_i\) (target Q-value) and \(Q(s_i, a_i; \theta)\) (predicted Q-value).
                \item \textbf{Optimization:} Minimize loss using algorithms like SGD or Adam.
            \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradients - Introduction}
    \begin{block}{Introduction to Policy Gradient Methods}
        Policy gradient methods are algorithms in reinforcement learning (RL) that directly optimize the policy, denoted as $\pi(a|s)$, instead of estimating a value function. These methods excel in high-dimensional action spaces or stochastic policies.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Value-Based Approaches:} Such as Q-learning, estimate action values and derive policies indirectly. They may struggle in large or continuous action spaces and can lead to instability.
        
        \item \textbf{Policy-Based Approaches:} Directly parameterize the policy (e.g., using neural networks) and optimize it using gradient ascent, promoting stability in learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradients - Advantages}
    \begin{block}{Advantages of Policy Gradient Methods}
        \begin{enumerate}
            \item \textbf{Flexible Policy Representation:} Suitable for complex policies in discrete and continuous action spaces.
            \item \textbf{Stochastic Policies:} Facilitate exploration in environments with multiple optimal actions.
            \item \textbf{High-Dimensional Action Spaces:} Ideal for applications like robotics or games with unordered actions.
            \item \textbf{Direct Optimization:} Maximizes expected reward, aiding tasks where value function estimation is difficult.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient - Key Concepts and Example}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{The Policy:} The policy $\pi(a|s)$ defines the probability of taking action $a$ in state $s$, parameterized by a neural network with weights $\theta$.
            \item \textbf{Objective Function:} Maximize expected return:
            \begin{equation}
                J(\theta) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]
            \end{equation}
            
            \item \textbf{Gradient Update:} The update rule is:
            \begin{equation}
                \theta_{\text{new}} = \theta + \alpha \nabla J(\theta)
            \end{equation}
            where $\alpha$ is the learning rate.
        \end{itemize}
    \end{block}

    \begin{block}{Example: REINFORCE Algorithm}
        \begin{enumerate}
            \item Sample trajectories to collect states, actions, rewards.
            \item Compute returns to each action.
            \item Update the policy parameters towards higher expected reward.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Overview}
    \begin{block}{Actor-Critic Approach}
        The actor-critic method combines two key components of reinforcement learning:
        \begin{itemize}
            \item \textbf{Actor}: Selects actions based on the current policy.
            \item \textbf{Critic}: Evaluates actions through value functions.
        \end{itemize}
        This combination allows for a balance between exploration and exploitation, leading to enhanced learning efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Process}
    \begin{enumerate}
        \item \textbf{Initialization}: 
            Both the actor and the critic start with random parameters.
        \item \textbf{Interaction with Environment}:
            \begin{itemize}
                \item The actor takes an action based on its policy.
                \item The environment provides a reward and a new state.
            \end{itemize}
        \item \textbf{Criticism}:
            The critic assesses the action taken by using:
            \begin{equation}
                V(s) \approx E[R_t | s]
            \end{equation}
            and computes the advantage:
            \begin{equation}
                A(s, a) = Q(s, a) - V(s)
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Updates}
    \begin{enumerate}[resume]
        \item \textbf{Policy Update}: 
            The actor updates its parameters based on the feedback from the critic:
            \begin{equation}
                \theta \leftarrow \theta + \alpha \cdot A(s, a) \cdot \nabla \log \pi_\theta(a | s)
            \end{equation}
            where \( \alpha \) is the learning rate.
        \item \textbf{Value Function Update}:
            The critic updates its value function:
            \begin{equation}
                V(s) \leftarrow V(s) + \beta \cdot \delta
            \end{equation}
            with the temporal-difference error:
            \begin{equation}
                \delta = r + \gamma V(s') - V(s)
            \end{equation}
            where \( \beta \) is the learning rate, \( r \) is the received reward, and \( \gamma \) is the discount factor.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Key Advantages}
    \begin{itemize}
        \item \textbf{Combining Benefits}: Merges the strengths of policy gradients and value-based methods.
        \item \textbf{Stability and Convergence}: More stable convergence compared to pure methods.
        \item \textbf{Reduces Variance}: The critic minimizes the high variance of policy gradients.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Actor-Critic Application}
    Consider a game-playing AI:
    \begin{itemize}
        \item The \textbf{actor} generates moves based on the current strategy (e.g., chess).
        \item The \textbf{critic} evaluates the outcome of each move to refine the actor's strategy for future plays.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation - Understanding the Balance}
    \begin{block}{Key Concepts}
        In Reinforcement Learning (RL), agents face a trade-off between:
    \end{block}
    
    \begin{itemize}
        \item \textbf{Exploration}: Trying new actions to discover their effects, which may not yield immediate rewards but can lead to better long-term outcomes.
        \item \textbf{Exploitation}: Utilizing known knowledge to maximize immediate rewards by selecting the best-known actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation - Importance}
    Balancing exploration and exploitation is crucial for effective learning:
    
    \begin{itemize}
        \item Too much exploration can lead to:
            \begin{itemize}
                \item Wasted resources and time
                \item Missing optimal actions
            \end{itemize}
        \item Too much exploitation can lead to:
            \begin{itemize}
                \item Preventing discovery of better strategies
                \item Suboptimal performance over time
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Balance Exploration and Exploitation}
    \begin{enumerate}
        \item \textbf{Epsilon-Greedy Strategy}
            \begin{itemize}
                \item With probability \( \epsilon \), select a random action (exploration).
                \item With probability \( 1 - \epsilon \), select the best-known action (exploitation).
                \item \textbf{Example}: If \( \epsilon = 0.1\), there is a 10\% chance of exploring a new action.
            \end{itemize}
        
        \item \textbf{Softmax Action Selection}
            \begin{itemize}
                \item Actions are chosen probabilistically based on their estimated value:
                \begin{equation}
                    P(a) = \frac{e^{Q(a)/\tau}}{\sum_{b} e^{Q(b)/\tau}}
                \end{equation}
                where \( Q(a) \) is the expected reward for action \( a \) and \( \tau \) controls exploration.
            \end{itemize}
        
        \item \textbf{Upper Confidence Bound (UCB)}
            \begin{itemize}
                \item Actions are chosen based on potential high rewards, balancing exploitation and uncertainty:
                \begin{equation}
                    UCB(a) = \overline{X}_a + c \sqrt{\frac{\ln(n)}{n_a}}
                \end{equation}
            \end{itemize}
        
        \item \textbf{Intrinsic Motivation}
            \begin{itemize}
                \item Introduce a reward signal for exploration activities, encouraging the agent to explore based on novelty or surprise.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Challenges in Deep Reinforcement Learning}
    \begin{block}{Introduction}
        Deep Reinforcement Learning (DRL) combines reinforcement learning with deep learning techniques to tackle complex decision-making problems. Despite its progress, DRL faces considerable challenges.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Common Challenges in DRL}
    \begin{enumerate}
        \item \textbf{Stability:}
        \begin{itemize}
            \item Small changes can lead to significant fluctuations in performance.
            \item \textit{Example:} In sparse reward environments, policy updates may lead to catastrophic forgetting.
        \end{itemize}

        \item \textbf{Convergence:}
        \begin{itemize}
            \item Non-stationary environments make it difficult to reach stable policies.
            \item \textit{Example:} In complex scenarios, agents may oscillate between policies rather than converge to an optimal one.
        \end{itemize}

        \item \textbf{Sample Inefficiency:}
        \begin{itemize}
            \item Requires many training samples, making learning slow and costly.
            \item \textit{Example:} Agents often need millions of episodes for effective learning; techniques like experience replay can help.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Implications for Designing Effective RL Agents}
    \begin{itemize}
        \item \textbf{Design Robust Algorithms:}
        \begin{itemize}
            \item Techniques like Double Q-learning and Periodic Target Networks reduce overestimation bias and improve stability.
        \end{itemize}

        \item \textbf{Leverage Transfer Learning:}
        \begin{itemize}
            \item Using knowledge from one task to aid learning in a related task can alleviate sample inefficiency.
        \end{itemize}

        \item \textbf{Incorporate Exploration Techniques:}
        \begin{itemize}
            \item Methods like Epsilon-Greedy or Upper Confidence Bounds enhance exploration, addressing stability and efficiency.
        \end{itemize}
    \end{itemize}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Understanding stability, convergence, and sample efficiency is crucial for effective DRL systems.
            \item Strategies to mitigate these challenges lead to more robust agents.
            \item Ongoing research is vital to address persistent challenges in DRL.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Experience Replay in Python}
    \begin{lstlisting}[language=Python]
import random
import numpy as np

class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
    
    def push(self, event):
        if len(self.memory) >= self.capacity:
            self.memory.pop(0)
        self.memory.append(event)
    
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

# Usage
replay_buffer = ReplayBuffer(10000)
event = (state, action, reward, next_state, done)
replay_buffer.push(event)
batch = replay_buffer.sample(32)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics in Reinforcement Learning}
    \begin{block}{Introduction}
        Performance metrics are essential for evaluating the effectiveness of reinforcement learning (RL) models. They provide quantitative measures of a model's success and help refine algorithms to ensure agents learn optimal behaviors efficiently.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Cumulative Rewards}
    \begin{itemize}
        \item \textbf{Definition}: 
            Cumulative rewards refer to the total reward an agent accumulates over a set of episodes or time steps.
        \item \textbf{Formula}: 
            \[
            G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
            \]
            where \( G_t \) is the cumulative reward at time \( t \), \( R_t \) is the reward received at time \( t \), and \( \gamma \) (0 ≤ \( \gamma \) < 1) is the discount factor.
        \item \textbf{Importance}:
            Cumulative rewards indicate an agent's capability to maximize returns—higher cumulative rewards signify better policy performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Convergence Rates}
    \begin{itemize}
        \item \textbf{Definition}: 
            Convergence rates measure the speed at which an RL algorithm approaches its optimal policy.
        \item \textbf{Explanation}: 
            Faster convergence indicates that the agent is learning and stabilizing its performance efficiently.
        \item \textbf{Visualization}: 
            Plotting the mean cumulative reward over episodes helps illustrate convergence.
        \item \textbf{Example}: 
            If performance metrics improve significantly over the first 100 episodes and then stabilize, it implies effective learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Performance metrics should align with specific RL objectives, as different tasks prioritize different aspects (e.g., exploration vs. exploitation).
            \item Real-world applications should consider multiple metrics, as relying solely on one may not provide a complete performance picture.
        \end{itemize}
    \end{block}
    Evaluating cumulative rewards and convergence rates is essential for understanding and improving reinforcement learning models. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    \begin{block}{Introduction to Ethical Implications in Deep Reinforcement Learning (DRL)}
        Deep Reinforcement Learning (DRL) has significant opportunities across various domains but also raises important ethical concerns that must be addressed.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Areas}
    \begin{enumerate}
        \item \textbf{Bias in Data}
            \begin{itemize}
                \item \textbf{Definition:} Systematic errors leading to unfair outcomes.
                \item \textbf{Example:} Training data focused on urban scenarios may cause poor performance in rural areas.
                \item \textbf{Impact:} Can perpetuate social inequalities, such as inadequate healthcare for minority groups.
            \end{itemize}
        
        \item \textbf{Transparency}
            \begin{itemize}
                \item \textbf{Definition:} Understanding how model decisions are made.
                \item \textbf{Example:} In finance, understanding loan approval decisions is critical.
                \item \textbf{Impact:} Enhances accountability and builds trust among stakeholders.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Continued}
    \begin{enumerate}
        \setcounter{enumii}{2} % Continue numbering from previous frame
        \item \textbf{Accountability}
            \begin{itemize}
                \item \textbf{Definition:} Responsibility for actions taken by AI systems.
                \item \textbf{Example:} Harm caused by DRL systems raises questions of liability (e.g., self-driving car accidents).
                \item \textbf{Impact:} Need for clear policies regarding developer, company, and user responsibility.
            \end{itemize}
        
        \item \textbf{Autonomy and Decision-Making}
            \begin{itemize}
                \item \textbf{Definition:} Capacity of DRL systems to make independent decisions.
                \item \textbf{Example:} Use in military for autonomous drones operating without human intervention.
                \item \textbf{Impact:} Ethical debates on machines making life-and-death decisions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Need for \textbf{bias detection} and remediation strategies.
            \item Importance of fostering \textbf{transparency} in RL models.
            \item Establishing \textbf{clear accountability frameworks}.
            \item Engaging in ongoing \textbf{dialogue among stakeholders}.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Addressing ethical implications proactively enhances fairness, transparency, and societal trust in AI technologies.
    \end{block}

    \begin{block}{Discussion Questions}
        \begin{itemize}
            \item What strategies can be implemented to identify and correct biases in training data?
            \item How can organizations enhance transparency in AI decision-making processes?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - References}
    \begin{itemize}
        \item ``Ethical Issues in Deep Reinforcement Learning,'' various authors (2020).
        \item ``AI Ethics: A Guide to the Future,'' Journal of AI Research (2021).
    \end{itemize}
    
    \begin{block}{Note}
        In future applications, ensure ethics in training and methodologies as students develop RL models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Deep Reinforcement Learning}
    \begin{block}{Introduction to Deep Reinforcement Learning (DRL)}
        Deep Reinforcement Learning is a subfield of machine learning where an agent learns to make decisions through interactions with an environment. The agent observes the state, takes actions, receives rewards, and updates its knowledge to maximize long-term rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Gaming}
    \begin{itemize}
        \item \textbf{Example: AlphaGo}
            \begin{itemize}
                \item Developed by DeepMind, AlphaGo became the first AI to defeat a human champion in the board game Go.
                \item Utilizes DRL techniques, particularly Monte Carlo Tree Search and deep neural networks, to evaluate potential outcomes of moves.
            \end{itemize}
        \item \textbf{Key Points}
            \begin{itemize}
                \item DRL enables agents to learn optimal strategies in complex, high-dimensional action spaces.
                \item Significant implications in games, simulations, and training environments.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Robotics and Automated Trading}
    \begin{columns}
        \column{0.5\textwidth}
            \begin{block}{Robotics}
                \begin{itemize}
                    \item \textbf{Example: Robot Manipulation}
                        \begin{itemize}
                            \item DRL is used for tasks like object grasping, navigation, and assembly.
                            \item Trained in simulated environments to adapt to real-world scenarios.
                        \end{itemize}
                    \item \textbf{Key Points}
                        \begin{itemize}
                            \item DRL promotes trial-and-error learning, enhancing efficiency over time.
                            \item Essential for real-time decision-making, especially in autonomous vehicles.
                        \end{itemize}
                \end{itemize}
            \end{block}
        \column{0.5\textwidth}
            \begin{block}{Automated Trading}
                \begin{itemize}
                    \item \textbf{Example: Algorithmic Trading Bots}
                        \begin{itemize}
                            \item Used by financial institutions to make investment decisions based on historical market data.
                            \item Continuously learns by rewarding profitable trades and penalizing losses.
                        \end{itemize}
                    \item \textbf{Key Points}
                        \begin{itemize}
                            \item DRL optimizes trade execution, risk management, and portfolio allocation.
                            \item Enhances decision-making in unpredictable market conditions.
                        \end{itemize}
                \end{itemize}
            \end{block}
        \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Conclusion}
    \begin{itemize}
        \item \textbf{Adaptability}: DRL thrives in dynamic and uncertain environments.
        \item \textbf{Efficiency}: Demonstrates increased efficiency and innovation across applications.
        \item \textbf{Future Potential}: Expanding applications across various industries, enhancing automation and personalization.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Deep Reinforcement Learning is shaping the future of industries such as entertainment, technology, and finance by enabling intelligent and adaptive systems. Understanding these applications inspires further exploration and innovation in the field.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continual Learning in Reinforcement Learning - Part 1}
    \begin{block}{Introduction to Continual Learning in RL}
        Continual learning (CL) in reinforcement learning (RL) refers to the ability of an agent to learn from a continuous stream of experiences while preserving previously acquired knowledge.
    \end{block}
    
    \begin{block}{Importance of Continual Learning}
        \begin{itemize}
            \item \textbf{Dynamic Environments:} Real-world scenarios often change over time, which requires agents to adapt.
            \item \textbf{Knowledge Retention:} Avoiding catastrophic forgetting is crucial as the agent learns new tasks while retaining past knowledge.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continual Learning in Reinforcement Learning - Part 2}
    \begin{block}{Mechanisms of Continual Learning in RL}
        \begin{itemize}
            \item \textbf{Experience Replay:} Storing past experiences and sampling them helps maintain knowledge from earlier interactions.
            \item \textbf{Meta-Learning:} Teaching the agent to learn how to learn and adapt its strategies based on prior experiences.
            \item \textbf{Task Interleaving:} Alternating between tasks during training encourages generalization across varied tasks.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Enhances adaptability of RL agents in dynamic environments.
            \item Vital mechanisms include experience replay, meta-learning, and task interleaving.
            \item Understanding challenges like catastrophic forgetting is critical for robustness.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continual Learning in Reinforcement Learning - Part 3}
    \begin{block}{Examples of Continual Learning}
        \begin{itemize}
            \item \textbf{Robotics:} A robotic arm modifying grip techniques based on new object shapes.
            \item \textbf{Video Games:} RL agents improving adaptive strategies in changing game environments without starting anew.
        \end{itemize}
    \end{block}

    \begin{block}{Challenges in Continual Learning}
        \begin{itemize}
            \item \textbf{Catastrophic Forgetting:} Maintaining old knowledge while exploring new is essential.
            \item \textbf{Resource Management:} Effective use of computational resources is crucial for retaining relevant old data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Future Directions in Deep Reinforcement Learning}

    \begin{block}{Key Points from This Week}
        \begin{enumerate}
            \item \textbf{Understanding Deep Reinforcement Learning (DRL)}: 
            Combines deep learning with reinforcement learning principles to enable agent-learning from interactions with environments.
            \item \textbf{Core Components of DRL}:
            \begin{itemize}
                \item \textbf{Agent}: The decision maker.
                \item \textbf{Environment}: The system providing feedback based on agent's actions.
                \item \textbf{Policy}: Strategy for selecting actions based on states.
                \item \textbf{Reward Function}: Feedback indicating the success of actions.
                \item \textbf{Value Function}: Estimates future rewards guiding long-term decisions.
            \end{itemize}
            \item \textbf{Key Algorithms Discussed}:
            \begin{itemize}
                \item DQN (Deep Q-Networks)
                \item Policy Gradient Methods
                \item Actor-Critic Models
            \end{itemize}
            \item \textbf{Importance of Continual Learning}: Essential for adapting to new tasks while retaining knowledge from previous tasks.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Deep Reinforcement Learning}

    \begin{block}{Research Focus Areas}
        \begin{enumerate}
            \item \textbf{Sample Efficiency}: Focus on enhancing data efficiency in training, exploring model-based RL.
            \item \textbf{Generalization}: Developing methods for agents to generalize across tasks using meta-learning.
            \item \textbf{Robustness and Safety}: Formalizing safety constraints to ensure RL agents explore safely.
            \item \textbf{Multi-Agent Systems}: Investigating cooperative and competitive strategies among multiple agents.
            \item \textbf{Interpretability}: Ensuring DRL models are interpretable to build trust in critical applications.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations and Implementation}

    \begin{block}{Q-Learning Update Formula}
        The Q-learning update formula is provided as:
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
        \end{equation}
        where:
        \begin{itemize}
            \item \( s \) = current state
            \item \( a \) = action taken
            \item \( r \) = reward received
            \item \( \gamma \) = discount factor
            \item \( \alpha \) = learning rate
        \end{itemize}
    \end{block}

    \begin{block}{Python Code Snippet for DQN}
        \begin{lstlisting}[language=Python]
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # discount rate
        self.epsilon = 1.0  # exploration rate
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

    # Method to act based on the current state
    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.choice(range(self.action_size))  # explore
        return np.argmax(self.model.predict(state))  # exploit
        \end{lstlisting}
    \end{block}
\end{frame}


\end{document}