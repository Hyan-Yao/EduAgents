\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 4: Temporal Difference Learning]{Week 4: Temporal Difference Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Temporal Difference Learning}
    \begin{block}{Overview}
        Temporal Difference (TD) Learning is a vital technique in Reinforcement Learning (RL). It combines elements from Monte Carlo methods and Dynamic Programming to enable agents to learn predictions about future rewards without a complete environment model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Temporal Difference (TD) Learning?}
    \begin{itemize}
        \item TD Learning updates estimates based on the difference between predicted and actual rewards.
        \item Agents refine action-value estimates using observed rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Bootstrapping}:
        \begin{itemize}
            \item Updates values incrementally without waiting for episode end.
            \item Uses current reward plus the value of the next state.
            \item \begin{equation}
              V(s) \leftarrow V(s) + \alpha \left( R_t + \gamma V(S_{t+1}) - V(s) \right)
              \end{equation}
            \item Where:
            \begin{itemize}
                \item $V(s)$: Current state value
                \item $R_t$: Reward received
                \item $S_{t+1}$: Next state
                \item $\gamma$: Discount factor
                \item $\alpha$: Learning rate
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of TD Learning in Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Online Learning}: Enables real-time learning without episode completion.
        \item \textbf{Efficiency}: Converges faster than Monte Carlo methods with fewer samples.
        \item \textbf{Foundation for Algorithms}: Serves as the basis for Q-Learning and SARSA algorithms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: TD Learning in Practice}
    \begin{itemize}
        \item \textbf{Scenario}: An agent navigating a grid world to reach a goal.
        \item \textbf{Actions}: Moves (up, down, left, right).
        \item \textbf{Rewards}: Positive for reaching the goal; negative for hitting walls.
    \end{itemize}
    \begin{block}{Iterative Updates}
        As the agent explores, it uses TD Learning to update value functions based on immediate rewards and future state values, gradually improving its policy to maximize cumulative rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item TD Learning enables learning from both immediate and future rewards.
        \item Incremental value updates make it effective in dynamic environments.
        \item Essential to understand before exploring algorithms like Q-Learning.
    \end{itemize}
    \begin{block}{Concluding Thought}
        Mastering TD Learning provides a foundational understanding of Reinforcement Learning principles applicable in various domains such as robotics, game AI, and adaptive systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Q-Learning - Introduction}
    Q-learning is a type of Reinforcement Learning (RL) algorithm that focuses on learning the value of taking actions in specific states to maximize cumulative future rewards. It is:
    \begin{itemize}
        \item Model-free: Does not require a model of the environment.
        \item Based on Temporal Difference (TD) learning: Learns values based on the difference between predicted and actual rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Q-Learning - Core Concepts}
    \begin{block}{A. Q-Value (Action-Value Function)}
        \begin{itemize}
            \item The Q-value, \( Q(s, a) \), represents the expected future rewards when taking action \( a \) in state \( s \) and following the optimal policy thereafter.
            \item Updated through learning to improve decision-making over time.
        \end{itemize}
    \end{block}
    
    \begin{block}{B. Learning Rate (\( \alpha \))}
        \begin{itemize}
            \item Controls how much new information overrides old information.
            \item Step size in Q-value updates: \( Q(s, a) \leftarrow Q(s, a) + \alpha (r + \gamma \max_{a'} Q(s', a') - Q(s, a)) \).
            \item Typically, \( \alpha \) ranges between 0 and 1.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Q-Learning - Exploration-Exploitation}
    \begin{block}{C. Exploration-Exploitation Balance}
        \begin{itemize}
            \item \textbf{Exploration:} Trying out new actions to discover their effects and potential rewards.
            \item \textbf{Exploitation:} Leveraging known information to maximize rewards based on current Q-values.
        \end{itemize}
        
        \textbf{Strategies for Balancing:}
        \begin{itemize}
            \item \(\epsilon\)-greedy Strategy: Choose a random action (exploration) with probability \( \epsilon \), and the best-known action (exploitation) with probability \( 1 - \epsilon \).
            \item Decrease \( \epsilon \) over time to favor exploitation as learning progresses.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Q-learning is a model-free RL approach.
            \item Proper updating of Q-values is crucial for convergence.
            \item Balancing exploration and exploitation is essential for effective learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning Algorithm - Overview}
    \begin{itemize}
        \item Q-learning is a foundational reinforcement learning algorithm.
        \item It enables agents to learn optimal actions through value estimation (Q-values).
        \item The goal: Maximize cumulative rewards over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components}
    \begin{itemize}
        \item \textbf{Q-value:} \( Q(s, a) \) - Expected utility of action \( a \) in state \( s \).
        \item \textbf{Learning Rate} \( \alpha \):
            \begin{itemize}
                \item Controls new information integration (0 to 1).
                \item \( \alpha = 0 \): No learning.
                \item \( \alpha = 1 \): Full overwrite of previous values.
            \end{itemize}
        \item \textbf{Discount Factor} \( \gamma \):
            \begin{itemize}
                \item Importance of future rewards (0 â‰¤ \( \gamma \) < 1).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Update Rule}
    \begin{block}{Q-value Update}
        The update rule for Q-learning is given by:
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
        \end{equation}
        \begin{itemize}
            \item \( s \): Current state
            \item \( a \): Action taken
            \item \( r \): Immediate reward
            \item \( s' \): Resulting state after action \( a \)
            \item \( \max_{a'} Q(s', a') \): Maximum Q-value of next state
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Iterative Process}
    \begin{enumerate}
        \item \textbf{Initialization:} Start with arbitrary Q-values (often zeros).
        \item \textbf{Agent-Environment Interaction:}
            \begin{itemize}
                \item Observe state \( s \).
                \item Select action \( a \) (e.g., using Îµ-greedy).
                \item Execute action, observe reward \( r \) and next state \( s' \).
                \item Update Q-value using the update rule.
            \end{itemize}
        \item \textbf{Repeat:} Continue until Q-values stabilize.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Q-value Update}
    Consider an agent in a grid world:
    \begin{itemize}
        \item \textbf{Current State \( s \)}: (1, 1)
        \item \textbf{Action \( a \)}: Right
        \item \textbf{Reward \( r \)}: 5
        \item \textbf{New State \( s' \)}: (1, 2)
    \end{itemize}
    
    Using \( Q(1, 1) = 2 \), \( Q(1, 2) = 3 \), \( \alpha = 0.1 \), and \( \gamma = 0.9 \):
    \[
    Q(1, 1) \leftarrow 2 + 0.1 \left( 5 + 0.9 \times 3 - 2 \right)
    \]
    Simplifying:
    \[
    = 2 + 0.1(5 + 2.7 - 2) = 2 + 0.57 = 2.57
    \]
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{itemize}
        \item Q-learning is \textbf{off-policy}, learning the optimal policy independently.
        \item It converges to optimal Q-values with sufficient exploration.
        \item Importance of balancing exploration (trying new actions) and exploitation (choosing the best-known action).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Overview}
    \begin{itemize}
        \item SARSA: State-Action-Reward-State-Action
        \item An on-policy reinforcement learning algorithm
        \item Evaluates the policy it is currently following
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of SARSA}
    \begin{itemize}
        \item \textbf{State (s)}: Current state of the agent
        \item \textbf{Action (a)}: Action taken by the agent in state \( s \)
        \item \textbf{Reward (r)}: Immediate reward received after action \( a \)
        \item \textbf{Next State (s')}: State reached after executing action \( a \)
        \item \textbf{Next Action (a')}: Action taken from state \( s' \) based on current policy
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Update Rule}
    The action-value function \( Q \) is updated using the rule:
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
    \end{equation}
    Where:
    \begin{itemize}
        \item \( \alpha \): Learning rate (0 < \( \alpha \) â‰¤ 1)
        \item \( \gamma \): Discount factor (0 â‰¤ \( \gamma \) < 1)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{On-Policy vs. Off-Policy}
    \begin{itemize}
        \item \textbf{On-Policy (SARSA)}: Updates based on actions taken, allowing for a more stable learning process.
        \item \textbf{Off-Policy (Q-learning)}: Uses maximum future Q-value, allowing for learning optimal policy irrespective of current actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example to Illustrate SARSA}
    \begin{enumerate}
        \item Agent in state \( s \) takes action \( a \) and receives reward \( r \).
        \item Transitions to state \( s' \) and chooses next action \( a' \) based on current policy.
        \item Updates \( Q(s, a) \) using the received reward and estimated value of \( a' \) in state \( s' \).
    \end{enumerate}
    This method teaches the agent to learn according to its behavior.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use SARSA?}
    \begin{itemize}
        \item \textbf{Interpretability}: Reflects actual actions taken, aiding performance tracking.
        \item \textbf{Exploration Excellence}: Adapts well in scenarios with frequently updated policies based on real-time experiences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item SARSA is an on-policy reinforcement learning technique.
        \item Updates Q-values based on actions taken by the agent.
        \item Emphasizes immediate rewards and future actions, aligning learning with behavior.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    Understanding SARSA is crucial for developing intelligent agents that effectively learn and navigate complex environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding the SARSA Algorithm}
    \begin{block}{What is SARSA?}
        \begin{itemize}
            \item SARSA stands for State-Action-Reward-State-Action.
            \item It is an on-policy temporal difference learning algorithm for reinforcement learning.
            \item Unlike Q-learning, it evaluates based on the current policy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Update Rule}
    The SARSA update rule can be expressed mathematically as:
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
    \end{equation}
    Where:
    \begin{itemize}
        \item \( Q(s, a) \): Current estimate of the action-value function.
        \item \( \alpha \): Learning rate (0 < \( \alpha \) â‰¤ 1).
        \item \( r \): Immediate reward after taking action \( a \) in state \( s \).
        \item \( \gamma \): Discount factor (0 â‰¤ \( \gamma \) < 1).
        \item \( s' \): New state after taking action \( a \).
        \item \( a' \): Next action to be taken in state \( s' \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Learning Rate (\( \alpha \))}:
            \begin{itemize}
                \item High \( \alpha \): quick learning, potential instability.
                \item Low \( \alpha \): stability, slower learning.
            \end{itemize}
        
        \item \textbf{Discount Factor (\( \gamma \))}:
            \begin{itemize}
                \item Close to 1: favors future rewards.
                \item Closer to 0: favors immediate rewards.
            \end{itemize}

        \item \textbf{On-Policy Learning}:
            \begin{itemize}
                \item Learns based on the current policy, making it adaptable to changing environments.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Application}
    \begin{block}{Gridworld Example}
        Consider a simple gridworld with states (S) and actions (A):
        \begin{itemize}
            \item \textbf{States}: S1, S2, S3 (with S1 as the starting state).
            \item \textbf{Actions}: \{Up, Down, Left, Right\}.
        \end{itemize}
        
        From state S1, choose action "Down", receive reward \( r = 1 \), landing in S2:
        \begin{itemize}
            \item Current State: \( s = S1 \)
            \item Current Action: \( a = Down \)
            \item Next State: \( s' = S2 \)
            \item Next Action: \( a' = Right \)
        \end{itemize}

        Applying the update rule:
        \begin{equation}
            Q(S1, Down) \leftarrow Q(S1, Down) + \alpha \left( 1 + \gamma Q(S2, Right) - Q(S1, Down) \right)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item SARSA adapts based on actions taken, making it useful for stochastic environments.
        \item The choices of \( \alpha \) and \( \gamma \) directly affect performance.
        \item Understanding SARSA is foundational for grasping advanced reinforcement learning algorithms.
    \end{itemize}
    By leveraging the SARSA algorithm, agents can learn dynamically based on their experiences.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{itemize}
        \item Q-learning and SARSA (State-Action-Reward-State-Action) are popular Temporal Difference (TD) learning methods.
        \item Both are used for solving reinforcement learning problems, but they have distinct strengths and weaknesses.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning: Strengths and Weaknesses}
    \begin{block}{Strengths}
        \begin{enumerate}
            \item \textbf{Off-policy Learning}:
            \begin{itemize}
                \item Uses the best possible action for updates, promoting exploration.
                \item Allows learning about optimal states even from suboptimal actions.
            \end{itemize}
            \item \textbf{Convergence to Optimal Policy}:
            \begin{itemize}
                \item Guarantees convergence if there is sufficient exploration.
            \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Weaknesses}
        \begin{enumerate}
            \item \textbf{Overestimation Bias}:
            \begin{itemize}
                \item Tends to overestimate next state's action values, leading to suboptimal policy learning.
            \end{itemize}
            \item \textbf{Higher Variance}:
            \begin{itemize}
                \item More variability in updates can require longer training.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA: Strengths and Weaknesses}
    \begin{block}{Strengths}
        \begin{enumerate}
            \item \textbf{On-policy Learning}:
            \begin{itemize}
                \item Updates based on the actual action taken, enhancing stability.
            \end{itemize}
            \item \textbf{Less Overestimation Bias}:
            \begin{itemize}
                \item Lower bias compared to Q-learning.
            \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Weaknesses}
        \begin{enumerate}
            \item \textbf{Convergence to Suboptimal Policies}:
            \begin{itemize}
                \item Can get stuck in suboptimal choices due to inadequate exploration.
            \end{itemize}
            \item \textbf{Sensitivity to Exploration Strategy}:
            \begin{itemize}
                \item Performance heavily influenced by exploration techniques.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences in Scenarios}
    \begin{itemize}
        \item \textbf{Exploration vs. Exploitation}:
        \begin{itemize}
            \item \textbf{Q-learning}: Best when optimal policy is crucial, especially in deterministic environments.
            \item \textbf{SARSA}: Better suited for stochastic environments where exploration is a priority.
        \end{itemize}
        \item \textbf{Training Time and Resources}:
        \begin{itemize}
            \item \textbf{Q-learning}: May need longer training to manage variance.
            \item \textbf{SARSA}: Can learn faster with effective exploration strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Formulas}
    \begin{itemize}
        \item Both algorithms play significant roles in the reinforcement learning landscape.
        \item Selection between Q-learning and SARSA depends on task requirements, including environment complexity and exploration needs.
    \end{itemize}

    \begin{block}{Formulas}
        \textbf{SARSA Update Rule}:
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
        \end{equation}

        \textbf{Q-learning Update Rule}:
        \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration Strategies - Introduction}
    In Temporal Difference (TD) learning and reinforcement learning, agents must explore their environment to enhance decision-making. 
    \begin{itemize}
        \item Exploration versus exploitation: Balancing trying new actions with choosing the best-known action.
        \item Key strategies: 
            \begin{itemize}
                \item Epsilon-Greedy
                \item Softmax Action Selection
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration Strategies - Epsilon-Greedy}
    \begin{block}{Concept}
        The epsilon-greedy strategy mostly selects the highest estimated action but occasionally explores random actions.
    \end{block}
    
    \begin{itemize}
        \item Probability of Exploration: With probability $\epsilon$, choose a random action.
        \item Probability of Exploitation: With probability $1 - \epsilon$, choose the action with the maximum estimated value.
    \end{itemize}

    \begin{equation}
    a = 
    \begin{cases} 
    \text{random action} & \text{with probability } \epsilon \\ 
    \arg\max_a Q(s, a) & \text{with probability } 1 - \epsilon 
    \end{cases}
    \end{equation}
    
    \textbf{Example:} For $\epsilon = 0.1$, explore randomly 10\% of the time.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration Strategies - Softmax Action Selection}
    \begin{block}{Concept}
        The softmax action selection evaluates actions based on their estimated values, with probabilities assigned accordingly.
    \end{block}
    
    \begin{itemize}
        \item Actions are selected based on the probability distribution derived from Q-values.
    \end{itemize}

    \begin{equation}
    P(a) = \frac{e^{\frac{Q(s, a)}{\tau}}}{\sum_{a'} e^{\frac{Q(s, a')}{\tau}}}
    \end{equation}
    \begin{itemize}
        \item Where $P(a)$ is the probability of selecting action $a$, and $\tau$ controls exploration level.
    \end{itemize}

    \textbf{Example:} Given $Q(s, a_1) = 2$ and $Q(s, a_2) = 1$ with $\tau = 1$:
    \begin{itemize}
        \item Compute probabilities: 
        \begin{itemize}
            \item $P(a_1) = \frac{e^{2}}{e^{2} + e^{1}}$
            \item $P(a_2) = \frac{e^{1}}{e^{2} + e^{1}}$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementing Q-learning in Python}
    \begin{block}{Overview of Q-learning}
        Q-learning is a model-free reinforcement learning algorithm that learns the value of actions in various states using the Bellman equation.
    \end{block}
    \begin{itemize}
        \item \textbf{States (S):} Possible situations for the agent.
        \item \textbf{Actions (A):} Choices the agent can make.
        \item \textbf{Q-values (Q):} Estimate of future rewards from actions in states.
        \item \textbf{Learning Rate ($\alpha$):} Controls the weight of new information.
        \item \textbf{Discount Factor ($\gamma$):} Future rewards are valued less than immediate rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning Update Rule}
    The Q-learning update can be represented as:
    \begin{equation}
        Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma \max_{A'} Q(S', A') - Q(S, A) \right]
    \end{equation}
    Where:
    \begin{itemize}
        \item $R$ is the immediate reward.
        \item $S'$ is the next state after taking action $A$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Implementation}
    Hereâ€™s an example demonstrating the Q-learning algorithm using Python and NumPy:
    \begin{lstlisting}[language=Python]
import numpy as np
import random

# Initialize parameters
num_states = 5
num_actions = 2
q_table = np.zeros((num_states, num_actions))  # Q-table initialized to zero
num_episodes = 1000
max_steps = 100
learning_rate = 0.1
discount_factor = 0.9
exploration_rate = 1.0
exploration_decay = 0.99
min_exploration_rate = 0.1 

# Define simulation functions
def choose_action(state):
    if random.uniform(0, 1) < exploration_rate:
        return random.randint(0, num_actions - 1)  # Explore action space
    else:
        return np.argmax(q_table[state])  # Exploit learned values

# Main Q-learning loop
for episode in range(num_episodes):
    state = random.randint(0, num_states - 1)  # Start from a random state
    for _ in range(max_steps):
        action = choose_action(state)
        next_state = (state + action) % num_states
        reward = 1 if next_state == num_states - 1 else 0 

        # Update Q-Table
        best_next_action = np.argmax(q_table[next_state])
        q_table[state, action] += learning_rate * (reward + discount_factor * q_table[next_state, best_next_action] - q_table[state, action])

        state = next_state
        
    exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)

# Display the learned Q-values
print("Learned Q-values:")
print(q_table)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points on Q-learning}
    \begin{itemize}
        \item \textbf{Exploration vs. Exploitation:} Balancing between exploring new actions and exploiting known rewarding actions is crucial. The $\epsilon$-greedy strategy helps achieve this balance.
        \item \textbf{Training Loop:} The algorithm updates Q-values iteratively across episodes, adjusting based on received rewards.
        \item \textbf{Parameter Tuning:} Tuning learning and exploration rates is essential for efficient learning; adjustments may be needed for different environments.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Implementing Q-learning in Python with NumPy offers a practical approach for creating and comprehending reinforcement learning agents. This foundational algorithm can be enhanced for more complex applications, facilitating advanced decision-making models.
\end{frame}

\begin{frame}
    \frametitle{Implementing SARSA in Python - Overview}
    \begin{itemize}
        \item \textbf{SARSA (State-Action-Reward-State-Action)} is an on-policy temporal difference learning algorithm for reinforcement learning.
        \item It updates Q-values based on the actions taken by the agent rather than the best future action, providing a conservative estimate.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementing SARSA in Python - Key Components}
    \begin{itemize}
        \item \textbf{Q-values}: Expected future rewards for state-action pairs.
        \item \textbf{Learning Rate} $\alpha$: Controls how much new information impacts existing Q-values.
        \item \textbf{Discount Factor} $\gamma$: Represents the importance of future rewards.
        \item \textbf{Exploration Rate} $\epsilon$: Facilitates exploration of the environment (via $\epsilon$-greedy policy).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing SARSA in Python - Algorithm Steps}
    \begin{enumerate}
        \item Initialize Q-values for all state-action pairs.
        \item For each episode:
            \begin{itemize}
                \item Start with an initial state.
                \item Choose an action using the $\epsilon$-greedy policy.
                \item For each step within the episode:
                    \begin{itemize}
                        \item Take the action and observe the reward and next state.
                        \item Choose the next action using the $\epsilon$-greedy policy.
                        \item Update Q-value using:
                        \begin{equation}
                        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
                        \end{equation}
                        \item Transition to the next state and action.
                    \end{itemize}
            \end{itemize}
        \item Continue until the episode ends.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing SARSA in Python - Python Code}
    \begin{lstlisting}[language=Python]
import numpy as np
import random

# Environment parameters
n_states = 5
n_actions = 2
epsilon = 0.1
alpha = 0.5
gamma = 0.9
num_episodes = 1000

# Initialize Q-table
Q = np.zeros((n_states, n_actions))

# SARSA Algorithm Implementation
for episode in range(num_episodes):
    state = random.randint(0, n_states - 1)
    action = np.argmax(Q[state] + np.random.randn(1, n_actions) * (1.0 / (episode + 1)))

    while True:
        next_state, reward = take_action(state, action)
        next_action = np.argmax(Q[next_state] + np.random.randn(1, n_actions) * (1.0 / (episode + 1)))
        Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])
        
        state, action = next_state, next_action
        
        if done:
            break
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Implementing SARSA in Python - Results and Evaluation}
    \begin{itemize}
        \item Analyze Q-values and their stabilisation over episodes.
        \item Measure the sum of rewards to evaluate performance and convergence.
        \item Visual results showcase the agent's learning improvement over episodes.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementing SARSA in Python - Key Takeaways}
    \begin{itemize}
        \item SARSA's on-policy nature relates learning directly to the policy being followed.
        \item Balancing exploration and exploitation is crucial for effective learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation of Temporal Difference Learning - Overview}
    \begin{itemize}
        \item Evaluating performance in temporal difference learning involves several key methods.
        \item Main methods discussed:
        \begin{enumerate}
            \item Convergence Rates
            \item Cumulative Rewards
            \item Policy Evaluation
            \item Mean Squared Error (MSE)
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation - Convergence Rates}
    \begin{block}{Convergence Rates}
        \begin{itemize}
            \item \textbf{Definition}: The rate at which a learning algorithm approaches its optimal policy or value function.
            \item \textbf{Importance}: Faster convergence indicates more efficient learning.
            \item \textbf{Measurement}: Assessed by plotting value function over time or calculating:
            \begin{equation}
                \text{Error} = |V_{t+1}(s) - V_t(s)|
            \end{equation}
            \item \textbf{Example}: In a grid world, observe how quickly state values stabilize over episodes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation - Cumulative Rewards & Other Metrics}
    \begin{block}{Cumulative Rewards}
        \begin{itemize}
            \item \textbf{Definition}: Total return an agent accumulates over time while following a policy, denoted as \( R_t \).
            \item \textbf{Formula}:
            \begin{equation}
                R_t = r_t + \gamma r_{t+1} + \gamma^2r_{t+2} + \ldots
            \end{equation}
            \item \textbf{Significance}: Higher cumulative rewards reflect better learning performance.
            \item \textbf{Example}: Points collected in a game scenario illustrate learning performance.
        \end{itemize}
    \end{block}

    \begin{block}{Mean Squared Error (MSE)}
        \begin{itemize}
            \item \textbf{Definition}: Assesses how far predictions are from actual returns.
            \item \textbf{Formula}:
            \begin{equation}
                \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (V_i - V^*_i)^2
            \end{equation}
            \item \textbf{Use}: Quantifies accuracy of the learned value function over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Convergence is criticalâ€”quicker means more efficient learning.
            \item Cumulative rewards provide a straightforward performance metric.
            \item Evaluate various policies to assess decision-making improvements.
            \item Utilize MSE for precise understanding of value estimate accuracy.
        \end{itemize}
        \item \textbf{Conclusion}: Performance evaluation in temporal difference learning combines convergence rates, cumulative rewards, policy evaluations, and error measurements.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Real-world Applications - Overview}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item **Temporal Difference (TD) Learning**: Reinforcement learning techniques for decision-making in uncertain environments.
            \item **Q-learning**: Model-free algorithm maximizing expected future rewards.
            \item **SARSA**: On-policy algorithm that updates estimates based on actual actions taken.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Real-world Applications - Fields}
    \begin{enumerate}
        \item **Gaming**
            \begin{itemize}
                \item Example: AI opponents in chess and Go using Q-learning, such as AlphaGo.
                \item Benefit: Learns strategies from millions of game states for high win rates.
            \end{itemize}
        \item **Robotics**
            \begin{itemize}
                \item Example: Navigation and control to avoid collisions in complex environments.
                \item Benefit: Adapts navigation policies through environmental feedback.
            \end{itemize}
        \item **Autonomous Systems**
            \begin{itemize}
                \item Example: Self-driving cars employing Q-learning for maneuver decisions.
                \item Benefit: Learns from real-time data to improve safety and efficiency.
            \end{itemize}
        \item **Healthcare**
            \begin{itemize}
                \item Example: SARSA optimizing personalized treatment plans.
                \item Benefit: Tailors effective treatment pathways based on patient feedback.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example Formula}
    \begin{block}{Key Points}
        \begin{itemize}
            \item **Adaptivity**: Q-learning and SARSA allow systems to learn continuously in dynamic environments.
            \item **Versatility**: Applicable in diverse fields including finance and climate modeling.
            \item **Exploration vs. Exploitation**: Balances exploring new strategies and using known successful ones.
        \end{itemize}
    \end{block}
    \begin{block}{Example Formula}
        \begin{equation}
        Q(s, a) \gets Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $s$: current state
            \item $a$: current action
            \item $r$: reward received
            \item $s'$: next state
            \item $\alpha$: learning rate
            \item $\gamma$: discount factor
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in TD Learning - Introduction}
    \begin{block}{Introduction to Ethical Implications}
        Temporal Difference (TD) Learning, including popular algorithms like Q-learning and SARSA, has transformative potential across many sectors. 
        However, its implementation raises important ethical questions. 
        As practitioners and researchers in the field, we must be aware of these ethical considerations to ensure responsible use of these technologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in TD Learning - Bias in Data}
    \begin{block}{Bias in Data}
        \begin{itemize}
            \item \textbf{Concept:} Data used in TD Learning algorithms may carry inherent biases reflecting systemic injustices or inequalities.
            \item \textbf{Example:} A TD Learning model trained on biased historical recruitment data may unfairly prioritize or eliminate candidates from certain demographics.
            \item \textbf{Illustration:} Imagine a job recruitment algorithm that shows a disproportionate preference for male candidates, perpetuating this bias.
            \item \textbf{Key Consideration:} Audit datasets and enhance diversity in the training data to mitigate bias.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in TD Learning - Algorithmic Transparency}
    \begin{block}{Algorithmic Transparency}
        \begin{itemize}
            \item \textbf{Concept:} There is a need for clear understanding of how TD Learning algorithms make decisions and predictions.
            \item \textbf{Challenges:} Many TD Learning models operate as "black boxes," leading to mistrust and misuse.
            \item \textbf{Example:} If an autonomous driving system uses a TD Learning algorithm for navigation without providing explanations for its decisions, safety becomes difficult to ascertain.
            \item \textbf{Key Point:} Understandable explanations for algorithmic actions increase trust and accountability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in TD Learning - Framework and Conclusion}
    \begin{block}{Ethical Framework and Guidelines}
        \begin{itemize}
            \item Establish \textbf{Guidelines:} Organizations should create ethical guidelines for the use of TD Learning.
            \item Consider \textbf{Best Practices:} Engage stakeholders during development, and regularly review and update algorithms for fairness.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        As we advance through TD Learning, it is our responsibility to be conscious of the ethical implications. 
        By prioritizing fairness and transparency, we can harness the power of TD Learning responsibly.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in TD Learning - Discussion Questions}
    \begin{block}{Discussion Questions}
        \begin{enumerate}
            \item How can we practically address data bias when designing TD Learning systems?
            \item What measures can ensure the transparency of TD Learning algorithms?
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Temporal Difference Learning}
    
    \begin{block}{Introduction}
        Temporal Difference (TD) Learning has shown tremendous promise in reinforcement learning (RL). Ongoing research focuses on enhancing efficiency and applicability across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Enhancing Algorithmic Efficiency}
    
    \begin{itemize}
        \item \textbf{Research Focus}: Improving existing TD algorithms to converge faster and with less data.
        \item \textbf{Example}: Utilizing sophisticated eligibility traces that combine TD(Î») principles with deep reinforcement learning to improve sample efficiency.
        \item \textbf{Key Point}: Reducing sample complexity is crucial for scenarios with limited data availability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integrating TD Learning with Neural Networks}
    
    \begin{itemize}
        \item \textbf{Research Focus}: Integrating neural networks with TD Learning, termed Deep Temporal Difference Learning.
        \item \textbf{Example}: Deep Q-Networks (DQN) utilize convolutional networks to efficiently represent large state spaces, employing TD learning for Q-value updates.
        \item \textbf{Key Point}: This hybrid approach enables tackling complex problems such as video game playing and robotic control tasks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Addressing Ethical Considerations}
    
    \begin{itemize}
        \item \textbf{Research Focus}: Mitigating data biases and ensuring algorithmic transparency.
        \item \textbf{Example}: Developing auditing mechanisms to evaluate how data impacts learning and to identify potential biases in decision-making.
        \item \textbf{Key Point}: The importance of ethical AI is rising, with researchers focusing on fairness alongside performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in Real-World Problems}
    
    \begin{itemize}
        \item \textbf{Research Focus}: Exploring new application domains for TD Learning.
        \item \textbf{Examples}:
            \begin{itemize}
                \item \textbf{Healthcare}: Personalized treatment plans predicting outcomes based on historical data.
                \item \textbf{Finance}: Algorithmic trading strategies adapting behavior based on market responses to optimize returns.
            \end{itemize}
        \item \textbf{Key Point}: The adaptability of TD Learning opens vast avenues for innovation in diverse fields.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Expanding Theoretical Foundations}
    
    \begin{itemize}
        \item \textbf{Research Focus}: Enhancing theoretical insights into convergence guarantees and optimality of TD Learning methods.
        \item \textbf{Example}: Establishing robust mathematical foundations for new TD methods to ensure effectiveness under various conditions.
        \item \textbf{Key Point}: A deeper theoretical understanding supports the development of practically effective and theoretically sound algorithms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    The field of TD Learning is dynamic, with exciting research avenues. By refining algorithms, integrating neural networks, addressing ethical issues, expanding applications, and enhancing theoretical insights, researchers can significantly impact complex problem-solving across domains.
    
    \begin{block}{Reminder}
        Balance advancing technology with ethical implications of powerful learning systems.
    \end{block}
\end{frame}


\end{document}