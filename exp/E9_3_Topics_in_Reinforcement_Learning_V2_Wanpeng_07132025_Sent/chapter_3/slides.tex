\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Dynamic Programming and Monte Carlo Methods]{Week 3: Dynamic Programming and Monte Carlo Methods}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview: Significance in Reinforcement Learning}
    Dynamic Programming (DP) and Monte Carlo (MC) methods are foundational approaches in reinforcement learning (RL) that:
    \begin{itemize}
        \item Help in solving decision-making problems under uncertainty.
        \item Enable evaluation and improvement of policies to achieve optimal behavior in various environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming (DP)}
    
    \textbf{Definition:} \\
    DP refers to a set of algorithms that solve problems by breaking them down into simpler subproblems in a recursive manner, particularly effective with Markov Decision Processes (MDPs).

    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Bellman Equation:} 
            \begin{equation}
                V(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]
            \end{equation}
            \item \textbf{Policy Evaluation:} Determines how good a policy is using the value function.
            \item \textbf{Policy Improvement:} Induces a new policy maximizing expected value based on the current value function.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Dynamic Programming}
    Consider a simple grid world where:
    \begin{itemize}
        \item An agent can move in four directions.
        \item The value function at each cell is determined by the expected cumulative reward from that point onward, calculated using the Bellman equation iteratively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo (MC) Methods}
    
    \textbf{Definition:} \\
    MC methods use random sampling to compute results and estimate the value of states or actions from samples of episodic sequences.

    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Episode:} A sequence of states, actions, and rewards from start to finish.
            \item \textbf{Return:} The total discounted reward from a given state onwards, given by:
            \begin{equation}
                G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots = \sum_{k=0}^\infty \gamma^k R_{t+k}
            \end{equation}
            \item \textbf{Exploration vs. Exploitation:} MC methods allow exploration of the state space utilizing random policies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Monte Carlo Methods}
    An agent plays a game multiple times, recording:
    \begin{itemize}
        \item Rewards for actions taken in various states.
        \item States encountered during episodes to average the returns.
        \item This helps construct an empirical estimate of the stateâ€™s value.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Applications}
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Both methods are essential for learning optimal policies in RL.
        \item DP offers structured solutions, while MC leverages randomness.
        \item Balances computational efficiency (DP) with sample efficiency (MC).
    \end{itemize}

    \textbf{Applications Include:}
    \begin{itemize}
        \item Game Playing (e.g., Chess, Go)
        \item Robotics for navigation and decision-making
        \item Finance for investment strategies
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    This week, we will dive deep into two fundamental concepts in Reinforcement Learning (RL) which are pivotal for understanding how agents can make optimal decisions based on their experiences in an environment. Our focus will be on:
    \begin{itemize}
        \item Policy Evaluation
        \item Value Iteration
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Policy Evaluation}
    \begin{block}{Understanding Policy Evaluation}
        \begin{itemize}
            \item \textbf{Definition}: Policy evaluation is the process of determining the value function for a given policy. It calculates the expected returns of states under a specific policy, providing insights into how good a policy is.
            \item \textbf{Key Formula}:
            \begin{equation}
                V^{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right]
            \end{equation}
            where \( G_t \) is the total return from state \( s \) onwards.
            \item \textbf{Example}: Consider an agent navigating a grid world. If the policy is to always move right when possible, policy evaluation helps compute expected future rewards for being in each state given this movement policy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Value Iteration}
    \begin{block}{Exploring Value Iteration}
        \begin{itemize}
            \item \textbf{Definition}: Value iteration is an algorithm used to compute the optimal policy by iteratively updating the value function until it converges to the optimal values.
            \item \textbf{Key Concept}: Starts with an initial guess of the value function and repeatedly applies the Bellman equation:
            \begin{equation}
                V_{k+1}(s) = \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V_k(s') \right]
            \end{equation}
            where:
            \begin{itemize}
                \item \( V_k(s) \): value function at iteration \( k \)
                \item \( P(s' \mid s, a) \): transition probability to state \( s' \)
                \item \( R(s, a, s') \): immediate reward for transition from state \( s \) to \( s' \) under action \( a \)
                \item \( \gamma \): discount factor.
            \end{itemize}
            \item \textbf{Example}: In the same grid world scenario, using value iteration helps identify the optimal policy that maximizes the expected return, considering all possible actions and outcomes recursively.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Additional Resources}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item Reinforcement Learning Context: Contribution to informed decision-making based on prior experiences.
            \item Iterative Nature: Importance of convergence towards optimal solutions.
            \item Practical Applications: Enhancements in robotics, game playing, and optimization problems.
        \end{itemize}
        \item \textbf{Additional Resources}:
        \begin{itemize}
            \item Practice implementing policy evaluation and value iteration algorithms using Python.
            \item Review concepts of Markov Decision Processes (MDPs) and their relation to policy evaluation and value iteration.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Evaluation - Definition}
    \begin{block}{Definition of Policy Evaluation}
        Policy Evaluation in reinforcement learning (RL) is the process of determining the value of a policy, which aims to compute the state-value function \( V^\pi(s) \) for each state \( s \) under a given policy \( \pi \).
        The value function represents the expected return (cumulative reward) when starting from state \( s \) and following policy \( \pi \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Evaluation - Key Concepts}
    \begin{itemize}
        \item \textbf{Policy} (\( \pi \)):
        \begin{itemize}
            \item A strategy defining actions in each state.
            \item Can be deterministic (specific action) or stochastic (probability distribution).
        \end{itemize}
        
        \item \textbf{Value Function} (\( V^\pi \)):
        \begin{equation}
        V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
        \end{equation}
        where \( G_t \) is the cumulative reward from time \( t \).

        \item \textbf{Return} (\( G_t \)):
        \begin{equation}
        G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
        \end{equation}
        with \( \gamma \) being the discount factor (\( 0 \leq \gamma < 1 \)).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Evaluation - Bellman Equation and Algorithm}
    \begin{block}{Bellman Equation}
        The value function can be defined by:
        \begin{equation}
        V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s', r} P(s', r | s, a) \left[ r + \gamma V^\pi(s') \right]
        \end{equation}
    \end{block}

    \begin{block}{Iterative Policy Evaluation Algorithm}
    \begin{enumerate}
        \item Initialize \( V(s) \) for all states \( s \).
        \item Update \( V(s) \) using the Bellman equation until convergence:
        \begin{equation}
        V_{k+1}(s) \gets \sum_{a} \pi(a|s) \sum_{s', r} P(s', r | s, a) \left[ r + \gamma V_k(s') \right]
        \end{equation}
        \item Continue until \( ||V_{k+1} - V_k|| < \theta \) for a small threshold \( \theta \).
    \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Evaluation - Example Illustration}
    \begin{block}{Example: Simple Grid World}
        Consider a grid world where:
        \begin{itemize}
            \item States are represented by grid cells.
            \item The policy specifies direction of movement based on the cell.
            \item Evaluating the policy estimates the expected total rewards from each cell.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Policy evaluation is crucial for assessing policy performance and informing improvements.
            \item It forms a foundation for advanced algorithms like Value Iteration and Policy Improvement.
            \item The Bellman equation is essential for deeper reinforcement learning understanding.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration - Overview}
    \begin{block}{Overview}
        Value iteration is a fundamental algorithm for solving Markov Decision Processes (MDPs). It enables us to determine the optimal policy that maximizes expected rewards.
    \end{block}
    \begin{itemize}
        \item Iteratively updates value estimates for each state until convergence.
        \item Guides decision-making in dynamic environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration - Key Concepts}
    \begin{itemize}
        \item \textbf{Markov Decision Process (MDP):} A framework for modeling decision-making, defined by states, actions, transition probabilities, and rewards.
        \item \textbf{Value Function (V(s))}: Represents the maximum expected return from state \( s \).
        \item \textbf{Optimal Policy (\(\pi^*\))}: Strategy that defines the best action at each state.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration - Algorithm Steps}
    \begin{enumerate}
        \item \textbf{Initialization:}
            \begin{itemize}
                \item Start with arbitrary values for all states, often initializing \( V(s) = 0 \).
            \end{itemize}
        
        \item \textbf{Update Step:}
            \begin{equation}
                V_{new}(s) = \max_a \left( \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V(s')] \right)
            \end{equation}
            \begin{itemize}
                \item \( P(s' | s, a) \): Transition probability from state \( s \) to \( s' \) after action \( a \).
                \item \( R(s, a, s') \): Immediate reward after transitioning.
                \item \( \gamma \): Discount factor (0 â‰¤ \( \gamma \) < 1).
            \end{itemize}

        \item \textbf{Convergence Check:}
            \begin{itemize}
                \item Repeat updates until convergence (when the change is less than a threshold \( \epsilon \)).
            \end{itemize}
        
        \item \textbf{Extract Policy:}
            \begin{equation}
                \pi^*(s) = \arg\max_a \left( \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V(s')] \right)
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration - Importance and Example}
    \begin{block}{Importance}
        \begin{itemize}
            \item \textbf{Efficiency:} Handles complex environments where exhaustive search is impractical.
            \item \textbf{Convergence Guarantee:} Converges to the optimal value function under certain conditions.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Consider a simple grid world where:
        \begin{itemize}
            \item Transition to the goal state yields a reward of +10.
            \item Moving to a wall results in -5.
            \item Initialize values, e.g., \( V(s) = 0 \).
            \item Update values iteratively until stabilization.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration - Key Points}
    \begin{itemize}
        \item Value iteration efficiently computes optimal policies even for large state spaces.
        \item Relies on the principle of optimality, ensuring that every policy component is optimal.
    \end{itemize}
    \begin{block}{Conclusion}
        Understanding and applying value iteration allows for a systematic approach to complex decision-making processes in various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations - Overview}
    \begin{itemize}
        \item Dynamic programming (DP) is a method for solving complex problems by breaking them into simpler subproblems.
        \item Key focus: Mathematical expressions and notations related to:
        \begin{itemize}
            \item Value Functions
            \item Bellman's Equation
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Function}
    \begin{block}{Definition}
        The value function \( V(s) \) quantifies the maximum expected return from state \( s \) under policy \( \pi \).
    \end{block}
    \begin{itemize}
        \item Notation:
        \begin{itemize}
            \item \( V(s) \): Value function of state \( s \)
            \item \( R(s, a) \): Reward after action \( a \) in state \( s \)
            \item \( P(s' | s, a) \): Transition probability to state \( s' \) after action \( a \) in state \( s \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman's Equation}
    \begin{block}{Definition}
        Bellman's equation relates the value of a state to values of subsequent states.
    \end{block}
    \begin{equation}
        V(s) = \max_{a} \left( R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right)
    \end{equation}
    \begin{itemize}
        \item Components:
        \begin{itemize}
            \item \( \max_{a} \): Maximum over actions \( a \)
            \item \( R(s, a) \): Immediate reward for action \( a \)
            \item \( \gamma \): Discount factor \( (0 \leq \gamma < 1) \)
            \item \( \sum_{s'} P(s' | s, a) V(s') \): Expected future value
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of the Discount Factor}
    \begin{itemize}
        \item Determines future reward valuation:
        \begin{itemize}
            \item \( \gamma = 0 \): Focus on immediate rewards.
            \item \( \gamma \to 1 \): Values future rewards, promoting long-term strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    \begin{itemize}
        \item Consider an agent in state \( s \) with two actions:
        \begin{itemize}
            \item Action 1: 
            \[
            V(s, a_1) = 5 + \gamma (0.5 \cdot 10) = 5 + 5\gamma
            \]
            \item Action 2: 
            \[
            V(s, a_2) = 2 + \gamma (0.3 \cdot 8) = 2 + 2.4\gamma
            \]
        \end{itemize}
        \item Maximizing \( V \) leads to an optimal policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item Mastery of value functions is crucial for understanding future implications of decisions.
        \item Bellman's equation forms the foundation of DP, enabling optimal policy derivation.
        \item Discount factor \( \gamma \) shapes decision-making strategies between short-term and long-term rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Methods - Overview}
    \begin{itemize}
        \item Monte Carlo methods utilize random sampling to obtain numerical results.
        \item Widely used in reinforcement learning (RL) for:
        \begin{itemize}
            \item Estimating value functions
            \item Optimizing policies
        \end{itemize}
        \item Particularly advantageous in complex or incomplete environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Methods - Key Concepts}
    \begin{enumerate}
        \item \textbf{Random Sampling}: 
            \begin{itemize}
                \item Explore state-space using random samples.
                \item Estimate expected returns and evaluate policies.
            \end{itemize}
        \item \textbf{Episodes and Returns}:
            \begin{itemize}
                \item An episode is a sequence of states, actions, and rewards.
                \item The return \( G_t \) from time step \( t \) is:
                \begin{equation}
                    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
                \end{equation}
                where \( \gamma \) is the discount factor.
            \end{itemize}
        \item \textbf{Policy Evaluation}: 
            \begin{itemize}
                \item Estimate values by averaging returns from multiple episodes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo vs Dynamic Programming}
    \begin{block}{Differences}
        \begin{tabular}{|l|l|}
            \hline
            \textbf{Dynamic Programming} & \textbf{Monte Carlo Methods} \\
            \hline
            Requires complete environment model. & Requires only access to episodes. \\
            Operates on full state space representation. & Relies on sampling from episodes. \\
            Involves iterative value updates. & Values updated after episode completion. \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Methods - Summary}
    \begin{itemize}
        \item \textbf{Convergence}: Converge to true value function with more samples.
        \item \textbf{Usage}: Ideal for complex dynamics and situations with knowledge gaps.
    \end{itemize}
    
    \textbf{Example in Pseudocode:}
    \begin{lstlisting}[language=Python]
for each episode:
    Initialize state S
    while S is not terminal:
        Choose action A based on policy Ï€
        Take action A, observe reward R and next state S'
        Store the experience (S, A, R, S')
        S = S'
    
    Calculate return G for each state-action pair in the episode
    Update value estimates for each state-action pair based on G
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Policy Evaluation}
    \begin{block}{Understanding Monte Carlo Policy Evaluation}
        Monte Carlo Policy Evaluation is a fundamental technique in reinforcement learning used to estimate the value functions of a policy through the analysis of sampled returns from episodes. This method differs from dynamic programming, as it relies on random sampling rather than requiring a complete model of the environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Value Function}: Represents the expected return for an agent from a given state following a specific policy.
            \begin{itemize}
                \item State Value Function: \( V^\pi(s) \)
                \item Action Value Function: \( Q^\pi(s, a) \)
            \end{itemize}
        \item \textbf{Policy}: Strategy for determining actions based on states, denoted as \( \pi(a|s) \).
        \item \textbf{Sample Return}: The total reward after taking an action and following the policy until a terminal state:
            \[
            G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
            \]
            where \( R_t \) is the reward at time \( t \) and \( \gamma \) is the discount factor (0 â‰¤ \( \gamma \) < 1).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Policy Evaluation Process}
    \begin{enumerate}
        \item \textbf{Collect Episodes}: Generate multiple episodes by following the current policy \( \pi \).
        \item \textbf{Calculate Sample Returns}: Compute total returns \( G_t \) for each state \( s \) encountered.
        \item \textbf{Average Returns}: Update the estimated value for state \( s \):
            \[
            V^\pi(s) = \frac{1}{N} \sum_{i=1}^{N} G_t^i
            \]
            where \( N \) is the visits to state \( s \).
        \item \textbf{Iterate}: Repeat the process to improve value function estimates.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation - Introduction}
    In many decision-making scenarios, particularly within Monte Carlo methods in Reinforcement Learning (RL), there exists a crucial trade-off between exploration and exploitation. 

    \begin{itemize}
        \item \textbf{Exploration:} Trying new actions to gather information about the environment or reward structure.
        \item \textbf{Exploitation:} Choosing actions that maximize immediate rewards based on current knowledge.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation - The Trade-Off}
    \begin{enumerate}
        \item \textbf{Why Exploration?}
        \begin{itemize}
            \item \textbf{Acquire Information:} Discover potential strategies that may outperform current actions.
            \item \textbf{Avoid Local Optima:} Solely exploiting known information can lead to suboptimal strategies.
        \end{itemize}

        \item \textbf{Why Exploitation?}
        \begin{itemize}
            \item \textbf{Maximize Rewards:} Focus on known successful actions for higher short-term rewards.
            \item \textbf{Efficiency:} Exploitation is more efficient in stable environments with well-understood reward structures.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation - Strategies}
    \textbf{Balancing the Two:}
    Finding an optimal strategy requires balancing exploration and exploitation. Here are some strategies:

    \begin{block}{Epsilon-Greedy Strategy}
        Most of the time ($1 - \epsilon$), choose the action with the highest estimated value (exploitation). With probability $\epsilon$, choose randomly (exploration).

        \begin{lstlisting}[language=Python]
def epsilon_greedy_action(Q, epsilon):
    if random.random() < epsilon:
        return random.choice(all_actions)  # Explore
    else:
        return np.argmax(Q)  # Exploit
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Softmax Action Selection}
        Actions receive probabilities based on their estimated values, facilitating proportional exploration according to confidence.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Dynamic Programming and Monte Carlo - Introduction}
    \begin{block}{Overview}
        Dynamic Programming (DP) and Monte Carlo methods are powerful techniques for solving complex problems across various fields. Understanding their applications highlights their versatility and reinforces their theoretical foundations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Dynamic Programming (DP):}
            \begin{itemize}
                \item A method for solving complex problems by breaking them down into simpler subproblems.
                \item Particularly useful in scenarios with overlapping subproblems.
            \end{itemize}
        \item \textbf{Monte Carlo Methods:}
            \begin{itemize}
                \item Rely on random sampling and statistical modeling to estimate numerical results.
                \item Ideal when computing exact results is infeasible; allows approximation through simulations.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}
    \begin{enumerate}
        \item \textbf{Gaming:}
            \begin{itemize}
                \item \textbf{DP Application:} Optimize strategies in games like chess and Go using DP techniques.
                \item \textbf{Monte Carlo Application:} Use Monte Carlo Tree Search (MCTS) for decision-making through random simulations of game outcomes.
            \end{itemize}
        \item \textbf{Robotics:}
            \begin{itemize}
                \item \textbf{DP Application:} A* algorithm for motion planning using DP for shortest path evaluation.
                \item \textbf{Monte Carlo Application:} Monte Carlo Localization for positional inference based on sensor data.
            \end{itemize}
        \item \textbf{Finance:}
            \begin{itemize}
                \item \textbf{DP Application:} Black-Scholes model for optimizing financial derivatives pricing.
                \item \textbf{Monte Carlo Application:} Simulate financial market behavior to predict option values.
            \end{itemize}
        \item \textbf{Healthcare:}
            \begin{itemize}
                \item \textbf{DP Application:} Optimize treatment pathways for improved patient outcomes.
                \item \textbf{Monte Carlo Application:} Estimate treatment outcomes in clinical trials through simulation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Introduction}
    \begin{block}{Overview}
        Performance metrics are essential tools for evaluating the effectiveness of algorithms in dynamic programming (DP) and Monte Carlo methods. 
    \end{block}

    \begin{itemize}
        \item Crucial for assessing convergence and accuracy.
        \item Helps in selecting appropriate algorithms for problem-solving.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Convergence}
    \begin{block}{Convergence}
        \begin{itemize}
            \item \textbf{Definition:} Approaches a final solution as iterations progress.
            \item \textbf{Importance:} Indicates reliability; consistent results over time.
        \end{itemize}
        
        \textbf{Example:} In value iteration for Markov Decision Processes (MDPs):
        \begin{equation}
            \max |V_{new} - V_{old}| < \epsilon
        \end{equation}
        Convergence occurs when the maximum change in value \(V\) approaches 0.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Accuracy}
    \begin{block}{Accuracy}
        \begin{itemize}
            \item \textbf{Definition:} Measures how closely the output matches the true solution.
            \item \textbf{Importance:} High accuracy ensures solutions are correct.
        \end{itemize}
        
        \textbf{Example:} Estimating the value of $\pi$ using Monte Carlo methods:
        \begin{equation}
            \text{Estimated } \pi = 4 \cdot \frac{N_{inside}}{N_{total}}
        \end{equation}
        where \(N_{inside}\) is the number of points inside the unit circle.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Key Metrics}
    \begin{itemize}
        \item \textbf{Mean Squared Error (MSE):}
        \begin{equation}
            MSE = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
        \end{equation}
        A lower MSE indicates better accuracy.

        \item \textbf{Time Complexity:} Evaluates runtime growth relative to input size.
        \begin{itemize}
            \item Example: DP solution may have polynomial complexity \(O(n^2)\).
        \end{itemize}

        \item \textbf{Sample Variance:}
        \begin{equation}
            Var(X) = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \bar{x})^2
        \end{equation}
        Represents the spread of outcomes in Monte Carlo methods.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Summary}
    \begin{itemize}
        \item \textbf{Convergence:} Vital for algorithm reliability.
        \item \textbf{Accuracy:} Ensures meaningful solutions.
        \item Understanding performance metrics aids in selecting the right algorithm and highlights potential improvements.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics - Conclusion}
    \begin{block}{Conclusion}
        By mastering performance metrics such as convergence and accuracy, practitioners can critically assess algorithm effectiveness, leading to better solutions in complex scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications - Overview}
    \begin{itemize}
        \item Discussion on the ethical implications of dynamic programming (DP) and Monte Carlo (MC) methods
        \item Focus on biases in reinforcement learning (RL) models
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding DP and MC Methods}
    \begin{block}{Dynamic Programming (DP)}
        \begin{itemize}
            \item Solves complex problems by breaking them down into simpler subproblems.
            \item Each subproblem is solved once and stored for efficiency.
        \end{itemize}
    \end{block}

    \begin{block}{Monte Carlo Methods}
        \begin{itemize}
            \item Algorithms that rely on repeated random sampling to obtain numerical results.
            \item Particularly useful for optimization and decision-making under uncertainty.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in RL Models}
    \begin{itemize}
        \item \textbf{Bias in RL Models:}
        \begin{itemize}
            \item Systematic favoritism in AI decision-making can lead to unfair outcomes.
            \item Sources of bias:
            \begin{itemize}
                \item Data bias: Non-representative datasets.
                \item Algorithm bias: Design of DP or MC methods may favor specific outcomes.
            \end{itemize}
        \end{itemize}

        \item \textbf{Consequences of Bias:}
        \begin{itemize}
            \item Reproducing social inequality.
            \item Loss of trust in automated systems.
            \item Increased legal and regulatory scrutiny.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Approaches to Address Bias}
    \begin{itemize}
        \item \textbf{Diverse Data Collection:} Ensure datasets represent various population segments.
        \item \textbf{Model Transparency:} Develop models transparently for better auditing and scrutiny.
        \item \textbf{Fairness Constraints:} Incorporate fairness metrics into the reward structure of RL models.
    \end{itemize}

    \begin{block}{Final Thoughts}
        \begin{itemize}
            \item Enhancing decision-making with DP and MC methods demands awareness of ethical implications.
            \item Striving for fairness and inclusivity is a social responsibility.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Introduction}
    \begin{block}{Overview}
        Dynamic Programming (DP) and Monte Carlo (MC) methods are powerful computational techniques utilized across various fields. 
        This presentation highlights case studies exemplifying their application in solving complex problems through:
    \end{block}
    \begin{itemize}
        \item Optimizing decisions
        \item Forecasting outcomes
        \item Managing uncertainty
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Optimal Inventory Management}
    \begin{block}{Case Study 1: Optimal Inventory Management with Dynamic Programming}
        \textbf{Context:} A retail company aims to determine the optimal order quantity for seasonal products to maximize profit and minimize inventory costs.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Dynamic Programming Approach:}
        \begin{itemize}
            \item \textbf{States:} Current inventory level, lead time
            \item \textbf{Decision Variables:} Order quantity
            \item \textbf{Recurrence Relation:} 
            \begin{equation}
                V(i) = \max_{q} \left\{ P \cdot D(q) - C(q) + V(i + q - D(q)) \right\}
            \end{equation}
            Where: 
            \begin{itemize}
                \item \( V(i) \): expected profit from inventory level \( i \)
                \item \( P \): selling price
                \item \( C \): costs associated with ordering
            \end{itemize}
        \end{itemize}
        \item \textbf{Outcome:} The DP model reduces costs and increases revenues through structured decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Game AI Development}
    \begin{block}{Case Study 2: Game AI Development using Monte Carlo Methods}
        \textbf{Context:} A gaming company employs Monte Carlo methods to enhance character decision-making in a strategic game.
    \end{block}

    \begin{itemize}
        \item \textbf{Monte Carlo Approach:}
        \begin{itemize}
            \item Perform simulations of thousands of game outcomes based on different strategies.
            \item Estimate values using the average outcome from these simulations.
        \end{itemize}
        \item \textbf{Key Steps:}
        \begin{enumerate}
            \item \textbf{Rollout:} Randomly simulate the game from a given state to compute immediate rewards.
            \item \textbf{Update Strategy:} Adjust decisions based on the average reward from simulations.
        \end{enumerate}
        \item \textbf{Outcome:} Allows for the analysis of complex strategies, improving gameplay quality.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Key Takeaways}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Dynamic Programming:} Efficient for problems with overlapping subproblems and optimal substructure, e.g., inventory management.
            \item \textbf{Monte Carlo Methods:} Stochastic approach, ideal for problems with uncertainty, such as games.
            \item Both methods significantly enhance decision-making processes, showcasing their versatility.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        DP excels in deterministic problems, while MC methods are suited for handling randomness and uncertainty. Understanding these methodologies enhances their application in real-world contexts.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 1}
    
    \begin{block}{Dynamic Programming (DP)}
        \begin{enumerate}
            \item \textbf{Definition:} 
            Dynamic Programming is an optimization method in algorithm design that breaks problems into smaller subproblems, solving each once and storing their solutions.
            
            \item \textbf{Key Concepts:}
            \begin{itemize}
                \item \textit{Optimal Substructure:} The optimal solution can be constructed from optimal solutions of its subproblems.
                \item \textit{Overlapping Subproblems:} Problems that can be broken down into subproblems reused multiple times.
            \end{itemize}
            
            \item \textbf{DP Approach:}
            \begin{itemize}
                \item Identify the optimal solution structure.
                \item Define the value of the optimal solution recursively.
                \item Implement using memoization (top-down) or tabulation (bottom-up).
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 2}
    
    \begin{block}{Example â€“ Fibonacci Numbers}
        \begin{itemize}
            \item Recursive approach may lead to exponential time complexity, while DP reduces it to linear time:
            \begin{equation}
                F(n) = F(n-1) + F(n-2) 
            \end{equation}
            \item DP Array Initialization: 
            \begin{equation}
                dp[0] = 0, \quad dp[1] = 1 
            \end{equation}
            \item Compute using:
            \begin{lstlisting}[language=Python]
for i in range(2, n+1):
    dp[i] = dp[i-1] + dp[i-2]
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 3}
    
    \begin{block}{Monte Carlo Methods}
        \begin{enumerate}
            \item \textbf{Definition:} 
            Monte Carlo Methods rely on repeated random sampling to obtain numerical results, useful for complex problem spaces.
            
            \item \textbf{Key Concepts:}
            \begin{itemize}
                \item \textit{Random Sampling:} Using randomness to estimate results.
                \item \textit{Law of Large Numbers:} As sample size increases, estimates converge to expected values.
            \end{itemize}
            
            \item \textbf{Applications:} 
            Used in finance (option pricing), physics simulations, risk assessment.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Monte Carlo Integration}
        A specific application to estimate definite integrals, for example:
        \begin{equation}
            \int_0^1 x^2 dx
        \end{equation}
        \begin{itemize}
            \item Sample uniformly random points \( x_i \) in \( [0, 1] \), compute \( f(x_i) = x_i^2 \), and average:
            \begin{equation}
                \text{Estimated Integral} = \frac{1}{N} \sum_{i=1}^{N} f(x_i)
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Conclusion}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item \textbf{Dynamic Programming} is essential for optimization problems by avoiding redundant calculations.
            \item \textbf{Monte Carlo Methods} provide techniques for solving complex problems through randomness, useful in simulations.
            \item Understanding when to apply each method is crucial for efficient algorithms.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Both Dynamic Programming and Monte Carlo Methods have wide applications and offer powerful tools to tackle diverse computational challenges. Mastering these techniques will enhance problem-solving skills in various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session}
    \begin{block}{Overview}
        This Q\&A session provides an opportunity for students to clarify concepts and address uncertainties regarding:
        \begin{itemize}
            \item Dynamic Programming (DP)
            \item Monte Carlo Methods (MCM)
        \end{itemize}
        Engaging with your peers and instructor can help reinforce your understanding and application of these techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming (DP)}
    \begin{block}{Definition}
        DP is an optimization approach used to solve complex problems by breaking them down into simpler subproblems.
    \end{block}
    \begin{itemize}
        \item \textbf{Overlapping Subproblems:} Seen in Fibonacci number calculation and shortest path algorithms.
        \item \textbf{Optimal Substructure:} The optimal solution can be constructed from optimal solutions of its subproblems.
    \end{itemize}
    \begin{block}{Example: Rod Cutting}
        \begin{lstlisting}[language=Python]
def rod_cutting(prices, n):
    if n == 0:
        return 0
    max_val = float('-inf')
    for i in range(1, n + 1):
        max_val = max(max_val, prices[i - 1] + rod_cutting(prices, n - i))
    return max_val
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Methods (MCM)}
    \begin{block}{Definition}
        MCM are stochastic techniques used to understand the impact of risk and uncertainty in prediction and forecasting models.
    \end{block}
    \begin{itemize}
        \item \textbf{Random Sampling:} Used to simulate complex systems and model probabilities.
        \item \textbf{Applications:} Financial modeling, risk assessment, and optimization problems.
    \end{itemize}
    \begin{block}{Example: Estimating $\pi$}
        \begin{lstlisting}[language=Python]
import random
def estimate_pi(num_samples):
    inside_circle = sum(1 for _ in range(num_samples)
                         if (random.random()**2 + random.random()**2) <= 1)
    return (4 * inside_circle) / num_samples
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engaging Questions}
    Discuss the following to clarify doubts:
    \begin{itemize}
        \item What specific areas of DP or MCM are you struggling with?
        \item Can you provide an example of where you've seen these methods in real-world scenarios?
        \item How do the principles of optimal substructure and overlapping subproblems manifest in your projects?
    \end{itemize}
    Letâ€™s engage in a discussion and clarify any doubts you might have!
\end{frame}


\end{document}