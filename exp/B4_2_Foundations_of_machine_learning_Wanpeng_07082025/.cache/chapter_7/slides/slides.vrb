\frametitle{Deep Q-Networks (DQN) and Policy Gradients}
    \begin{itemize}
        \item \textbf{Deep Q-Networks (DQN):}
        \begin{itemize}
            \item \textbf{Definition:} An extension of Q-learning that uses deep neural networks to approximate the Q-value function instead of maintaining a Q-table.
            \item \textbf{Key Points:}
            \begin{itemize}
                \item Combines Q-learning with deep learning techniques.
                \item Uses experience replay to store previous experiences, improving learning efficiency.
                \item Implements target networks to stabilize training by updating Q-values less frequently.
            \end{itemize}
            \item \textbf{Example:} DQNs have been successfully used in playing video games like Atari, where the network learns to evaluate the best actions based on visual input rather than discrete states.
        \end{itemize}

        \item \textbf{Policy Gradients:}
        \begin{itemize}
            \item \textbf{Definition:} A family of algorithms that optimize the policy directly instead of estimating the value function. The policy is a probability distribution over actions given a state.
            \item \textbf{Key Formula:}
            \begin{equation}
                J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} r_t \right]
            \end{equation}
            \begin{itemize}
                \item Where:
                \begin{itemize}
                    \item $J(\theta)$ = expected return for the policy parameterized by $\theta$
                    \item $\tau$ = trajectory followed by the agent
                    \item $r_t$ = reward at time step $t$
                \end{itemize}
            \end{itemize}
            \item \textbf{Example:} In robotics, policy gradient methods can train a robot to complete tasks through trial and error by continuously updating its approach based on the effectiveness of actions taken.
        \end{itemize}
    \end{itemize}
