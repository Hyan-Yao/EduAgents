\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Supervised Learning Techniques]{Weeks 4-8: Supervised Learning Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning}
    \begin{block}{Overview of Supervised Learning}
        Supervised Learning is a predominant approach in machine learning where the model is trained on a labeled dataset. This technique is vital for applications in AI as it allows computers to make predictions based on data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Supervised Learning}
    \begin{enumerate}
        \item \textbf{Definition of Supervised Learning}: 
            \begin{itemize}
                \item Utilizes a labeled dataset for training.
                \item Each instance includes input data and a corresponding output label.
                \item The goal is to learn a function that can generalize new inputs.
            \end{itemize}
        
        \item \textbf{Labeled Data}: 
            \begin{itemize}
                \item Comprises features (inputs) and labels (outputs).
                \item Example: Emails labeled as 'spam' or 'not spam'.
            \end{itemize}
        
        \item \textbf{Model Training}: 
            \begin{itemize}
                \item Involves feeding training data to the model.
                \item The model adjusts parameters to minimize prediction errors.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Supervised Learning}
    \begin{block}{Real-World Applications}
        \begin{itemize}
            \item \textbf{Classification Tasks}: E.g., image recognition (cats vs. dogs).
            \item \textbf{Regression Tasks}: E.g., predicting house prices based on features.
        \end{itemize}
    \end{block}

    \begin{block}{Performance Measurement}
        Evaluate models using metrics like accuracy, precision, recall, and F1 score.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Supervised Learning}
    \begin{enumerate}
        \item \textbf{Classification Example}:
            \begin{itemize}
                \item Dataset of bank customers with features like age and income, labeled for loan defaults.
                \item Algorithm predicts default risk for new customers.
            \end{itemize}
        
        \item \textbf{Regression Example}:
            \begin{itemize}
                \item Housing price dataset with features like square footage and number of bedrooms.
                \item Model predicts prices based on learned correlations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Supervised learning requires a labeled dataset for effective training.
        \item Applicable to various real-world scenarios, including both classification and regression.
        \item Performance is quantified with multiple metrics, crucial for model evaluation.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Supervised learning is foundational in machine learning, essential for developing systems that make informed decisions based on historical data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula and Code Snippet}
    \begin{block}{Mean Squared Error (MSE)}
        \begin{equation}
            MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \end{equation}
        Where \(y_i\) is the actual output, \(\hat{y}_i\) is the predicted output, and \(n\) is the number of observations.
    \end{block}

    \begin{block}{Sample Python Code for Linear Regression}
        \begin{lstlisting}[language=Python]
from sklearn.linear_model import LinearRegression
import numpy as np

# Sample data
X = np.array([[1], [2], [3], [4]])
y = np.array([1, 2, 3, 4])

# Creating a Linear Regression model
model = LinearRegression().fit(X, y)

# Predicting based on new data
predictions = model.predict(np.array([[5], [6]]))
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Supervised Learning - Definition}
    \begin{block}{Definition of Supervised Learning}
        Supervised learning is a type of machine learning where a model is trained on a dataset that includes both input features and their corresponding output labels. The primary goal is to learn a mapping from inputs to outputs so that the model can make accurate predictions on unseen data.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Point:} Training under supervision means the model learns from labeled examples.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Supervised Learning - Labeled Data}
    \begin{block}{Labeled Data}
        Labeled data consists of data points that are associated with their corresponding output. Each instance in the dataset includes both the features (input) and the label (output), which represents the correct answer.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Example:} 
        \begin{itemize}
            \item \textbf{Features:} Size (square footage), Location, Number of Bedrooms
            \item \textbf{Label:} Actual price of the house
            \item \textbf{Input:} (Size: 2000 sq ft, Location: Suburb, Bedrooms: 3)
            \item \textbf{Output:} Price: \$300,000
        \end{itemize}
        \item \textbf{Importance:} Quality labeled data is critical for the success of supervised learning models.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Supervised Learning - Model Training}
    \begin{block}{Model Training}
        Model training is the process of using the labeled data to teach the model how to predict the output from the input. This involves feeding the data into an algorithm that adjusts its parameters based on the loss (error) in predicting the labels.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Steps in Model Training:}
        \begin{enumerate}
            \item Data Preparation: Clean and preprocess the data to ensure quality.
            \item Algorithm Selection: Choose an appropriate algorithm (e.g., decision trees, support vector machines, neural networks).
            \item Training Process:
            \begin{itemize}
                \item The model makes initial predictions and receives feedback based on accuracy.
                \item It then iteratively adjusts its parameters using techniques like gradient descent.
            \end{itemize}
        \end{enumerate}
        \item \textbf{Formula for Loss Function (Example using Mean Squared Error):}
        \begin{equation}
        Loss = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
        \end{equation}
        where \( y_i \) is the true label and \( \hat{y}_i \) is the predicted label.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Introduction}
    \begin{block}{What is Linear Regression?}
        Linear Regression is a fundamental supervised learning algorithm used for predicting continuous outcomes. It establishes a linear relationship between a dependent variable (target) and one or more independent variables (predictors).
    \end{block}
    \begin{block}{Objective}
        The goal is to find the best-fitting line through the data points, minimizing the differences between observed and predicted values.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Key Concepts}
    \begin{itemize}
        \item \textbf{Dependent Variable (Y)}: The outcome we wish to predict (e.g., house prices).
        \item \textbf{Independent Variables (X)}: Predictors influencing the dependent variable (e.g., size of the house, number of bedrooms).
        \item \textbf{Equation}:
        \begin{equation}
            Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
        \end{equation}
        where:
        \begin{itemize}
            \item \( Y \): Predicted outcome
            \item \( \beta_0 \): Intercept
            \item \( \beta_1, \beta_2, ..., \beta_n \): Coefficients
            \item \( X_1, X_2, ..., X_n \): Independent variables
            \item \( \epsilon \): Error term
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Assumptions and Applications}
    \begin{block}{Assumptions of Linear Regression}
        \begin{enumerate}
            \item \textbf{Linearity}: Relationship should be linear.
            \item \textbf{Independence}: Residuals should not be correlated.
            \item \textbf{Homoscedasticity}: Constant variance of residuals.
            \item \textbf{Normality}: Residuals should be approximately normally distributed.
        \end{enumerate}
    \end{block}

    \begin{block}{Applications}
        \begin{itemize}
            \item \textbf{Real Estate}: Predicting property prices.
            \item \textbf{Finance}: Estimating future stock prices.
            \item \textbf{Healthcare}: Understanding impacts on patient outcomes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations of Linear Regression - Overview}
    
    \begin{block}{Overview of Linear Regression}
        Linear regression is a supervised learning technique used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship.
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations of Linear Regression - Cost Function}
    
    \begin{block}{Cost Function}
        The cost function quantifies how well our linear model fits the data. 
        
        \textbf{Definition:} The most common cost function for linear regression is the Mean Squared Error (MSE), given by:
        
        \begin{equation}
            J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
        \end{equation}
        
        where:
        \begin{itemize}
            \item \( J(\theta) \): Cost function
            \item \( m \): Number of training examples
            \item \( y_i \): Actual outputs
            \item \( \hat{y}_i = \theta_0 + \theta_1 x_i \): Predicted outputs
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations of Linear Regression - Optimization and Least Squares Method}
    
    \begin{block}{Goal of Optimization}
        The main goal in linear regression is to \textbf{optimize the model parameters} (denoted as \(\theta\)) to minimize the cost function:
        
        \begin{itemize}
            \item Find \( \theta_0 \) (intercept) and \( \theta_1 \) (slope) that minimize \( J(\theta) \).
        \end{itemize}
    \end{block}
    
    \begin{block}{Least Squares Method}
        The Least Squares Method is a standard approach to minimize differences between observed and predicted values:
        
        \begin{itemize}
            \item For each data point, calculate the difference between actual and predicted values.
            \item Square these differences to ensure they are positive.
            \item Optimize parameters to minimize the sum of squared differences.
        \end{itemize}
        
        The optimal parameters can be calculated using:
        
        \begin{equation}
            \theta = (X^T X)^{-1} X^T y
        \end{equation}
        
        where:
        \begin{itemize}
            \item \( X \): Matrix of input features
            \item \( y \): Vector of observed outputs
            \item \( \theta \): Vector of parameter estimates
        \end{itemize}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Overview}
    \begin{block}{What is Linear Regression?}
        Linear Regression is a fundamental algorithm in supervised learning that models the relationship between a dependent variable (target) and one or more independent variables (predictors). 
    \end{block}
    
    \begin{block}{Equation of Linear Regression}
        \begin{equation}
            y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon 
        \end{equation}
        Where:  
        \begin{itemize}
            \item \(y\) = dependent variable  
            \item \(x_n\) = independent variables  
            \item \(\beta_0\) = y-intercept  
            \item \(\beta_n\) = coefficients  
            \item \(\epsilon\) = error term  
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Steps}
    \begin{enumerate}
        \item \textbf{Importing Libraries:} Load necessary libraries for data manipulation and modeling.
        \begin{lstlisting}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
        \end{lstlisting}
        
        \item \textbf{Loading the Data:} Use a CSV file to load your dataset.
        \begin{lstlisting}
data = pd.read_csv('your_data.csv')
print(data.head())
        \end{lstlisting}
        
        \item \textbf{Preparing the Data:} Select features and target variable.
        \begin{lstlisting}
X = data[['Feature1', 'Feature2']]  # Example feature selection
y = data['Target']
        \end{lstlisting}
        
        \item \textbf{Splitting the Data:} Divide data into training and testing sets.
        \begin{lstlisting}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Completion}
    \begin{enumerate}[resume]
        \item \textbf{Creating the Model:} Instantiate the Linear Regression model.
        \begin{lstlisting}
model = LinearRegression()
        \end{lstlisting}
        
        \item \textbf{Fitting the Model:} Fit the model to the training data.
        \begin{lstlisting}
model.fit(X_train, y_train)
        \end{lstlisting}
        
        \item \textbf{Making Predictions:} Use the model to predict on the testing set.
        \begin{lstlisting}
predictions = model.predict(X_test)
        \end{lstlisting}
       
        \item \textbf{Evaluating the Model:} Assess performance with metrics like MAE or R-squared.
        \begin{lstlisting}
from sklearn.metrics import mean_absolute_error, r2_score

mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f'MAE: {mae}, R-squared: {r2}')
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Introduction}
    \begin{block}{Overview}
        Logistic regression is a statistical method used for binary classification problems, where the outcome variable can take on two possible values (coded as 0 and 1). It predicts the probability that a given input belongs to a certain category rather than predicting continuous outcomes like linear regression.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Key Concepts}
    \begin{enumerate}
        \item \textbf{Binary Classification}:
        \begin{itemize}
            \item Used to categorize observations into one of two classes (e.g., spam detection, disease diagnosis).
        \end{itemize}
        \item \textbf{Logistic Function (Sigmoid Function)}:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-z}}
        \end{equation}
        where \( z = \beta_0 + \beta_1 X_1 + ... + \beta_n X_n \).
        \item \textbf{Decision Boundary}:
        \begin{itemize}
            \item Separates two classes based on a cutoff probability (commonly 0.5).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Example}
    \begin{block}{Example: Student Exam Pass Prediction}
        \begin{itemize}
            \item Predicting whether a student will pass (1) or fail (0) based on hours studied.
            \item Example equation: 
            \begin{equation}
                z = -4 + 0.8 \times \text{hours\_studied}
            \end{equation}
            \item Predict the probability for a student who studies 10 hours:
            \begin{equation}
                z = -4 + 0.8 \times 10 = 4
            \end{equation}
            \begin{equation}
                P(Y=1 | X) = \frac{1}{1 + e^{-4}} \approx 0.982
            \end{equation}
            \item \textbf{Interpretation:} 98.2\% probability that the student will pass.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Implementation}
    \begin{block}{Python Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression

# Training data: hours studied (X) and pass/fail labels (y)
X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]
y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]

# Create and fit the model
model = LogisticRegression()
model.fit(X, y)

# Make a prediction
probability = model.predict_proba([[10]])[0][1]
print(f"Probability of passing for a student who studies 10 hours: {probability:.2f}")
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding the Logistic Function - Overview}
    
    \begin{block}{What is the Logistic Function?}
        The logistic function, denoted as \( S(x) \), models the probability of a binary outcome (0 or 1). It maps real-valued numbers into the range between 0 and 1, defined as:
        \[
        S(x) = \frac{1}{1 + e^{-x}}
        \]
        where \( e \) is the base of the natural logarithm and \( x \) is a linear combination of input features.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding the Logistic Function - Characteristics}

    \begin{enumerate}
        \item \textbf{S-shaped Curve}: The logistic function produces an S-shaped (sigmoid) curve, approaching 0 as \( x \to -\infty \) and 1 as \( x \to +\infty \).
        
        \item \textbf{Decision Threshold}: The decision boundary is typically at \( S(x) = 0.5 \):
        \begin{itemize}
            \item If \( S(x) > 0.5 \), classify as 1 (positive class).
            \item If \( S(x) \leq 0.5 \), classify as 0 (negative class).
        \end{itemize}
        
        \item \textbf{Interpretability}: Outputs from the logistic function represent the probability of being in the positive class, facilitating easier interpretation of results.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding the Logistic Function - Example and Conclusion}

    \begin{block}{Example of the Logistic Function}
        Applying the logistic function to a linear equation \( z = w_0 + w_1x_1 + w_2x_2 \):
        \[
        P(y=1|x) = S(z) = \frac{1}{1 + e^{-z}}
        \]
        
        **Example Values**:
        \begin{itemize}
            \item If \( z = 0 \): \( P(y=1|x) = 0.5 \)
            \item If \( z = 2 \): \( P(y=1|x) \approx 0.88 \)
            \item If \( z = -2 \): \( P(y=1|x) \approx 0.12 \)
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        The logistic function is essential for transforming outputs of linear equations into probabilities. Understanding the decision boundary at \( S(x) = 0.5 \) is crucial for binary classification tasks.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Logistic Regression}
    % Guidelines for implementing logistic regression with Python.
    Logistic regression is a statistical method for binary classification, estimating probabilities of outcomes.

    In this section, we will:
    \begin{itemize}
        \item Explore the logistic function and decision boundary
        \item Discuss steps for implementation in Python
        \item Provide an example of application
        \item Summarize key points and conclusions
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Logistic Function}: Given by:
        \begin{equation}
        S(t) = \frac{1}{1 + e^{-t}}
        \end{equation}
        Outputs values between 0 and 1 for modeling probabilities.
        
        \item \textbf{Decision Boundary}: The point where predicted probability equals 0.5, dividing classes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps for Implementation in Python}
    \begin{enumerate}
        \item \textbf{Import Necessary Libraries}:
        \begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
        \end{lstlisting}

        \item \textbf{Load and Prepare Data}:
        \begin{lstlisting}[language=Python]
data = pd.read_csv('health_data.csv')
X = data[['age', 'blood_pressure', 'cholesterol']]
y = data['disease_present']  # binary: 0 = No, 1 = Yes
        \end{lstlisting}

        \item \textbf{Split Data}:
        \begin{lstlisting}[language=Python]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps for Implementation in Python (cont)}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering
        \item \textbf{Create and Train the Model}:
        \begin{lstlisting}[language=Python]
model = LogisticRegression()
model.fit(X_train, y_train)
        \end{lstlisting}

        \item \textbf{Make Predictions}:
        \begin{lstlisting}[language=Python]
y_pred = model.predict(X_test)
        \end{lstlisting}

        \item \textbf{Evaluate the Model}:
        \begin{lstlisting}[language=Python]
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", cm)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Example Scenario}
    Imagine you're a healthcare data analyst determining whether patients are likely to have a disease based on various measurements.

    After implementing logistic regression, you find:
    - An accuracy of 85\%
    - A confusion matrix indicating the model's precision and recall for each class.
    
    This outcome helps your medical team identify patients needing further examination.
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Logistic regression is effective for binary classification tasks.
        \item Model performance can be evaluated using metrics like accuracy and confusion matrix.
        \item Proper data preprocessing and feature selection enhance outcomes.
    \end{itemize}

    By understanding the logistic function and decision boundary, along with practical steps in Python, you can effectively implement logistic regression for predictive analytics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Introduction}
    \begin{block}{Introduction to Decision Trees}
        Decision Trees are powerful and intuitive supervised learning models used for classification and regression tasks. They structure data in a tree-like diagram, splitting it into subsets based on feature values—ultimately leading to predictions or decisions. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Structure}
    \begin{block}{Structure of Decision Trees}
        \begin{itemize}
            \item \textbf{Nodes:}
                \begin{itemize}
                    \item \textbf{Root Node:} The top node representing the entire dataset.
                    \item \textbf{Decision Nodes:} Intermediate nodes that split the data into subsets based on feature values.
                    \item \textbf{Leaf Nodes:} Terminal nodes that provide the final output or classification.
                \end{itemize}
            \item \textbf{Branches:} Show the flow from a node to another (indicating how the data is split).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Simple Decision Tree}
    \begin{center}
        \begin{verbatim}
          [Root: Weather]
              /       \
           Sunny      Rainy
           /   \         |
       [Humidity]   [Windy]
           /  \         / \
         High  Normal  Weak Strong
           |      |    |      |
         No      Yes  Yes     No
        \end{verbatim}
    \end{center}
    \begin{block}{Explanation}
        In this example:
        \begin{itemize}
            \item The decision tree starts with the root node "Weather."
            \item If the weather is "Sunny," it splits further into "Humidity."
            \item Depending on the humidity level (High or Normal), the final decision leads to either "Yes" (play) or "No" (don't play).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Data Partitioning}
    \begin{block}{How Decision Trees Partition Data}
        The partitioning of data is made through a series of decision points:
        \begin{itemize}
            \item At each node, the model evaluates features to determine the best way to split the data using criteria like Gini impurity or entropy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Terminology}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Intuitive Visualization}: Graphically represents decisions, making it easier to understand how outcomes are derived.
            \item \textbf{Flexibility}: Can handle both categorical and numerical data.
            \item \textbf{Interpretability}: Easy to interpret for stakeholders, essential for domains requiring transparency, like healthcare or finance.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Terminology}
        \begin{itemize}
            \item \textbf{Splitting}: The process of dividing a dataset at each node.
            \item \textbf{Pruning}: A subsequent process (covered in the next slide) used to remove branches that have little importance, helping simplify the model and improve generalization.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Gini Impurity Formula}
    \begin{block}{Formula for Gini Impurity (for splitting criteria)}
        \begin{equation}
            Gini(D) = 1 - \sum_{k=1}^{K} p_k^2 
        \end{equation}
        Where:
        \begin{itemize}
            \item \( D \) is the dataset,
            \item \( K \) is the number of classes,
            \item \( p_k \) is the proportion of samples in class \( k \).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        Decision Trees are a fundamental concept in supervised learning, providing a versatile approach to data analysis and decision-making processes. Understanding their structure and how they partition data helps set the foundation for more complex techniques discussed in upcoming slides.
    \end{block}
    
    \begin{block}{Next Steps}
        \begin{itemize}
            \item \textbf{Building Decision Trees}: Deep dive into the criteria for splitting and the role of pruning for optimal tree structure.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Overview}
    \begin{itemize}
        \item Objective: Create splits that maximize class separation in the dataset.
        \item Criteria for splitting:
        \begin{itemize}
            \item Gini impurity
            \item Entropy
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Gini Impurity}
    \begin{block}{Definition}
        Gini impurity measures the probability of mislabeling an element chosen randomly from the dataset based on the class distribution.
    \end{block}
    
    \begin{block}{Formula}
        \begin{equation}
        \text{Gini}(D) = 1 - \sum_{i=1}^{C} p_i^2
        \end{equation}
        Where:
        \begin{itemize}
            \item \(D\) is the dataset,
            \item \(C\) is the number of classes,
            \item \(p_i\) is the proportion of class \(i\).
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        \begin{itemize}
            \item Class A: 4 instances
            \item Class B: 1 instance
            \item Class C: 1 instance
        \end{itemize}
        \begin{equation}
        \text{Gini}(D) = 1 - \left(\left(\frac{4}{6}\right)^2 + \left(\frac{1}{6}\right)^2 + \left(\frac{1}{6}\right)^2\right) = 0.5
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Entropy and Pruning}
    \begin{block}{Entropy}
        \begin{itemize}
            \item Measures uncertainty in the dataset.
            \item Lower entropy means a more pure node.
        \end{itemize}
        
        \begin{block}{Formula}
            \begin{equation}
            \text{Entropy}(D) = -\sum_{i=1}^{C} p_i \log_2(p_i)
            \end{equation}
        \end{block}
        
        \begin{block}{Example}
            \begin{equation}
            \text{Entropy}(D) \approx 1.3
            \end{equation}
        \end{block}
    \end{block}
    
    \begin{block}{Tree Pruning}
        \begin{itemize}
            \item Removes branches with little importance.
            \item Types:
            \begin{enumerate}
                \item Pre-Pruning: Stops early if a split is not beneficial.
                \item Post-Pruning: Builds complete tree then removes less informative nodes.
            \end{enumerate}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ensemble Methods - Overview}
    \begin{block}{Definition}
        Ensemble methods are techniques in machine learning that combine multiple models to improve predictive accuracy and robustness.
    \end{block}
    \begin{itemize}
        \item Leverage strengths of various models
        \item Mitigate weaknesses of individual models
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ensemble Methods - Key Concepts}
    \begin{enumerate}
        \item \textbf{What are Ensemble Methods?}
            \begin{itemize}
                \item Combine multiple weak learners to create a stronger overall model.
                \item The group of weak learners can outperform a single strong learner.
            \end{itemize}
        \item \textbf{Types of Ensemble Techniques:}
            \begin{itemize}
                \item \textbf{Bagging (Bootstrap Aggregating)}
                    \begin{itemize}
                        \item Create subsets by sampling with replacement.
                        \item Train a model on each subset and aggregate results.
                        \item Example: Random Forest.
                    \end{itemize}
                \item \textbf{Boosting}
                    \begin{itemize}
                        \item Sequentially train models that correct previous errors.
                        \item Examples: AdaBoost, Gradient Boosting, XGBoost.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ensemble Methods - Illustrations and Code}
    \begin{block}{Random Forests}
        \begin{itemize}
            \item Consists of multiple decision trees.
            \item Final output is aggregated from individual tree predictions.
            \item \textit{Illustration:} Each tree as a person voting on the best course of action.
        \end{itemize}
    \end{block}

    \begin{block}{Using Random Forest in Python}
        \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier

# Create a random forest classifier
rfc = RandomForestClassifier(n_estimators=100)

# Fit the model to the data
rfc.fit(X_train, y_train)

# Make predictions
predictions = rfc.predict(X_test)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks}
    \begin{block}{What are Neural Networks?}
        Neural Networks are computational models inspired by the human brain's structure and function. They consist of interconnected nodes (neurons) organized in layers, which can learn complex patterns from data through a process called training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Architecture}
    \begin{enumerate}
        \item \textbf{Input Layer}
        \begin{itemize}
            \item Represents the input features of the dataset.
            \item Each neuron corresponds to a feature in the data.
        \end{itemize}
        
        \item \textbf{Hidden Layers}
        \begin{itemize}
            \item Intermediate layers where computations are performed.
            \item The number of hidden layers and neurons can significantly affect performance.
        \end{itemize}
        
        \item \textbf{Output Layer}
        \begin{itemize}
            \item Returns the final prediction or classification.
            \item The number of neurons corresponds to the number of target classes.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Illustration}
        Input Layer $\rightarrow$ [Hidden Layer 1] $\rightarrow$ [Hidden Layer 2] $\rightarrow$ Output Layer
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Neural Networks}
    \begin{itemize}
        \item \textbf{Neuron}:
        \begin{itemize}
            \item Basic unit receiving inputs, processing them, and producing output.
            \item Equation: \( \text{output} = \text{activation}(\text{weights} \cdot \text{inputs} + \text{bias}) \)
        \end{itemize}
        
        \item \textbf{Weights and Bias}:
        \begin{itemize}
            \item Weights determine the strength of input relationships.
            \item Bias allows shifting of the activation function.
        \end{itemize}
        
        \item \textbf{Activation Functions}:
        \begin{itemize}
            \item Functions applied at the neuron's output.
            \item Common types: Sigmoid, ReLU (Rectified Linear Unit), and Tanh.
        \end{itemize}
        
        \item \textbf{Example of Sigmoid Function}:
        \begin{equation}
            f(x) = \frac{1}{1 + e^{-x}}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks}
    \begin{block}{Overview}
        Training a neural network involves adjusting its parameters (weights and biases) to minimize prediction error. This is achieved through three interrelated concepts:
        \begin{itemize}
            \item Backpropagation
            \item Activation Functions
            \item Optimization Algorithms
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation}
    \begin{block}{Definition}
        Backpropagation is a supervised learning algorithm used to train neural networks by calculating the gradient of the loss function with respect to each weight.
    \end{block}
    
    \begin{block}{Process}
        \begin{enumerate}
            \item \textbf{Forward Pass}: Input data is passed through the network to obtain predictions.
            \item \textbf{Loss Calculation}: The loss function quantifies the difference between predicted and actual values.
            \item \textbf{Backward Pass}:
            \begin{itemize}
                \item Compute gradients of the loss function using the chain rule.
                \item Update weights to reduce loss: 
                \begin{equation}
                    w \leftarrow w - \eta \frac{\partial L}{\partial w}
                \end{equation}
                where \(w\) is the weight, \(\eta\) is the learning rate, and \(L\) is the loss.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions and Optimization Algorithms}
    \begin{block}{Activation Functions}
        Activation functions introduce non-linearity, allowing models to learn complex patterns. Common activation functions include:
        \begin{itemize}
            \item \textbf{Sigmoid}: 
            \begin{equation}
                \sigma(x) = \frac{1}{1 + e^{-x}}
            \end{equation}
            \item \textbf{ReLU} (Rectified Linear Unit):
            \begin{equation}
                f(x) = \max(0, x)
            \end{equation}
            \item \textbf{Softmax}:
            \begin{equation}
                \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
            \end{equation}
        \end{itemize}
    \end{block}

    \begin{block}{Optimization Algorithms}
        Optimization algorithms dictate how weights are updated based on gradients:
        \begin{itemize}
            \item \textbf{Stochastic Gradient Descent (SGD)}: Updates weights using a single training example; can be noisy.
            \item \textbf{Adam (Adaptive Moment Estimation)}: 
            \begin{equation}
                w \leftarrow w - \frac{\eta \cdot m_t}{\sqrt{v_t} + \epsilon}
            \end{equation}
            where \(m_t\) and \(v_t\) are moving averages of gradients and their squares.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Introduction}
    \begin{block}{Introduction}
        Model evaluation metrics are crucial for assessing the performance of supervised learning models. They help quantify how well our model performs and guide improvements. 
        We will explore key metrics: Accuracy, Precision, Recall, F1 Score, and ROC-AUC.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Accuracy}
    \begin{block}{1. Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: Measures the ratio of correctly predicted instances to total instances.
            \[
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
            \]
            \item \textbf{Example}: If 80 out of 100 instances are predicted correctly, the accuracy is \( 0.8 \) or 80\%.
            \item \textbf{Key Point}: Accuracy can be misleading for imbalanced datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Precision and Recall}
    \begin{block}{2. Precision}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of true positive predictions to total positive predictions.
            \[
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \]
            \item \textbf{Example}: With 30 positive predictions and 20 correct, precision is \( \frac{20}{30} = 0.67 \) or 67\%.
            \item \textbf{Key Point}: High precision indicates a low false positive rate.
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the ratio of true positives to actual positives.
            \[
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \]
            \item \textbf{Example}: With 50 actual positives and 45 identified correctly, recall is \( \frac{45}{50} = 0.9 \) or 90\%.
            \item \textbf{Key Point}: High recall means successful capturing of relevant instances.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - F1 Score and ROC-AUC}
    \begin{block}{4. F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: Harmonic mean of precision and recall.
            \[
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \]
            \item \textbf{Example}: For precision \(0.67\) and recall \(0.9\):
            \[
            \text{F1 Score} \approx 0.76
            \]
            \item \textbf{Key Point}: Useful for imbalanced classes, balancing precision and recall.
        \end{itemize}
    \end{block}
    
    \begin{block}{5. ROC-AUC}
        \begin{itemize}
            \item \textbf{Definition}: Measures a model's ability to distinguish between classes; plots true positive rate vs. false positive rate.
            \item \textbf{Key Features}:
            \begin{itemize}
                \item AUC ranges from 0 to 1.
                \item 1 indicates a perfect model, 0.5 indicates random guessing.
            \end{itemize}
            \item \textbf{Example}: An AUC of 0.85 suggests a well-performing model.
            \item \textbf{Key Point}: Especially useful for binary classification problems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Conclusion}
    \begin{block}{Conclusion}
        Understanding evaluation metrics is fundamental for developing and validating supervised learning models. 
        By measuring accuracy, precision, recall, F1 Score, and ROC-AUC, data scientists can ensure models meet specific project needs.
        Comparing these metrics helps stakeholders select models that align with their objectives.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Supervised Learning}
    \begin{block}{Overview of Supervised Learning}
        Supervised learning is a type of machine learning where models learn from labeled data to make predictions or decisions. 
        The model is trained using input-output pairs, adjusting based on prediction errors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Domains of Application}
    \begin{enumerate}
        \item \textbf{Healthcare}
            \begin{itemize}
                \item \textbf{Application}: Disease Diagnosis
                \item \textbf{Example}: Predicting if a patient has a disease (e.g., diabetes) based on age, blood pressure, and BMI.
                \item \textbf{Technique}: Classification algorithms (Logistic Regression, Decision Trees, SVM).
            \end{itemize}

        \item \textbf{Finance}
            \begin{itemize}
                \item \textbf{Application}: Credit Scoring
                \item \textbf{Example}: Evaluating customer default likelihood based on credit history and income.
                \item \textbf{Technique}: Regression analysis (Logistic Regression).
            \end{itemize}

        \item \textbf{Retail}
            \begin{itemize}
                \item \textbf{Application}: Customer Segmentation
                \item \textbf{Example}: Categorizing customers based on purchase behavior.
                \item \textbf{Technique}: Clustering algorithms with supervised methods.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Domains of Application (continued)}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering from previous frame
        \item \textbf{Marketing}
            \begin{itemize}
                \item \textbf{Application}: Churn Prediction
                \item \textbf{Example}: Identifying customers likely to unsubscribe based on service usage.
                \item \textbf{Technique}: Classification methods (Random Forest, Gradient Boosted Trees).
            \end{itemize}

        \item \textbf{Transportation}
            \begin{itemize}
                \item \textbf{Application}: Predictive Maintenance
                \item \textbf{Example}: Predicting machinery failure based on sensor data.
                \item \textbf{Technique}: Regression models for time-series data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Predicting Loan Default}
    \begin{itemize}
        \item \textbf{Dataset Features}: Age, Income, Credit Score, Employment Status.
        \item \textbf{Model}: Logistic Regression
        \item \textbf{Goal}: Predict the binary outcome (Default or No Default).
    \end{itemize}
    \begin{block}{Formula Used in Logistic Regression}
        \begin{equation}
            P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Supervised learning provides powerful tools for informed decision-making across various fields.
        \item The chosen model often depends on whether the problem involves classification (discrete outcomes) or regression (continuous outcomes).
        \item Real-world applications show the importance of accurate model predictions, impacting both financial and social outcomes.
    \end{itemize}

    \begin{block}{Next Steps}
        Next, we will discuss the \textbf{Ethical Considerations in Supervised Learning}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Ethical Considerations}
    \begin{block}{Overview}
        As we deploy supervised learning models across various applications, it is crucial to address ethical considerations to ensure fairness, transparency, and accountability in their outcomes. This presentation will explore the common ethical challenges faced when applying supervised learning techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Challenges}
    \begin{enumerate}
        \item Bias and Fairness
        \item Transparency and Explainability
        \item Data Privacy and Security
        \item Accountability in Decision-Making
        \item Societal Impact
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bias and Fairness}
    \begin{itemize}
        \item \textbf{Definition:} Bias in supervised learning occurs when the model's predictions are systematically prejudiced due to skewed training data.
        \item \textbf{Example:} A hiring model trained predominantly on one demographic may disadvantage others.
        \item \textbf{Key Point:} Regularly audit datasets to identify and mitigate any inherent biases.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transparency and Explainability}
    \begin{itemize}
        \item \textbf{Definition:} Many models operate as "black boxes," making decisions difficult to understand.
        \item \textbf{Example:} A loan model may reject applicants without clear reasons.
        \item \textbf{Key Point:} Implement tools like LIME or SHAP to enhance model explainability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Privacy and Security}
    \begin{itemize}
        \item \textbf{Definition:} Models often require large datasets, which may include sensitive personal information.
        \item \textbf{Example:} A healthcare model might use patient records that can expose confidential health data.
        \item \textbf{Key Point:} Adhere to regulations like GDPR to protect data privacy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accountability in Decision-Making}
    \begin{itemize}
        \item \textbf{Definition:} Determining responsibility for decisions made by models can be complex.
        \item \textbf{Example:} In autonomous vehicles, determining liability in accidents is problematic.
        \item \textbf{Key Point:} Establish clear accountability frameworks to address adverse outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Societal Impact}
    \begin{itemize}
        \item \textbf{Definition:} Deployment of models can significantly influence societal norms.
        \item \textbf{Example:} Predictive policing may lead to over-policing in certain neighborhoods.
        \item \textbf{Key Point:} Assess the broader social implications and strive for equitable outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Key Takeaway}
        Ethical considerations are vital in the development of supervised learning models. By understanding and addressing these challenges, we can foster trust and ensure accountability in AI systems.
    \end{block}
    \vfill
    \textbf{Discussion Prompt:} How would you handle ethical challenges in your work?
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustration Idea}
    \begin{block}{Flowchart Outline}
        \textbf{Data Collection} \\
        $\rightarrow$ Bias Check \\
        $\rightarrow$ Privacy Protection \\
        $\rightarrow$ Transparency Measures \\
        $\rightarrow$ Deploy Model Ethically
    \end{block}
\end{frame}


\end{document}