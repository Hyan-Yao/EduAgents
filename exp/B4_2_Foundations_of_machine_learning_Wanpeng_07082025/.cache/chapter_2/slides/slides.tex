\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Data Preprocessing]{Weeks 2-3: Data Preprocessing and Feature Engineering}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{Overview}
        Data preprocessing is a critical step in the machine learning pipeline that transforms raw data into a clean and usable format, ensuring better performance and reliable predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Definition:} Data preprocessing enhances the quality and usability of data for machine learning algorithms.
        
        \item \textbf{Purpose:}
        \begin{itemize}
            \item Improve Model Accuracy
            \item Mitigate Errors
            \item Facilitate Analysis
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Preprocessing Steps}
    \begin{enumerate}
        \item \textbf{Data Cleaning:}
        \begin{itemize}
            \item Handling Missing Values:
            \begin{itemize}
                \item Techniques: removal or imputation (mean, median, mode).
                \item \textbf{Example:} Filling missing housing prices with the median price.
            \end{itemize}
        \end{itemize}

        \item \textbf{Data Transformation:}
        \begin{itemize}
            \item Normalization/Standardization:
            \begin{equation}
                x' = \frac{x - \min(X)}{\max(X) - \min(X)}
            \end{equation}
        \end{itemize}

        \item \textbf{Encoding Categorical Variables:}
        \begin{itemize}
            \item Techniques like One-Hot Encoding.
            \item \textbf{Example:} Transforming a "Color" feature (Red, Blue, Green) into binary features.
        \end{itemize}
        
        \item \textbf{Feature Scaling:}
        \item \textbf{Feature Selection/Extraction:} Techniques like PCA to retain variance while reducing dimensionality.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Importance of Data Quality - Introduction}
    \begin{block}{Introduction to Data Quality}
        Data quality refers to the condition of a dataset based on factors such as accuracy, completeness, consistency, and timeliness. It directly influences the effectiveness and reliability of machine learning models.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Data Quality - Impact on Model Performance}
    \begin{block}{Impact on Model Performance}
        \begin{enumerate}
            \item \textbf{Accuracy}: Poor data quality can lead to inaccurate predictions.
            \begin{itemize}
                \item \textit{Example}: A model predicting spam emails can misclassify legitimate emails if the dataset contains 10\% incorrect labels.
            \end{itemize}
            
            \item \textbf{Generalization}: High-quality data enables models to generalize better. 
            \begin{itemize}
                \item \textit{Example}: Regression models trained on sales data with outliers may perform poorly on unseen data due to overfitting.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Data Quality - Impact on Validity}
    \begin{block}{Impact on Validity}
        \begin{enumerate}
            \item \textbf{Internal Validity}: High-quality data ensures trustworthy results that reflect true relationships.
            \begin{itemize}
                \item \textit{Illustration}: Erroneous sales data might falsely indicate a marketing strategy's effectiveness.
            \end{itemize}
            
            \item \textbf{External Validity}: Significant biases in data can mislead conclusions about broader populations.
            \begin{itemize}
                \item \textit{Example}: A model trained on data from one region may not be reliable when applied to a different region with distinct characteristics.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data - Introduction}
    \begin{block}{Introduction}
        In any data analysis or machine learning task, handling missing data is crucial because:
        \begin{itemize}
            \item Missing values can bias results.
            \item They can reduce the efficiency of algorithms.
            \item They may lead to incorrect conclusions.
        \end{itemize}
        The methods used to address missing data can significantly impact the model's performance and the insights derived.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data - Techniques}
    \begin{block}{Techniques for Handling Missing Data}
        \begin{enumerate}
            \item \textbf{Deletion}
                \begin{itemize}
                    \item \textbf{Listwise Deletion}: Remove any row with a missing value.
                    \item \textbf{Pairwise Deletion}: Use available data for each calculation, excluding missing values as they occur.
                \end{itemize}
            \item \textbf{Imputation}
                \begin{itemize}
                    \item \textbf{Mean/Median/Mode Imputation}: Fill missing values with their respective statistical measure.
                    \item \textbf{K-Nearest Neighbors (KNN)}: Replace missing values based on similar data points.
                \end{itemize}
            \item \textbf{Using Models}
                \begin{itemize}
                    \item \textbf{Regression Models}: Predict missing values based on other features.
                    \item \textbf{Machine Learning Algorithms}: Use advanced models to find relationships.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data - Summary}
    \begin{block}{Summary}
        Handling missing data effectively is essential to:
        \begin{itemize}
            \item Maintain data quality.
            \item Improve model performance.
        \end{itemize}
        The choice among deletion, imputation, and modeling should depend on:
        \begin{itemize}
            \item The context of the data.
            \item The proportion of missingness.
            \item The nature of the analysis.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data - Next Steps}
    In the next slide, we will delve deeper into specific imputation methods including:
    \begin{itemize}
        \item Mean Imputation
        \item Median Imputation
        \item Mode Imputation
        \item K-Nearest Neighbors (KNN)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Imputation Methods}
    \begin{block}{Introduction to Imputation}
        Imputation is the process of filling in missing values in a dataset. Addressing missing data is crucial as it can significantly influence the performance of machine learning models. Here we will cover four key imputation techniques:
    \end{block}
    \begin{itemize}
        \item Mean Imputation
        \item Median Imputation
        \item Mode Imputation
        \item K-Nearest Neighbors (KNN) Imputation
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Mean Imputation}
    \begin{itemize}
        \item \textbf{Definition:} Replacing missing values with the average of available values.
        \item \textbf{Formula:} 
        \begin{equation}
            \text{Mean} = \frac{\sum_{i=1}^{n} x_i}{n}
        \end{equation}
        \item \textbf{Example:} For the dataset: [5, 6, NaN, 8, 9], the mean is calculated as follows:
        \begin{equation}
            \text{Mean} = \frac{5 + 6 + 8 + 9}{4} = 7
        \end{equation}
        Here, NaN is replaced with 7.
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Best for normally distributed data.
            \item Can underestimate variability, leading to biased models.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Median Imputation}
    \begin{itemize}
        \item \textbf{Definition:} Replacing missing values with the median of the available data.
        \item \textbf{Example:} Using the dataset: [5, 6, NaN, 8, 9]. Arranging it gives [5, 6, 8, 9], yielding:
        \begin{equation}
            \text{Median} = \frac{6 + 8}{2} = 7
        \end{equation}
        Replace NaN with 7.
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Robust to outliers.
            \item More appropriate for skewed distributions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Mode Imputation}
    \begin{itemize}
        \item \textbf{Definition:} Fills missing values with the most frequent value.
        \item \textbf{Example:} For the dataset [1, 2, 2, NaN, 3], the mode is 2, replacing NaN with 2.
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Useful for categorical data.
            \item May introduce bias if not representative.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. K-Nearest Neighbors (KNN) Imputation}
    \begin{itemize}
        \item \textbf{Definition:} Estimates missing values based on similarity to k-nearest neighbors.
        \item \textbf{Example:} If neighbors have values 2, 3, and 5, then:
        \begin{equation}
            \text{Imputed Value} = \frac{2 + 3 + 5}{3} = 3.33
        \end{equation}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Captures local structure of data.
            \item Computationally expensive with large datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Imputation Techniques}
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Method} & \textbf{Best For} & \textbf{Advantages} & \textbf{Disadvantages} \\
        \hline
        Mean & Normally distributed data & Simple, easily implemented & Underestimates variance \\
        \hline
        Median & Skewed distributions & Robust to outliers & Loss of information \\
        \hline
        Mode & Categorical data & Simple, preserves dominance & Possible bias \\
        \hline
        KNN & Complex datasets & Captures local structure & Computationally intensive \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Choosing the right imputation method is critical for maintaining the integrity and predictive power of your model. Properly handling missing data can lead to improved model accuracy and insightful analyses.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Slide: Data Normalization}
    In the following slide, we will explore normalization techniques including Min-Max scaling and Z-score standardization to further prepare our datasets for modeling.
\end{frame}

\begin{frame}
    \frametitle{Data Normalization - Overview}
    \begin{block}{Overview}
        Data normalization is a critical preprocessing step in data science aimed at transforming features to a similar scale for effective model training. 
        This prevents any single feature from dominating due to larger values, enhancing the performance of many machine learning algorithms.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Normalization Techniques}
    \begin{enumerate}
        \item \textbf{Min-Max Scaling}
        \begin{itemize}
            \item \textbf{Definition}: Rescales the feature to a fixed range, typically [0, 1].
            \item \textbf{Formula}:
            \begin{equation}
                X' = \frac{X - X_{min}}{X_{max} - X_{min}}
            \end{equation}
            \item \textbf{Example}: For the feature 'age' = {22, 25, 27, 35, 45}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Min-Max Scaling - Example}
    \begin{itemize}
        \item Min: 22, Max: 45
        \item Normalized values:
        \begin{itemize}
            \item For age 22: 
            \begin{equation}
                X' = \frac{22 - 22}{45 - 22} = 0
            \end{equation}
            \item For age 35: 
            \begin{equation}
                X' = \frac{35 - 22}{45 - 22} \approx 0.41
            \end{equation}
            \item For age 45: 
            \begin{equation}
                X' = \frac{45 - 22}{45 - 22} = 1
            \end{equation}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Z-score Standardization}
    \begin{enumerate}
        \item \textbf{Definition}: Transforms the feature to have a mean of 0 and a standard deviation of 1.
        \item \textbf{Formula}:
        \begin{equation}
            Z = \frac{X - \mu}{\sigma}
        \end{equation}
        Where:
        \begin{itemize}
            \item $\mu$ = mean of the feature
            \item $\sigma$ = standard deviation
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Z-score Standardization - Example}
    \begin{itemize}
        \item Using the same dataset: {22, 25, 27, 35, 45}
        \item Mean ($\mu$): 30.8, Standard Deviation ($\sigma$): approx. 8.74
        \item Standardized values:
        \begin{itemize}
            \item For age 22:
            \begin{equation}
                Z \approx \frac{22 - 30.8}{8.74} \approx -1.00
            \end{equation}
            \item For age 35:
            \begin{equation}
                Z \approx \frac{35 - 30.8}{8.74} \approx 0.48
            \end{equation}
            \item For age 45:
            \begin{equation}
                Z \approx \frac{45 - 30.8}{8.74} \approx 1.63
            \end{equation}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points}
    \begin{itemize}
        \item Normalization is crucial for algorithms sensitive to the scale of input features (e.g., K-means, neural networks).
        \item Min-Max scaling is preferred for bounded and uniform data.
        \item Z-score standardization is suitable for normally distributed data.
        \item Applying these methods improves model training speed and convergence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet - Python Example}
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import numpy as np

# Sample Data
data = np.array([[22], [25], [27], [35], [45]])

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
data_min_max = min_max_scaler.fit_transform(data)

# Z-score Standardization
standard_scaler = StandardScaler()
data_standard = standard_scaler.fit_transform(data)

print("Min-Max Scaled Data:\n", data_min_max)
print("Z-Score Standardized Data:\n", data_standard)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Why Normalize Data? - Introduction}
  Normalization is a crucial step in data preprocessing, transforming features to a common scale while preserving differences in value ranges. This is particularly important for machine learning models sensitive to input data scale.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Why Normalize Data? - Benefits of Normalization}
  \begin{enumerate}
    \item \textbf{Improved Model Convergence}:
      \begin{itemize}
        \item Gradient Descent Optimization: Features on different scales elongate the cost function, causing zigzagging in optimization paths.
        \item Example: One feature ranges from 0-1 and another from 1,000-10,000. Normalization promotes even weight adjustment.
      \end{itemize}
      
    \item \textbf{Enhanced Model Performance}:
      \begin{itemize}
        \item Uniform Contribution: Normalized data prevents bias towards features with larger ranges, aiding in balanced model training.
        \item Example: In K-Means clustering, large values from one feature can skew results. Normalization balances impacts.
      \end{itemize}

    \item \textbf{Faster Training Time}:
      \begin{itemize}
        \item Normalized data leads to quicker optimization, reducing overall training time.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Why Normalize Data? - Key Considerations}
  \begin{block}{When to Normalize}
    Normalize data for distance-based algorithms (e.g., K-Means, SVM) and gradient-based optimizations.
  \end{block}

  \begin{block}{Normalization Techniques}
    \begin{itemize}
      \item \textbf{Min-Max Scaling}: Rescales features to a range [0, 1].\\
      Formula: \(X' = \frac{X - X_{min}}{X_{max} - X_{min}}\)
      \item \textbf{Z-score Standardization}: Centers data around the mean with a standard deviation of 1.\\
      Formula: \(Z = \frac{X - \mu}{\sigma}\)
    \end{itemize}
  \end{block}
  
  \begin{block}{Conclusion}
    Normalization enhances convergence speed, model performance, and training efficiency when applied appropriately.
  \end{block}
  
  \textbf{Key Takeaway:} Normalizing ensures equal feature contribution, leading to improved performance and faster convergence rates.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Overview}
    \begin{block}{Definition of Feature Selection}
        Feature selection is the process of identifying and selecting a subset of relevant features (variables, predictors) for model construction. It involves evaluating the importance of each feature in contributing to predictive power and retaining only those that provide significant capability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Feature Selection}
    \begin{itemize}
        \item \textbf{Improves Model Accuracy:}
        \begin{itemize}
            \item Relevant features help the model focus on important data, leading to better predictions.
            \item \textit{Example:} In housing price prediction, 'number of bedrooms' and 'square footage' are more impactful than 'color of the front door'.
        \end{itemize}
        
        \item \textbf{Reduces Overfitting:}
        \begin{itemize}
            \item Overfitting occurs when a model learns noise in the training data.
            \item \textit{Illustration:}
            \begin{itemize}
                \item Before Feature Selection: High complexity with many irrelevant features.
                \item After Feature Selection: Streamlined model focusing on key features, improving generalization to new data.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Curse of Dimensionality:} More features require more data for effective generalization.
        \item \textbf{Sensitivity to Noise:} Too many features lead to less stable predictions; reducing features enhances robustness.
        \item \textbf{Computational Efficiency:} Fewer features result in faster training and evaluation times, conserving resources.
    \end{itemize}

    \begin{block}{Example Techniques for Feature Selection}
        \begin{itemize}
            \item \textbf{Univariate Selection:} Evaluates each feature individually to find the strongest relationships with the output variable.
            \item \textbf{Recursive Feature Elimination (RFE):} Iteratively removes the least significant features until the desired number is reached.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Feature selection is crucial in data preprocessing, significantly enhancing model performance. Understanding effective feature selection is fundamental in data science.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula for Feature Selection Metrics}
    You may encounter various metrics for feature selection, such as information gain or correlation coefficients. For example, correlation can be calculated as:
    
    \begin{equation}
        \text{Correlation}(X, Y) = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
    \end{equation}
    
    Where \(Cov\) is covariance, and \(\sigma\) represents standard deviation. This formula can be applied for univariate feature selection to assess the strength of relationships between independent variable \(X\) and dependent variable \(Y\).
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Introduction}
    \begin{block}{Introduction to Feature Selection Techniques}
        Feature selection is a crucial step in data preprocessing, as it helps identify the most relevant features in a dataset that contribute to the prediction of the target variable. 
        By utilizing the right feature selection technique, we can enhance model performance and reduce the risk of overfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Methods}
    \begin{enumerate}
        \item \textbf{Filter Methods}
        \item \textbf{Wrapper Methods}
        \item \textbf{Embedded Methods}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Filter Methods}
    \begin{block}{Filter Methods}
        Evaluate the relevance of features based on their intrinsic properties, independent of any machine learning model.
    \end{block}
    \begin{itemize}
        \item Simple and computationally efficient.
        \item Statistical functions like correlation, chi-square tests, and information gain are used.
    \end{itemize}
    \begin{block}{Example: Pearson Correlation Coefficient}
        Measures linear correlation between features and the target variable.
        Features with high correlation should be selected.
    \end{block}
    \begin{equation}
        r = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Wrapper Methods}
    \begin{block}{Wrapper Methods}
        Evaluate multiple subsets of features by applying a machine learning model.
    \end{block}
    \begin{itemize}
        \item More accurate than filter methods but computationally expensive.
        \item Use search strategies like forward selection, backward elimination, or genetic algorithms.
    \end{itemize}
    \begin{block}{Example: Recursive Feature Elimination (RFE)}
        Iteratively removes the least important features based on model performance until the optimal set is found.
    \end{block}
    \begin{lstlisting}[language=Python]
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
rfe = RFE(model, 5)
fit = rfe.fit(X_train, y_train)
selected_features = fit.support_
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Embedded Methods}
    \begin{block}{Embedded Methods}
        Integrate feature selection within the model training process, performing both tasks simultaneously.
    \end{block}
    \begin{itemize}
        \item Balances the advantages of filter and wrapper methods.
        \item Includes techniques like Lasso (L1 penalty) and Ridge (L2 penalty) regression.
    \end{itemize}
    \begin{block}{Example: Lasso Regression}
        Encourages sparsity by adding a penalty term for large coefficients, effectively shrinking less important coefficients to zero.
    \end{block}
    \begin{lstlisting}[language=Python]
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.01)
lasso.fit(X_train, y_train)
selected_features = np.where(lasso.coef_ != 0)[0]
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Summary and Next Steps}
    \begin{block}{Summary}
        \begin{itemize}
            \item \textbf{Filter Methods}: Fast and independent of models; focus on statistics.
            \item \textbf{Wrapper Methods}: Provide high accuracy by considering feature interactions; computationally intensive.
            \item \textbf{Embedded Methods}: Efficiently combine feature selection with model training, yielding a robust feature set.
        \end{itemize}
    \end{block}
    \begin{block}{Next Steps}
        Understanding how these techniques differ from feature extraction and diving deeper into feature extraction methods like PCA and LDA.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Extraction vs. Feature Selection}
    \begin{itemize}
        \item \textbf{Feature Selection}:
            \begin{itemize}
                \item Chooses a subset of relevant features from the original set.
                \item Improves computational efficiency and reduces overfitting.
                \item \textbf{Example}: Selecting age and blood pressure in a diabetes prediction dataset.
            \end{itemize}
        \item \textbf{Feature Extraction}:
            \begin{itemize}
                \item Transforms original features into a lower-dimensional space.
                \item Captures essential information by combining existing features.
                \item \textbf{Example}: Using PCA in image processing to convert pixel values.
            \end{itemize}
    \end{itemize}
    \begin{block}{Key Point}
        Feature selection focuses on existing features, whereas feature extraction generates new features.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to PCA}
    \begin{itemize}
        \item \textbf{Principal Component Analysis (PCA)}:
            \begin{itemize}
                \item Used for dimensionality reduction while preserving variance.
                \item Identifies principal components that maximize data variance.
                \item Principal components are orthogonal, avoiding multicollinearity.
            \end{itemize}
        \item \textbf{Mathematical Representation}:
            \begin{enumerate}
                \item Center the data (subtract the mean).
                \item Compute covariance matrix: \( C = \frac{1}{n-1} (X^TX) \).
                \item Calculate eigenvalues and eigenvectors of \( C \).
                \item Select top \( k \) eigenvectors corresponding to the largest eigenvalues.
            \end{enumerate}
        \item \textbf{Example}: In facial recognition, PCA reduces dimensions while retaining essential features.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to LDA}
    \begin{itemize}
        \item \textbf{Linear Discriminant Analysis (LDA)}:
            \begin{itemize}
                \item Maximizes class separability rather than variance.
                \item Finds a linear combination of features that best separates classes.
            \end{itemize}
        \item \textbf{Mathematical Concept}:
            \begin{enumerate}
                \item Compute mean vectors for each class.
                \item Compute within-class and between-class scatter matrices.
                \item Solve the generalized eigenvalue problem for optimal projection.
            \end{enumerate}
        \item \textbf{Example}: LDA can distinguish between flower species using petal length and width.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Code Snippet for PCA}
    \begin{itemize}
        \item \textbf{Summary}:
            \begin{itemize}
                \item Feature Selection: Chooses the best subset of original features.
                \item Feature Extraction: Creates new features from combinations of existing ones.
                \item Techniques: PCA focuses on variance maximization, while LDA focuses on class separability.
            \end{itemize}
        \item \textbf{Code Snippet for PCA in Python}:
        \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Assume X is your data array
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print("Explained variance ratios:", pca.explained_variance_ratio_)
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Principal Component Analysis (PCA)}
  \begin{block}{What is PCA?}
    Principal Component Analysis (PCA) is a powerful statistical technique used for dimensionality reduction while preserving as much variance (information) as possible in a dataset. PCA transforms the original variables into a new set of uncorrelated variables called principal components.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{PCA - How Does It Work?}
  \begin{enumerate}
    \item \textbf{Standardization}:
      \begin{itemize}
        \item Standardize data to have mean of zero and standard deviation of one.
        \item \textbf{Formula}: 
        \[
        z_i = \frac{x_i - \mu}{\sigma}
        \]
      \end{itemize}
      
    \item \textbf{Covariance Matrix Computation}:
      \begin{itemize}
        \item Calculate the covariance matrix to see how variables vary together.
        \item \textbf{Formula}: 
        \[
        \text{Cov}(X) = \frac{1}{n-1} (X^T X)
        \]
      \end{itemize}
      
    \item \textbf{Eigenvalue and Eigenvector Calculation}:
      \begin{itemize}
        \item Compute eigenvalues and eigenvectors of the covariance matrix.
      \end{itemize}
      
    \item \textbf{Selecting Principal Components}:
      \begin{itemize}
        \item Sort eigenvalues and choose top \( k \) eigenvalues.
      \end{itemize}
      
    \item \textbf{Transformation}:
      \begin{itemize}
        \item Project original data into new feature space.
        \item \textbf{Formula}:
        \[
        Y = XW
        \]
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example and Applications of PCA}
  \begin{block}{Example}
    Imagine a dataset with three features: height, weight, and age of individuals. PCA can reduce these three dimensions to two principal components that best represent the variance in the data.
    \begin{itemize}
      \item \textbf{Component 1}: Most variance (e.g., relationships in height and weight)
      \item \textbf{Component 2}: Second most variance (e.g., age correlating with height)
    \end{itemize}
  \end{block}
  
  \begin{block}{Key Points}
    \begin{itemize}
      \item PCA simplifies models and reduces computational costs.
      \item It retains as much variability as possible, avoiding loss of critical information.
      \item Useful for visualizing high-dimensional data in 2D or 3D plots.
    \end{itemize}
  \end{block}
  
  \begin{block}{Applications}
    \begin{itemize}
      \item Image compression (reducing file sizes while maintaining quality)
      \item Preprocessing step for machine learning
      \item Exploring patterns in datasets like genetic and financial data
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of PCA - What is PCA?}
    \begin{block}{Principal Component Analysis (PCA)}
        PCA is a statistical technique used for dimensionality reduction. It transforms high-dimensional data into a lower-dimensional space, preserving as much variance as possible. This is particularly beneficial when dealing with datasets that contain numerous features, leading to issues such as the curse of dimensionality.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of PCA - Real-World Applications}

    \begin{enumerate}
        \item \textbf{Face Recognition:}
        \begin{itemize}
            \item PCA is utilized in image processing for recognizing faces in photographs.
            \item Transforms images into a lower-dimensional space (eigenfaces).
            \item \textbf{Benefit:} Reduces computational burden and improves recognition speed and accuracy.
        \end{itemize}

        \item \textbf{Genomics:}
        \begin{itemize}
            \item Helps analyze gene expression data involving thousands of genes.
            \item Enables visualization and identification of patterns in gene expression.
            \item \textbf{Benefit:} Clearer interpretation of biological data and distinction between disease subtypes.
        \end{itemize}

        \item \textbf{Finance:}
        \begin{itemize}
            \item Analyzes stock market performance across various sectors.
            \item Identifies underlying factors that drive asset returns.
            \item \textbf{Benefit:} Facilitates investment decisions by simplifying data without significant loss of information.
        \end{itemize}

        \item \textbf{Image Compression:}
        \begin{itemize}
            \item Compresses image files by reducing necessary dimensions.
            \item \textbf{Benefit:} Saves storage and bandwidth while maintaining quality.
        \end{itemize}

        \item \textbf{Market Research:}
        \begin{itemize}
            \item Identifies customer segments by analyzing surveys and purchasing patterns.
            \item \textbf{Benefit:} Provides actionable insights for targeted marketing strategies.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of PCA - Key Benefits}
    
    \begin{itemize}
        \item \textbf{Reduces Overfitting:} Simplifies the model to avoid overcomplexity in predictive analysis.
        \item \textbf{Improves Visualization:} Allows for easier visualization of high-dimensional data in 2D or 3D plots.
        \item \textbf{Increases Performance:} Many machine learning algorithms perform better with less noise and lower dimensionality.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of PCA - Mathematical Basis}
    
    The core of PCA involves eigenvalue decomposition of the covariance matrix of the dataset. For a dataset represented as \(X\):
    
    \begin{enumerate}
        \item \textbf{Center the data:} Subtract the mean.
        \item \textbf{Calculate the covariance matrix:} 
        \[
        C = \frac{1}{n - 1} X^T X
        \]
        \item \textbf{Eigenvalue decomposition:} Find eigenvalues and eigenvectors of \(C\).
        \item \textbf{Select principal components:} Choose the top \(k\) eigenvectors corresponding to the \(k\) largest eigenvalues to form a new feature space.
    \end{enumerate}

    \begin{block}{Example Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA

# Example code snippet
pca = PCA(n_components=2)  # Targeting reduction to 2 dimensions
X_reduced = pca.fit_transform(X)  # X is the original high-dimensional dataset
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Introduction to Feature Engineering and Preprocessing}
  Feature engineering and data preprocessing are critical steps in the data science workflow. 
  \begin{itemize}
    \item Enhance the quality of data and are crucial for building robust models.
    \item Combining different techniques can lead to more powerful predictive features and improve model performance.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Best Practices for Combining Techniques}
  \begin{enumerate}
    \item \textbf{Understand Your Data}
      \begin{itemize}
        \item Analyze dataset and relationships between features.
        \item \textit{Example}: Use visualizations (histograms, box plots).
      \end{itemize}
    \item \textbf{Start with Simple Techniques}
      \begin{itemize}
        \item Begin with handling missing values, scaling, and encoding.
        \item \textit{Example}: Mean imputation for numerical data.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Best Practices for Combining Techniques (cont'd)}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Iterate Through Transformations}
      \begin{itemize}
        \item Combine techniques iteratively; assess each individually.
        \item \textit{Example}: Apply normalization, then polynomial features.
      \end{itemize}
    \item \textbf{Use Domain Knowledge}
      \begin{itemize}
        \item Create meaningful features based on expertise.
        \item \textit{Example}: "Price per square foot" in housing models.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Best Practices for Combining Techniques (cont'd)}
  \begin{enumerate}
    \setcounter{enumi}{4}
    \item \textbf{Feature Selection}
      \begin{itemize}
        \item Use RFE or Lasso regression for selecting significant features.
        \item \textbf{Key Point}: Prioritize feature relevance over quantity.
      \end{itemize}
    \item \textbf{Dimensionality Reduction}
      \begin{itemize}
        \item Use PCA to capture variance and reduce dimensionality.
        \item \textit{Example}: Apply PCA after combining quantitative features.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Best Practices for Combining Techniques (cont'd)}
  \begin{enumerate}
    \setcounter{enumi}{6}
    \item \textbf{Automated Feature Engineering}
      \begin{itemize}
        \item Explore tools like FeatureTools or Tsfresh for scalability.
        \item \textbf{Key Point}: Automation saves time while providing extensive features.
      \end{itemize}
    \item \textbf{Cross-Validation and Testing}
      \begin{itemize}
        \item Validate effectiveness using cross-validation methods.
        \item \textit{Example}: Employ k-fold cross-validation for model performance.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Summary}
  \begin{itemize}
    \item Combining preprocessing and feature engineering methods is systematic and iterative.
    \item A combination of techniques enhances data quality and model performance.
    \item Continuous evaluation and refinement are keys to successful feature engineering.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Code Snippet Example: Basic Preprocessing in Python}
  \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Load dataset
data = pd.read_csv('data.csv')

# Impute missing values
imputer = SimpleImputer(strategy='mean')
data['feature'] = imputer.fit_transform(data[['feature']])

# Scale features
scaler = StandardScaler()
data[['feature1', 'feature2']] = scaler.fit_transform(data[['feature1', 'feature2']])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2)
  \end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Key Takeaway}
  The combination of feature engineering techniques is a dynamic process that requires:
  \begin{itemize}
    \item Understanding data
    \item Iterative testing
    \item Leveraging automated and domain-driven insights
  \end{itemize}
  This approach leads to better model performance and more meaningful predictions.
\end{frame}

\begin{frame}
    \frametitle{Real-World Case Studies in Data Preprocessing and Feature Engineering}
    \begin{block}{Introduction}
        Data preprocessing and feature engineering are crucial steps in the data analysis pipeline. Effective techniques can significantly enhance model performance. Let's explore some real-world case studies highlighting successful applications of these methodologies.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Case Study 1: Credit Scoring Model}
    \textbf{Context:} A financial institution aimed to improve its credit scoring system.

    \begin{itemize}
        \item \textbf{Data Preprocessing Techniques Used:}
        \begin{itemize}
            \item \underline{Handling Missing Values:} Employed K-Nearest Neighbors (KNN) to impute missing values based on similarity to other borrowers.
            \item \underline{Normalization:} Standardized numeric features using Min-Max scaling to ensure equal weight in the model.
        \end{itemize}

        \item \textbf{Feature Engineering:}
        \begin{itemize}
            \item \underline{Creating New Features:} Developed a feature for 'Debt-to-Income Ratio' by dividing total monthly debt by gross monthly income.
            \item \underline{Categorical Encoding:} Applied One-Hot Encoding for categorical variables like 'Employment Status' to create binary features.
        \end{itemize}

        \item \textbf{Impact:} Resulted in a 15\% increase in predictive accuracy of the credit scoring model.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Case Study 2: E-commerce Recommendation System}
    \textbf{Context:} An e-commerce platform aimed to enhance product recommendations for customers.

    \begin{itemize}
        \item \textbf{Data Preprocessing Techniques Used:}
        \begin{itemize}
            \item \underline{Outlier Detection:} Utilized the Z-score method to identify and remove outliers in the purchase history.
            \item \underline{Text Preprocessing:} Cleaned product descriptions by removing stop words and performing stemming.
        \end{itemize}

        \item \textbf{Feature Engineering:}
        \begin{itemize}
            \item \underline{Collaborative Filtering:} Created user-item interaction matrices to capture user preferences based on previous purchases.
            \item \underline{Feature Transformation:} Applied log transformation to skewed sales data to reduce variance and stabilize the mean.
        \end{itemize}

        \item \textbf{Impact:} Improved recommendation accuracy by 20\%, contributing to a 10\% increase in sales.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{enumerate}
        \item \textbf{Importance of Data Quality:} Quality data ensures robust model performance. Preprocessing techniques like imputation and normalization are fundamental.
        
        \item \textbf{Feature Selection and Creation:} Deriving and selecting the right features directly influences model effectiveness. Techniques like collaborative filtering in recommendation systems exemplify this.
        
        \item \textbf{Iterative Process:} Data preprocessing and feature engineering are not one-time tasks. They require iterative refinement to adapt to new data and changing patterns.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    These case studies demonstrate that thoughtful application of data preprocessing and feature engineering can lead to significant improvements in predictive modeling outcomes. By learning from real-world scenarios, we can better understand how to approach our projects effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example: KNN Imputation}
    \begin{lstlisting}[language=Python]
from sklearn.impute import KNNImputer
import pandas as pd

# Sample data
data = pd.DataFrame({
    'income': [50000, 60000, None, 45000, 80000],
    'debt': [5000, 7000, 3000, None, 9000]
})

# KNN Imputation
imputer = KNNImputer(n_neighbors=2)
imputed_data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)
print(imputed_data)
    \end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Challenges in Data Preprocessing}
  Data preprocessing is a critical step in preparing data for analysis and modeling. 
  It ensures clean, relevant, and structured data for machine learning algorithms. 
  However, common challenges can complicate this process. 
  This presentation will outline these challenges and their implications.
\end{frame}

\begin{frame}
  \frametitle{1. Missing Data}
  \begin{itemize}
    \item \textbf{Explanation}: Missing values can arise from data entry errors or incomplete surveys.
    \item \textbf{Implications}: Leads to biased results and reduced model accuracy.
    \item \textbf{Common Techniques}:
      \begin{itemize}
        \item \textbf{Imputation}: Fill missing values using mean, median, or mode.
        \item \textbf{Removal}: Exclude rows or columns with significant missing data.
      \end{itemize}
    \item \textbf{Example}: Removing rows with missing "age" values can lead to loss of valuable data.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{2. Outliers}
  \begin{itemize}
    \item \textbf{Explanation}: Extreme values that deviate significantly from other observations.
    \item \textbf{Implications}: Can negatively impact machine learning models' performance.
    \item \textbf{Common Techniques}:
      \begin{itemize}
        \item \textbf{Z-Score Method}: Identify outliers by measuring standard deviations from the mean.
        \item \textbf{Interquartile Range (IQR)}: Set thresholds for acceptable values using IQR.
      \end{itemize}
    \item \textbf{Example}: A record of \$1 billion in sales could skew financial dataset analysis.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{3. Categorical Data}
  \begin{itemize}
    \item \textbf{Explanation}: Requires converting categorical variables into a suitable numeric format.
    \item \textbf{Techniques}:
      \begin{itemize}
        \item \textbf{Label Encoding}: Assign unique integers to each category.
        \item \textbf{One-Hot Encoding}: Create binary columns for each category level.
      \end{itemize}
    \item \textbf{Example}:
    \begin{lstlisting}[language=python]
import pandas as pd

# Example DataFrame
df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})
# One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=['Color'])
    \end{lstlisting}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{4. Data Scaling}
  \begin{itemize}
    \item \textbf{Explanation}: Features on different scales can lead to biased results.
    \item \textbf{Common Techniques}:
      \begin{itemize}
        \item \textbf{Normalization}: Scale values to a range of [0, 1].
        \item \textbf{Standardization}: Convert features to have a mean of 0 and standard deviation of 1.
      \end{itemize}
    \item \textbf{Example}:
    \begin{lstlisting}[language=python]
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)  # Scaling data between 0 and 1
    \end{lstlisting}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{5. Data Leakage}
  \begin{itemize}
    \item \textbf{Explanation}: Occurs when outside information influences model training.
    \item \textbf{Implications}: Undermines model generalization and leads to incorrect conclusions.
    \item \textbf{Prevention Techniques}:
      \begin{itemize}
        \item \textbf{Proper Train-Test Splits}: Respect the boundaries between training and testing data.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Conclusion}
  Recognizing and addressing challenges in data preprocessing enhances the quality of analysis and modeling, leading to more reliable insights and successful project outcomes.
  \begin{itemize}
    \item Data preprocessing is essential, not optional.
    \item Be vigilant against common pitfalls like missing data and outliers.
    \item Always validate preprocessing steps to avoid data leakage.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools for Data Preprocessing - Overview}
    \begin{itemize}
        \item Data preprocessing is essential for preparing datasets for analysis and modeling.
        \item Utilizing the right tools can enhance the efficiency of this process.
        \item We will explore three popular libraries:
        \begin{itemize}
            \item \textbf{Pandas}
            \item \textbf{NumPy}
            \item \textbf{Scikit-learn}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools for Data Preprocessing - Pandas}
    \begin{block}{Pandas: The DataFrame Champion}
        \begin{itemize}
            \item \textbf{What is it?}  
            Pandas is a powerful Python library for data manipulation and analysis.
            \item \textbf{Key Features:}
            \begin{itemize}
                \item Data cleaning (handling missing values, duplicates)
                \item Data transformation (filtering, aggregating)
                \item Supports various file formats (CSV, Excel, SQL)
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load data
df = pd.read_csv('data.csv')

# Handling missing values
df.fillna(method='ffill', inplace=True)

# Dropping duplicates
df.drop_duplicates(inplace=True)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools for Data Preprocessing - NumPy and Scikit-learn}
    \begin{block}{NumPy: The Numerical Backbone}
        \begin{itemize}
            \item \textbf{What is it?}  
            NumPy (Numerical Python) is foundational for numerical computing in Python.
            \item \textbf{Key Features:}
            \begin{itemize}
                \item Efficient operations on large datasets
                \item Basic linear algebra and statistical operations
                \item High-performance multidimensional array object
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
import numpy as np

# Create an array
arr = np.array([1, 2, 3, np.nan])

# Replace NaN with 0
arr = np.nan_to_num(arr)
    \end{lstlisting}
    
    \begin{block}{Scikit-learn: The Modeling Companion}
        \begin{itemize}
            \item \textbf{What is it?}  
            Scikit-learn is a library for machine learning in Python.
            \item \textbf{Key Features:}
            \begin{itemize}
                \item Feature scaling (StandardScaler, MinMaxScaler)
                \item Encoding categorical variables (OneHotEncoder, LabelEncoder)
                \item Train-Test split utility
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 1}
    \begin{block}{Importance of Data Preprocessing}
        Data preprocessing is the foundational step in any machine learning workflow. Properly cleaned and transformed data is essential for building robust models. The insights derived from a model are only as good as the data fed into it.
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{enumerate}
            \item \textbf{Identifying Relevant Features}
            \item \textbf{Handling Missing Values}
            \item \textbf{Data Normalization/Standardization}
            \item \textbf{Encoding Categorical Variables}
            \item \textbf{Outlier Detection}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{0} % Continue enumeration from previous frame
        \item \textbf{Identifying Relevant Features:}
        \begin{itemize}
            \item Selection and engineering of features greatly influence model performance.
            \item Example: In predicting house prices, features like location, square footage, and number of bedrooms are vital.
        \end{itemize}
        
        \item \textbf{Handling Missing Values:}
        \begin{itemize}
            \item Missing data can skew results. Techniques such as imputation (e.g., mean, median, mode) or removal of records are crucial.
            \item Example: If 10\% of the data on property prices is missing, imputing with the average price can provide a more reliable dataset.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue enumeration from previous frame
        \item \textbf{Data Normalization/Standardization:}
        \begin{itemize}
            \item Standardization (Z-score normalization) and Min-Max scaling are techniques that can help models learn better when features have different scales.
            \item Example: Feature values ranging from 0-100 versus values in the thousands need to be scaled appropriately.
        \end{itemize}
        
        \item \textbf{Encoding Categorical Variables:}
        \begin{itemize}
            \item Machine learning models require numerical input. Techniques like One-Hot Encoding transform categorical variables into a format that can be provided to machine learning algorithms.
            \item Example: The categorical variable "Color" with values \{Red, Blue, Green\} can be encoded to binary columns.
        \end{itemize}
        
        \item \textbf{Outlier Detection:}
        \begin{itemize}
            \item Outliers can mislead model interpretation; therefore, identifying and managing them (either by removal or transformation) improves model reliability.
            \item Example: A property listed with an extremely high price compared to similar properties might need investigation.
        \end{itemize}
        
        \begin{block}{Final Thoughts}
            Good data preprocessing is a vital part of understanding the underlying patterns in your data. Always visualize your data before and after preprocessing and experiment with different techniques for optimal results.
        \end{block}
    \end{enumerate}
\end{frame}


\end{document}