\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Font Setup
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}

% Title Page Information
\title[Week 2: Data Processing Frameworks]{Week 2: Data Processing Frameworks}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Data Processing Frameworks?}
    \begin{block}{Definition}
        Data Processing Frameworks are structured environments that facilitate the organization, transformation, and analysis of large sets of data. They enable efficient processing by utilizing resources and tools that help in managing the complexities of big data applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Big Data}
    \begin{itemize}
        \item \textbf{Volume:} Manage, store, and process large datasets swiftly.
        \item \textbf{Variety:} Handle multiple data formats to enable comprehensive analysis.
        \item \textbf{Velocity:} Analyze data in real-time for immediate insights and decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Data Processing Frameworks}
    \begin{enumerate}
        \item \textbf{Distributed Processing:} 
        \begin{itemize}
            \item Leverages multiple nodes for parallel processing of large datasets.
        \end{itemize}
        \item \textbf{Data Storage:} 
        \begin{itemize}
            \item Integrates with storage solutions like HDFS to support data lakes and warehouses.
        \end{itemize}
        \item \textbf{Data Transformation:} 
        \begin{itemize}
            \item Applies transformations such as filtering, aggregating, or enriching data for analysis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Popular Data Processing Frameworks}
    \begin{itemize}
        \item \textbf{Apache Hadoop:} 
        \begin{itemize}
            \item Use Case: Processing vast amounts of log file data from web servers.
        \end{itemize}
        \item \textbf{Apache Spark:} 
        \begin{itemize}
            \item Use Case: Running machine learning algorithms on large datasets.
        \end{itemize}
        \item \textbf{Apache Flink:} 
        \begin{itemize}
            \item Use Case: Real-time fraud detection in transactions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Efficiency:} Streamlines big data workflows for faster analysis.
        \item \textbf{Scalability:} Adapts to increasing data loads without redesigns.
        \item \textbf{Flexibility:} Supports a variety of data formats for versatile analytics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Engagement Activity}
    \begin{block}{Conclusion}
        Understanding data processing frameworks is crucial for navigating big data challenges, enhancing processing capabilities, and deriving actionable insights.
    \end{block}
    \begin{block}{Engagement Activity}
        \textbf{Discussion Prompt:} Consider a dataset you frequently encounter. How could a data processing framework enhance your analysis?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Quiz Question}
    \begin{itemize}
        \item Name a data processing framework optimized for real-time analysis and describe a potential use case.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Understanding ETL Processes - Part 1}
    \frametitle{What is ETL?}
    ETL stands for \textbf{Extraction, Transformation, and Loading}. 
    It is a fundamental process in data processing frameworks that moves data from multiple sources into a consolidated data warehouse or database for analysis and reporting.

    \begin{enumerate}
        \item \textbf{Extraction:}
          \begin{itemize}
              \item Data is collected from various sources including databases, flat files, APIs, cloud storage, or web scraping.
              \item \textit{Example:} Extracting customer data from a CRM system, sales data from a transactional database, and inventory data from an ERP system.
          \end{itemize}

        \item \textbf{Transformation:}
          \begin{itemize}
              \item Data is transformed to ensure suitability for analysis, including cleaning, aggregating, or changing formats.
              \item \textit{Example:} Converting all customer names to uppercase, filtering out invalid email addresses, or calculating total sales per month.
          \end{itemize}

        \item \textbf{Loading:}
          \begin{itemize}
              \item Transformed data is loaded into a destination system like a data warehouse, with options to refresh or append data as needed.
              \item \textit{Example:} Loading data into SQL data warehouses like Amazon Redshift or Google BigQuery.
          \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Understanding ETL Processes - Part 2}
    \frametitle{Role of ETL in Data Processing}
    ETL processes are essential for:

    \begin{itemize}
        \item \textbf{Data Integration:} Combining data from different sources into a unified view.
        \item \textbf{Data Quality:} Improving the reliability and accuracy of data through cleansing and validation.
        \item \textbf{Performance Optimization:} Ensuring efficient query performance in data storage solutions.
        \item \textbf{Regulatory Compliance:} Adhering to legal standards such as GDPR and HIPAA.
    \end{itemize}
    
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item \textbf{Importance of ETL:} Crucial for businesses relying on data-driven insights.
        \item \textbf{Automation:} Many ETL processes can be automated, reducing manual labor and risks of errors.
        \item \textbf{ETL vs. ELT:} ETL processes data before loading; ELT loads data before processing, especially relevant in cloud computing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Understanding ETL Processes - Part 3}
    \frametitle{Diagram Overview and Conclusion}
    \textbf{Diagram Overview:}
    Here, visualize a flowchart depicting the ETL process:
    
    \begin{itemize}
        \item Data Sources
        \item Extraction
        \item Transformation
        \item Loading
        \item Data Warehouse
    \end{itemize}
    
    \textbf{Conclusion:}
    Understanding ETL processes is fundamental for anyone working in data processing. 
    They form the backbone of data integration and quality management, essential for effective data analytics.
\end{frame}

\begin{frame}[fragile]{Hadoop Overview}
    \begin{block}{Introduction to Hadoop}
        Hadoop is an open-source framework designed for the distributed processing of large data sets across clusters of computers using simple programming models. It scales from a single server to thousands of machines, each offering local computation and storage.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Components of Hadoop}
    \begin{enumerate}
        \item \textbf{Hadoop Distributed File System (HDFS)}:
        \begin{itemize}
            \item \textbf{Functionality}: Stores data across multiple nodes; handles large volumes.
            \item \textbf{Features}:
            \begin{itemize}
                \item High Fault Tolerance: Data is replicated for reliability.
                \item Scalability: Nodes can be added without downtime.
            \end{itemize}
            \item \textbf{Example}: HDFS can store transaction logs from multiple retail stores for trend analysis.
        \end{itemize}
        
        \item \textbf{MapReduce}:
        \begin{itemize}
            \item \textbf{Functionality}: A programming model for processing large data sets using a parallel algorithm.
            \item \textbf{Process}:
            \begin{itemize}
                \item \textbf{Map Phase}: Divides data into chunks, processes them in parallel, and transforms them into key-value pairs.
                \item \textbf{Reduce Phase}: Aggregates key-value pairs to produce final output.
            \end{itemize}
            \item \textbf{Example}: Counting item sales from different stores using MapReduce for data analysis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Benefits of Hadoop}
    \begin{itemize}
        \item \textbf{Cost-Effective}: Uses commodity hardware, significantly reducing costs.
        \item \textbf{Flexibility}: Handles various data types and formats.
        \item \textbf{High Availability}: Provides redundancy through data replication.
    \end{itemize}
    
    \begin{block}{Key Points to Emphasize}
        - Hadoop is engineered for scalability, flexibility, and fault tolerance.
        - Understanding HDFS and MapReduce is crucial for leveraging Hadoop's capabilities in big data processing.
        - Many organizations utilize Hadoop for data warehousing, analytics, machine learning, and real-time data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Additional Resources}
    \begin{itemize}
        \item \textbf{Apache Hadoop Documentation}: For in-depth technical details.
        \item \textbf{Community Forums}: Engage with users and developers for support and tips.
    \end{itemize}
    
    By using Hadoop, organizations can efficiently process vast amounts of data to gain valuable insights and make data-driven decisions. Explore the next slide to understand the benefits and specific use cases of Hadoop.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Benefits and Use Cases - Introduction}
    \begin{block}{What is Hadoop?}
        Hadoop is an open-source framework designed for distributed storage and processing of large datasets using a cluster of commodity hardware.
    \end{block}
    \begin{itemize}
        \item \textbf{HDFS (Hadoop Distributed File System):} A scalable and fault-tolerant file system that stores data across multiple nodes.
        \item \textbf{MapReduce:} A programming model for processing large data sets with a distributed algorithm.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Benefits - Key Advantages}
    \begin{enumerate}
        \item \textbf{Scalability}
            \begin{itemize}
                \item Allows seamless scaling by adding more nodes.
                \item \textit{Example:} Start with a few nodes and expand to thousands.
            \end{itemize}
        
        \item \textbf{Cost-Effectiveness}
            \begin{itemize}
                \item Utilizes commodity hardware, lowering costs.
                \item \textit{Example:} Build a Hadoop cluster with standard machines instead of expensive servers.
            \end{itemize}
        
        \item \textbf{Flexibility}
            \begin{itemize}
                \item Handles structured, semi-structured, and unstructured data.
                \item \textit{Example:} Analyze data from social media, logs, and databases.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Benefits - Continued}
    \begin{enumerate}[resume]
        \item \textbf{Fault Tolerance}
            \begin{itemize}
                \item Automatically replicates data across nodes.
                \item \textit{Illustration:} HDFS stores three copies of each data block.
            \end{itemize}

        \item \textbf{Processing Speed}
            \begin{itemize}
                \item Quick processing through parallel data operations.
                \item \textit{Example:} Market analysis can be conducted on terabytes of data quickly.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Typical Use Cases for Hadoop}
    \begin{enumerate}
        \item \textbf{Big Data Analytics}
            \begin{itemize}
                \item Used to analyze datasets for business insights.
                \item \textit{Example:} Retailers analyze customer behavior to optimize inventory.
            \end{itemize}
        
        \item \textbf{Data Lake Creation}
            \begin{itemize}
                \item Stores raw data in its native format for analysis.
                \item \textit{Example:} Financial institutions gather varied data for risk analysis.
            \end{itemize}
        
        \item \textbf{Machine Learning and AI}
            \begin{itemize}
                \item Handles large datasets for training algorithms.
                \item \textit{Example:} Healthcare providers predict health risks using patient data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Use Cases for Hadoop}
    \begin{enumerate}[resume]
        \item \textbf{Log and Event Processing}
            \begin{itemize}
                \item Processes large volumes of log data for monitoring.
                \item \textit{Example:} IT departments analyze logs for security anomalies.
            \end{itemize}

        \item \textbf{Research and Development}
            \begin{itemize}
                \item Used in scientific research and genomic analysis.
                \item \textit{Example:} Genomic researchers analyze DNA sequences to identify genetic markers.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Hadoop is a robust solution for handling big data challenges. Its distributed architecture, cost-effectiveness, and flexibility make it a preferred choice across industries. The diverse applications, from analytics to machine learning, showcase its expanding role in the evolving data landscape.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Spark - Overview}
    \begin{block}{What is Apache Spark?}
        Apache Spark is an open-source, distributed computing system designed for fast and flexible data processing. It allows data professionals to analyze large datasets efficiently by utilizing both in-memory processing and cluster computing.
    \end{block}
    
    \begin{itemize}
        \item Unified analytics engine for big data processing
        \item Excels in machine learning and streaming analytics
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Spark - Key Features}
    \begin{itemize}
        \item \textbf{Speed}: In-memory data processing leads to faster computations.
        \item \textbf{Ease of Use}: User-friendly APIs available in multiple languages (Python, Java, Scala, R).
        \item \textbf{Versatile Processing}: Supports batch processing, streaming, machine learning, and graph processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Apache Spark}
    \begin{enumerate}
        \item \textbf{Driver Program}: Main entry point for Spark applications, coordinating task execution.
        \item \textbf{Cluster Manager}: Allocates resources across the cluster (supports Apache Mesos, Hadoop YARN).
        \item \textbf{Worker Nodes}: Execute tasks assigned by the driver.
        \item \textbf{Executors}: Processes running on worker nodes that handle computation and data storage.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Spark - RDDs}
    \begin{block}{Resilient Distributed Datasets (RDDs)}
        \begin{itemize}
            \item \textbf{Definition}: Fundamental data structure of Spark, immutable collections processed in parallel.
            \item \textbf{Characteristics}:
            \begin{itemize}
                \item Fault Tolerance: RDDs can be rebuilt on failure using lineage.
                \item Lazy Evaluation: Operations executed only when an action is called.
            \end{itemize}
        \end{itemize}
    
        \begin{lstlisting}[language=Python]
from pyspark import SparkContext

sc = SparkContext("local", "My App")
data = sc.parallelize([1, 2, 3, 4, 5])
squared_data = data.map(lambda x: x ** 2)
print(squared_data.collect())  # Output: [1, 4, 9, 16, 25]
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Spark - DataFrames}
    \begin{block}{DataFrames}
        \begin{itemize}
            \item \textbf{Definition}: Distributed collection of data organized into named columns.
            \item \textbf{Advantages}:
            \begin{itemize}
                \item Optimized Execution via Catalyst optimizer.
                \item Ease of Use with a rich set of operations.
            \end{itemize}
        \end{itemize}
        
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("My Spark App").getOrCreate()
df = spark.read.json("path/to/data.json")
df.show()  # Displays the contents of the DataFrame
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Spark Key Points}
    \begin{itemize}
        \item Spark is designed for fast, user-friendly processing of large datasets.
        \item RDDs enable parallel processing with benefits like fault tolerance and lazy evaluation.
        \item DataFrames offer structured data manipulation and query optimization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Spark Benefits and Use Cases - Introduction}
    \begin{block}{Introduction}
        Apache Spark is an open-source cluster computing framework widely used for big data processing. Below, we explore the key advantages of using Spark and common applications in big data environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Benefits of Using Spark}
    \begin{enumerate}
        \item \textbf{Speed and Performance}
            \begin{itemize}
                \item \textbf{In-Memory Processing:} Accelerates data processing tasks by storing data in memory.
                \item \textbf{Lazy Evaluation:} Optimizes execution of operations until an action is called.
                \item \textit{Example:} Transformation that takes hours with Hadoop takes minutes with Spark.
            \end{itemize}
        \item \textbf{Ease of Use}
            \begin{itemize}
                \item \textbf{High-Level APIs:} Available in Scala, Python, Java, and R.
                \item \textbf{DataFrames and Spark SQL:} Facilitate intuitive data manipulation.
                \item \textit{Example:} \texttt{df.filter(df.age > 21).groupBy("city").count().show()}.
            \end{itemize}
        \item \textbf{Unified Analytics Engine}
            \begin{itemize}
                \item Supports batch processing, interactive queries, stream processing, and machine learning.
                \item \textit{Example:} Using Spark Streaming for live data ingestion and MLlib for predictive modeling.
            \end{itemize}
        \item \textbf{Scalability}
            \begin{itemize}
                \item Easily scales from a single server to thousands of nodes.
            \end{itemize}
        \item \textbf{Rich Libraries}
            \begin{itemize}
                \item Includes MLlib, GraphX, and Spark Streaming for diverse applications.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Use Cases for Apache Spark}
    \begin{enumerate}
        \item \textbf{Data Transformation and ETL}
            \begin{itemize}
                \item Efficiently performs Extract, Transform, Load (ETL) processes.
                \item \textit{Example:} Cleaning and loading transaction data into data warehouses.
            \end{itemize}
        \item \textbf{Real-time Stream Processing}
            \begin{itemize}
                \item Spark Streaming processes data in real-time for applications such as fraud detection.
                \item \textit{Example:} Identifying unusual patterns in online banking transactions.
            \end{itemize}
        \item \textbf{Machine Learning}
            \begin{itemize}
                \item Builds predictive models using MLlib on large datasets.
                \item \textit{Example:} Analyzing social media interactions for customer churn prediction.
            \end{itemize}
        \item \textbf{Graph Processing}
            \begin{itemize}
                \item Processes graph data for social networks and recommendation systems.
                \item \textit{Example:} Developing recommendation engines utilizing user relationships.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Code Example}
    \begin{block}{Conclusion}
        Spark's combination of speed, ease of use, and a rich set of features makes it valuable in modern data processing scenarios. Its ability to handle diverse workloads and scalability ensures it remains a top choice for big data analytics.
    \end{block}
    
    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder.appName("DataProcessing").getOrCreate()

# Load data
data = spark.read.csv("data.csv", header=True, inferSchema=True)

# Data Transformation
filtered_data = data.filter(data.age > 21).groupBy("city").count()

# Display results
filtered_data.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Comparing Hadoop and Spark - Overview}
    \begin{itemize}
        \item Hadoop and Spark are popular frameworks for big data processing.
        \item Differences exist in architecture, performance, and use cases.
        \item This slide contrasts both frameworks for better understanding.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Comparing Hadoop and Spark - Architecture}
    \begin{block}{Hadoop}
        \begin{itemize}
            \item **Core Components**: HDFS (storage) and MapReduce (data processing).
            \item **Batch Processing**: Primarily processes data in large blocks.
            \item **Disk-Based**: Heavily relies on disk storage for intermediate processing.
        \end{itemize}
    \end{block}
    
    \begin{block}{Spark}
        \begin{itemize}
            \item **Unified Engine**: Includes libraries for SQL, machine learning, etc.
            \item **In-Memory Computing**: Speeds up tasks using in-memory processing.
            \item **Micro-batching**: Can handle batch and real-time data processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Comparing Hadoop and Spark - Performance}
    \begin{itemize}
        \item **Speed**:
            \begin{itemize}
                \item Hadoop: Slower due to disk I/O, processing can take hours.
                \item Spark: Significantly faster, up to 100 times faster for iterative tasks.
            \end{itemize}
        
        \item **Latency**:
            \begin{itemize}
                \item Hadoop: Suitable for high-latency tasks.
                \item Spark: Effective for low-latency scenarios, ideal for real-time analytics.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Comparing Hadoop and Spark - Programming Model}
    \begin{block}{Hadoop}
        \begin{itemize}
            \item **Language Support**: Mainly Java, presenting a learning curve.
            \item **Complexity**: Requires more boilerplate code for jobs.
        \end{itemize}
    \end{block}

    \begin{block}{Spark}
        \begin{itemize}
            \item **Language Support**: Supports Python, Scala, R, and Java.
            \item **API**: Higher-level APIs allow for shorter and more readable code.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Comparing Hadoop and Spark - Use Cases}
    \begin{itemize}
        \item **Hadoop**:
            \begin{itemize}
                \item Ideal for historical data processing in data lakes.
                \item Effective for fault-tolerant data storage.
            \end{itemize}
        
        \item **Spark**:
            \begin{itemize}
                \item Best for iterative machine learning tasks and real-time analytics.
                \item Suitable for streaming data and interactive data queries.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Comparing Hadoop and Spark - Key Points}
    \begin{itemize}
        \item **Speed**: Spark is faster due to in-memory processing.
        \item **Flexibility**: Spark's varied library support surpasses Hadoop's fixed methodology.
        \item **Complexity vs. Usability**: Spark's API allows for rapid development compared to Hadoop's complexity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example Code Snippet - Spark DataFrame Creation}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("Example").getOrCreate()

# Load data into DataFrame
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Show data
df.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Comparing Hadoop and Spark - Conclusion}
    \begin{itemize}
        \item Understand strengths and weaknesses to select the right tool.
        \item Each framework has its unique applications; evaluate based on use case.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing a Basic Data Processing Pipeline}
    \begin{block}{Introduction}
        A \textbf{data processing pipeline} is a series of steps whereby the output of one step serves as the input for the next. In big data contexts, frameworks such as \textbf{Hadoop} and \textbf{Spark} are crucial for building these pipelines efficiently.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Hadoop and Spark}
    \begin{itemize}
        \item \textbf{Hadoop}: A distributed computing framework that uses the MapReduce programming model for processing large datasets stored in HDFS (Hadoop Distributed File System).
        
        \item \textbf{Spark}: An in-memory distributed computing framework that processes data faster than Hadoop by allowing data access in RAM, significantly reducing task completion time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up a Simple Data Processing Pipeline}
    \begin{enumerate}
        \item \textbf{Data Ingestion}
        \begin{itemize}
            \item Upload dataset of user log data to HDFS.
            \begin{lstlisting}
hadoop fs -put user_logs.csv /data/user_logs/
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Data Transformation}
        \begin{itemize}
            \item Read and filter data using Spark.
            \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create Spark Session
spark = SparkSession.builder \
    .appName("Simple Data Pipeline") \
    .getOrCreate()

# Read the data
logs_df = spark.read.csv("hdfs:///data/user_logs/user_logs.csv", header=True, inferSchema=True)

# Data Transformation: Filter for 'login' activity
filtered_logs = logs_df.filter(logs_df.activity == 'login')
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Analysis and Output}
    \begin{enumerate}[resume]
        \item \textbf{Data Analysis}
        \begin{itemize}
            \item Count number of logins by user.
            \begin{lstlisting}[language=Python]
logins_count = filtered_logs.groupBy("user_id").count()
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Data Output}
        \begin{itemize}
            \item Write the processed data back to HDFS for future use.
            \begin{lstlisting}[language=Python]
logins_count.write.csv("hdfs:///data/processed_logins/")
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Hadoop} is suited for batch processing, while \textbf{Spark} excels in real-time data processing.
        
        \item Understand the data flow: \textbf{Ingestion → Transformation → Analysis → Output}.
        
        \item Real-world applications include ETL processes, log processing, and social media analysis.
    \end{itemize}
    \begin{block}{Conclusion}
        Implementing a basic data processing pipeline using Hadoop and Spark enables efficient handling of large datasets. Begin to build your own pipelines for data extraction, transformation, and analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: ETL in Action}
    \begin{block}{Overview of ETL Process}
        ETL (Extract, Transform, Load) is a crucial process in data warehousing that consolidates data from various sources, transforms it into a suitable format, and loads it into a repository for analysis. 
    \end{block}
    \begin{block}{Context of the Case Study}
        \begin{itemize}
            \item \textbf{Scenario}: A retail company consolidates sales data from transaction databases, website logs, and customer feedback.
            \item \textbf{Objective}: Build a scalable ETL pipeline using Hadoop for storage and Spark for processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step ETL Process}
    \begin{enumerate}
        \item \textbf{Extract}
        \begin{itemize}
            \item \textbf{Tools Used}: Apache Sqoop for relational databases and Spark’s DataFrames for NoSQL.
            \item \textbf{Action}: Extract data from MySQL databases and JSON logs stored in HDFS.
            \item \textbf{Example Code}:
            \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ETL_Example").getOrCreate()
sales_data = spark.read.format("jdbc").options(
    url="jdbc:mysql://database-url/dbname",
    dbtable="sales_table",
    user="username",
    password="password").load()
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Transform}
        \begin{itemize}
            \item \textbf{Processes Included}: Cleaning, filtering, aggregating, enriching data.
            \item \textbf{Common Transformations}:
            \begin{itemize}
                \item Data Cleaning: Remove duplicates, handle missing values.
                \item Filtering: Extract data for a specific date range.
                \item Aggregation: Summarize sales per category.
            \end{itemize}
            \item \textbf{Example Transformation}:
            \begin{lstlisting}[language=Python]
from pyspark.sql.functions import col

transformed_data = sales_data.filter(col('transaction_date') >= '2022-01-01')\
                               .groupBy('item_category')\
                               .agg({'sales_amount': 'sum', 'transaction_id': 'count'})
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step ETL Process (Cont'd)}
    \begin{enumerate}[resume]
        \item \textbf{Load}
        \begin{itemize}
            \item \textbf{Destination}: A data warehouse solution (e.g., Amazon Redshift, Google BigQuery).
            \item \textbf{Action}: Store structured and cleaned data for reporting and analysis.
            \item \textbf{Example Code}:
            \begin{lstlisting}[language=Python]
transformed_data.write \
    .format("jdbc") \
    .options(
        url="jdbc:redshift://endpoint:port/database",
        dbtable="aggregated_sales",
        user="username",
        password="password") \
    .mode("overwrite") \
    .save()
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Scalability with Hadoop and Spark.
            \item Real-time data processing capabilities.
            \item Data enrichment through integration of external datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - Introduction}
    \begin{block}{Introduction to Ethical Considerations}
        Data processing involves the collection, storage, and utilization of vast amounts of information. 
        With this capability comes significant ethical responsibilities. Ensuring that data is handled responsibly 
        is crucial for maintaining public trust and compliance with legal frameworks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - Key Implications}
    \begin{enumerate}
        \item \textbf{Data Privacy and Protection}
            \begin{itemize}
                \item \textbf{Definition}: Privacy refers to the right of individuals to control their personal information and how it's collected, accessed, and used.
                \item \textbf{Importance}: Breaches can lead to identity theft, discrimination, and loss of trust in organizations.
            \end{itemize}
        
        \item \textbf{Informed Consent}
            \begin{itemize}
                \item \textbf{Definition}: Organizations must obtain explicit permission before collecting, using, or sharing data.
                \item \textbf{Example}: A website requests users to accept a privacy policy before signing up for a newsletter.
            \end{itemize}

        \item \textbf{Data Minimization}
            \begin{itemize}
                \item \textbf{Definition}: Collect only data necessary for the stated purpose.
                \item \textbf{Best Practice}: Only gather what is essential for service delivery.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regulatory Frameworks}
    \begin{enumerate}
        \item \textbf{General Data Protection Regulation (GDPR)}
            \begin{itemize}
                \item \textbf{Overview}: A regulation by the EU to protect individuals' privacy.
                \item \textbf{Key Components}:
                    \begin{itemize}
                        \item \textbf{Right to Access}: Individuals can request access to their data.
                        \item \textbf{Right to be Forgotten}: Individuals can request deletion of unnecessary data.
                        \item \textbf{Fines}: Organizations can face penalties up to €20 million or 4\% of global turnover.
                    \end{itemize}
            \end{itemize}

        \item \textbf{Health Insurance Portability and Accountability Act (HIPAA)}
            \begin{itemize}
                \item \textbf{Overview}: A U.S. law to protect patient information.
                \item \textbf{Key Components}:
                    \begin{itemize}
                        \item \textbf{Privacy Rule}: National standards for protection of health information.
                        \item \textbf{Security Rule}: Requirements for safeguarding ePHI.
                        \item \textbf{Penalties}: Fines ranging from \$100 to \$50,000 per violation, maximum \$1.5 million annually.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Governance}
    \begin{block}{Introduction to Data Governance}
        Data governance is a management framework that ensures data accuracy, availability, and security while adhering to regulations.
    \end{block}
    \begin{itemize}
        \item Critical for organizations handling sensitive data
        \item Mitigates risks related to data breaches and non-compliance
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Strategies for Effective Data Governance}
    \begin{enumerate}
        \item Establish a Data Governance Framework
        \item Data Classification and Inventory
        \item Compliance with Regulations
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Strategy: Establish a Data Governance Framework}
    \begin{itemize}
        \item \textbf{Definition:} A structured approach including roles, responsibilities, and policies.
        \item \textbf{Example:} A steering committee overseeing data strategy with data stewards managing data quality.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Classification and Inventory}
    \begin{itemize}
        \item \textbf{Definition:} Categorizing data based on sensitivity and value.
        \item \textbf{Example:} Classifying customer data as "Highly Sensitive" and company financials as "Confidential."
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Compliance with Regulations}
    \begin{itemize}
        \item \textbf{GDPR:} Protects EU residents' personal data, requiring explicit consent.
            \begin{block}{Key Point}
                Fines can be up to €20 million or 4\% of global turnover for non-compliance.
            \end{block}
        \item \textbf{HIPAA:} Regulates healthcare information with strict data access measures.
            \begin{block}{Key Point}
                Fines can reach \$1.5 million per violation annually.
            \end{block}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{More Key Strategies}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item Implement Data Quality Controls
        \item Data Access Management
        \item Regular Audits and Assessments
        \item Training and Awareness Programs
        \item Incident Response Plan
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Takeaways}
    \begin{itemize}
        \item Implementing best practices in data governance aids compliance and trust.
        \item Importance of establishing frameworks, maintaining data quality, and training.
        \item Regular audits and proactive incident response are crucial.
    \end{itemize}
    \begin{block}{Next Steps}
        Review specific case studies where best practices have mitigated governance failures.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Future Directions - Part 1}
    \begin{block}{Key Points Recap}
        In this chapter, we explored essential concepts of data processing frameworks and their importance in data governance. Here are the key points covered:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Data Processing Frameworks}
        \begin{itemize}
            \item \textbf{Hadoop}: A distributed file system for large data sets.
            \item \textbf{Spark}: An open-source engine for data processing with fault tolerance.
            \item \textbf{Airflow}: A platform for scheduling and monitoring workflows.
        \end{itemize}

        \item \textbf{Data Governance}
        \begin{itemize}
            \item Management of data availability, integrity, and security.
            \item Strategies include data stewardship roles and access controls.
        \end{itemize}

        \item \textbf{Best Practices}
        \begin{itemize}
            \item Regular updates to frameworks for compatibility.
            \item Scalable architectures for growing data volumes.
            \item Prioritizing data quality to avoid garbage in, garbage out.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Future Directions - Part 2}
    \begin{block}{Future Directions}
        Several trends are shaping the future of data processing:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Increased Automation}
        \begin{itemize}
            \item AI and ML will automate workflows, enhancing efficiency.
            \item Example: DBT automates the transformation stage in data pipelines.
        \end{itemize}

        \item \textbf{Real-time Data Processing}
        \begin{itemize}
            \item Growing demand for near real-time analytics.
            \item Technologies like Apache Kafka and Pulsar are crucial for streaming processing.
        \end{itemize}

        \item \textbf{Serverless Architectures}
        \begin{itemize}
            \item Simplifies scaling and reduces infrastructure management costs.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Future Directions - Part 3}
    \begin{block}{Future Directions Continued}
        Additional trends to consider:
    \end{block}

    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Data Privacy and Ethics}
        \begin{itemize}
            \item Investments in technologies to protect user information, such as differential privacy.
        \end{itemize}

        \item \textbf{Interoperability of Frameworks}
        \begin{itemize}
            \item Seamless integration across various data processing frameworks.
            \item Importance of standards and APIs for effective data integration.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Conclusion}
        Mastering data processing frameworks is vital for data-driven organizations. Embrace emerging trends to ensure effective data governance.
    \end{block}
\end{frame}


\end{document}