\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Title Page Information
\title[Decision Trees and Can Trees]{Week 5: Decision Trees and Can Trees}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Decision Trees - Overview}
    Decision Trees are a powerful and intuitive method used in data mining, machine learning, and statistical analysis for making predictions or classifications. 
    They serve as a visual representation of decision-making processes and can handle both categorical and continuous data.
    
    \begin{block}{Key Features}
        \begin{itemize}
            \item \textbf{Hierarchical Structure}: Flowchart representation with internal nodes as tests on attributes, branches as outcomes, and leaves as decisions.
            \item \textbf{Interpretability}: Easy to understand and interpret, making them accessible for non-experts.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Decision Trees - Significance and Applications}
    \begin{block}{Significance in Data Mining}
        Decision Trees are significant because they:
        \begin{itemize}
            \item \textbf{Facilitate Automation}: Enable automated decisions based on historical data.
            \item \textbf{Handle Missing Values}: Make predictions even with incomplete data.
            \item \textbf{Capture Non-linear Relationships}: Model complex relationships without transforming data.
        \end{itemize}
    \end{block}

    \begin{block}{Real-World Applications}
        \begin{enumerate}
            \item \textbf{Healthcare}: Disease diagnosis based on symptoms and test results.
            \item \textbf{Finance}: Credit scoring models for loan approval decisions.
            \item \textbf{Marketing}: Customer segmentation for targeted campaigns.
            \item \textbf{Manufacturing}: Quality control to predict product failures.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Decision Trees - Example and Conclusion}
    \begin{block}{Example Decision Tree}
        Consider a simple Decision Tree for predicting whether a person will buy a computer:
        \begin{enumerate}
            \item \textbf{Node 1}: Is the person's income > \$50,000?
                \begin{itemize}
                    \item Yes → Go to Node 2
                    \item No → No Purchase
                \end{itemize}
            \item \textbf{Node 2}: Is the person's age < 30?
                \begin{itemize}
                    \item Yes → Purchase
                    \item No → No Purchase
                \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Intuitive and versatile for classification and regression.
            \item Widely applicable, offering significant advantages in automation and interpretability.
            \item Can be combined with ensemble methods like Random Forests for better accuracy.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding Decision Trees is crucial for leveraging data-driven insights, empowering informed business decisions, and understanding complex datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Definition}
    \begin{block}{Definition}
        A \textbf{decision tree} is a flowchart-like structure used for making decisions or predictions based on a set of rules derived from data. It is a supervised learning algorithm commonly used in data mining and machine learning applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Structure}
    \begin{block}{Components of a Decision Tree}
        The decision tree consists of three key components:
        \begin{itemize}
            \item \textbf{Nodes}:
            \begin{itemize}
                \item \textbf{Root Node}: The top-most node representing the entire dataset.
                \item \textbf{Internal Nodes}: Points where the dataset is split based on specific feature values.
            \end{itemize}
            
            \item \textbf{Branches}: Connections between nodes that represent the outcome of tests performed at each node.
            
            \item \textbf{Leaves}: Terminal nodes that provide the predicted outcome or final decision.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Diagram}
    \begin{block}{Diagram of a Decision Tree}
        \begin{center}
            \includegraphics[width=0.6\linewidth]{decision_tree_diagram.png} % Placeholder for a diagram image
        \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Example}
    \begin{block}{Example Scenario}
        Imagine predicting if a person buys a computer based on age and income. 
        \begin{enumerate}
            \item \textbf{Root Node}: Age < 30?
            \begin{enumerate}
                \item Yes: Go to Node A
                \item No: Go to Node B
            \end{enumerate}
            \item \textbf{Node A}: Income < \$50,000?
            \begin{enumerate}
                \item Yes: Leaf 1 (Not Buy)
                \item No: Leaf 2 (Buy)
            \end{enumerate}
            \item \textbf{Node B}: Income < \$80,000?
            \begin{enumerate}
                \item Yes: Leaf 3 (Not Buy)
                \item No: Leaf 4 (Buy)
            \end{enumerate}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Key Points}
    \begin{itemize}
        \item Decision trees are intuitive and visually interpretable.
        \item They can handle both classification and regression tasks.
        \item Decision trees can work with categorical and continuous data.
        \item Overfitting is a potential problem if the tree is too deep.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Conclusion}
    \begin{block}{Conclusion}
        Understanding decision trees is vital for analyzing data patterns and making informed decisions based on those patterns. 
        They provide an accessible way to utilize data effectively in real-world situations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics - Introduction}
    \begin{block}{Key Characteristics of Decision Trees}
        Decision trees are versatile tools in machine learning. This presentation discusses their key characteristics, focusing on:
        \begin{itemize}
            \item Interpretability
            \item Transparency
            \item Structure
            \item Non-Linear Relationships
            \item Robustness to Irregular Data
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics - Interpretability and Transparency}
    \begin{enumerate}
        \item \textbf{Interpretability}
            \begin{itemize}
                \item \textbf{Definition:} Decision trees provide a clear, visual representation of decisions.
                \item \textbf{How It Works:} Each node represents a feature, leading to branches for different outcomes.
                \item \textbf{Example:} A medical diagnosis decision tree starting with the symptom "Fever".
            \end{itemize}

        \item \textbf{Transparency}
            \begin{itemize}
                \item \textbf{Definition:} The decision-making process can easily be traced from root to leaf.
                \item \textbf{Advantages:} Unlike complex models, the logic behind conclusions is clear.
                \item \textbf{Example:} In credit scoring, paths can be followed based on criteria like "Credit Score".
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics - Structure and Robustness}
    \begin{enumerate}
        \setcounter{enumi}{2} % Start from item 3
        \item \textbf{Structure}
            \begin{itemize}
                \item \textbf{Nodes:} Feature or decision point (e.g., "Age > 30?").
                \item \textbf{Branches:} Outcome of a decision (e.g., "Yes" or "No").
                \item \textbf{Leaves:} Final outcome (e.g., "Approve Loan" or "Deny Loan").
            \end{itemize}

        \item \textbf{Robust to Irregular Data}
            \begin{itemize}
                \item Decision trees handle missing values without imputation.
                \item They are less impacted by outliers compared to linear models.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustration of a Decision Tree Structure}
    \begin{center}
        \texttt{
            \begin{tabular}{c}
              [Age > 30?] \\
                / \ \ \ \ \ \ \ \ \ \ \ \\
              Yes \ \ \ \ \ \ \ \ \ \ \ \ No \\
               / \ \ \ \ \ \ \ \ \ \ \ \  \\
           [Income > 50k?] \ \ \ \ \ \ [Approve Loan] \\
                / \ \ \ \ \ \ \ \ \ \ \\
              Yes \ \ \ \ \ \ \ \ \ \ \  No \\
               / \ \ \ \ \ \ \ \ \ \ \  \\
       [Approve Loan] \ \ \ \ \ \ \ \ \ [Deny Loan]
            \end{tabular}
        }
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        The characteristics of decision trees, particularly their interpretability and transparency, make them an effective choice for communicating complex, data-driven insights. Understanding the algorithms used to construct these trees is essential for grasping their foundational role in machine learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Tree Algorithms}
    \begin{block}{Overview}
        Decision trees are a powerful tool in machine learning, particularly for classification and regression tasks. 
        Several algorithms have been developed to construct decision trees effectively. 
        We will discuss three prominent algorithms: ID3, C4.5, and CART.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ID3 (Iterative Dichotomiser 3)}
    \begin{itemize}
        \item \textbf{Concept}: Introduced by Ross Quinlan in 1986; uses a top-down approach for recursive data splitting.
        \item \textbf{Splitting Criterion}: ID3 uses \textbf{Information Gain} based on entropy to decide the best attribute to split the data.
        \begin{equation}
            \text{Entropy}(S) = -\sum_{i=1}^{c} p_i \log_2 p_i
        \end{equation}
        Where \(p_i\) is the probability of class \(i\) in the dataset \(S\).
        \item \textbf{Limitations}: Cannot handle continuous attributes directly and does not support pruning, leading to potential overfitting.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{C4.5 and CART}
    \begin{itemize}
        \item \textbf{C4.5}
        \begin{itemize}
            \item \textbf{Concept}: An extension of ID3 introduced in 1993; addresses ID3 limitations.
            \item \textbf{Splitting Criterion}: Uses \textbf{Gain Ratio} to reduce bias towards attributes with many levels.
            \begin{equation}
                \text{Gain Ratio} = \frac{\text{Information Gain}}{\text{Split Information}}
            \end{equation}
            \item \textbf{Continuous Attributes}: Handles both categorical and continuous attributes via thresholding.
            \item \textbf{Pruning}: Includes a post-pruning step to reduce overfitting.
        \end{itemize}

        \item \textbf{CART (Classification and Regression Trees)}
        \begin{itemize}
            \item \textbf{Concept}: Introduced by Breiman et al. in 1986; supports both classification and regression tasks.
            \item \textbf{Splitting Criterion}: For classification uses the \textbf{Gini Index}, for regression uses \textbf{Mean Squared Error (MSE)}.
            \begin{equation}
                Gini(D) = 1 - \sum_{i=1}^{c} p_i^2
            \end{equation}
            \item \textbf{Binary Trees}: Creates binary trees—each internal node results in two child nodes.
            \item \textbf{Pruning}: Uses pruning to improve generalization on unseen data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Activity}
    \begin{itemize}
        \item \textbf{Key Takeaways}:
        \begin{itemize}
            \item \textbf{ID3}: Effective for simple datasets; biased towards attributes with many values.
            \item \textbf{C4.5}: Enhances ID3 with support for continuous data and pruning.
            \item \textbf{CART}: Versatile for both classification and regression with a focus on binary splits and generalization.
        \end{itemize}
        \item \textbf{Conclusion}: These algorithms provide a robust foundation for creating effective decision trees essential for various machine learning tasks.
        \item \textbf{In-Class Activity}: Students to implement each algorithm on a dataset in groups and compare performance, reinforcing understanding through practical experience.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Introduction}
    \begin{block}{Overview}
        Decision trees are a popular machine learning technique used for classification and regression tasks.
        They model decisions and their possible consequences, including chance event outcomes, resource costs, and utility.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Building a Decision Tree}
    \begin{enumerate}
        \item \textbf{Data Preparation}
        \begin{itemize}
            \item Collecting Data: Start with a dataset that includes both input features and target outcomes.
            \item Cleaning Data: Handle missing values and ensure data is in the correct format.
        \end{itemize}

        \item \textbf{Splitting Criteria}
        \begin{itemize}
            \item Entropy and Information Gain:
            \begin{equation*}
                \text{Entropy}(S) = - \sum_{i=1}^{C} p_i \log_2(p_i)
            \end{equation*}
            \begin{equation*}
                \text{Information Gain}(S, A) = \text{Entropy}(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Entropy}(S_v)
            \end{equation*}
            \item Gini Impurity:
            \begin{equation*}
                Gini(S) = 1 - \sum_{i=1}^{C} (p_i)^2
            \end{equation*}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Tree Structure and Pruning}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Creating Tree Nodes}
        \begin{itemize}
            \item Root Node: Represents the entire dataset.
            \item Decision Nodes: Split into further branches based on a chosen attribute.
            \item Leaf Nodes: Terminal nodes that represent the final outcome.
        \end{itemize}

        \item \textbf{Pruning the Tree}
        \begin{itemize}
            \item After building, pruning simplifies the tree by removing sections that provide little predictive power.
            \item Techniques include:
            \begin{itemize}
                \item Cost Complexity Pruning
                \item Post-pruning
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Example and Key Points}
    \begin{block}{Example}
        Suppose we have a dataset of patients with features like age, blood pressure, and cholesterol level.
        \begin{enumerate}
            \item Start with all patient data at the root node.
            \item Calculate Information Gain for each feature.
            \item Select feature with maximum Information Gain (e.g., "Cholesterol Level").
            \item Split dataset into branches for high and low cholesterol.
            \item Repeat for each branch until leaf nodes are reached.
            \item Prune the tree to prevent overfitting.
        \end{enumerate}
    \end{block}

    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Decision Trees are intuitive and easy to visualize.
            \item Effective feature selection is crucial for a good model.
            \item Overfitting can lead to poor predictive performance, hence pruning is necessary.
            \item Various criteria (Entropy, Gini Impurity) can guide how splits are made.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Decision Tree Implementation}
    \begin{block}{What is a Decision Tree?}
        A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. 
        It mimics human decision-making by creating a model that predicts the value of a target variable based on several input features.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Part 1}
    \begin{enumerate}
        \item Import Required Libraries
        \begin{itemize}
            \item \textbf{Python Example:}
            \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
            \end{lstlisting}
            \item \textbf{R Example:}
            \begin{lstlisting}[language=R]
library(rpart)
library(rpart.plot)
            \end{lstlisting}
        \end{itemize}
        
        \item Load the Dataset
        \begin{itemize}
            \item \textbf{Python Example:}
            \begin{lstlisting}[language=Python]
data = pd.read_csv('iris.csv')
            \end{lstlisting}
            \item \textbf{R Example:}
            \begin{lstlisting}[language=R]
data <- read.csv('iris.csv')
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Part 2}
    \begin{enumerate}[resume]
        \item Prepare the Data
        \begin{itemize}
            \item \textbf{Python Example:}
            \begin{lstlisting}[language=Python]
X = data.drop('species', axis=1)
y = data['species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            \end{lstlisting}
            \item \textbf{R Example:}
            \begin{lstlisting}[language=R]
set.seed(42)
index <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[index, ]
test_data <- data[-index, ]
            \end{lstlisting}
        \end{itemize}

        \item Create the Decision Tree Model
        \begin{itemize}
            \item \textbf{Python Example:}
            \begin{lstlisting}[language=Python]
model = DecisionTreeClassifier()
model.fit(X_train, y_train)
            \end{lstlisting}
            \item \textbf{R Example:}
            \begin{lstlisting}[language=R]
model <- rpart(species ~ ., data = train_data, method = "class")
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Part 3}
    \begin{enumerate}[resume]
        \item Make Predictions
        \begin{itemize}
            \item \textbf{Python Example:}
            \begin{lstlisting}[language=Python]
y_pred = model.predict(X_test)
            \end{lstlisting}
            \item \textbf{R Example:}
            \begin{lstlisting}[language=R]
predictions <- predict(model, test_data, type = "class")
            \end{lstlisting}
        \end{itemize}

        \item Evaluate the Model
        \begin{itemize}
            \item \textbf{Python Example:}
            \begin{lstlisting}[language=Python]
accuracy = metrics.accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
            \end{lstlisting}
            \item \textbf{R Example:}
            \begin{lstlisting}[language=R]
confusion_matrix <- table(test_data$species, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste('Accuracy:', accuracy))
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Decision Trees are intuitive and easy to visualize.
        \item The implementation process involves several steps: importing libraries, loading data, preprocessing, model creation, prediction, and evaluation.
        \item Python and R provide robust libraries for Decision Tree implementations (e.g., \texttt{sklearn} for Python and \texttt{rpart} for R).
        \item Always evaluate your model's performance using metrics like accuracy, precision, recall, and F1-score.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Understanding how to implement a Decision Tree in Python or R equips you for powerful data analysis and model development.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation}
    Understanding the methods for evaluating decision tree performance:
    \begin{itemize}
        \item Accuracy
        \item Precision
        \item Recall
        \item F1-score
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Overview}
    When evaluating decision tree performance, it’s important to understand:
    \begin{itemize}
        \item **Accuracy**: The ratio of correctly predicted instances to the total instances.
        \item **Precision**: Proportion of positive identifications that were actually correct.
        \item **Recall**: Proportion of actual positives correctly identified.
        \item **F1-score**: Harmonic mean of precision and recall.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy}
    \begin{block}{Definition}
        Accuracy is defined as the ratio of correctly predicted instances to total instances.
    \end{block}
    \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    Where:
    \begin{itemize}
        \item \(TP\) = True Positives
        \item \(TN\) = True Negatives
        \item \(FP\) = False Positives
        \item \(FN\) = False Negatives
    \end{itemize}
    Example Calculation:
    \begin{equation}
        \text{Accuracy} = \frac{90}{100} = 0.90 \text{ or } 90\%
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision}
    \begin{block}{Definition}
        Precision indicates how many of the instances predicted as positive are actually positive.
    \end{block}
    \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
    Example Calculation:
    \begin{equation}
        \text{Precision} = \frac{30}{40} = 0.75 \text{ or } 75\%
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall (Sensitivity)}
    \begin{block}{Definition}
        Recall measures how many actual positive instances were correctly identified.
    \end{block}
    \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
    Example Calculation:
    \begin{equation}
        \text{Recall} = \frac{30}{50} = 0.60 \text{ or } 60\%
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-score}
    \begin{block}{Definition}
        The F1-score combines precision and recall, capturing their balance.
    \end{block}
    \begin{equation}
        \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
    Example Calculation:
    \begin{equation}
        \text{F1-score} = 2 \times \frac{0.75 \times 0.60}{0.75 + 0.60} = 0.6667 \text{ or } 66.67\%
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item High accuracy does not always indicate a good model, especially in imbalanced datasets.
        \item Precision and recall should be considered based on application context, such as medical diagnostics.
        \item The F1-score is effective when seeking a balance between precision and recall.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Application}
    \begin{block}{Example}
        In a banking scenario:
        \begin{itemize}
            \item A decision tree predicts loan defaults.
            \item High recall is essential to identify as many defaulters as possible.
            \item This minimizes the risk of significant financial losses from false negatives.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Overview}
    \begin{itemize}
        \item Decision trees are a popular predictive modeling tool.
        \item Key advantages include:
        \begin{itemize}
            \item Ease of interpretation
            \item Handling non-linear data
            \item Minimal data preparation
            \item Incorporation of interactions
            \item Robustness to outliers
            \item Versatile application across fields
            \item Support for ensemble methods
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Key Points}
    \begin{enumerate}
        \item \textbf{Ease of Interpretation}
        \begin{itemize}
            \item Visual representation of decision-making.
            \item Example: Loan granting based on credit score, income, and debt-to-income ratio.
        \end{itemize}

        \item \textbf{Handling Non-Linear Data}
        \begin{itemize}
            \item Models complex feature relationships without assumptions.
            \item Example: Housing price predictions based on location, bedrooms, and amenities.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - More Key Points}
    \begin{enumerate}[resume]
        \item \textbf{Minimal Data Preparation}
        \begin{itemize}
            \item Less processing required; handles various data types directly.
            \item Example: Analyzing retail customer data with categorical variables.
        \end{itemize}

        \item \textbf{Incorporation of Interactions}
        \begin{itemize}
            \item Automatically captures interactions between variables.
            \item Example: Medical diagnosis based on combined symptoms.
        \end{itemize}

        \item \textbf{Robust to Outliers}
        \begin{itemize}
            \item Less affected by extreme values.
            \item Example: User income dataset robustness.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Final Key Points}
    \begin{enumerate}[resume]
        \item \textbf{Versatile Application}
        \begin{itemize}
            \item Applicable in finance, marketing, healthcare, etc.
            \item Example: Assessing diabetes risk based on patient factors.
        \end{itemize}

        \item \textbf{Support for Ensemble Methods}
        \begin{itemize}
            \item Acts as a base learner for methods like Random Forest and Gradient Boosting.
            \item Example: Random Forest improves performance by averaging multiple trees.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Decision Trees are intuitive and effective for diverse datasets.
            \item They excel in capturing complex relationships and interactions.
            \item Their benefits make them valuable in real-world applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Overview}
    Decision Trees, while powerful, have several notable limitations that can affect their performance in data analysis and predictive modeling. Understanding these constraints is pivotal for appropriate application and effective utilization.
    
    \begin{itemize}
        \item Overfitting
        \item Sensitivity to noisy data
        \item Biases in decision making
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Overfitting}
    \begin{block}{Overfitting}
        \begin{itemize}
            \item **Explanation**: Captures noise in the training data, leading to poor performance on unseen data.
            \item **Example**: A tree perfectly classifying all training samples through memorization rather than generalization.
            \item **Key Point**: Model complexity must be balanced; techniques like pruning can help combat overfitting.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Sensitivity to Noisy Data}
    \begin{block}{Sensitivity to Noisy Data}
        \begin{itemize}
            \item **Explanation**: Noise can significantly skew partitions leading to incorrect splits and misclassifications.
            \item **Example**: Erroneous customer data causing misclassification in predictions.
            \item **Key Point**: Careful data preprocessing is essential; ensemble methods like Random Forest can mitigate sensitivity.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Biases in Decision Making}
    \begin{block}{Biases in Decision Making}
        \begin{itemize}
            \item **Explanation**: Decision Trees may favor features with more levels, dominating the tree's structure.
            \item **Example**: Over-reliance on a feature like "Job Title" in predicting loan approval, overshadowing other relevant features.
            \item **Key Point**: Employing feature selection and data balancing strategies can reduce the impact of biased features.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Summary}
    Decision Trees are valuable tools, but recognizing their limitations is essential for effective application.
    
    \begin{itemize}
        \item Key limitations: Overfitting, sensitivity to noisy data, and biases in decision making.
        \item Addressing issues with techniques such as pruning, data preprocessing, and ensemble methods is crucial.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Decision Trees - Introduction}
    
    \begin{block}{Introduction}
        A decision tree is a flowchart-like structure used for decision-making and predictive modeling. 
        It breaks down a dataset into smaller subsets while developing a tree structure based on various feature values.
    \end{block}
    
    \begin{block}{Importance}
        Widely used due to their simplicity and interpretability, decision trees allow for clear visualization of decision-making processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Decision Trees - Case Studies}
    
    \textbf{1. Healthcare: Disease Diagnosis}
    \begin{itemize}
        \item \textbf{Case Study:} Predicting Diabetes
        \item \textbf{How it works:} Analyzes patient data (age, weight, blood pressure) to predict the likelihood of diabetes.
        \item \textbf{Outcome:} Enables early identification of at-risk patients for proactive medical intervention.
    \end{itemize}
    
    \textbf{2. Finance: Credit Scoring}
    \begin{itemize}
        \item \textbf{Case Study:} Loan Approval
        \item \textbf{Application:} Automated evaluation using income, credit history, and debts.
        \item \textbf{Outcome:} Reduces bias, speeds up approval while ensuring accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Decision Trees - Continued}
    
    \textbf{3. Marketing: Customer Segmentation}
    \begin{itemize}
        \item \textbf{Case Study:} Targeted Advertising
        \item \textbf{How it works:} Categorizes customers based on demographics and purchase history.
        \item \textbf{Outcome:} Leads to targeted marketing strategies and increased conversion rates.
    \end{itemize}
    
    \textbf{4. Manufacturing: Quality Control}
    \begin{itemize}
        \item \textbf{Case Study:} Defect Prediction
        \item \textbf{Application:} Predicts defects based on conditions like temperature and humidity.
        \item \textbf{Outcome:} Early identification of quality issues leading to improved reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Decision Trees - Final Cases and Conclusion}
    
    \textbf{5. Agriculture: Crop Yield Prediction}
    \begin{itemize}
        \item \textbf{Case Study:} Yield Optimization
        \item \textbf{How it works:} Analyzes variables such as soil type and rainfall to predict crop yields.
        \item \textbf{Outcome:} Informs practices for increased yields and sustainability.
    \end{itemize}
    
    \begin{block}{Conclusion}
        \begin{itemize}
            \item \textbf{Versatility:} Decision trees enhance decision-making across diverse fields.
            \item \textbf{Interactivity:} Their structure encourages stakeholder engagement and understanding.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Can Trees}
    Can Trees represent an evolved form of traditional decision trees, designed to tackle some of the limitations associated with them. 
    \begin{itemize}
        \item Traditional decision trees are powerful tools for classification and regression tasks.
        \item Often struggle with overfitting and robustness in high-dimensional spaces.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences from Traditional Decision Trees - 1}
    \begin{enumerate}
        \item \textbf{Structure and Flexibility}:
            \begin{itemize}
                \item \textbf{Traditional Decision Trees}: 
                    \begin{itemize}
                        \item Have a binary split at each node.
                        \item Rigid structure; may not capture complex patterns.
                    \end{itemize}
                \item \textbf{Can Trees}:
                    \begin{itemize}
                        \item Allow for multi-way splits at each node.
                        \item Increased flexibility for more expressive models.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Handling Missing Values}:
            \begin{itemize}
                \item \textbf{Traditional Decision Trees}:
                    \begin{itemize}
                        \item Often omit instances or use imputation techniques.
                    \end{itemize}
                \item \textbf{Can Trees}:
                    \begin{itemize}
                        \item Natively handle missing values using probabilistic approaches.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences from Traditional Decision Trees - 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Robustness}:
            \begin{itemize}
                \item \textbf{Traditional Decision Trees}:
                    \begin{itemize}
                        \item Prone to overfitting, especially in small datasets.
                    \end{itemize}
                \item \textbf{Can Trees}:
                    \begin{itemize}
                        \item Incorporate regularization techniques to prevent overfitting.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case}
    \begin{itemize}
        \item \textbf{Modeling Customer Behavior}:
        \begin{itemize}
            \item Traditional decision trees may classify based on two distinct categories.
            \item Can Trees can categorize customers based on multiple attributes:
                \begin{itemize}
                    \item Gender, Age, Shopping Frequency, Purchase Type.
                \end{itemize}
        \end{itemize}
        \item \textbf{Benefits}:
        \begin{itemize}
            \item Provides nuanced classification.
            \item Enhances targeted marketing strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Can Trees introduce advanced methodologies to decision trees, enabling:
    \begin{itemize}
        \item Improved performance in various fields (finance, marketing).
        \item Better handling of complex relationships.
        \item Robustness against overfitting and improved missing value management.
    \end{itemize}
    Understanding these differences is essential for leveraging the full potential of decision tree models in practical applications.
\end{frame}

\begin{frame}[fragile]` and `\end{frame}

\begin{frame}[fragile]
    \frametitle{Can Tree Characteristics - Introduction}
    \begin{itemize}
        \item Can Trees are an advanced form of decision trees.
        \item They address limitations of traditional decision trees.
        \item This slide will highlight key characteristics and improvements in decision-making processes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Can Tree Characteristics - Key Features}
    \begin{enumerate}
        \item \textbf{Adaptive Structure}
        \begin{itemize}
            \item Allows for multi-way splits.
            \item \textit{Example:} Age categories can be "0-18", "19-35", "36-65", "65+".
        \end{itemize}

        \item \textbf{Reduced Overfitting}
        \begin{itemize}
            \item Employs regularization techniques.
            \item \textit{Illustration:} Visual comparison of pre-pruning versus post-pruning.
        \end{itemize}
        
        \item \textbf{Enhanced Ensemble Learning}
        \begin{itemize}
            \item Suitable for Bagging and Boosting methods.
            \item \textit{Key Point:} Multiple Can Trees improve accuracy and robustness.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Can Tree Characteristics - Further Features}
    \begin{enumerate}[resume]
        \item \textbf{Support for Categorical and Continuous Data}
        \begin{itemize}
            \item Handles diverse datasets effectively.
            \item \textit{Real-world Example:} Financial analysis assessing categorical (customer type) and continuous (transaction amount) data.
        \end{itemize}
        
        \item \textbf{Interpretability and Visual Comprehension}
        \begin{itemize}
            \item Balances complexity with model interpretability.
            \item \textit{Diagram Suggestion:} Simplified decision path to illustrate decision-making process.
        \end{itemize}
        
        \item \textbf{Advantages Over Traditional Decision Trees}
        \begin{itemize}
            \item Flexibility in modeling for varied data patterns.
            \item Less prone to bias, improving decision accuracy.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Can Tree Characteristics - Summary}
    \begin{itemize}
        \item Can Trees evolve decision tree technology.
        \item They provide robust, flexible structures for complex datasets.
        \item Enhance interpretability while emphasizing performance.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementing Can Trees}
    \begin{block}{Introduction to Can Trees}
        A Can Tree (Categorical and Numerical Tree) is an advanced version of decision trees designed to handle both categorical and numerical data efficiently while addressing common limitations seen in traditional decision trees.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Benefits of Can Trees}
    \begin{itemize}
        \item Reduce overfitting through a more sophisticated splitting method.
        \item Ability to incorporate domain knowledge in decision-making.
        \item Provide improved interpretability and performance.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Algorithm Steps for Implementation}
    \begin{enumerate}
        \item \textbf{Data Preparation}: Convert categorical features to numerical encoding (e.g., one-hot encoding) and normalize numerical values if necessary.
        \item \textbf{Tree Structure Initialization}: Start with the root node containing the entire dataset.
        \item \textbf{Splitting Criteria}: Choose an optimal feature to split using criteria like Gini impurity or information gain.
        \item \textbf{Recursive Tree Construction}: Create child nodes based on the split and recursively repeat the splitting process until stopping criteria are met.
        \item \textbf{Pruning}: Post-process the tree to remove nodes that add little predictive power to enhance generalization.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier
import pandas as pd

# Load your dataset
data = pd.read_csv('data.csv')

# Prepare features and labels
features = data[['feature1', 'feature2']]  # Replace with relevant features
labels = data['target']  # Replace with the target variable

# Instantiate and fit the Can Tree model (Decision Tree in this context)
model = DecisionTreeClassifier(criterion='gini', max_depth=5)
model.fit(features, labels)

# Make predictions
predictions = model.predict(features)

# Display decision tree
from sklearn.tree import export_text
tree_rules = export_text(model, feature_names=list(features.columns))
print(tree_rules)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Real-World Application}
    \begin{block}{Use Case: Customer Segmentation}
        Can trees can be used in marketing to segment customers based on both numerical data (age, income) and categorical data (gender, purchase history), allowing businesses to tailor their advertising strategies effectively.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Can Trees improve upon traditional decision trees by precisely handling diverse data types and applying advanced methodologies for building interpretable predictive models.
        \item Implementing Can Trees requires attention to data preprocessing, model selection, and ongoing evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Decision Trees and Can Trees - Introduction}
    \begin{block}{Overview}
        Decision Trees and Can Trees are tree-based algorithms utilized in machine learning for classification and regression tasks. 
        Understanding their differences in performance, use cases, and efficiencies is crucial for model selection.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Decision Trees and Can Trees - Key Differences}
   
    \begin{enumerate}
        \item \textbf{Structure and Representation:}
            \begin{itemize}
                \item \textbf{Decision Trees}:
                    \begin{itemize}
                        \item Hierarchical structure utilizing nodes to split datasets based on feature values.
                        \item Each node represents a feature with branches showing decision outcomes.
                    \end{itemize}
                \item \textbf{Can Trees}:
                    \begin{itemize}
                        \item Designed to handle missing features with a compact probabilistic representation.
                        \item Maintains likelihoods at nodes, aiding in generalization on incomplete data.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Performance:}
            \begin{itemize}
                \item \textbf{Decision Trees}: Prone to overfitting as they grow deeper, capturing noise.
                \item \textbf{Can Trees}: Less likely to overfit due to their probabilistic approach.
            \end{itemize}    
    \end{enumerate}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Decision Trees and Can Trees - Use Cases and Efficiency}
     
    \begin{enumerate}
        \setcounter{enumi}{2} % To continue numbering from the previous frame
        \item \textbf{Use Cases:}
            \begin{itemize}
                \item \textbf{Decision Trees}: Best for structured data scenarios with complete feature sets.
                \item \textbf{Can Trees}: Ideal for datasets with missing values, common in healthcare analytics.
            \end{itemize}
        \item \textbf{Efficiency:}
            \begin{itemize}
                \item \textbf{Decision Trees}: Less intensive training but can be inefficient with large datasets.
                \item \textbf{Can Trees}: More computationally demanding but effectively handle uncertainty.
            \end{itemize}    
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Decision Trees and Can Trees - Summary and Key Takeaway}
    
    \begin{block}{Summary}
        - Decision Trees excel in structured environments. \\
        - Can Trees thrive in situations with uncertainty and missing data.
    \end{block}
    
    \begin{block}{Key Takeaway}
        Choose the appropriate tree-based algorithm by assessing the dataset characteristics and task requirements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion on Decision Trees and Can Trees}
    In this chapter, we explored the fundamentals of Decision Trees and Can Trees, elucidating their significant roles in data mining. 
    Both techniques allow us to convert complex data relationships into easily interpretable visual formats, which is crucial for decision-making processes in various fields.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points Covered}
    \begin{enumerate}
        \item \textbf{What are Decision Trees?}
            \begin{itemize}
                \item Used for classification and regression tasks.
                \item Represent data through branching pathways leading to decisions.
                \item Internal nodes represent feature tests, branches represent outcomes, and leaf nodes represent class labels.
                \item \textit{Example:} Predicting customer purchases based on age, income, and browsing history.
            \end{itemize}
        
        \item \textbf{What are Can Trees?}
            \begin{itemize}
                \item Extend traditional Decision Trees to enhance branch splits for categorical features.
                \item Prioritize critical features with multiple categories showing superior performance.
                \item \textit{Example:} Classifying plant species based on flower color, leaf shape, and habitat type.
            \end{itemize}

        \item \textbf{Comparative Analysis}
            \begin{itemize}
                \item Decision Trees can overfit data; Can Trees manage categorical data better.
                \item Wide use in finance, healthcare, and marketing for Decision Trees. Can Trees excel in bioinformatics and customer analytics.
                \item Computational costs lower for Decision Trees; Can Trees may need more resources but offer better accuracy.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance in Data Mining}
    \begin{itemize}
        \item \textbf{Insight Generation:} Both tree types convert raw data into actionable insights.
        \item \textbf{Scalability:} Models can handle large datasets and provide real-time insights.
        \item \textbf{Model Explainability:} Enhance transparency in machine learning, critical in sectors like healthcare.
    \end{itemize}
    
    \begin{block}{Key Formula}
        \textit{Understanding feature splitting:}
        \begin{equation}
            Gini = 1 - \sum (p_i^2) \quad \text{where } p_i \text{ is the probability of class } i
        \end{equation}
        \begin{equation}
            H(S) = -\sum (p_i \log_2(p_i))
        \end{equation}
    \end{block}

    \begin{block}{Final Thought}
        Mastering Decision Trees and Can Trees is essential for aspiring data professionals. The ability to interpret and apply these models effectively can lead to powerful data-driven decisions across multiple industries.
    \end{block}
\end{frame}


\end{document}