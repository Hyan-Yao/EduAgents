\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 7: k-Nearest Neighbors and Ensemble Methods]{Week 7: k-Nearest Neighbors and Ensemble Methods}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is k-Nearest Neighbors (k-NN)?}
    \begin{itemize}
        \item k-Nearest Neighbors (k-NN) is a \textbf{supervised machine learning algorithm}.
        \item Used for \textbf{classification} and \textbf{regression tasks}.
        \item Relies on \textbf{proximity} in feature space to make predictions based on the \textbf{k} closest data points.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{History and Significance of k-NN}
    \begin{block}{History}
        \begin{itemize}
            \item Roots in the \textbf{1950s}, gaining momentum with more data availability.
            \item Initially described in \textbf{pattern recognition} in the 1940s and 1950s.
            \item Became practical in \textbf{1980s} with evolving data mining and machine learning fields.
        \end{itemize}
    \end{block}
    
    \begin{block}{Significance in Data Mining}
        \begin{itemize}
            \item \textbf{Versatility}: Applicable to problems like image recognition and recommendation systems.
            \item \textbf{Intuition}: Similar things are grouped together (e.g., shared interests).
            \item \textbf{Simplicity}: Non-parametric, requiring no assumptions about data distribution.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Determination of 'k'}: Choice of \textbf{k} affects performance; small values prone to noise, large values may smooth distinctions.
        \item \textbf{Distance Metrics}:
        \begin{itemize}
            \item \textbf{Euclidean Distance}: \( d(p, q) = \sqrt{\sum (p_i - q_i)^2} \)
            \item \textbf{Manhattan Distance}: \( d(p, q) = \sum |p_i - q_i| \)
        \end{itemize}
        \item \textbf{Data Preparation}: Requires feature scaling (normalizing or standardizing) to avoid bias.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    \begin{block}{Scenario}
        Consider classifying a new fruit based on attributes (weight, color, texture). By examining the closest fruits (using a chosen \textbf{k}), we derive a classification based on the majority label of its neighbors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item k-NN is a foundational algorithm in machine learning.
        \item Provides insights into how proximity influences predictions.
        \item Understanding k-NN lays the groundwork for exploring more complex machine learning methods.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Theoretical Background of k-NN - Understanding Proximity}
    \begin{itemize}
        \item \textbf{Proximity Definition:} In k-NN, proximity refers to the closeness of data points in a multi-dimensional space.
        \item The goal is to classify a new data point based on the majority label of its 'k' nearest neighbors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Theoretical Background of k-NN - Distance Metrics}
    \begin{block}{Importance of Distance}
        The selection of the distance metric significantly influences the performance of the k-NN algorithm, affecting how similar or different two data points are.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Euclidean Distance:} 
        \begin{equation}
            D(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
        \end{equation}
        \item \textbf{Description:} Straight-line distance between two points; best used when scales are uniform.
        \item \textbf{Example:} For points \( A(2, 3) \) and \( B(5, 7) \):
        \begin{equation}
            D(A, B) = 5
        \end{equation}
        
        \item \textbf{Manhattan Distance:} 
        \begin{equation}
            D(x, y) = \sum_{i=1}^{n} |x_i - y_i|
        \end{equation}
        \item \textbf{Description:} The sum of absolute differences; useful for non-comparable dimensions.
        \item \textbf{Example:} For points \( A(2, 3) \) and \( B(5, 7) \):
        \begin{equation}
            D(A, B) = 7
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Theoretical Background of k-NN - The Importance of 'k' Value}
    \begin{itemize}
        \item \textbf{Definition of 'k':} Represents the number of nearest neighbors influencing the classification of a data point.
        \item \textbf{Choosing 'k':} 
        \begin{itemize}
            \item A small 'k' (e.g., 1) captures complex patterns but may lead to overfitting.
            \item A large 'k' smooths predictions but can ignore important patterns, leading to underfitting.
        \end{itemize}
        \item \textbf{Best Practices:} Find optimal value for 'k' by:
        \begin{itemize}
            \item Cross-validation
            \item Examining data distribution
            \item Grid search for optimizing accuracy metrics
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Correct choice of distance metrics affects classification accuracy.
            \item The 'k' value is crucial for bias-variance tradeoff; experimentation is essential.
            \item k-NN applications: widely used in recommendation systems, image classification, and pattern recognition.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How k-NN Works - Overview}
    \begin{block}{Overview of the k-Nearest Neighbors (k-NN) Algorithm}
        The k-Nearest Neighbors (k-NN) algorithm is a simple and intuitive method used for classification and regression. 
        Its primary principle is to classify data points based on the proximity to other points in the dataset.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How k-NN Works - Step-by-Step Breakdown}
    \begin{enumerate}
        \item \textbf{Data Input}
            \begin{itemize}
                \item \textbf{Training Data}: A labeled dataset where outcomes (classes) are known.
                \item \textbf{New Data Point}: An unlabeled data point for classification.
            \end{itemize}

        \item \textbf{Distance Calculation}
            \begin{itemize}
                \item The "distance" between points determines their similarity. Common distance metrics:
                \begin{itemize}
                    \item \textbf{Euclidean Distance}:
                    \begin{equation*}
                    d(p, q) = \sqrt{\sum_{i=1}^{n}(p_i - q_i)^2}
                    \end{equation*}
                    \item \textbf{Manhattan Distance}:
                    \begin{equation*}
                    d(p, q) = \sum_{i=1}^{n}|p_i - q_i|
                    \end{equation*}
                \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How k-NN Works - Classification and Key Points}
    \begin{enumerate}[resume]
        \item \textbf{Neighbor Selection}
            \begin{itemize}
                \item Choose a value for \(k\) (number of nearest neighbors), typically 3 or 5.
                \item Identify the \(k\) nearest neighbors based on calculated distances.
            \end{itemize}
        
        \item \textbf{Classification}
            \begin{itemize}
                \item \textbf{Majority Voting}: Assign the class of the new point based on the majority class among the \(k\) neighbors.
                \item For regression, take the average of the values from the \(k\) neighbors.
            \end{itemize}
    \end{enumerate}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Feature Scaling}: Critical for accurate distance calculations.
            \item \textbf{Value of \(k\)}: Impacts model performance; small \(k\) can be sensitive to noise, large \(k\) may smooth class boundaries.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Disadvantages of k-NN}
    k-Nearest Neighbors (k-NN) is a popular non-parametric method for classification and regression based on identifying the 'k' closest training examples. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of k-NN}
    \begin{itemize}
        \item k-NN is a \textbf{non-parametric algorithm} used for both classification and regression.
        \item It identifies the 'k' closest training examples to a given query point.
        \item The output is predicted based on the majority class of these neighbors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of k-NN}
    \begin{enumerate}
        \item \textbf{Simplicity and Ease of Implementation} 
        \begin{itemize}
            \item Straightforward to understand and implement.
            \item Minimal setup (choice of 'k' and distance metric).
            \item Example: Classifying an unknown flower based on nearest 'k' flowers via majority voting.
        \end{itemize}
        
        \item \textbf{Effective with Large Datasets} 
        \begin{itemize}
            \item Works well with large datasets and complex relationships.
            \item No prior assumption about data distribution.
        \end{itemize}
        
        \item \textbf{No Training Phase} 
        \begin{itemize}
            \item A lazy learner; all computations occur during classification.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Disadvantages of k-NN}
    \begin{enumerate}
        \item \textbf{High Computational Cost}
        \begin{itemize}
            \item Requires calculating distances to all training instances (O(n)).
            \item Can be resource-intensive for large datasets.
            \item Illustration: Querying a huge database can lead to long response times.
        \end{itemize}
        
        \item \textbf{Sensitivity to Irrelevant Features}
        \begin{itemize}
            \item Considers all features equally; affected by noise and irrelevant data.
            \item Distance calculations can be distorted.
        \end{itemize}
        
        \item \textbf{Curse of Dimensionality}
        \begin{itemize}
            \item Increased features lead to sparse data, making distance measurements less meaningful.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Metrics}
    \begin{itemize}
        \item In summary, k-NN is intuitive and useful for many applications, especially with well-structured data.
        \item Consider computational demands and susceptibility to irrelevant features.
    \end{itemize}

    \textbf{Distance Metric Example:}
    \begin{lstlisting}[language=Python]
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')
    \end{lstlisting}

    \textbf{Reminder:} Choose 'k' carefully! A small 'k' can be noisy; a large 'k' may introduce bias.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of k-NN - Introduction}
    k-NN is a simple, yet powerful machine learning algorithm used for classification and regression tasks. 
    It operates by identifying the 'k' nearest data points to a given input and determining the output based on their labels.
    
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Instance-Based Learning:} k-NN doesn’t require a model to be built; it simply stores the training instances.
            \item \textbf{Distance Metrics:} Commonly uses Euclidean distance, though other metrics such as Manhattan or Minkowski can be applied.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of k-NN - Real-World Applications}
    \begin{enumerate}
        \item \textbf{Healthcare}
        \begin{itemize}
            \item \textbf{Disease Diagnosis:} Predicting diseases using historical data and identified similarities.
            \item \textit{Example:} Predicting whether a patient has diabetes by analyzing traits like age, BMI, and blood pressure.
        \end{itemize}
        
        \item \textbf{Marketing}
        \begin{itemize}
            \item \textbf{Customer Segmentation:} Segmenting customers based on purchasing behavior and demographics.
            \item \textit{Example:} Targeting marketing campaigns by predicting customer responses to promotions.
        \end{itemize}
        
        \item \textbf{Finance}
        \begin{itemize}
            \item \textbf{Credit Scoring:} Assessing risk of lending by comparing with previous applicants.
            \item \textit{Example:} Predicting the likelihood of repayment based on income and credit history.
        \end{itemize}
        
        \item \textbf{Recommendation Systems}
        \begin{itemize}
            \item \textbf{Product Recommendations:} Identifying users with similar patterns for product suggestions.
            \item \textit{Example:} Suggesting book Y to User B because of similarity to User A's preferences.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Considerations When Using k-NN}
    \begin{itemize}
        \item \textbf{Choosing ‘k’:} 
        \begin{itemize}
            \item The choice of 'k' significantly affects results; a small k may lead to noisy predictions, while a large k can smooth out distinctions.
        \end{itemize}
        
        \item \textbf{Feature Scaling:} 
        \begin{itemize}
            \item Since k-NN relies on distance calculations, feature values should be standardized or normalized for fair comparisons.
        \end{itemize}
    \end{itemize}

    \begin{block}{Summary}
        k-NN is effectively applied across various sectors, demonstrating its versatility. Understanding its practical applications reveals its real-world relevance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Introduction to Ensemble Methods - Definition}
  \begin{block}{Definition of Ensemble Methods}
    Ensemble methods are machine learning techniques that combine multiple models to improve the overall performance of predictive tasks. By aggregating the predictions from various models, these methods aim to achieve greater accuracy and robustness compared to individual models.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Introduction to Ensemble Methods - Purpose}
  \begin{block}{Purpose of Ensemble Methods}
    \begin{enumerate}
      \item \textbf{Increased Accuracy:} Combining forecasts from several models often leads to better predictive performance as the ensemble captures a wider range of data patterns.
      \item \textbf{Reduction of Overfitting:} Individual models may overfit noise in the training data, but an ensemble averages these errors, leading to more generalized predictions.
      \item \textbf{Model Diversity:} Leveraging different models (e.g., decision trees, linear models) helps balance out individual weaknesses, resulting in more reliable predictions.
      \item \textbf{Confidence Estimation:} Ensemble methods can provide measures of uncertainty (e.g., prediction intervals) by examining the variability in model outputs.
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Introduction to Ensemble Methods - Example and Techniques}
  \begin{block}{Real-World Example}
    \begin{itemize}
      \item \textbf{Healthcare:} In predicting patient readmissions, an ensemble method can combine logistic regression (which focuses on demographic factors) with a decision tree (which captures interactions between symptoms) for more reliable predictions.
    \end{itemize}
  \end{block}

  \begin{block}{Common Ensemble Techniques}
    \begin{itemize}
      \item \textbf{Bagging:} Creates multiple versions of a training dataset through sampling and builds separate models for each. Example: \textbf{Random Forests}.
      \item \textbf{Boosting:} Builds models sequentially, correcting errors made by previous ones. Example: \textbf{AdaBoost}.
      \item \textbf{Stacking:} Blends different models (e.g., decision trees, neural networks) at a higher level using another model as a combiner.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Introduction to Ensemble Methods - Mathematical Representation}
  \begin{block}{Mathematical Representation}
    If \( f_1, f_2, \ldots, f_n \) are the individual models, the ensemble prediction \( F \) can be represented as:
    \begin{equation}
      F(x) = \frac{1}{n} \sum_{i=1}^{n} f_i(x)
    \end{equation}
    This average helps smooth out individual errors.
  \end{block}
  
  \begin{block}{Key Points}
    Ensemble methods capitalize on the idea that "the whole is better than the sum of its parts" and can be applied flexibly across different algorithms, creating tailored solutions based on specific dataset characteristics.
  \end{block}
\end{frame}

\begin{frame}
    \frametitle{Types of Ensemble Methods}
    \begin{block}{Overview of Ensemble Techniques}
        Ensemble methods are powerful techniques in machine learning that aim to improve predictive accuracy by combining multiple models. This slide provides an overview of three popular ensemble methods: \textbf{Bagging, Boosting,} and \textbf{Stacking}.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Bagging (Bootstrap Aggregating)}
    \begin{itemize}
        \item \textbf{Definition}: Creating multiple subsets of the training data using bootstrapping (sampling with replacement) and training a separate model on each subset. The final prediction is made by averaging (for regression) or voting (for classification).
        
        \item \textbf{Example}: 
        \begin{itemize}
            \item **Random Forest** builds multiple decision trees and merges their outputs:
            \begin{itemize}
                \item Each tree is trained on a random subset of data.
                \item Each tree votes for classification; for regression, the mean of all trees is taken.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Reduces variance and helps prevent overfitting.
            \item Suitable for high-dimensional datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Boosting}
    \begin{itemize}
        \item \textbf{Definition}: An iterative technique that adjusts the weight of instances based on the errors made by previous models, sequentially training models that focus more on misclassified data points.
        
        \item \textbf{Example}: 
        \begin{itemize}
            \item **AdaBoost (Adaptive Boosting)** trains each subsequent model to correct the errors of the previous model. Predictions are combined using a weighted sum based on misclassified instances' weight.
        \end{itemize}
        
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Increases prediction accuracy and reduces bias.
            \item More sensitive to noise in data than bagging.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Stacking}
    \begin{itemize}
        \item \textbf{Definition}: Involves training multiple models (base learners) and combining their predictions using another model (meta-learner), which learns the best way to combine the base learners' outputs.
        
        \item \textbf{Example}: 
        \begin{itemize}
            \item Using decision trees and logistic regression as base learners, with an SVM as the meta-learner.
        \end{itemize}
        
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Leverages the strengths of multiple algorithms.
            \item Often results in superior performance compared to individual learners.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Conclusion}
    \begin{itemize}
        \item Ensemble methods harness the collective power of multiple models to enhance predictive capabilities.
        \begin{itemize}
            \item \textbf{Bagging} focuses on reducing variance.
            \item \textbf{Boosting} reduces bias.
            \item \textbf{Stacking} exploits the strengths of diverse models.
        \end{itemize}
        
        \item Utilizing ensemble methods can significantly improve model performance in various complex problems in fields like finance, healthcare, and marketing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Random Forest Implementation}
    Here’s a simple implementation using Python’s \texttt{scikit-learn} for a Random Forest model:
    \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier

# Create the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100)

# Fit the model to the training data
rf_model.fit(X_train, y_train)

# Make predictions
predictions = rf_model.predict(X_test)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - What is Bagging?}
    \begin{itemize}
        \item \textbf{Bagging}, short for Bootstrap Aggregating, is an ensemble learning technique that aims to enhance the stability and accuracy of machine learning models.
        \item It combines predictions from multiple subsets of the training dataset to reduce variance and combat overfitting.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - Key Concepts}
    \begin{enumerate}
        \item \textbf{Bootstrap Sampling}:
            \begin{itemize}
                \item Randomly draw subsets of data (with replacement) from the original dataset.
                \item Each subset varies in size and may contain duplicates.
            \end{itemize}
            
        \item \textbf{Model Training}:
            \begin{itemize}
                \item Train separate models (e.g., decision trees) on each bootstrapped subset.
                \item Typically, bagging employs identical model types.
            \end{itemize}
            
        \item \textbf{Aggregation}:
            \begin{itemize}
                \item For regression: Average the predictions.
                \item For classification: Use majority voting to determine the final class.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - Advantages}
    \begin{itemize}
        \item \textbf{Reduction of Overfitting}: Averages predictions to mitigate overfitting in complex models.
        \item \textbf{Increased Robustness}: Improves performance on unseen data by lowering variance.
        \item \textbf{Parallel Processing}: Models can be trained independently, enhancing computational efficiency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - Example: Random Forest}
    \begin{itemize}
        \item \textbf{Random Forest}:
            \begin{itemize}
                \item Key implementation of bagging with decision trees.
                \item Multiple decision trees are built using bootstrap samples.
                \item Each tree uses a random subset of features during splits to add randomness.
            \end{itemize}
        
        \item \textbf{Prediction Process}:
            \begin{itemize}
                \item Aggregate outputs from all decision trees: Majority vote for classification or average for regression.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - Real-World Applications}
    \begin{itemize}
        \item \textbf{Image Recognition}:
            \begin{itemize}
                \item Create various classifiers to handle different image aspects (brightness, contrast, color).
            \end{itemize}
        \item \textbf{Financial Forecasting}:
            \begin{itemize}
                \item Analyze various financial indicators for predictions that leverage insights from multiple trees.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - Key Points}
    \begin{itemize}
        \item Bagging reduces prediction variance, enhancing model accuracy.
        \item Random forests utilize bagging to generate robust models for classification and regression.
        \item The core idea of bagging: "Many models are better than one".
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification

# Create a synthetic dataset
X, y = make_classification(n_samples=100, n_features=20, random_state=42)

# Initialize the base classifier
base_classifier = DecisionTreeClassifier()

# Create the Bagging Classifier
bagging_model = BaggingClassifier(base_classifier, n_estimators=50)

# Fit the model
bagging_model.fit(X, y)

# Predict
predictions = bagging_model.predict(X)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging Explained - Summary}
    \begin{itemize}
        \item Bagging, especially through Random Forest, is a powerful ensemble method that enhances prediction accuracy and model robustness.
        \item Understanding these principles is essential for effective machine learning implementations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Boosting Explained}
    \begin{block}{Understanding Boosting Techniques}
        Boosting is an ensemble learning technique that combines predictions of multiple base models to improve overall accuracy. It trains models sequentially such that each new model focuses on the errors made by the previous ones, effectively reducing bias and variance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Boosting}
    \begin{enumerate}
        \item \textbf{Base Learner:} A simple model that is weak on its own but can strengthen the overall ensemble.
        \item \textbf{Weight Adjustment:} Misclassified instances are given more weight for the subsequent models to learn from errors.
        \item \textbf{Final Prediction:} Combined predictions of all models, often using a weighted average.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Boosting Techniques}
    \begin{block}{Types of Boosting}
        \begin{itemize}
            \item \textbf{AdaBoost:}
            \begin{itemize}
                \item Equal weights for all instances initially.
                \item Train base learners, adjust weights for misclassified examples.
                \item Combine using a weighted vote.
            \end{itemize}
        
            \item \textbf{Gradient Boosting:}
            \begin{itemize}
                \item Sequentially trains models to minimize residual errors.
                \item Utilizes gradient descent on a loss function.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulations}
    \begin{block}{AdaBoost Formula}
        \begin{equation}
            \text{Final Prediction} = \sum_{m=1}^{M} \alpha_m h_m(x) 
        \end{equation}
        where $\alpha_m$ is the weight for each weak learner $h_m(x)$.
    \end{block}
    
    \begin{block}{Gradient Boosting Formula}
        \begin{equation}
            \theta = \theta - \eta \nabla L(y, \hat{f}(x; \theta)) 
        \end{equation}
        where:
        \begin{itemize}
            \item $\theta$: parameters,
            \item $\eta$: learning rate,
            \item $L$: loss function (e.g., mean squared error).
        \end{itemize}
    \end{block}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}
    Boosting methods have diverse applications:
    \begin{itemize}
        \item \textbf{Finance:} Used in credit scoring models.
        \item \textbf{Marketing:} Customer segmentation and targeting.
        \item Predicting student outcomes based on study habits and participation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion on Boosting}
    \begin{block}{Empowering Predictions}
        Boosting effectively combines multiple learners to improve model accuracy. Understanding its application can greatly enhance predictive modeling across various fields.
    \end{block}
\end{frame}

\begin{frame}
    \title{Comparative Analysis: k-NN vs. Ensemble Methods}
    \date{\today}
    \maketitle
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Definitions}

    \begin{block}{k-Nearest Neighbors (k-NN)}
        \begin{itemize}
            \item Instance-based learning algorithm for classification and regression.
            \item Classifies new instances by examining the 'k' closest training examples.
            \item Majority class among neighbors determines the class of the instance.
        \end{itemize}
    \end{block}

    \begin{block}{Ensemble Methods}
        \begin{itemize}
            \item Techniques that combine multiple models to improve performance.
            \item Examples: Random Forest, AdaBoost, Gradient Boosting.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Effectiveness Comparison}

    \textbf{1. Accuracy}
    \begin{itemize}
        \item \textbf{k-NN:}
        \begin{itemize}
            \item Strengths: High accuracy on small, well-defined datasets.
            \item Weaknesses: Susceptible to noise, issues with high dimensionality (Curse of dimensionality).
            \item \textit{Example}: Works well with distinct and evenly distributed handwritten digit samples.
        \end{itemize}

        \item \textbf{Ensemble Methods:} 
        \begin{itemize}
            \item Strengths: Generally outperform k-NN by reducing overfitting.
            \item Weaknesses: Require careful parameter tuning, can be less interpretable.
            \item \textit{Example}: Random Forest excels in classifying complex datasets like customer churn.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Computational Efficiency}

    \textbf{k-NN:}
    \begin{itemize}
        \item Speed: Slow during prediction (computation of distances).
        \item Complexity: $O(n \cdot d)$ for $n$ samples and $d$ dimensions.
    \end{itemize}

    \begin{lstlisting}[language=Python]
# Example of k-NN implementation in Python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)  # Training
prediction = knn.predict(X_test)  # Prediction
    \end{lstlisting}

    \textbf{Ensemble Methods:}
    \begin{itemize}
        \item Speed: Faster predictions after training (especially in Random Forest).
        \item Complexity: $O(m \cdot n \cdot d)$ where $m$ is the number of models.
    \end{itemize}

    \begin{lstlisting}[language=Python]
# Example of Random Forest implementation in Python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)  # Training
prediction = rf.predict(X_test)  # Prediction
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Cases and Summary}

    \begin{block}{Use Cases}
        \begin{itemize}
            \item \textbf{k-NN:} Best for small, simple datasets and critical interpretability.
            \item \textbf{Ensemble Methods:} Optimal for large, complex datasets where accuracy is vital.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item k-NN can degrade in high-dimensional or noisy datasets.
            \item Ensemble methods typically produce superior results but require more resources.
            \item Choice should consider dataset characteristics and predictive goals.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Method}
    \begin{block}{Overview}
        When deciding between \textbf{k-Nearest Neighbors (k-NN)} and \textbf{ensemble methods}, it's essential to consider the characteristics of your data and the specific goals of your project. This presentation provides guidelines to help you make an informed choice.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding k-NN and Ensemble Methods}
    \begin{itemize}
        \item \textbf{k-NN:} A simple, instance-based learning algorithm that classifies data points based on the majority class among their nearest neighbors. It is easy to understand and implement.
        \item \textbf{Ensemble Methods:} Techniques that combine multiple models to produce a powerful and robust prediction. Common ensemble methods include Random Forest, AdaBoost, and Gradient Boosting.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Factors for Choosing the Right Method}
    \begin{block}{Nature of the Data}
        \begin{itemize}
            \item \textbf{Size of the Dataset:}
                \begin{itemize}
                    \item \textbf{k-NN:} Performs well with smaller datasets.
                    \item \textbf{Ensemble:} Effective for larger datasets due to aggregation.
                \end{itemize}
            \item \textbf{Dimensionality:}
                \begin{itemize}
                    \item \textbf{k-NN:} Struggles in high-dimensional space.
                    \item \textbf{Ensemble:} Handles high dimensions better (e.g., Random Forest).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Distribution and Prediction Speed}
    \begin{block}{Data Distribution}
        \begin{itemize}
            \item \textbf{k-NN:} Effective for evenly distributed or well-clustered data.
            \item \textbf{Ensemble:} Works better with complex, imbalanced, or noisy distributions.
        \end{itemize}
    \end{block}
    \begin{block}{Prediction Speed}
        \begin{itemize}
            \item \textbf{k-NN:} Slower during prediction due to distance calculations.
            \item \textbf{Ensemble:} Typically faster once trained, as they can predict using aggregated models.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Specific Project Goals}
    \begin{itemize}
        \item \textbf{Accuracy vs. Interpretability:}
            \begin{itemize}
                \item Use k-NN for interpretability.
                \item Use ensemble for high accuracy in complex tasks.
            \end{itemize}
        \item \textbf{Robustness to Overfitting:}
            \begin{itemize}
                \item Prefer ensemble methods if the risk of overfitting is high.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenarios}
    \begin{block}{Scenario 1: Image Classification}
        \begin{itemize}
            \item \textbf{Data Type:} High-dimensional pixel data.
            \item \textbf{Choice:} Ensemble method (e.g., Random Forest).
        \end{itemize}
    \end{block}
    \begin{block}{Scenario 2: Simple Classification Tasks}
        \begin{itemize}
            \item \textbf{Data Type:} Small, well-defined datasets.
            \item \textbf{Choice:} k-NN due to its simplicity.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{Summary}
        Assess the dataset’s size, dimensionality, distribution, and project goals:
        \begin{itemize}
            \item Use k-NN for smaller, interpretable tasks.
            \item Use ensemble methods for larger, more complex datasets to improve accuracy and robustness.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Practical Implementation with Python}
    \begin{block}{Summary}
        This presentation provides a practical coding walkthrough for implementing the k-Nearest Neighbors (k-NN) algorithm and ensemble methods using Python's Scikit-learn library. Key concepts include:
        \begin{itemize}
            \item Overview of k-NN and ensemble methods
            \item Step-by-step implementation of k-NN
            \item Implementation of Random Forest as an ensemble method
            \item Key considerations for model selection and evaluation
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction to k-NN and Ensemble Methods}
    \begin{itemize}
        \item \textbf{k-NN}:
        \begin{itemize}
            \item Predicts the class of a data point based on its k nearest neighbors.
            \item Simple but effective for classification tasks.
        \end{itemize}
        \item \textbf{Ensemble Methods}:
        \begin{itemize}
            \item Combine predictions from multiple models to enhance accuracy.
            \item Examples include:
            \begin{itemize}
                \item Bagging: e.g., Random Forest
                \item Boosting: e.g., AdaBoost
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing k-NN using Scikit-learn}
    \begin{enumerate}
        \item \textbf{Import Necessary Libraries:}
        \begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
        \end{lstlisting}

        \item \textbf{Load and Prepare the Dataset:}
        \begin{lstlisting}[language=Python]
# Example dataset: Iris dataset
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        \end{lstlisting}

        \item \textbf{Normalize Data:}
        \begin{lstlisting}[language=Python]
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continuing with k-NN: Model Training and Evaluation}
    \begin{enumerate}[resume]
        \item \textbf{Create and Train the k-NN Model:}
        \begin{lstlisting}[language=Python]
knn = KNeighborsClassifier(n_neighbors=3)  # k=3
knn.fit(X_train, y_train)
        \end{lstlisting}

        \item \textbf{Make Predictions and Evaluate:}
        \begin{lstlisting}[language=Python]
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Ensemble Methods using Scikit-learn}
    \begin{enumerate}
        \item \textbf{Import Random Forest Classifier:}
        \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier
        \end{lstlisting}

        \item \textbf{Create and Train a Random Forest Model:}
        \begin{lstlisting}[language=Python]
rf = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 trees
rf.fit(X_train, y_train)
        \end{lstlisting}

        \item \textbf{Make Predictions and Evaluate:}
        \begin{lstlisting}[language=Python]
rf_pred = rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
print(f'Random Forest Accuracy: {rf_accuracy:.2f}')
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Choice of 'k' in k-NN:} 
        \begin{itemize}
            \item Smaller k can lead to noise sensitivity; larger k may mask patterns.
            \item Cross-validation assists in selecting optimal k.
        \end{itemize}
        \item \textbf{Ensemble Strength:} 
        \begin{itemize}
            \item Utilizing multiple algorithms enhances generalization on unseen data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item Implementing k-NN and ensemble methods with Scikit-learn is intuitive.
        \item Data preparation, parameter selection, and model evaluation are crucial.
        \item Encourage experimentation with various datasets to understand model behaviors.
        \item Consider applying skills to real-world problems, such as customer churn classification or image recognition.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Pitfalls and Best Practices in k-Nearest Neighbors and Ensemble Methods}
    \begin{itemize}
        \item Powerful machine learning tools, but can be compromised by pitfalls.
        \item Understanding pitfalls and best practices is essential for optimal results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Pitfalls}
    \begin{enumerate}
        \item \textbf{Choosing the Wrong Value of k in k-NN}
            \begin{itemize}
                \item Small k may yield noisy predictions; large k can smooth essential patterns.
                \item \textit{Best Practice}: Use cross-validation to determine optimal k (range 1-20).
            \end{itemize}
        \item \textbf{Ignoring Feature Scaling}
            \begin{itemize}
                \item Distance calculations affected by feature scales.
                \item \textit{Best Practice}: Apply Min-Max Normalization or Z-score Standardization.
            \end{itemize}
        \item \textbf{Overfitting in Ensemble Models}
            \begin{itemize}
                \item Inadequately tuned ensembles (like Random Forest) may overfit.
                \item \textit{Best Practice}: Use pruning, limit depth, and set minimum samples to split a node.
            \end{itemize}
        \item \textbf{Not Considering Class Imbalance}
            \begin{itemize}
                \item Models may favor the majority class in imbalanced datasets.
                \item \textit{Best Practice}: Use resampling techniques or imbalance-specific ensembles (Balanced Random Forest).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices}
    \begin{block}{For k-NN}
        \begin{itemize}
            \item Experiment with different distance metrics (Euclidean, Manhattan).
            \item Use PCA for dimensionality reduction with high-dimensional data.
            \item Preprocess data by addressing missing values and outliers.
        \end{itemize}
    \end{block}

    \begin{block}{For Ensemble Methods}
        \begin{itemize}
            \item Use diverse algorithms in ensembles to improve predictions.
            \item Tune hyperparameters with grid search or randomized search.
            \item Employ k-fold cross-validation for robust evaluation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Example Code Snippet}
    \begin{itemize}
        \item Recognizing pitfalls and following best practices improves k-NN and ensemble methods' performance.
        \item Effective modeling depends on data handling, tuning, and evaluation strategies.
    \end{itemize}
    \begin{block}{Example Code Snippet for k-NN with Cross-Validation}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

# Load dataset
X, y = load_iris(return_X_y=True)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# k-NN Model
knn = KNeighborsClassifier(n_neighbors=5)

# Cross-validation
scores = cross_val_score(knn, X_scaled, y, cv=5)
print("Cross-Validation Scores:", scores)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Learnings}
    \begin{enumerate}
        \item \textbf{Understanding k-Nearest Neighbors (k-NN)}:
            \begin{itemize}
                \item k-NN is a simple yet powerful classification algorithm that relies on feature distance.
                \item \textbf{Example}: In customer segmentation, k-NN classifies new customers based on similarity to previous customers.
            \end{itemize}
        
        \item \textbf{Ensemble Methods}:
            \begin{itemize}
                \item Ensemble methods improve model accuracy by combining weak learners into a stronger model.
                \item \textbf{Examples}: Random Forests and Gradient Boosting aggregate predictions from multiple decision trees.
            \end{itemize}
        
        \item \textbf{Advantages and Challenges}:
            \begin{itemize}
                \item k-NN is intuitive and requires no training phase, but it can be computationally expensive for large datasets.
                \item Ensemble methods can suffer from overfitting if not properly tuned.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Future Trends}
    \begin{enumerate}
        \item \textbf{Scalability of k-NN}:
            \begin{itemize}
                \item Investigate approximate nearest neighbor algorithms (e.g., KD-Trees and Ball Trees) to enhance scalability for big data applications.
            \end{itemize}
        
        \item \textbf{Integration with Deep Learning}:
            \begin{itemize}
                \item Research may focus on combining k-NN with neural networks for tasks like image recognition, enhancing predictive performance.
            \end{itemize}
        
        \item \textbf{Enhanced Ensemble Techniques}:
            \begin{itemize}
                \item Developments in stacking, blending, and using diverse algorithms for model performance will continue to evolve.
                \item Boosting and bagging strategies will become more refined for specific datasets and problem types.
            \end{itemize}
        
        \item \textbf{Real-time Predictions}:
            \begin{itemize}
                \item Advances in hardware and optimization algorithms will allow for real-time k-NN predictions.
            \end{itemize}
        
        \item \textbf{Ethics and Explainability}:
            \begin{itemize}
                \item Future models will prioritize transparency and ethical considerations to ensure accountability in decisions made.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Points to Emphasize}
    \begin{itemize}
        \item Both k-NN and ensemble methods are foundational in data mining, requiring understanding of their theoretical frameworks and practical applications.
        \item Continuous technological advancements will shape the usage of these algorithms; practitioners must stay informed about developments in the field.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session}
    \begin{block}{Overview}
        This slide serves as an open discussion platform aimed at clarifying any uncertainties surrounding k-Nearest Neighbors (k-NN) and Ensemble Methods.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts to Discuss - k-Nearest Neighbors (k-NN)}
    \begin{enumerate}
        \item \textbf{Definition:} A supervised learning algorithm for classification and regression based on proximity in feature space.
        
        \item \textbf{How It Works:} It computes distances (commonly Euclidean) to find 'k' nearest neighbors for making predictions.
        
        \item \textbf{Example:} Classifying an animal based on height and weight by looking at the closest 'k' animals.
        
        \item \textbf{Key Considerations:} 
        \begin{itemize}
            \item Choice of 'k' affects the influence of noise.
            \item Different distance metrics (e.g., Manhattan, Minkowski) can be used based on context.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts to Discuss - Ensemble Methods}
    \begin{enumerate}
        \item \textbf{Definition:} Techniques that combine multiple models to improve overall prediction accuracy.
        
        \item \textbf{Popular Types:} 
        \begin{itemize}
            \item \textbf{Bagging:} Builds models from random data subsets (e.g., Random Forest).
            \item \textbf{Boosting:} Combines weak learners sequentially to correct previous errors (e.g., AdaBoost).
        \end{itemize}
        
        \item \textbf{Example:} Using different models to predict student performance based on varied criteria.
        
        \item \textbf{Key Points:} 
        \begin{itemize}
            \item Increased Model Accuracy.
            \item Robustness against Overfitting.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Discussion Points}
    \begin{block}{Examples to Illustrate}
        \begin{itemize}
            \item \textbf{k-NN Use Case:} Virtual assistant recommendations based on user behavior.
            \item \textbf{Ensemble Use Case:} Credit scoring utilizing various models for comprehensive risk assessment.
        \end{itemize}
    \end{block}
    
    \begin{block}{Discussion Points}
        \begin{itemize}
            \item Challenges in implementing k-NN or ensemble methods?
            \item Deciding between simple models versus ensemble models?
            \item References to real-world applications of these methods.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engagement Strategy and Conclusion}
    \begin{block}{Engagement Strategy}
        Encourage students to share experiences with k-NN or ensemble methods and pose questions on supervised learning importance.
    \end{block}

    \begin{block}{Conclusion}
        Use this session to clarify doubts and deepen understanding of k-NN and ensemble methods for enhancing predictive analytics.
    \end{block}
\end{frame}


\end{document}