\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Data Preprocessing]{Chapter 3: Data Preprocessing and Feature Engineering (continued)}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{Overview}
        Data preprocessing is the critical step of transforming raw data into a format suitable for modeling. It involves techniques that prepare the data for analysis, ensuring its quality and improving the performance of machine learning algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Data Preprocessing Important?}
    \begin{enumerate}
        \item \textbf{Improves Data Quality:} Quality data affects model learning ability; inaccurate or inconsistent data can lead to misleading results.
        
        \item \textbf{Enhances Model Performance:} Properly preprocessed data aids algorithms in making better predictions, minimizing errors and increasing accuracy.
        
        \item \textbf{Facilitates Feature Engineering:} Supports the creation of new features from existing data, enhancing the model's explanatory power.
        
        \item \textbf{Saves Time and Resources:} Addresses data issues upfront, avoiding potential pitfalls downstream in the modeling process.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Data Cleaning:} 
        Correcting or removing erroneous records and handling missing values.
        \begin{itemize}
            \item \textit{Example:} Negative or excessively high age entries may need correction or removal.
        \end{itemize}
        
        \item \textbf{Data Transformation:} 
        Normalization, standardization, or scaling of numeric attributes.
        \begin{itemize}
            \item \textit{Example:} Min-Max Scaling:
            \begin{equation}
            X' = \frac{X - X_{min}}{X_{max} - X_{min}}
            \end{equation}
        \end{itemize}
        
        \item \textbf{Data Reduction:} 
        Reducing volume while retaining analytical results using feature selection or dimensionality reduction.
        \begin{itemize}
            \item \textit{Example:} PCA can reduce dimensions while retaining primary variance.
        \end{itemize}
        
        \item \textbf{Data Integration:} 
        Combining data from multiple sources into a coherent dataset.
        \begin{itemize}
            \item \textit{Example:} Merging customer data with transaction data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        Data preprocessing lays the groundwork for meaningful insights and effective machine learning models. Proper attention here can significantly impact predictive analytics success.
    \end{block}
    \begin{itemize}
        \item Data preprocessing is essential for improving model accuracy.
        \item Key steps include data cleaning, transformation, reduction, and integration.
        \item Investing time in preprocessing saves resources and leads to better outcomes in machine learning projects.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Introduction}
    \begin{block}{Introduction to Data Quality}
        Data quality refers to the condition of a dataset based on several factors, including:
        \begin{itemize}
            \item Accuracy
            \item Completeness
            \item Reliability
            \item Relevance
        \end{itemize}
        High-quality data is crucial for effective data analysis and machine learning models.
    \end{block}
    This section addresses two significant data quality issues: **missing values** and **outliers**.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Missing Values}
    \begin{block}{1. Missing Values}
        \textbf{Definition}: Missing values occur when no data value is stored for a variable in an observation. 

        \textbf{Impact of Missing Values}:
        \begin{itemize}
            \item Reduces sample size and statistical power.
            \item Biased estimates if the data is not random.
        \end{itemize}

        \textbf{Types of Missing Data}:
        \begin{enumerate}
            \item MCAR (Missing Completely At Random)
            \item MAR (Missing At Random)
            \item MNAR (Missing Not At Random)
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Types of Missing Data Examples}
    \begin{block}{Examples of Missing Data}
        \begin{itemize}
            \item \textbf{MCAR}: A survey response is mistakenly lost due to a technical issue.
            \item \textbf{MAR}: Higher-income individuals might not respond to financial questions.
            \item \textbf{MNAR}: People who are ill may skip health-related questions, leading to biased outcomes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Outliers}
    \begin{block}{2. Outliers}
        \textbf{Definition}: Outliers are data points that significantly differ from the other observations.

        \textbf{Causes of Outliers}:
        \begin{itemize}
            \item Measurement error
            \item Unusual but legitimate variance in data
            \item Data entry errors
        \end{itemize}

        \textbf{Detection of Outliers}:
        \begin{itemize}
            \item Visual Methods: Use box plots or scatter plots.
            \item Statistical Methods: Z-score method.
        \end{itemize}
        \begin{equation}
            Z = \frac{(X - \mu)}{\sigma}
        \end{equation}
        where \( X \) is the data point, \( \mu \) is the mean, and \( \sigma \) is the standard deviation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Quality of data is paramount for machine learning success.
            \item Addressing missing values and outliers is essential.
            \item Understanding the nature of missing data and properly identifying outliers are critical steps.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Addressing data quality issues strengthens dataset integrity, leads to more reliable models, and ensures better decision-making based on analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Introduction}
    Data cleaning is a crucial step in data preprocessing that addresses common data quality issues such as:

    \begin{itemize}
        \item Missing values
        \item Outliers
    \end{itemize}

    These techniques are essential for ensuring that the data is accurate and usable for analysis, thus improving the performance of machine learning algorithms and statistical models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Handling Missing Data}
    \begin{block}{Common Techniques}
        \begin{itemize}
            \item \textbf{Deletion Methods:}
            \begin{itemize}
                \item \textbf{Listwise Deletion:} Remove all records with any missing values.
                \item \textbf{Pairwise Deletion:} Use all available data without deleting entire rows.
            \end{itemize}

            \item \textbf{Imputation Methods:}
            \begin{itemize}
                \item \textbf{Mean/Median/Mode Imputation:} Replace missing values with the mean, median, or mode.
                \item \textbf{K-Nearest Neighbors (KNN) Imputation:} Predict missing values based on similar data points.
                \item \textbf{Regression Imputation:} Estimate and fill in missing values based on regression models.
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Choose methods based on dataset characteristics.
            \item Over-imputation may introduce bias.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Removing Outliers}
    \begin{block}{Identifying Outliers}
        \begin{itemize}
            \item \textbf{Statistical Methods:}
            \begin{itemize}
                \item \textbf{Z-Score Method:} Identify values with Z-scores beyond specified thresholds.
                \begin{equation}
                    Z = \frac{(X - \mu)}{\sigma}
                \end{equation}
                \item \textbf{IQR Method:} Use Interquartile Range to find outliers.
                \item Example: Identify values below \( Q1 - 1.5 \times IQR \) or above \( Q3 + 1.5 \times IQR \).
            \end{itemize}
        \end{itemize}
    \end{block}
  
    \begin{block}{Techniques for Outlier Treatment}
        \begin{itemize}
            \item \textbf{Removing Outliers}
            \item \textbf{Capping or Winsorizing}
            \item \textbf{Transformations (Log or Square Root)}
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Outliers can skew results and may indicate data quality issues.
            \item Always visualize data before and after outlier removal.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Conclusion}
    Applying appropriate data cleaning techniques for missing data and outliers is critical for producing high-quality datasets. 

    \begin{itemize}
        \item Employ methods such as imputation for missing values.
        \item Use statistical analysis for outlier detection.
    \end{itemize}

    This preparation is essential for achieving more accurate analyses and predictions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Topic: Normalization and Scaling}
    Transition to the importance of scaling, which enhances the performance of algorithms in the data preprocessing journey.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization and Scaling}
    \begin{block}{What are Normalization and Scaling?}
        Normalization and scaling are crucial preprocessing techniques for preparing numerical data for machine learning algorithms. 
        Their primary purpose is to adjust the range and distribution of feature values for effective analysis and modeling.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Normalization and Scaling}
    \begin{enumerate}
        \item \textbf{Improves Convergence Speed:}
            \begin{itemize}
                \item Gradient descent and other optimization algorithms converge faster with similar feature scales.
                \item Algorithms like Logistic Regression and Neural Networks are sensitive to input data scale.
            \end{itemize}
        \item \textbf{Avoids Bias:}
            \begin{itemize}
                \item Features with larger ranges can dominate model behavior, leading to inaccurate predictions.
                \item Example: Features with ranges [0, 100] versus [0, 1].
            \end{itemize}
        \item \textbf{Enhances Performance:}
            \begin{itemize}
                \item Algorithms calculating distances, like KNN and SVM, perform better with normalized data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques}
    \begin{enumerate}
        \item \textbf{Min-Max Scaling:}
            \begin{equation}
            X' = \frac{X - X_{min}}{X_{max} - X_{min}}
            \end{equation}
            Example: Transforming [10, 20, 30] results in [0, 0.5, 1].
        
        \item \textbf{Standardization (Z-score Normalization):}
            \begin{equation}
            X' = \frac{X - \mu}{\sigma}
            \end{equation}
            Where:
            \begin{itemize}
                \item $\mu$ = Mean of the feature
                \item $\sigma$ = Standard deviation of the feature
            \end{itemize}
            Example: Heights [150, 160, 170] become Z-scores [-1, 0, 1].

        \item \textbf{Robust Scaling:}
            \begin{equation}
            X' = \frac{X - \text{median}(X)}{\text{IQR}(X)}
            \end{equation}
            Example: More robust to extreme outliers than Min-Max scaling.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Code Snippet}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Choose the appropriate method based on data distribution.
            \item Preprocessing is critical for optimal model performance.
            \item Visualize the data before and after scaling for better understanding.
        \end{itemize}
    \end{block}

    \begin{lstlisting}[language=Python, caption={Normalization and Scaling Example}]
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

# Sample data
data = [[10], [20], [30]]

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
normalized_data = min_max_scaler.fit_transform(data)

# Standardization
standard_scaler = StandardScaler()
standardized_data = standard_scaler.fit_transform(data)

# Robust Scaling
robust_scaler = RobustScaler()
robust_data = robust_scaler.fit_transform(data)

print("Normalized Data:", normalized_data)
print("Standardized Data:", standardized_data)
print("Robust Data:", robust_data)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Encoding Categorical Variables}
    \begin{block}{Introduction to Categorical Variables}
        Categorical variables represent distinct categories or groups. 
        Examples include:
        \begin{itemize}
            \item \textbf{Nominal}: No intrinsic order (e.g., colors: Red, Blue, Green).
            \item \textbf{Ordinal}: Meaningful order (e.g., survey ratings: Poor, Fair, Good, Excellent).
        \end{itemize}
        Machine learning algorithms usually require numerical input, necessitating the transformation of categorical variables.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Common Methods for Encoding Categorical Variables}
    \begin{enumerate}
        \item \textbf{Label Encoding}:
            \begin{itemize}
                \item Converts categories to unique integers.
                \item Useful for ordinal data.
                \item Example: 
                \begin{quote}
                    Colors: Red $\rightarrow$ 1, Blue $\rightarrow$ 2, Green $\rightarrow$ 3 
                \end{quote}
                \item \textbf{Python Code}:
                    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import LabelEncoder

labels = ['Red', 'Blue', 'Green']
le = LabelEncoder()
encoded_labels = le.fit_transform(labels) # Result: [2, 0, 1]
                    \end{lstlisting}
            \end{itemize}

        \item \textbf{One-Hot Encoding}:
            \begin{itemize}
                \item Creates binary columns for each category.
                \item Suitable for nominal data.
                \item Example:
                \begin{quote}
                    Original: Red $\rightarrow$ [1, 0, 0], Blue $\rightarrow$ [0, 1, 0], Green $\rightarrow$ [0, 0, 1]
                \end{quote}
                \item \textbf{Python Code}:
                    \begin{lstlisting}[language=Python]
import pandas as pd

df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})
one_hot_encoded_df = pd.get_dummies(df, columns=['Color'])
                    \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Common Methods for Encoding Categorical Variables (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Binary Encoding}:
            \begin{itemize}
                \item Combines Label Encoding and One-Hot Encoding.
                \item Converts categories to binary numbers.
                \item Example: 
                \begin{quote}
                    Red $\rightarrow$ 01, Blue $\rightarrow$ 10, Green $\rightarrow$ 11
                \end{quote}
                \item Two columns represent the binary digits.
            \end{itemize}

        \item \textbf{Target Encoding}:
            \begin{itemize}
                \item Replaces categories with mean of the target variable.
                \item Example: 
                \begin{quote}
                    Red $\rightarrow$ 50000, Blue $\rightarrow$ 60000 
                \end{quote}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Choose encoding method based on variable type (nominal vs. ordinal) and machine learning algorithm.
        \item One-Hot Encoding may increase dimensionality significantly; consider Binary Encoding or Target Encoding as alternatives.
        \item Always assess the impact of encoding on model performance and interpretability.
    \end{itemize}

    Through careful preprocessing of categorical variables, we can enhance model performance and accuracy. 
    Next, we will explore \textbf{Feature Extraction} techniques that can further enrich our datasets.
\end{frame}

\begin{frame}
    \frametitle{Feature Extraction}
    % Overview of techniques and methods for feature extraction.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Feature Extraction}
    \begin{itemize}
        \item Feature extraction is a crucial step in data preprocessing.
        \item It transforms raw data into relevant features to improve machine learning performance.
        \item The goal is to reduce data complexity while enhancing necessary information for modeling.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Definition}: 
            \begin{itemize}
                \item Conversion of raw data into informative, non-redundant features.
            \end{itemize}
        
        \item \textbf{Importance}:
            \begin{itemize}
                \item Enhances model accuracy and reduces computation time.
                \item Helps overcome the curse of dimensionality.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Feature Extraction}
    \begin{enumerate}
        \item \textbf{Statistical Measures}
            \begin{itemize}
                \item Mean, median, variance, skewness summarize data properties.
                \item \textit{Example}: Average house price per neighborhood.
            \end{itemize}

        \item \textbf{Text-Based Feature Extraction}
            \begin{itemize}
                \item Bag of Words, TF-IDF.
                \item \textit{Example}: Top 1000 words from reviews using TF-IDF.
            \end{itemize}
        
        \item \textbf{Time-Series Feature Extraction}
            \begin{itemize}
                \item Extract trends, seasonality, and lags.
                \item \textit{Example}: Moving averages in stock price data.
            \end{itemize}
        
        \item \textbf{Image Feature Extraction}
            \begin{itemize}
                \item Edge detection, HOG, and color histograms.
                \item \textit{Example}: Facial recognition features like nose shape.
            \end{itemize}
         
        \item \textbf{Domain-Specific Techniques}
            \begin{itemize}
                \item Tailored features based on data type (e.g., MFCC for audio signals).
                \item \textit{Example}: MFCC in music classification for genre distinction.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Feature Engineering vs. Feature Extraction}
            \begin{itemize}
                \item Extraction creates new features; engineering selects the best ones.
            \end{itemize}
        
        \item \textbf{Iterative Process}:
            \begin{itemize}
                \item Requires multiple iterations to refine effective features.
            \end{itemize}
        
        \item \textbf{Impact on Models}:
            \begin{itemize}
                \item Right features enhance interpretability, efficiency, and prediction performance.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: TF-IDF Example in Python}
    \begin{lstlisting}[language=Python]
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample text documents
documents = ["I love programming in Python", "Python is great for Data Science"]

# Creating the TF-IDF model
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Display features
print(vectorizer.get_feature_names_out())
print(tfidf_matrix.toarray())
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    % Summary of importance of feature extraction.
    Feature extraction is essential in machine learning workflows. Transforming raw data into meaningful features enhances model learning effectiveness and efficiency. As we continue, we will explore related concepts such as dimensionality reduction for further refinement of the feature space.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction}
    \begin{block}{What is Dimensionality Reduction?}
        Dimensionality reduction is a process that reduces the number of random variables or features under consideration while retaining significant information. It simplifies datasets, reduces costs, and enhances model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Dimensionality Reduction}
    \begin{enumerate}
        \item \textbf{Computational Efficiency:} Lower dimensions reduce computational costs, speeding up processing and training.
        \item \textbf{Noise Reduction:} Eliminates redundant features, improving model accuracy.
        \item \textbf{Visualization:} Makes it easier to visualize high-dimensional data, providing insightful analysis.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques}
    \begin{itemize}
        \item \textbf{Principal Component Analysis (PCA)}
        \begin{itemize}
            \item Transforms the feature space, preserving max variance.
            \item Projects data to principal components.
            \item \begin{equation} Z = XW \end{equation} where \( W \) is the matrix of principal components.
            \item \textit{Example:} Reduce dataset from 10 features to 2/3 components capturing 95\% variance.
        \end{itemize}
        
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
        \begin{itemize}
            \item Excels at preserving local structures for visualization.
            \item Converts distances into probabilities minimizing Kullback-Leibler divergence.
            \item \textit{Key Point:} Non-linear and effective for complex datasets.
            \item \textit{Example:} Clusters of handwritten digits visualized despite high-dimensional spread.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Dimensionality reduction should be implemented carefully to avoid losing important information.
        \item PCA is suitable for linear data; t-SNE for non-linear relationships.
        \item Often, applying PCA before t-SNE is beneficial for efficiency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Dimensionality reduction techniques like PCA and t-SNE are crucial for efficient data handling and insightful analysis, significantly enhancing model performance and interpretability.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Feature Selection Strategies}
    % Overview of strategies for selecting relevant features
    Feature selection is a crucial step in the data preprocessing process. It involves selecting a subset of relevant features (variables, predictors) for use in model construction. The goals are to improve model performance, reduce overfitting, and shorten training time.
\end{frame}

\begin{frame}
    \frametitle{Importance of Feature Selection}
    % Why is feature selection important?
    \begin{itemize}
        \item \textbf{Reduces Overfitting:} Eliminates noise or irrelevant data.
        \item \textbf{Improves Model Interpretability:} Simplifies models by reducing complexity.
        \item \textbf{Enhances Computational Efficiency:} Decreases training time and resource consumption.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Feature Selection Strategies}
    % Overview of the categories of feature selection methods
    There are three main categories of feature selection methods:
    \begin{enumerate}
        \item \textbf{Filter Methods}
        \item \textbf{Wrapper Methods}
        \item \textbf{Embedded Methods}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods}
    % Details on Filter Methods
    \begin{itemize}
        \item Evaluate relevance by their relationship with the target variable using statistical tests.
        \item \textbf{Examples:}
        \begin{itemize}
            \item \textit{Correlation Coefficient:} Measures linear relationships (Pearson’s r).
            \item \textit{Chi-Squared Test:} Evaluates categorical features against the target variable.
        \end{itemize}
    \end{itemize}
    \begin{lstlisting}[language=Python]
from sklearn.feature_selection import chi2
from sklearn.preprocessing import LabelEncoder
chi2_scores = chi2(X_train, y_train)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrapper Methods}
    % Details on Wrapper Methods
    \begin{itemize}
        \item Use a predictive model to evaluate combinations of features, selecting based on performance.
        \item \textbf{Examples:}
        \begin{itemize}
            \item \textit{Recursive Feature Elimination (RFE):} Trains the model recursively to eliminate the least significant features.
        \end{itemize}
    \end{itemize}
    \begin{lstlisting}[language=Python]
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(model, 5)  # Selecting top 5 features
X_rfe = rfe.fit_transform(X_train, y_train)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Embedded Methods}
    % Details on Embedded Methods
    \begin{itemize}
        \item Perform feature selection as part of the model training process.
        \item \textbf{Examples:}
        \begin{itemize}
            \item \textit{Lasso Regression:} Regularization technique that shrinks some coefficients to zero, effectively performing feature selection.
        \end{itemize}
    \end{itemize}
    \begin{lstlisting}[language=Python]
from sklearn.linear_model import Lasso
model = Lasso(alpha=0.01)
model.fit(X_train, y_train)
selected_features = X_train.columns[model.coef_ != 0]
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    % Summary of key points regarding feature selection
    \begin{itemize}
        \item Use \textbf{filter methods} for initial analysis of features.
        \item \textbf{Wrapper methods} provide more tailored results but with higher computational costs.
        \item \textbf{Embedded methods} combine training and selection, useful for complex models like tree-based algorithms.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    % Final thoughts
    Selecting the right features is pivotal for building efficient models. Understanding different strategies allows data scientists to tailor their approach based on dataset characteristics and modeling objectives. Employing these methods enhances model performance and maintains interpretability.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods for Feature Selection}
    \begin{block}{Overview}
        Filter methods evaluate feature relevance based on intrinsic properties, independent of machine learning algorithms.
    \end{block}
    \begin{itemize}
        \item \textbf{Independence from Algorithms:} Fast and efficient.
        \item \textbf{High Dimensional Data Handling:} Suitable for eliminating irrelevant features quickly.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Filter Methods}
    \begin{enumerate}
        \item \textbf{Correlation Coefficient}
        \begin{itemize}
            \item \textbf{Definition:} Measure of how two variables change together, ranging from -1 to +1.
            \item \textbf{Formula:}
            \begin{equation}
                r = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum (X_i - \bar{X})^2} \sqrt{\sum (Y_i - \bar{Y})^2}}
            \end{equation}
            \item \textbf{Application:} Assess relationship strength between features and target variable.
        \end{itemize}
        
        \item \textbf{Chi-Squared Test}
        \begin{itemize}
            \item \textbf{Definition:} Evaluates significant associations between categorical variables.
            \item \textbf{Formula:}
            \begin{equation}
                \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
            \end{equation}
            \item \textbf{Application:} Assess relationships in categorical features.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item \textbf{Speed and Efficiency:} Computationally faster than wrapper methods.
        \item \textbf{Pre-Processing Step:} Ideal for initial feature selection.
        \item \textbf{Independence from Model Accuracy:} Does not guarantee model performance improvement.
    \end{itemize}
    \begin{block}{Summary}
        Filter methods like correlation coefficient and chi-squared tests are crucial for assessing and selecting relevant features, enhancing the data preprocessing stage in machine learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Wrapper Methods for Feature Selection}
  \begin{block}{Overview}
      Wrapper methods are a technique for feature selection that assesses the usefulness of feature subsets using the performance of a predictive model. Unlike filter methods, they consider feature interactions and are thereby more integrative.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts of Wrapper Methods}
  \begin{itemize}
    \item \textbf{What are Wrapper Methods?}
      \begin{itemize}
        \item Use a predictive model to score feature subsets based on accuracy.
        \item Seek optimal feature combinations through systematic search.
      \end{itemize}
    
    \item \textbf{How They Work:}
      \begin{itemize}
        \item \textbf{Search Strategies:}
          \begin{itemize}
            \item \textbf{Forward Selection:} Start with no features and add features iteratively.
            \item \textbf{Backward Elimination:} Start with all features and remove the least significant ones.
            \item \textbf{Recursive Feature Elimination (RFE):} Recursively fit and eliminate the weakest features.
          \end{itemize}
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example and Advantages of Wrapper Methods}
  \begin{block}{Example}
      Consider a dataset on house prices with features like number of bedrooms and area. Using a regression model, a wrapper method evaluates combinations to determine the best predictors.
      \begin{itemize}
          \item \textbf{Forward Selection Approach:}
            \begin{enumerate}
              \item Start with no features.
              \item Test individual features to find the one that maximizes accuracy.
              \item Add features iteratively until no significant improvement is observed.
            \end{enumerate}
      \end{itemize}
  \end{block}
  
  \begin{block}{Advantages}
      \begin{itemize}
        \item Higher accuracy due to model-specific evaluations.
        \item Capture interaction effects among features.
        \item Customizable approach based on the problem context.
      \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Code Snippet Example}
  Here’s a simple example of Recursive Feature Elimination (RFE) in Python using scikit-learn:

  \begin{lstlisting}[language=Python]
from sklearn.datasets import load_boston
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Load dataset
X, y = load_boston(return_X_y=True)

# Create linear regression model
model = LinearRegression()

# Create RFE model and select top 5 features
rfe = RFE(model, 5)
fit = rfe.fit(X, y)

# Selected features
print("Selected features: ", fit.support_)
print("Feature ranking: ", fit.ranking_)
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  Wrapper methods for feature selection effectively balance computational complexity and model accuracy. They excel in scenarios requiring consideration of feature interactions. Understanding these methods is crucial for effective data preparation and modeling in machine learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Embedded Methods for Feature Selection}
    % Overview of embedded methods in feature selection.
    Embedded methods are techniques that incorporate feature selection into the model training process. This is different from wrapper methods, which evaluate subsets of features separately. Embedded methods identify the best features while the model is being trained, improving efficiency and reducing overfitting.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Embedded Methods}
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Incorporation within the Learning Algorithm:} 
                Feature selection occurs alongside model training, allowing efficient processes.
                
            \item \textbf{Regularization Techniques:} 
                Use regularization to penalize less important features to prevent overfitting, with techniques like Lasso (L1) and Ridge (L2).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Embedded Methods}
    \begin{enumerate}
        \item \textbf{Lasso Regression (L1 Regularization)}
            \begin{itemize}
                \item Mechanism: Shrinks some coefficients to zero.
                \item Formula: 
                \[
                \min_{w} \left( \text{Loss}(y, Xw) + \lambda \|w\|_1 \right)
                \]
            \end{itemize}
        
        \item \textbf{Decision Trees and Tree-Based Models}
            \begin{itemize}
                \item Provide feature importance scores based on model performance.
                \item Example: Income and credit history may be key features in credit scoring.
            \end{itemize}

        \item \textbf{Elastic Net}
            \begin{itemize}
                \item Combines L1 and L2 regularization for balance in feature selection.
                \item Formula: 
                \[
                \min_{w} \left( \text{Loss}(y, Xw) + \lambda_1 \|w\|_1 + \lambda_2 \|w\|_2^2 \right)
                \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Embedded Methods}
    \begin{itemize}
        \item \textbf{Efficiency:} Embedded methods result in faster training times.
        \item \textbf{Reduced Overfitting:} Regularization techniques improve generalization.
        \item \textbf{Automatic Handling of Irrelevant Features:} These methods discard irrelevant features inherently.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Embedded methods are a powerful approach to feature selection that integrates selection into model training. They offer automation, efficiency, and robust predictive capabilities by highlighting relevant features while ensuring the model remains simple yet effective.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Exploration}
    Next, we will discuss how to evaluate the importance of features selected by embedded methods and analyze their impact on model performance.
\end{frame}

\begin{frame}
    \frametitle{Evaluating Feature Importance}
    \begin{block}{Introduction}
        Evaluating feature importance is crucial in data preprocessing and feature engineering as it helps identify which features significantly contribute to the performance of a machine learning model. Understanding feature importance enables optimal feature selection and enhances model interpretability.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Methods for Evaluating Feature Importance - Part 1}
    \begin{enumerate}
        \item \textbf{Feature Importance from Tree-Based Models}
        \begin{itemize}
            \item Tree-based models like Random Forest and Gradient Boosting assess feature importance based on impurity reduction.
            \item \textbf{Example:} Importance score in Random Forest is the average decrease in node impurity.
        \end{itemize}
        \begin{block}{Code Snippet}
            \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, y_train)
importance = model.feature_importances_
            \end{lstlisting}
        \end{block}

        \item \textbf{Permutation Importance}
        \begin{itemize}
            \item Measures the increase in prediction error after permuting the feature's values.
            \item \textbf{Example:} If accuracy drops from 90\% to 70\%, the feature is important.
        \end{itemize}
        \begin{block}{Code Snippet}
            \begin{lstlisting}[language=Python]
from sklearn.inspection import permutation_importance
result = permutation_importance(model, X_test, y_test, n_repeats=10)
sorted_idx = result.importances_mean.argsort()
            \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Methods for Evaluating Feature Importance - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}  % Continue numbering from the previous frame
        \item \textbf{SHAP Values (SHapley Additive exPlanations)}
        \begin{itemize}
            \item Provide a unified measure of feature importance based on cooperative game theory.
            \item \textbf{Example:} Features consistently increasing predicted probability have high SHAP values.
        \end{itemize}
        \begin{block}{Code Snippet}
            \begin{lstlisting}[language=Python]
import shap
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)
            \end{lstlisting}
        \end{block}

        \item \textbf{Lasso Regularization}
        \begin{itemize}
            \item L1 regularization shrinks less important feature coefficients to zero for feature selection.
            \item \textbf{Example:} Features with non-zero coefficients are deemed important.
        \end{itemize}
        \begin{block}{Code Snippet}
            \begin{lstlisting}[language=Python]
from sklearn.linear_model import Lasso
lasso = Lasso(alpha=0.01)
lasso.fit(X_train, y_train)
importance = lasso.coef_
            \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Model Dependence:} Feature importance can vary across models; evaluate within context.
            \item \textbf{Interpretability:} Understanding important features enhances model explainability.
            \item \textbf{Iterative Process:} Evaluating feature importance is ongoing during model development.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding feature importance is essential for building effective predictive models. Methods like tree-based feature importance, permutation importance, SHAP values, and regularization can inform feature engineering strategies effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Feature Engineering in Practice}
    \begin{block}{Overview}
        Feature engineering is a crucial step in the machine learning pipeline, transforming raw data into a format that effectively captures underlying patterns. This case study illustrates feature engineering techniques used in a retail sales prediction project.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Retail Sales Prediction}
    \begin{itemize}
        \item \textbf{Objective:} Build a model for accurate sales predictions to improve inventory management and pricing strategies.
        \item \textbf{Dataset:} Sales data over several years, including:
        \begin{itemize}
            \item Date
            \item Store ID
            \item Sales Amount
            \item Temperature (°C)
            \item Promotions (binary)
            \item Competitor Prices
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Steps}
    \begin{enumerate}
        \item \textbf{Handling Missing Values:}
        \begin{itemize}
            \item Filled missing sales data using linear interpolation based on the date.
            \item Imputed competitor prices with the mean value for each store.
        \end{itemize}
        
        \item \textbf{Data Type Conversion:}
        \begin{itemize}
            \item Converted 'Date' to datetime objects for feature extraction (e.g., day of the week).
        \end{itemize}
        
        \item \textbf{Normalization:}
        \begin{itemize}
            \item Min-Max scaling for temperature, transforming values to a range of [0, 1].
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering Techniques}
    \begin{enumerate}
        \item \textbf{Datetime Features:}
        \begin{itemize}
            \item Extracted features: Day of the week, Month, Is weekend.
            \item Example: June 15, 2023, gives Day of the week = 3, Is weekend = 0.
        \end{itemize}
        
        \item \textbf{Lag Features:}
        \begin{itemize}
            \item Created features sales from previous week and month to capture temporal dependencies.
        \end{itemize}
        \begin{equation}
            \text{sales\_lag\_7} = \text{sales}_{t-7}
        \end{equation}
        
        \item \textbf{One-Hot Encoding:}
        \begin{itemize}
            \item Transformed categorical variables (e.g., 'Store ID', 'Promotions') into numerical vectors.
        \end{itemize}
        
        \item \textbf{External Data Integration:}
        \begin{itemize}
            \item Included weather data (e.g., rainfall, holidays) using APIs for daily conditions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Feature engineering impacts model performance; the right features boost accuracy.
        \item Understanding business context is vital for effective feature creation.
        \item Iteration is key: continually assess and refine features based on model performance.
    \end{itemize}
    \begin{block}{Conclusion}
        This case study highlights the significance of thorough data preprocessing and innovative feature engineering in achieving robust predictive models for retail sales.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet - Creating Datetime Features}
    \begin{lstlisting}[language=Python]
    import pandas as pd

    # Load the dataset
    data = pd.read_csv('sales_data.csv')

    # Convert 'Date' to datetime
    data['Date'] = pd.to_datetime(data['Date'])

    # Extract useful features
    data['Day_of_Week'] = data['Date'].dt.dayofweek
    data['Is_Weekend'] = (data['Day_of_Week'] >= 5).astype(int)
    data['Month'] = data['Date'].dt.month
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Ethical Considerations in Data Preprocessing}
    \begin{block}{Introduction}
        Data preprocessing and feature engineering are critical steps in building effective machine learning models. However, these processes come with ethical implications that can significantly impact our analyses. 
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Ethical Considerations}
    \begin{itemize}
        \item \textbf{Bias in Data Representation}
            \begin{itemize}
                \item Definition: Bias occurs when certain groups are underrepresented or misrepresented in the dataset.
                \item Example: A facial recognition system trained on predominantly light-skinned faces may underperform on individuals with darker skin tones.
            \end{itemize}
        
        \item \textbf{Feature Selection Bias}
            \begin{itemize}
                \item Definition: The selection of features can introduce bias, particularly if the chosen features reflect societal prejudices.
                \item Example: Using zip codes as a feature for predicting loan approval may reinforce existing inequalities.
            \end{itemize}
        
        \item \textbf{Implicit Assumptions}
            \begin{itemize}
                \item Definition: Assumptions made during feature engineering can perpetuate systemic biases.
                \item Example: Assuming higher education levels correlate with job performance can exclude valuable candidates from non-traditional backgrounds.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Strategies to Mitigate Bias}
    \begin{enumerate}
        \item \textbf{Diverse Data Collection}
            \begin{itemize}
                \item Ensure datasets are representative and include diverse samples.
            \end{itemize}
        
        \item \textbf{Bias Auditing}
            \begin{itemize}
                \item Regularly audit datasets and features for possible biases. 
            \end{itemize}
        
        \item \textbf{Inclusive Feature Engineering}
            \begin{itemize}
                \item Engage stakeholders from various backgrounds to ensure respectful feature selection.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        Ethical considerations in data preprocessing and feature engineering are essential for building equitable and trustworthy models. As data scientists, we must actively work to mitigate biases and strive for fairness in our algorithms.
    \end{block}
    
    \begin{itemize}
        \item Bias can exist in data representation and feature selection.
        \item Vigilance in data collection and feature engineering is essential.
        \item Diverse perspectives in feature selection enhance fairness and effectiveness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Example function to check for bias in loan application data
def check_bias(dataframe, feature, target):
    fairness_analysis = dataframe.groupby(feature)[target].value_counts(normalize=True)
    return fairness_analysis

# Load a sample dataset
data = pd.read_csv("loan_applications.csv")
bias_report = check_bias(data, 'zip_code', 'approval_status')
print(bias_report)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Data Preprocessing - Introduction}
    \begin{block}{Introduction}
        Data preprocessing and feature engineering are critical steps in the machine learning pipeline. 
        These processes enhance the quality of data, enabling models to learn more effectively. 
        Below are best practices to follow in this phase.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Data Preprocessing - Key Practices}
    \begin{enumerate}
        \item \textbf{Understand Your Data:}
        \begin{itemize}
            \item Perform exploratory data analysis (EDA) to understand data distributions.
            \item Use visualizations to assess variable relationships.
        \end{itemize}
        
        \item \textbf{Handle Missing Values:}
        \begin{itemize}
            \item Use imputation or remove excessive missing values.
            \item Imputation methods should be realistic to the data context.
        \end{itemize}

        \item \textbf{Normalize and Scale Features:}
        \begin{itemize}
            \item Normalize using Min-Max Scaling:
            \begin{equation}
                X' = \frac{X - X_{min}}{X_{max} - X_{min}}
            \end{equation}
            \item Standardize using Z-Score Scaling:
            \begin{equation}
                X' = \frac{X - \mu}{\sigma}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Data Preprocessing - Advanced Practices}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Feature Selection:}
        \begin{itemize}
            \item Eliminate irrelevant features through correlation analysis.
            \item Use PCA for dimensionality reduction.
        \end{itemize}
        
        \item \textbf{Categorical Encoding:}
        \begin{itemize}
            \item Apply One-Hot Encoding for binary format.
            \item Use Label Encoding for ordinal variables.
        \end{itemize}

        \item \textbf{Check for Outliers:}
        \begin{itemize}
            \item Identify outliers using IQR or Z-scores.
            \item Cap or remove outliers based on their impact.
        \end{itemize}

        \item \textbf{Keep an Eye on Data Leakage:}
        \begin{itemize}
            \item Fit preprocessing steps on training data only to avoid leakage.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Data Preprocessing - Conclusion}
    \begin{block}{Conclusion}
        By adhering to these best practices, data scientists can create robust datasets for machine learning model training. 
        Each step enhances the model's ability to learn and make accurate predictions based on improved data quality.
    \end{block}

    \begin{block}{Next Steps}
        Moving forward, we will wrap up this chapter and discuss how these preprocessing techniques will lead into deeper considerations for machine learning applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Key Takeaways}
  
  \begin{block}{Key Takeaways from Chapter 3}
    Reflecting on core concepts of data preprocessing and feature engineering:
  \end{block}
  
  \begin{enumerate}
    \item \textbf{Importance of Data Quality}:
    \begin{itemize}
      \item Definition: Accuracy, completeness, and reliability of data.
      \item Example: Missing values or incorrect labels can hurt model performance.
    \end{itemize}
    
    \item \textbf{Best Practices in Data Preprocessing}:
    \begin{itemize}
      \item Cleaning: Remove noise and irrelevant data.
      \item Normalization/Standardization: Scale features for algorithms that assume this.
      \item Encoding Categorical Variables: Convert categorical data into numerical format.
    \end{itemize}
    
    \item \textbf{Feature Engineering Techniques}:
    \begin{itemize}
      \item Creating New Features: Derive attributes for better model context.
      \item Dimensionality Reduction: Techniques like PCA to retain critical information.
    \end{itemize}
    
    \item \textbf{Impact of Feature Selection}:
    \begin{itemize}
      \item Filter and Wrapper methods to select relevant features based on performance.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Moving Forward}
  
  \begin{block}{Next Steps in Machine Learning}
    Skills and concepts for practical applications:
  \end{block}
  
  \begin{itemize}
    \item \textbf{Model Selection and Training}: Choose models based on problem type (e.g., regression, classification).
    \item \textbf{Evaluation Metrics}: Learn to measure model performance (accuracy, precision, recall, F1-score).
    \item \textbf{Hyperparameter Tuning}: Optimize model parameters for better performance.
    \item \textbf{Deployment and Monitoring}: Deploy models and monitor performance over time.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Call to Action}
  
  \begin{block}{Call to Action}
    Recommendations for furthering your skills:
  \end{block}
  
  \begin{itemize}
    \item \textbf{Experimentation and Practice}: Work on datasets from platforms like Kaggle or UCI Machine Learning Repository.
    \item \textbf{Stay Curious}: Explore ongoing literature in data science and machine learning to discover new techniques.
  \end{itemize}
  
  \textbf{Preparing for Future Challenges}: 
  With these tools and knowledge, you are ready to tackle more complex machine learning challenges ahead!
\end{frame}


\end{document}