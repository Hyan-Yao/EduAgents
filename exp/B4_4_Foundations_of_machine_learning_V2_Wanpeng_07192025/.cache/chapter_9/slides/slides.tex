\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Overview of Supervised Learning}
    \begin{itemize}
        \item Supervised learning is a branch of machine learning where a model is trained on \textbf{labeled datasets}.
        \item Each training sample consists of \textbf{input data} paired with the \textbf{correct output}.
        \item The model learns to minimize the error between predictions and actual outcomes by mapping inputs to outputs.
    \end{itemize}
    
    \begin{block}{Key Characteristics:}
        \begin{itemize}
            \item \textbf{Labeled Data:} Each data point has a corresponding label.
            \item \textbf{Training Phase:} Parameters adjusted based on error during training.
            \item \textbf{Outcome Prediction:} Model can predict outcomes for new data after training.
        \end{itemize}
    \end{block}

    \begin{block}{Examples:}
        \begin{itemize}
            \item \textbf{Image Classification:} Classifying animals based on labeled images.
            \item \textbf{Spam Detection:} Identifying emails as \textit{spam} or \textit{not spam} using labeled data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Structure and Concepts}
    \begin{itemize}
        \item Neural networks are algorithms modeled after the human brain, designed to \textbf{recognize patterns}.
        \item They consist of interconnected groups of \textbf{nodes (neurons)} organized in layers.
    \end{itemize}

    \begin{block}{Basic Structure:}
        \begin{itemize}
            \item \textbf{Input Layer:} Takes input features.
            \item \textbf{Hidden Layers:} Intermediate layers that process inputs.
            \item \textbf{Output Layer:} Produces the final prediction.
        \end{itemize}
    \end{block}

    \begin{block}{Key Concepts:}
        \begin{itemize}
            \item \textbf{Activation Function:} Determines neuron output (e.g., ReLU, Sigmoid).
            \item \textbf{Weights and Biases:} Parameters adjusted to minimize prediction error.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Networks - Example and Formula}
    \begin{block}{Simple Example:}
        Consider a scenario where an input layer consists of features representing pixels of a handwritten digit image. The neural network learns to output the corresponding digit based on patterns identified in the training data.
    \end{block}

    \begin{block}{Formula:}
        The output \( y \) of a neuron is expressed as:
        \begin{equation}
            y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( w_i \): weight of input \( x_i \)
            \item \( b \): bias term
            \item \( f \): activation function
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architectures}
    \begin{block}{Understanding Neural Network Architectures}
        Neural networks come in various architectures, each suited to different types of tasks. Here, we will discuss three primary types:
        \begin{itemize}
            \item Feedforward Neural Networks
            \item Convolutional Neural Networks
            \item Recurrent Neural Networks
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedforward Neural Networks (FNN)}
    \begin{itemize}
        \item \textbf{Definition}: The simplest type of artificial neural network where connections between nodes do not form cycles. Information moves in one direction—forward.
        \item \textbf{Structure}: Composed of an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next.
        \item \textbf{Example Use Cases}: Binary classification tasks, e.g., predicting email spam.
    \end{itemize}

    \begin{block}{Key Concept}
        \textbf{Activation Function}: Determines the output of a neuron. Common functions include sigmoid or ReLU (Rectified Linear Unit).
    \end{block}

    \begin{equation}
        y = f(W \cdot X + b)
    \end{equation}
    Where: 
    \begin{itemize}
        \item \( W \) = weights,
        \item \( X \) = input vector,
        \item \( b \) = bias,
        \item \( f \) = activation function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Convolutional Neural Networks (CNN)}
    \begin{itemize}
        \item \textbf{Definition}: Specialized neural networks primarily used in processing grid-like data such as images. They use convolutional layers to learn spatial hierarchies of features.
        \item \textbf{Structure}: Composed of convolutional layers, pooling layers, and fully connected layers.
        \item \textbf{Key Feature}: \textbf{Convolution operation}—filters applied to input data create feature maps.
    \end{itemize}

    \begin{block}{Example Use Cases}
        Image recognition, facial recognition, and object detection.
    \end{block}

    \begin{block}{Key Concept}
        \textbf{Pooling Layer}: Reduces the spatial dimensions of the feature maps while maintaining important information (e.g., Max Pooling).
    \end{block}

    % Optional: Insert a diagram here showing the flow of CNN
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNN)}
    \begin{itemize}
        \item \textbf{Definition}: Neural networks that allow for cycles in connections, designed to recognize patterns in sequences of data.
        \item \textbf{Structure}: Incorporates loops, allowing information to persist and be fed back into the network.
    \end{itemize}

    \begin{block}{Example Use Cases}
        Natural language processing, speech recognition, and time-series forecasting.
    \end{block}

    \begin{block}{Key Concept}
        \textbf{Memory}: RNNs maintain state (memory) that informs future outputs based on past inputs, making them powerful for sequence prediction tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps}
    \begin{itemize}
        \item \textbf{Feedforward Neural Networks}: Best for straightforward classification tasks with static data.
        \item \textbf{Convolutional Neural Networks}: Ideal for spatial data analysis like images.
        \item \textbf{Recurrent Neural Networks}: Suited for sequential data tasks requiring memory of previous inputs.
    \end{itemize}

    \begin{block}{Next Steps}
        We'll explore \textbf{Activation Functions}, which are crucial for determining how information is processed within these architectures.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Introduction}
    \begin{block}{Introduction to Activation Functions}
        Activation functions are a crucial component of neural networks. They determine the output of a neuron given an input or set of inputs. By introducing non-linearity into the model, activation functions enable neural networks to learn complex patterns in data.
    \end{block}
    \begin{block}{Importance}
        Without activation functions, a neural network would behave like a linear regressor, incapable of capturing intricate relationships within data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Common Types}
    \begin{enumerate}
        \item \textbf{Sigmoid Function}
            \begin{itemize}
                \item Equation: 
                \begin{equation}
                \sigma(x) = \frac{1}{1 + e^{-x}}
                \end{equation}
                \item Range: \( (0, 1) \)
                \item Advantages: Smooth gradient; suitable for optimization.
                \item Disadvantages: Can cause vanishing gradient problems.
                \item Example Usage: Output layer for binary classification.
            \end{itemize}
        \item \textbf{ReLU (Rectified Linear Unit)}
            \begin{itemize}
                \item Equation: 
                \begin{equation}
                \text{ReLU}(x) = \max(0, x)
                \end{equation}
                \item Range: \( [0, \infty) \) 
                \item Advantages: Computationally efficient; mitigates vanishing gradient problem.
                \item Disadvantages: Can lead to "dying ReLU" issue.
                \item Example Usage: Hidden layers of deep neural networks.
            \end{itemize}
        \item \textbf{Tanh (Hyperbolic Tangent)}
            \begin{itemize}
                \item Equation: 
                \begin{equation}
                \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
                \end{equation}
                \item Range: \( (-1, 1) \)
                \item Advantages: Outputs values centered around zero; smooth gradient.
                \item Disadvantages: Still suffers from vanishing gradient, but less severely than sigmoid.
                \item Example Usage: Hidden layers where output benefits from zero centering.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Key Points and Implementation}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Choice of activation function matters; ReLU is often preferred in modern architectures.
            \item Different frameworks (e.g., TensorFlow, PyTorch) provide easy selection of these functions.
        \end{itemize}
    \end{block}
    \begin{block}{Code Snippet (Python Example)}
        \begin{lstlisting}[language=Python]
import numpy as np

# Sigmoid Function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ReLU Function
def relu(x):
    return np.maximum(0, x)

# Tanh Function
def tanh(x):
    return np.tanh(x)

# Examples of usage
inputs = np.array([-2, -1, 0, 1, 2])
print("Sigmoid:", sigmoid(inputs))
print("ReLU:", relu(inputs))
print("Tanh:", tanh(inputs))
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition to Next Topic}
    Understanding these activation functions is vital as they are foundational to tuning deep learning architectures and optimizing performance on tasks such as classification and regression. Next, we will dive into how data moves through a neural network during training, focusing on \textbf{Forward and Backward Propagation}.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Forward and Backward Propagation - Summary}
  \begin{block}{Overview}
    Understanding the concepts of forward propagation and backward propagation in the training of neural networks is crucial for optimizing their learning processes.
  \end{block}
  \begin{itemize}
    \item Forward propagation generates predictions based on input data.
    \item Backward propagation updates the weights based on prediction errors.
    \item These processes are iterative and performed over multiple epochs.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Forward Propagation}
  \begin{block}{Definition}
    Forward propagation is the process by which input data is fed through the neural network to produce an output.
  \end{block}
  
  \begin{enumerate}
    \item \textbf{Input Layer}: Input features are fed into the network.
    \item \textbf{Hidden Layers}:
      \begin{itemize}
        \item Calculate the weighted sum:
          \[
          z = w \cdot x + b
          \]
        \item Apply the activation function:
          \[
          a = f(z)
          \]
      \end{itemize}
    \item \textbf{Output Layer}: Produces the final predicted output.
  \end{enumerate}
  
  \begin{block}{Example}
    A neural network with an input layer of 2 units, a hidden layer of 2 units, and an output layer of 1 unit processes inputs through weights and activation functions to yield a single output.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Backward Propagation}
  \begin{block}{Definition}
    Backward propagation is the process of updating the weights of the network by applying the gradients of the loss function concerning each weight.
  \end{block}
  
  \begin{enumerate}
    \item \textbf{Calculate Loss}:
      \[
      L = \frac{1}{n} \sum (y_{\text{true}} - y_{\text{pred}})^2
      \]
    \item \textbf{Compute Gradients}:
      \[
      \frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}
      \]
    \item \textbf{Update Weights}:
      \[
      w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w}
      \]
      where \( \eta \) is the learning rate.
  \end{enumerate}
  
  \begin{block}{Example}
    If the error in predicting the output is high, backward propagation adjusts the weights that contributed most to the error to reduce the loss.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Functions - Overview}
    \begin{block}{Overview of Loss Functions}
        Loss functions are crucial components in training neural networks. They measure how well the neural network's predictions align with the actual data (the true labels). By quantifying the difference between predicted outputs and actual results, loss functions guide the optimization process, helping the model improve its performance over time.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Definition:} A loss function, or cost function, quantifies the error of a neural network's predictions. The goal during training is to minimize this error.
        \item \textbf{Significance:} The choice of loss function directly influences the learning outcome, affecting both convergence speed and the quality of the final model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Loss Functions}
    \begin{enumerate}
        \item \textbf{Mean Squared Error (MSE)}
            \begin{itemize}
                \item \textbf{Usage:} Primarily for regression tasks.
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
                \end{equation}
                \item \textbf{Key Point:} Penalizes larger errors heavily, making it sensitive to outliers.
            \end{itemize}
        
        \item \textbf{Binary Cross-Entropy Loss}
            \begin{itemize}
                \item \textbf{Usage:} Used for binary classification problems.
                \item \textbf{Formula:} 
                \begin{equation}
                    L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
                \end{equation}
                \item \textbf{Key Point:} Measures the performance of a classification model whose output is a probability value between 0 and 1.
            \end{itemize}
        
        \item \textbf{Categorical Cross-Entropy Loss}
            \begin{itemize}
                \item \textbf{Usage:} Suitable for multi-class classification tasks.
                \item \textbf{Formula:}
                \begin{equation}
                    L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
                \end{equation}
                \item \textbf{Key Point:} Similar to binary cross-entropy but applicable when there are more than two classes; emphasizes the confidence in the model's predictions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Loss Functions (cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Hinge Loss}
            \begin{itemize}
                \item \textbf{Usage:} Primarily used for "maximum-margin" classification, notably for Support Vector Machines.
                \item \textbf{Formula:} 
                \begin{equation}
                    L(y, \hat{y}) = \sum_{i=1}^{n} \max(0, 1 - y_i \hat{y}_i)
                \end{equation}
                \item \textbf{Key Point:} Encourages the model to correctly classify and maximize the margin between classes.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Example Illustration}
        \textbf{Scenario:} Predicting house prices (regression).\\
        If the actual price is \$200,000 and the predicted price is \$180,000:
        \begin{equation}
            \text{MSE} = (200,000 - 180,000)^2 = 4,000,000,000
        \end{equation}
        The loss indicates the degree of error in predictions, aiding the model in adjustment.
    \end{block}
    
    \begin{block}{Conclusion}
        Choosing the appropriate loss function is paramount in optimizing a neural network. It impacts how the model learns and its effectiveness in making predictions. Understanding the characteristics of different loss functions will enable practitioners to select the right one for their specific problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques}
    \begin{block}{Understanding Optimization in Neural Networks}
        Optimization in the context of neural networks refers to the method of fine-tuning weights and biases to minimize the loss function, which is crucial for effective learning from training data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Optimization Algorithms}
    \begin{enumerate}
        \item \textbf{Gradient Descent}
    \end{enumerate}
    \begin{itemize}
        \item \textbf{Concept}: Adjusts weights by taking small steps in the opposite direction of the gradient.
        \item \textbf{Formula}:
        \begin{equation}
            w = w - \eta \cdot \nabla L(w)
        \end{equation}
        \item \textbf{Example}: Like hiking down a hill to find the quickest path down.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Optimization Algorithms (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Adam (Adaptive Moment Estimation)}
    \end{enumerate}
    \begin{itemize}
        \item \textbf{Concept}: Computes adaptive learning rates for each parameter.
        \item \textbf{Benefits}:
        \begin{itemize}
            \item Combines features of AdaGrad and RMSProp.
            \item Adjusts learning rates based on moments of gradients.
        \end{itemize}
        \item \textbf{Formulas}:
        \begin{align}
            m_t & = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
            v_t & = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
            \hat{m_t} & = \frac{m_t}{1 - \beta_1^t} \quad \hat{v_t} = \frac{v_t}{1 - \beta_2^t} \\
            w & = w - \frac{\eta \cdot \hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
        \end{align}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Optimization Algorithms (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{RMSprop (Root Mean Square Propagation)}
    \end{enumerate}
    \begin{itemize}
        \item \textbf{Concept}: Addresses vanishing learning rates by scaling according to recent average of gradients.
        \item \textbf{How it Works}: Maintains moving average of squared gradients.
        \item \textbf{Formulas}:
        \begin{align}
            v_t & = \beta v_{t-1} + (1 - \beta)g_t^2 \\
            w & = w - \frac{\eta}{\sqrt{v_t} + \epsilon} g_t
        \end{align}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Learning Rate}: Critical hyperparameter influencing model adjustments.
        \item \textbf{Convergence}: Aim to minimize the loss function for better performance.
        \item Different algorithms exhibit varied convergence behaviors—recognizing their characteristics aids in proper selection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Code Snippet}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(input_dim,))
])
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(train_data, train_labels, epochs=50)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Mastering optimization techniques is crucial for training neural networks effectively, ensuring they converge to optimal solutions for their tasks.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Overfitting and Underfitting - Overview}
  \begin{block}{Understanding Overfitting and Underfitting}
    In supervised learning, particularly neural networks, two common issues are **overfitting** and **underfitting**. 
    These affect a model's ability to generalize to unseen data, critical for enhancing model performance.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Overfitting vs. Underfitting - Definitions}
  \begin{itemize}
    \item \textbf{Overfitting}: 
      \begin{itemize}
        \item Occurs when a model learns the training data too well, capturing noise instead of underlying patterns.
        \item Performs well on training data but poorly on validation/test data.
        \item \textit{Example}: A model memorizing a small dataset fails to generalize.
      \end{itemize}
  
    \item \textbf{Underfitting}: 
      \begin{itemize}
        \item Happens when a model is too simple to capture the data structure.
        \item Results in poor performance on both training and test sets.
        \item \textit{Example}: A linear regression model fitting a complex nonlinear relationship.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Balancing Overfitting and Underfitting}
  \begin{block}{Key Points}
    Achieving a balance between underfitting and overfitting is fundamental. 
    The objective is to create a model that generalizes well to new data.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Methods to Prevent Overfitting and Underfitting}
  \begin{enumerate}
    \item \textbf{Regularization Techniques}:
      \begin{itemize}
        \item **L1 Regularization (Lasso)**: 
          \begin{equation}
          L = L_0 + \lambda \sum |w_i|
          \end{equation}
        \item **L2 Regularization (Ridge)**: 
          \begin{equation}
          L = L_0 + \lambda \sum w_i^2
          \end{equation}
        \item **Dropout**: Random nodes are ignored during training to promote better feature learning.
      \end{itemize}

    \item \textbf{Cross-Validation}: 
      \begin{itemize}
        \item Use k-fold cross-validation to evaluate model performance across data subsets.
      \end{itemize}

    \item \textbf{Early Stopping}:
      \begin{itemize}
        \item Stop training when performance on the validation set begins to degrade.
      \end{itemize}

    \item \textbf{Data Augmentation}:
      \begin{itemize}
        \item Enhance the dataset with transformations to introduce variability.
      \end{itemize}

    \item \textbf{Complexity Control}:
      \begin{itemize}
        \item Limit model complexity by reducing layers or neurons.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  Understanding overfitting and underfitting is crucial for implementing neural networks successfully. By applying regularization techniques and following best practices in model evaluation, we can build models that effectively generalize to new data.
\end{frame}

\begin{frame}
    \frametitle{Hyperparameter Tuning}
    \begin{block}{Overview}
        Hyperparameter tuning is the process of optimizing parameters in a neural network that are set prior to training. These parameters significantly impact model performance, accuracy, training time, and generalization.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{What are Hyperparameters?}
    \begin{itemize}
        \item \textbf{Learning Rate:} Step size during gradient descent; crucial for convergence.
        \item \textbf{Batch Size:} Number of training samples per iteration; affects gradient estimation and stability.
        \item \textbf{Number of Epochs:} Total dataset iterations; influences fitting and overfitting.
        \item \textbf{Network Architecture:} Configuration of layers and units; impacts model capacity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Techniques for Hyperparameter Tuning}
    \begin{enumerate}
        \item \textbf{Grid Search:}
            \begin{itemize}
                \item Systematic evaluation of combinations from a defined grid.
                \item Example: 3 learning rates × 3 batch sizes = 9 combinations.
            \end{itemize}
        \item \textbf{Random Search:}
            \begin{itemize}
                \item Samples a subset of combinations; often faster in high dimensions.
            \end{itemize}
        \item \textbf{Bayesian Optimization:}
            \begin{itemize}
                \item Predicts performance based on previous evaluations; strikes a balance between exploration and exploitation.
            \end{itemize}
        \item \textbf{Learning Rate Scheduling:}
            \begin{itemize}
                \item Adjust learning rate during training to effectively navigate the loss landscape.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Hyperparameters greatly influence a model's generalization ability and performance on unseen data.
        \item There is no one-size-fits-all solution; tuning often requires extensive experimentation.
        \item Use evaluation metrics (e.g., accuracy, F1-score) to guide tuning decisions.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Formula Example}
    To demonstrate the learning rate's impact on convergence, consider the following update formula:
    \begin{equation}
        \theta = \theta - \eta \nabla J(\theta)
    \end{equation}
    Where:
    \begin{itemize}
        \item $\theta$: model parameters
        \item $\eta$: learning rate (hyperparameter)
        \item $\nabla J(\theta)$: gradient of the cost function
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Grid Search (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier

param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50)],
    'activation': ['tanh', 'relu'],
    'learning_rate_init': [0.001, 0.01, 0.1]
}

grid_search = GridSearchCV(MLPClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
print("Best parameters:", grid_search.best_params_)
    \end{lstlisting}
    This snippet utilizes Grid Search to find optimal hyperparameters for an MLP classifier.
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Hyperparameter tuning is essential for building effective neural network models. By systematically exploring various configurations, we can optimize performance and enhance generalization to new data.
\end{frame}

\begin{frame}
    \frametitle{Neural Network Libraries}
    \begin{block}{Introduction}
        In the field of machine learning, various libraries and frameworks simplify and accelerate the building, training, and deploying of neural networks. This slide introduces two of the most popular libraries: \textbf{TensorFlow} and \textbf{PyTorch}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{TensorFlow}
    \begin{itemize}
        \item \textbf{Overview}: Developed by Google Brain, TensorFlow is an open-source library for dataflow programming and numerical computation.
        \item \textbf{Key Features}:
            \begin{itemize}
                \item \textbf{High-Level APIs}: Includes Keras, a high-level API for quick prototyping.
                \item \textbf{Scalability}: Suitable for CPUs and GPUs, scalable to large cloud environments.
                \item \textbf{Deployment}: Supports TensorFlow Serving and TensorFlow Lite for production and mobile deployment.
            \end{itemize}
    \end{itemize}
    \begin{block}{Code Example}
        \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow import keras

# Create a simple feedforward neural network
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PyTorch}
    \begin{itemize}
        \item \textbf{Overview}: Developed by Facebook's AI Research lab, PyTorch is renowned for its flexibility and user-friendliness.
        \item \textbf{Key Features}:
            \begin{itemize}
                \item \textbf{Dynamic Computation Graphs}: Allows architectural changes during runtime.
                \item \textbf{Pythonic Nature}: Integrates seamlessly with Python, making it intuitive to use.
                \item \textbf{Rich Ecosystem}: Includes tools like TorchVision for image processing and TorchText for natural language processing.
            \end{itemize}
    \end{itemize}
    \begin{block}{Code Example}
        \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = SimpleNN()
optimizer = optim.Adam(model.parameters(), lr=0.001)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Summary and Conclusion}
    \begin{itemize}
        \item \textbf{Ease of Use}: Both TensorFlow and PyTorch support beginners and seasoned developers with extensive documentation and community.
        \item \textbf{Performance}: TensorFlow excels in production, while PyTorch is favored for rapid experimentation.
        \item \textbf{Choice Based on Needs}: Selecting between these libraries depends on project requirements and deployment strategies.
    \end{itemize}
    \begin{block}{Conclusion}
        TensorFlow and PyTorch have transformed neural network development, offering powerful tools for machine learning practitioners. Familiarity with these libraries is crucial for implementing supervised learning effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Implementations of Neural Networks}
    \begin{block}{Introduction}
        Neural networks have revolutionized various fields by enabling machines to perform tasks that traditionally required human intelligence. Their capacity to learn from data and recognize patterns makes them invaluable in real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Neural Networks - Part 1}
    \begin{enumerate}
        \item \textbf{Image Recognition}
        \begin{itemize}
            \item \textbf{Concept:} Identifying objects, people, or features in images using deep learning.
            \item \textbf{Example:} Convolutional Neural Networks (CNNs) are used in facial recognition and medical image analysis.
        \end{itemize}
        
        \item \textbf{Natural Language Processing (NLP)}
        \begin{itemize}
            \item \textbf{Concept:} Interaction between computers and human language.
            \item \textbf{Example:} RNNs and Transformers for language translation and chatbots.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Neural Networks - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Speech Recognition}
        \begin{itemize}
            \item \textbf{Concept:} Converting spoken language into text.
            \item \textbf{Example:} Voice assistants like Amazon Alexa use neural networks for understanding user commands.
        \end{itemize}
        
        \item \textbf{Autonomous Vehicles}
        \begin{itemize}
            \item \textbf{Concept:} Perceiving environments and making driving decisions using various sensors.
            \item \textbf{Example:} Object detection and decision-making processes in self-driving cars.
        \end{itemize}
        
        \item \textbf{Financial Forecasting}
        \begin{itemize}
            \item \textbf{Concept:} Analyzing historical data to predict market trends.
            \item \textbf{Example:} LSTM networks for time-series prediction tasks in finance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Neural networks excel in high-dimensional data tasks (e.g., images, text).
            \item They improve with more data, enhancing accuracy and performance.
            \item Architecture choice (CNN, RNN, LSTM, Transformer) is crucial based on application.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        The diverse applications of neural networks showcase their impact across industries. By leveraging their capabilities, they are at the forefront of technological advancements in artificial intelligence.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Note to Students}
    Explore these applications in deeper detail through hands-on projects or case studies in our upcoming sessions!
\end{frame}

\begin{frame}
    \frametitle{Case Study: Neural Network Application}
    \begin{block}{Objectives of the Case Study}
        \begin{itemize}
            \item Understand how neural networks can be applied to solve real-world problems.
            \item Analyze the end-to-end process of implementing a neural network.
            \item Highlight key considerations such as data preparation, model selection, and evaluation metrics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Real-World Problem: Handwritten Digit Recognition}
    \begin{block}{Context}
        A widely recognized application of neural networks is the recognition of handwritten digits from images. The MNIST dataset, a benchmark in the field, comprises 60,000 training images and 10,000 testing images of handwritten digits (0-9).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation}
    \begin{enumerate}
        \item \textbf{Data Collection:}
            \begin{itemize}
                \item \textbf{Dataset:} MNIST
                \item \textbf{Features:} 28x28 pixel grayscale images, each labeled from 0 to 9.
                \item \textbf{Goal:} Classify and predict the digits based on image input.
            \end{itemize}
            
        \item \textbf{Data Preprocessing:}
            \begin{itemize}
                \item Normalization: Scale pixel values (0-255) to a range between 0 and 1 for better model performance.
                \item Reshaping: Flatten each 28x28 image into a 784-dimensional vector.
            \end{itemize}
            \begin{lstlisting}[language=Python]
# Example Code for Normalization:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 28*28))
            \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Model Selection:}
            \begin{itemize}
                \item Architecture: Create a simple feedforward neural network:
                \begin{itemize}
                    \item Input Layer: 784 neurons
                    \item Hidden Layer: 128 neurons (with ReLU activation)
                    \item Output Layer: 10 neurons (softmax activation for multi-class classification)
                \end{itemize}
            \end{itemize}
            \begin{lstlisting}[language=Python]
# TensorFlow/Keras Example
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(784,)))
model.add(Dense(10, activation='softmax'))
            \end{lstlisting}
        
        \item \textbf{Model Training:}
            \begin{itemize}
                \item Loss Function: Categorical Crossentropy
                \item Optimizer: Adam
                \item Metrics: Accuracy
            \end{itemize}
            \begin{lstlisting}[language=Python]
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train_scaled, y_train, epochs=10, batch_size=32)
            \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation Continued}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Model Evaluation:}
            \begin{itemize}
                \item Use the testing dataset to evaluate model performance.
                \item Metrics to consider:
                \begin{itemize}
                    \item Accuracy: Percentage of correctly classified images.
                    \item Confusion Matrix: To analyze classification errors.
                \end{itemize}
            \end{itemize}
            \begin{lstlisting}[language=Python]
# Model Evaluation Example
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)
print(f'Test Accuracy: {test_accuracy:.2f}')
            \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Importance of Data:} Quality and quantity of data play crucial roles in model performance.
        \item \textbf{Model Complexity:} Balance between too simple (underfitting) and too complex (overfitting).
        \item \textbf{Real-World Implications:} Applications extend beyond digit recognition, impacting fields like autonomous driving, healthcare diagnostics, and more.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    This case study not only highlights how neural networks can effectively handle image classification tasks but also showcases the systematic approach to implementation—from data preparation to model evaluation—emphasizing the importance of each step in building successful AI applications.
    
    \begin{block}{}
        Feel free to ask questions or seek clarifications on any part of this case study as we explore the powerful world of neural networks!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations}
    \begin{block}{Understanding Ethical Implications in Neural Networks}
        Neural networks, as powerful tools in supervised learning, offer significant benefits across various applications—from healthcare diagnostics to financial forecasting. However, they also raise critical ethical concerns that must be considered.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations}
    \begin{enumerate}
        \item \textbf{Bias in Training Data}
            \begin{itemize} 
                \item \textit{Definition}: Bias occurs when a neural network learns patterns that reflect societal prejudices, often embedded in the training data.
                \item \textit{Example}: A facial recognition system trained predominantly on images of individuals from one ethnic group may perform poorly on others, leading to misidentifications and reinforcing inequalities.
            \end{itemize}
        
        \item \textbf{Transparency and Explainability}
            \begin{itemize} 
                \item \textit{Definition}: Neural networks often function as "black boxes," making it difficult to understand how decisions are made.
                \item \textit{Example}: In a hiring process, if a neural network recommends candidates without explaining its decision logic, it can lead to distrust and frustration among applicants.
            \end{itemize}
    
        \item \textbf{Data Privacy}
            \begin{itemize} 
                \item \textit{Definition}: The collection of vast amounts of personal data for training poses risks to individual privacy.
                \item \textit{Example}: Using health records to train predictive models might expose sensitive information if proper safeguards aren't implemented.
            \end{itemize}
    
        \item \textbf{Accountability}
            \begin{itemize} 
                \item \textit{Definition}: Determining who is responsible when a neural network causes harm (e.g., wrongful arrests due to flawed predictive policing).
                \item \textit{Example}: If a self-driving car accident occurs, who is liable? The developers, the manufacturers, or the data providers?
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Addressing Ethical Concerns}
    \begin{itemize}
        \item \textbf{Bias Mitigation}: Implement strategies such as diverse training datasets and regular audits of models to identify and address bias.
        \item \textbf{Promoting Transparency}: Utilizing model interpretability tools (e.g., LIME, SHAP) can help explain predictions, fostering trust and understanding.
        \item \textbf{Ensuring Ethical Data Use}: Adopting data minimization principles and obtaining informed consent protects user privacy and builds goodwill.
        \item \textbf{Establishing Accountability Frameworks}: Implement regulatory guidelines to clarify responsibilities in case of malfunction or misuse.
    \end{itemize}
    
    \begin{block}{Conclusion}
        While neural networks can transform industries and improve decision-making, it is crucial for developers, researchers, and policymakers to prioritize ethical considerations, ensuring a fair and just society.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{itemize}
        \item Neural networks have transformed various fields, such as image recognition and natural language processing.
        \item Effective application and responsible AI development require understanding the challenges and limitations in training and deploying these models.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges}
    \begin{enumerate}
        \item \textbf{Overfitting}
            \begin{itemize}
                \item \textit{Explanation:} Capturing noise and irrelevant details leads to poor generalization to unseen data.
                \item \textit{Example:} A model performs well on known images but fails with new variations.
                \item \textit{Key Points:} 
                    \begin{itemize}
                        \item Use cross-validation.
                        \item Techniques: Regularization (L1, L2) and dropout.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Data Requirements}
            \begin{itemize}
                \item \textit{Explanation:} Large amounts of high-quality structured data are required for effective training.
                \item \textit{Example:} Thousands of labeled images needed for image classification models.
                \item \textit{Key Points:} 
                    \begin{itemize}
                        \item Collecting quality data is resource-intensive.
                        \item Data augmentation can be beneficial for limited datasets.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges (Continued)}
    \begin{enumerate}[resume]
        \item \textbf{Computational Resources}
            \begin{itemize}
                \item \textit{Explanation:} Substantial computational power is required for training complex neural networks.
                \item \textit{Example:} Training on a standard CPU may take weeks; a GPU reduces this to days.
                \item \textit{Key Points:}
                    \begin{itemize}
                        \item Investigate cloud-based solutions.
                        \item Manage model complexity to optimize resource usage.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Interpretability}
            \begin{itemize}
                \item \textit{Explanation:} Neural networks are often "black boxes," making it hard to understand their decisions.
                \item \textit{Example:} Critical to understand model's reasoning in medical diagnosis systems.
                \item \textit{Key Points:}
                    \begin{itemize}
                        \item Techniques like SHAP and LIME can improve interpretability.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Bias and Ethical Concerns}
            \begin{itemize}
                \item \textit{Explanation:} Models can learn and propagate biases from training data.
                \item \textit{Example:} Facial recognition systems may underperform on underrepresented demographics.
                \item \textit{Key Points:}
                    \begin{itemize}
                        \item Regular audits for bias in models.
                        \item Use fairness-aware algorithms and guidelines.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formulas}
    \begin{block}{Loss Function}
        \[
        L(y, \hat{y}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y_i}) + (1 - y_i) \log(1 - \hat{y_i}) \right]
        \]
        where:
        \begin{itemize}
            \item $y$: actual output
            \item $\hat{y}$: predicted output
            \item $N$: number of samples
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Understanding challenges and limitations is essential for sound AI development.
        \item Addressing overfitting, data quality, and bias enhances reliability and applicability of neural networks.
        \item Improving interpretability fosters trust and accountability in AI applications.
    \end{itemize}
    \begin{block}{Visual Suggestions}
        \item Consider using visuals such as charts and diagrams for enhanced understanding of topics discussed.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks}
    \begin{block}{Overview}
        Explore the emerging trends and future directions of neural network research and applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Part 1}
    \begin{itemize}
        \item \textbf{Explainable AI (XAI)}
        \begin{itemize}
            \item Enhances transparency of AI decision-making processes.
            \item Example: LIME provides insights into model predictions.
            \item Importance in sensitive fields like healthcare and finance.
        \end{itemize}

        \item \textbf{Self-Supervised Learning}
        \begin{itemize}
            \item Models learn from unlabeled data, generating their own labels.
            \item Example: Contrastive learning distinguishes between data pairs.
            \item Leads to better transfer learning across tasks.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Part 2}
    \begin{itemize}
        \item \textbf{Neuromorphic Computing}
        \begin{itemize}
            \item Mimics human brain processing with efficient hardware.
            \item Example: IBM's TrueNorth chip consumes less power than GPUs.
            \item Impact on real-time processing in robotics and IoT.
        \end{itemize}

        \item \textbf{Federated Learning}
        \begin{itemize}
            \item Trains models on-device while keeping data localized.
            \item Example: Google’s keyboard prediction learns from user patterns.
            \item Enhances privacy and security during model training.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Part 3}
    \begin{itemize}
        \item \textbf{Transfer Learning and Domain Adaptation}
        \begin{itemize}
            \item Pre-trained models fine-tuned for specific tasks.
            \item Example: Adapting ImageNet-trained models for medical imaging.
            \item Reduces training time and resource requirement.
        \end{itemize}

        \item \textbf{Integration with Quantum Computing}
        \begin{itemize}
            \item Combines neural networks with quantum computing for faster computation.
            \item Potential uses in optimization and simulation tasks.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Landscape of neural networks is evolving rapidly.
        \item Emerging trends will shape AI systems and ethical considerations.
        \item Staying informed is crucial for advancing in AI and machine learning.
    \end{itemize}
    \begin{block}{Conclusion}
        The future of neural networks is marked by significant innovations leading to powerful and responsible AI applications.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Collaborative Project Overview}
    \begin{block}{Objectives of the Project}
        The purpose of this collaborative project is to apply the concepts learned about neural networks in a practical setting. 
        Students will enhance their understanding of supervised learning techniques, particularly neural network architectures and their applications.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Expectations}
    \begin{enumerate}
        \item \textbf{Team Collaboration:}
        \begin{itemize}
            \item Teams of 3-5 members should ensure active contribution from all members.
            \item Regular meetings to discuss milestones and distribute tasks effectively are recommended.
        \end{itemize}
        
        \item \textbf{Project Proposal:}
        \begin{itemize}
            \item Outline the problem statement or research question to address.
            \item Specify the chosen dataset and rationale for its selection.
            \item Describe the neural network architecture to be used (e.g., Feedforward Networks, CNN).
        \end{itemize}

        \item \textbf{Implementation:}
        \begin{itemize}
            \item Train the neural network model with the selected dataset.
            \item Experiment with at least two configurations, documenting the training process.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deliverables and Key Points}
    \begin{block}{Deliverables}
        \begin{enumerate}
            \item \textbf{Project Report:}
            \begin{itemize}
                \item Introduction, Literature Review, Methodology, Results, Conclusion.
            \end{itemize}
            \item \textbf{Presentation:}
            \begin{itemize}
                \item 10-minute overview including key findings with visual aids.
                \item Demonstration of model performance using relevant metrics.
            \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Importance of collaboration and communication.
            \item Maintain clarity and organization in all deliverables.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Takeaways}
    
    \begin{enumerate}
        \item \textbf{Understanding Neural Networks}
        \item \textbf{Architecture of Neural Networks}
        \item \textbf{Activation Functions}
        \item \textbf{Training Neural Networks}
        \item \textbf{Overfitting and Regularization}
        \item \textbf{Applications of Neural Networks}
        \item \textbf{Collaborative Project Insights}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Detailed Insights}
    
    \begin{itemize}
        \item \textbf{Understanding Neural Networks:}
            \begin{itemize}
                \item Inspired by the human brain.
                \item Composed of layers of interconnected neurons.
            \end{itemize}
        
        \item \textbf{Architecture:}
            \begin{itemize}
                \item Input Layer, Hidden Layers, Output Layer.
            \end{itemize}

        \item \textbf{Activation Functions:}
            \begin{itemize}
                \item Sigmoid for binary classification.
                \item ReLU for faster training.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Discussion Points}
    
    \begin{itemize}
        \item \textbf{Training Neural Networks:}
            \begin{itemize}
                \item Forward Propagation, Loss Function, Backpropagation.
            \end{itemize}
        
        \item \textbf{Overfitting and Regularization:}
            \begin{itemize}
                \item Techniques: Dropout, L2 Regularization.
            \end{itemize}
        
        \item \textbf{Applications:}
            \begin{itemize}
                \item Used in image recognition, NLP, and game AI.
            \end{itemize}
            
        \item \textbf{Questions and Discussion:}
            \begin{itemize}
                \item Clarifications on challenging concepts.
                \item Practical applications of neural networks.
            \end{itemize}
    \end{itemize}
\end{frame}


\end{document}