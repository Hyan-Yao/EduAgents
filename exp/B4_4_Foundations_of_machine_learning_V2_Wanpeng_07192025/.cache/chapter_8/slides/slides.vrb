\frametitle{Activation Functions - Common Types}
    \begin{block}{Common Activation Functions}
        \begin{enumerate}
            \item \textbf{Sigmoid Function}
            \begin{itemize}
                \item \textbf{Formula:} \( f(x) = \frac{1}{1 + e^{-x}} \)
                \item \textbf{Range:} (0, 1)
                \item \textbf{Pros:} Smooth gradient, easy to predict probabilities.
                \item \textbf{Cons:} Saturation problem - gradients near 0 for extreme inputs.
                \item \textbf{Example:} Used in binary classification (e.g., spam vs. not spam).
            \end{itemize}

            \item \textbf{Tanh Function (Hyperbolic Tangent)}
            \begin{itemize}
                \item \textbf{Formula:} \( f(x) = \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \)
                \item \textbf{Range:} (-1, 1)
                \item \textbf{Pros:} Zero-centered output leads to faster convergence.
                \item \textbf{Cons:} Still suffers from saturation.
                \item \textbf{Example:} Useful in multi-class classification problems.
            \end{itemize}

            \item \textbf{ReLU (Rectified Linear Unit)}
            \begin{itemize}
                \item \textbf{Formula:} \( f(x) = \max(0, x) \)
                \item \textbf{Range:} [0, âˆž)
                \item \textbf{Pros:} Efficient computation, mitigates the vanishing gradient problem.
                \item \textbf{Cons:} Can lead to the dying ReLU problem (inactive neurons).
                \item \textbf{Example:} Widely used in hidden layers of neural networks.
            \end{itemize}
        \end{enumerate}
    \end{block}
