\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{What is Data Preprocessing?}
        Data preprocessing is the process of cleaning and transforming raw data into a format suitable for analysis and modeling in machine learning. It encompasses various techniques to improve data quality and effectiveness.
    \end{block}
    \begin{block}{Significance in the Machine Learning Pipeline}
        The quality of input data directly impacts the performance and accuracy of machine learning models. Effective preprocessing reduces misleading results and enhances model efficiency, serving as a foundational step in the machine learning pipeline.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Data Cleaning}
        \item \textbf{Data Transformation}
        \item \textbf{Feature Selection}
        \item \textbf{Encoding Categorical Variables}
        \item \textbf{Data Splitting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Data Preprocessing - Details}
    \begin{itemize}
        \item \textbf{Data Cleaning:}
            \begin{itemize}
                \item Handling missing values through strategies like imputation or removal.
                \item \textit{Example:} Replacing missing student scores with the average score.
            \end{itemize}
        \item \textbf{Data Transformation:}
            \begin{itemize}
                \item Normalization (Min-Max Scaling) and Standardization.
                \item \textit{Formulas:}
                  \begin{equation}
                        x' = \frac{x - \min(X)}{\max(X) - \min(X)}
                  \end{equation}
                  \begin{equation}
                        z = \frac{x - \mu}{\sigma}  \quad (\mu = \text{mean}, \sigma = \text{std deviation})
                  \end{equation}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Data Preprocessing - More Details}
    \begin{itemize}
        \item \textbf{Feature Selection:}
            \begin{itemize}
                \item Choosing relevant features to reduce complexity.
                \item Techniques include correlation matrix and recursive feature elimination.
                \item \textit{Example:} Removing irrelevant features like owner's name in house price predictions.
            \end{itemize}
        \item \textbf{Encoding Categorical Variables:}
            \begin{itemize}
                \item Converting categories to numerical format.
                \item Techniques include label encoding and one-hot encoding.
                \item \textit{Example:} Converting 'Color' categories into binary columns.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Data Preprocessing - Final Points}
    \begin{itemize}
        \item \textbf{Data Splitting:}
            \begin{itemize}
                \item Dividing the dataset into training and test sets, commonly with an 80/20 split.
            \end{itemize}
        \item \textbf{Key Points to Remember:}
            \begin{itemize}
                \item Data preprocessing is essential for high accuracy.
                \item Data quality impacts model performance significantly.
                \item Common steps include cleaning, transforming, selecting features, encoding, and splitting data.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Preprocessing}
    \begin{block}{Introduction to Data Preprocessing}
        Data preprocessing is the essential step in data analysis and machine learning, which prepares raw data for further analysis. It ensures that the data is in the right format and quality, allowing machine learning models to generate reliable and high-quality predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality}
    \begin{block}{Why Data Quality Matters}
        Data quality directly impacts your model’s accuracy and effectiveness. High-quality data means fewer errors and more accurate results, leading to better decision-making. Key aspects of data quality include:
    \end{block}
    
    \begin{itemize}
        \item \textbf{Completeness}: All required data should be present.
        \item \textbf{Consistency}: Data should be uniform across sources and formats.
        \item \textbf{Accuracy}: Data must be free from errors and truthful representations.
        \item \textbf{Timeliness}: Data should be up-to-date.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Enhancements Through Preprocessing}
    \begin{block}{Key Enhancements}
        \begin{itemize}
            \item \textbf{Handling Missing Values:} Impute missing values to avoid bias.
            \item \textbf{Normalization:}
                \begin{equation}
                X' = \frac{X - X_{min}}{X_{max} - X_{min}} 
                \end{equation}
                Scales features to improve model convergence.
            \item \textbf{Standardization:} Centers data
                \begin{equation}
                X' = \frac{X - \mu}{\sigma}
                \end{equation}
                where $\mu$ is the mean and $\sigma$ is the standard deviation.
            \item \textbf{Outlier Detection and Treatment:} Adjust outliers to prevent distortion in models.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion – Enhancing Predictive Power}
        A robust preprocessing strategy improves model learning and generalization capabilities. Techniques like cleaning, normalization, and outlier handling pave the way for higher accuracy and better performance.
    \end{block}
    
    \begin{itemize}
        \item Effective data preprocessing is crucial for accurate predictions.
        \item Addressing missing values, scaling features, and mitigating outliers are vital steps.
        \item Enhanced data quality leads to better models and insights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Data Preprocessing Techniques}
    \begin{block}{Introduction}
        Data preprocessing is a critical step in the data analysis pipeline that prepares raw data for modeling. This slide provides an overview of essential preprocessing techniques: normalization, standardization, and data cleaning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization}
    \begin{itemize}
        \item Normalization is the process of scaling individual data points to a common range, typically [0, 1].
        \item Important when features have different units or scales.
    \end{itemize}
    
    \begin{block}{Example: Min-Max Scaling}
        The Min-Max scaling technique transforms each feature \( x \) using the formula:
        \begin{equation}
            x' = \frac{x - \text{min}(X)}{\text{max}(X) - \text{min}(X)}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( x' \) is the normalized value.
            \item \( \text{min}(X) \) and \( \text{max}(X) \) are the minimum and maximum values of the feature.
        \end{itemize}
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Useful in algorithms that compute distances (e.g., k-NN).
            \item Ensures all features contribute equally to distance calculations.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Standardization}
    \begin{itemize}
        \item Standardization transforms data to have a mean of 0 and a standard deviation of 1.
        \item Crucial for algorithms that assume a Gaussian distribution of features.
    \end{itemize}
    
    \begin{block}{Example: Z-Score Transformation}
        The z-score standardization is applied as follows:
        \begin{equation}
            x' = \frac{x - \mu}{\sigma}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( x' \) is the standardized value.
            \item \( \mu \) is the mean of the feature.
            \item \( \sigma \) is the standard deviation of the feature.
        \end{itemize}
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Beneficial for models like logistic regression, SVM, and neural networks.
            \item Handles outliers better than normalization.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning}
    \begin{itemize}
        \item Data cleaning involves identifying and correcting errors or inconsistencies in the dataset.
        \begin{itemize}
            \item Handling missing values
            \item Removing duplicates
            \item Correcting inaccuracies
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Common Techniques}
        \begin{itemize}
            \item \textbf{Handling Missing Values:} Techniques include imputation (mean, median, mode) or removing rows/columns.
            \item \textbf{Removing Duplicates:} Identify duplicate records and retain only unique instances.
            \item \textbf{Correcting Inaccuracies:} Validating data entries against known rules (e.g., postal codes) to ensure accuracy.
        \end{itemize}
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item High-quality data directly influences model performance and reliability.
            \item Continuous data cleaning is essential, especially in real-time applications.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Understanding and effectively applying normalization, standardization, and data cleaning ensures that the data is both high-quality and suitable for analysis.
        \item Preprocessing transforms raw data into a structured format that models can efficiently process, leading to improved accuracy and reliability in predictions.
    \end{itemize}
    
    \begin{block}{Next Step}
        In the following slide, we will delve deeper into normalization techniques, specifically focusing on Min-Max scaling and its implementation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization - Overview}
    \begin{block}{What is Normalization?}
        Normalization is the process of scaling individual data points to fall within a specific range, often [0, 1]. 
        This technique is crucial when features have different units or varying ranges to ensure that no single feature dominates the learning algorithm.
    \end{block}

    \begin{block}{Why Normalize Data?}
        \begin{itemize}
            \item \textbf{Uniform Scale:} Sensitive algorithms benefit from scaled input.
            \item \textbf{Improved Convergence:} Faster convergence rates with features on a similar scale.
            \item \textbf{Interpretability:} Models yield more meaningful outputs with normalized inputs.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques - Min-Max Scaling}
    \begin{block}{Min-Max Scaling}
        Min-Max scaling transforms features to a fixed range, typically [0, 1].
        
        \begin{equation}
        X' = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
        \end{equation}

        Where:
        \begin{itemize}
            \item \(X'\) is the normalized value.
            \item \(X\) is the original value.
            \item \(X_{\min}\) and \(X_{\max}\) are the minimum and maximum values of the feature.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        For “Age” with values \([20, 30, 50]\):
        \begin{itemize}
            \item \(X_{\min} = 20\), \(X_{\max} = 50\)
            \item Normalized Ages:
            \begin{itemize}
                \item Age 20: \(0\)
                \item Age 30: \(0.33\)
                \item Age 50: \(1\)
            \end{itemize}
            \item Result: Normalized ages are \([0, 0.33, 1]\).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Apply Normalization}
    \begin{block}{When to Apply Normalization}
        Normalization is useful in the following scenarios:
        \begin{itemize}
            \item \textbf{Input Features with Different Ranges:} Varying units (e.g., income, age).
            \item \textbf{Distance-Based Algorithms:} Essential for algorithms like KNN and SVM.
            \item \textbf{Neural Networks:} Helps achieve faster training convergence.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Rescale features to a standardized range.
            \item Min-Max scaling is simple and widely used.
            \item Use the same parameters for training and test sets.
        \end{itemize}
    \end{block}
    
    \begin{block}{Implementation Example (Python)}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler

# Sample data
data = [[20], [30], [50]]

# Initialize scaler
scaler = MinMaxScaler()

# Fit and transform data
normalized_data = scaler.fit_transform(data)
print(normalized_data)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Standardization - Overview}
    \begin{block}{Understanding Standardization}
        \begin{itemize}
            \item Definition: A data preprocessing technique transforming data to have a mean of 0 and a standard deviation of 1.
            \item Purpose: Facilitates comparison across different scales or units in statistics and machine learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Z-Score Normalization}
    \begin{block}{What is Z-Score Normalization?}
        \begin{itemize}
            \item Rescaling based on mean and standard deviation.
            \item Formula: 
            \begin{equation}
                z = \frac{x - \mu}{\sigma}
            \end{equation}
            where:
            \begin{itemize}
                \item \(z\) = z-score
                \item \(x\) = raw score
                \item \(\mu\) = mean
                \item \(\sigma\) = standard deviation
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Dataset: 70, 75, 80, 85, 90 \\
        Steps:
        \begin{enumerate}
            \item Mean (\(\mu\)): 
            \begin{equation}
                \mu = \frac{70 + 75 + 80 + 85 + 90}{5} = 80
            \end{equation}
            \item Standard Deviation (\(\sigma\)):
            \begin{equation}
                \sigma \approx 7.07
            \end{equation}
            \item Z-scores:
            \begin{itemize}
                \item For \(x = 70\): \(z \approx -1.41\)
                \item For \(x = 90\): \(z \approx 1.41\)
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relevance in Normal Distribution}
    \begin{block}{Why Standardization Matters}
        \begin{itemize}
            \item **Compare Different Datasets**: Enables fair comparisons across varying scales.
            \item **Facilitate Machine Learning**:
                \begin{itemize}
                    \item Many algorithms perform better with standardized data.
                \end{itemize}
            \item **Interpret Z-Scores**:
                \begin{itemize}
                    \item Helps identify outliers and understand data distribution.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Standardization is essential in data analysis and machine learning, improving comparisons and algorithm performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data}
    \begin{block}{Understanding Missing Data}
        Missing data occurs when no data value is stored for a variable in an observation. This can significantly impact the quality of analysis and lead to biased results. Common reasons for missing data include:
    \end{block}
    \begin{itemize}
        \item \textbf{Data Collection Issues:} Errors during entry, technology malfunctions, or survey non-responses.
        \item \textbf{Data Processing Errors:} Data corruption or loss during migration or storage.
        \item \textbf{Natural Occurrences:} Values may be genuinely absent; for example, a person may choose not to answer specific survey questions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods to Address Missing Values}
    \begin{enumerate}
        \item \textbf{Deletion Methods}
        \begin{itemize}
            \item \textbf{Listwise Deletion:} Remove any observation with missing values. Best used when missing data is minimal and random.
            \item \textbf{Pairwise Deletion:} Use available data for each pair of variables. Maintains more data points but complicates interpretations.
        \end{itemize}
        
        \item \textbf{Imputation Methods}
        \begin{itemize}
            \item \textbf{Mean/Median/Mode Imputation:} Replace missing values with the mean, median, or mode of the available data.
            \item \textbf{K-Nearest Neighbors (KNN):} Impute missing values based on the k closest observations.
            \item \textbf{Interpolation:} Estimate missing values using existing data points.
        \end{itemize}
        
        \item \textbf{Prediction Methods}
        \begin{itemize}
            \item \textbf{Regression Models:} Predict the missing value based on other variables.
            \item \textbf{Machine Learning Techniques:} Advanced methods that can learn patterns in data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion on Missing Data}
    Handling missing data is crucial for maintaining the integrity of analysis. The choice between deletion, imputation, or prediction depends on the context and nature of the data. Evaluate the following:
    \begin{itemize}
        \item The proportion of missing values
        \item The possible impact on results
    \end{itemize}
    \begin{block}{Important Note}
        Consider the type of missing data (e.g., MCAR, MAR, NMAR) to apply the most suitable method effectively.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Encoding Categorical Variables}
    \begin{block}{Introduction to Categorical Variables}
        Categorical variables represent categories or groups. 
        They are qualitative data and cannot be used directly in most machine learning algorithms.
        Thus, we need to convert them into a numerical format through encoding.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Common Techniques for Encoding}
    \begin{enumerate}
        \item \textbf{Label Encoding}
        \item \textbf{One-Hot Encoding}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Label Encoding}
    \begin{block}{Definition}
        Converts each category into a unique integer. 
    \end{block}
    \begin{block}{Use Case}
        Best suited for ordinal categorical variables with a natural order.
    \end{block}
    \begin{block}{Example}
        \begin{itemize}
            \item Categories of "Size": Small, Medium, Large.
            \item Encoding: Small → 0, Medium → 1, Large → 2.
        \end{itemize}
    \end{block}
    \begin{block}{Implementation (Python)}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import LabelEncoder

sizes = ['Small', 'Medium', 'Large']
label_encoder = LabelEncoder()
sizes_encoded = label_encoder.fit_transform(sizes)
print(sizes_encoded)  # Output: [0 1 2]
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{One-Hot Encoding}
    \begin{block}{Definition}
        Creates binary columns for each category, 
        where each column indicates the presence (1) or absence (0) of a category.
    \end{block}
    \begin{block}{Use Case}
        Ideal for nominal categorical variables with no order.
    \end{block}
    \begin{block}{Example}
        \begin{itemize}
            \item For "Size": 
            \begin{itemize}
                \item Size\_Small → 1 if Small, else 0 
                \item Size\_Medium → 1 if Medium, else 0 
                \item Size\_Large → 1 if Large, else 0
            \end{itemize}
            \item One-hot encoded representation:
            \[
            \begin{array}{c|c|c}
            \text{Size\_Small} & \text{Size\_Medium} & \text{Size\_Large} \\
            \hline
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1 \\
            \end{array}
            \]
        \end{itemize}
    \end{block}
    \begin{block}{Implementation (Python)}
        \begin{lstlisting}[language=Python]
import pandas as pd

sizes = pd.Series(['Small', 'Medium', 'Large'])
one_hot_encoded = pd.get_dummies(sizes, prefix='Size')
print(one_hot_encoded)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Why Encoding Matters:} 
        Many machine learning algorithms require numeric input.
        Proper encoding allows models to effectively learn from data.
        
        \item \textbf{Choosing the Right Technique:} 
        \begin{itemize}
            \item Use \textbf{label encoding} for ordinal variables (e.g., ratings, sizes).
            \item Use \textbf{one-hot encoding} for nominal variables (e.g., colors, types).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        Encoding categorical variables is crucial for data preprocessing. 
        Choosing the correct technique influences the performance of predictive models.
    \end{block}
    \begin{block}{Next Steps}
        In the following slide, we will discuss feature extraction, 
        which further enhances model performance by reducing dimensionality and improving interpretability.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Feature Extraction}
    \begin{block}{Introduction}
        Feature extraction is a crucial step in the data preprocessing pipeline that transforms raw data into a set of features for machine learning algorithms. It is key for handling high-dimensional data and reduces dimensionality, leading to improved model performance.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Importance of Feature Extraction}
    \begin{itemize}
        \item \textbf{Dimensionality Reduction}: Simplifies the model without significant information loss.
        \item \textbf{Improved Performance}: Models trained on relevant features yield better predictive performance.
        \item \textbf{Reduced Overfitting}: Fewer features decrease model complexity, minimizing overfitting risk.
        \item \textbf{Enhanced Interpretability}: A lower number of features makes models easier to interpret.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Feature Extraction Methods}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA)}
            \begin{itemize}
                \item Captures maximum variance using orthogonal variables.
                \item Key Formula: 
                \begin{equation}
                    Z = XW
                \end{equation}
                \item Example: Image compression retains the most significant features.
            \end{itemize}
        
        \item \textbf{Linear Discriminant Analysis (LDA)}
            \begin{itemize}
                \item Separates classes using a linear combination of features.
                \item Key Formula: 
                \begin{equation}
                    w = S_W^{-1}(m_1 - m_2)
                \end{equation}
                \item Example: Classifying flower species based on sepal and petal measurements.
            \end{itemize}
        
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
            \begin{itemize}
                \item Non-linear technique for visualizing high-dimensional data.
                \item Example: Visualizing customer purchase clusters in retail datasets.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example - PCA}
    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standardizing the data
data = # your dataset
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Applying PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(scaled_data)

# Creating a DataFrame with the PCA results
import pandas as pd
pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{block}{Key Takeaways}
        Feature extraction is essential in machine learning for efficient modeling and analysis of complex data. Understanding how to properly extract and utilize features is crucial for success in data science.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Overview}
    \begin{block}{Overview}
        Feature selection is a critical step in the data preprocessing phase of machine learning that involves selecting the most relevant features from the dataset to improve model performance and reduce overfitting. By discarding irrelevant or redundant features, we can build simpler models that are computationally efficient.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Techniques}
    \begin{block}{Techniques for Feature Selection}
        There are three primary methods for feature selection:
        \begin{itemize}
            \item Filter Methods
            \item Wrapper Methods
            \item Embedded Methods
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Filter Methods}
    \begin{block}{1. Filter Methods}
        \begin{itemize}
            \item \textbf{Definition:} Evaluate relevance by intrinsic properties, independent of machine learning algorithms using statistical techniques.
            \item \textbf{Common Techniques:}
            \begin{itemize}
                \item \textbf{Correlation Coefficient:} Measures linear relationships (e.g., Pearson’s correlation).
                \item \textbf{Chi-Squared Test:} Tests independence for categorical variables.
            \end{itemize}
        \end{itemize}
        \textbf{Example:} Strong correlation between house size and price leads to selecting the "size" feature.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Wrapper Methods}
    \begin{block}{2. Wrapper Methods}
        \begin{itemize}
            \item \textbf{Definition:} Evaluate subsets of features using a predictive model based on model accuracy.
            \item \textbf{Common Techniques:}
            \begin{itemize}
                \item \textbf{Recursive Feature Elimination (RFE):} Iteratively removes the least important features until optimal subset is reached.
                \item \textbf{Forward Selection:} Starts with no features, adding one by one based on model performance.
                \item \textbf{Backward Elimination:} Starts with all features, removing them one by one.
            \end{itemize}
        \end{itemize}
        \textbf{Example:} RFE may eliminate features like "garden size" to improve accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Embedded Methods}
    \begin{block}{3. Embedded Methods}
        \begin{itemize}
            \item \textbf{Definition:} Feature selection is part of the model training process.
            \item \textbf{Common Techniques:}
            \begin{itemize}
                \item \textbf{Lasso Regularization (L1):} Forces less important coefficients to zero.
                \item \textbf{Decision Trees:} Provide importance scores for feature selection.
            \end{itemize}
        \end{itemize}
        \textbf{Example:} Lasso Regression may select only highly predictive variables from an initial set.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Key Points and Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Effective feature selection enhances model performance.
            \item Trade-offs: Filter methods are fast but may ignore interactions; wrapper methods are accurate but computationally expensive; embedded methods balance both.
        \end{itemize}
    \end{block}
    
    \begin{block}{Summary}
        Choosing an appropriate feature selection method depends on the dataset, model complexity, and computational resources. Proper application can significantly improve the performance of machine learning models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Code Example}
    \begin{block}{Code Snippet Example}
        \begin{lstlisting}[language=Python]
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Assume X is your feature set and y is your target variable
model = LogisticRegression()
selector = RFE(model, n_features_to_select=3)  # Select top 3 features
selector = selector.fit(X, y)

selected_features = X.columns[selector.support_]
print("Selected Features: ", selected_features)
        \end{lstlisting}
    \end{block}
    \textbf{Note:} Incorporating feature selection methods leads to better performance and interpretability of models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Feature Engineering}
    \begin{block}{Overview}
        Feature engineering involves creating new input features from existing data using domain knowledge to enhance model performance. This process includes selecting, modifying, or constructing features that help machine learning algorithms better understand data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Feature Engineering}
    \begin{enumerate}
        \item \textbf{Model Performance}:
            Well-engineered features can significantly improve the accuracy of models, often more than the choice of algorithm itself.
        \item \textbf{Reducing Overfitting}:
            Meaningful features help the model generalize to unseen data, reducing the risk of overfitting.
        \item \textbf{Interpretability}:
            Features provide insights into the problem domain, making models easier to interpret.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering Techniques}
    \begin{itemize}
        \item \textbf{Transformation}:
            \begin{itemize}
                \item \textbf{Normalization}: Scaling features to a range using the formula:
                \[
                x' = \frac{x - \text{min}(X)}{\text{max}(X) - \text{min}(X)}
                \]
                \item \textbf{Log Transformation}: Converting data to a more normal distribution.
            \end{itemize}
        \item \textbf{Creation}:
            \begin{itemize}
                \item \textbf{Polynomial Features}: Generating squares or interactions (e.g., \(x_1 \times x_2\)).
                \item \textbf{Datetime Features}: Extracting day, month, or year from timestamps.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Feature Engineering - Overview}
    \begin{block}{Introduction}
        Feature engineering is a critical step in the machine learning pipeline, enabling the transformation of raw data into meaningful features that enhance model performance. 
    \end{block}
    \begin{itemize}
        \item Focus on two common techniques: 
        \begin{itemize}
            \item Polynomial Features
            \item Interaction Terms
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Feature Engineering - Polynomial Features}
    \begin{block}{Definition}
        Polynomial features help capture non-linear relationships between features by generating new features from polynomial expressions.
    \end{block}
    
    \begin{block}{Example}
        Given a feature \( x \):
        \begin{itemize}
            \item Original: \( x = [1, 2, 3] \)
            \item Polynomial Features (degree 2): \( [1^2, 2^2, 3^2] = [1, 4, 9] \)
        \end{itemize}
        By incorporating polynomial features, we can better model relationships, e.g., house sizes predicting prices with \( \text{size}^2 \).
    \end{block}
    
    \begin{block}{Implementation in Python}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import PolynomialFeatures

# Example feature array
X = [[2], [3], [5]]
poly = PolynomialFeatures(degree=2)
poly_features = poly.fit_transform(X)
print(poly_features)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Feature Engineering - Interaction Terms}
    \begin{block}{Definition}
        Interaction terms capture the combined effect of two or more features on the target variable, which may not be evident when features are considered separately.
    \end{block}
    
    \begin{block}{Example}
        For features \( x_1 \) and \( x_2 \):
        \begin{itemize}
            \item Interaction term: \( x_1 \times x_2 \)
            \item Example: Ads spending and sales level where interaction can inform how spending impacts sales.
        \end{itemize}
    \end{block}

    \begin{block}{Implementation in Python}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import PolynomialFeatures

# Example feature array
X = [[1, 2], [2, 3], [3, 1]]
poly = PolynomialFeatures(interaction_only=True, include_bias=False)
interaction_features = poly.fit_transform(X)
print(interaction_features)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Non-linearity:} Polynomial features help model non-linear relationships.
        \item \textbf{Interactions:} Understanding how features work together can significantly enhance model accuracy.
        \item \textbf{Use with Care:} Overfitting can occur with too many features, making validation essential.
    \end{itemize}
    By mastering these techniques, you can improve your models' predictive performance across various tasks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Application of Preprocessing}
    \begin{block}{Introduction to Data Preprocessing}
        Data preprocessing and feature engineering are critical steps in the machine learning pipeline. They involve transforming raw data into a format suitable for modeling, enhancing model performance, and enabling accurate predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Predicting House Prices}
    \begin{itemize}
        \item \textbf{Scenario}: A real estate company wants to predict house prices based on historical sales data.
        \item \textbf{Challenges}:
        \begin{itemize}
            \item Missing values for important features (e.g., square footage, number of bedrooms).
            \item Outliers such as extremely high or low prices.
        \end{itemize}
        \item \textbf{Preprocessing Steps}:
        \begin{itemize}
            \item Imputation of missing values using the median price of similar houses.
            \item Normalization through Min-Max scaling.
        \end{itemize}
        \item \textbf{Outcome}: Improved model accuracy by 25\%.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Customer Churn Prediction}
    \begin{itemize}
        \item \textbf{Scenario}: A telecommunications company aims to predict customer churn based on user data.
        \item \textbf{Challenges}:
        \begin{itemize}
            \item Categorical variables that need conversion to numerical formats.
            \item Over 50 features leading to risk of overfitting.
        \end{itemize}
        \item \textbf{Preprocessing Steps}:
        \begin{itemize}
            \item One-Hot Encoding for categorical variables.
            \item Feature engineering for new insights (e.g., average monthly spend).
            \item Dimensionality reduction using PCA from 50 to 10 features.
        \end{itemize}
        \item \textbf{Outcome}: Reduced model complexity while maintaining predictive performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Impacts}
    \begin{itemize}
        \item \textbf{Improved Model Accuracy}: Proper preprocessing enhances model performance by addressing issues like missing values and outliers.
        \item \textbf{Feature Engineering Importance}: Designed features provide greater insights.
        \item \textbf{Real-Life Impact}: Effective preprocessing facilitates better business decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula and Example: Feature Scaling}
    \begin{block}{Min-Max Scaling Formula}
        \begin{equation}
            X' = \frac{X - X_{min}}{X_{max} - X_{min}}
        \end{equation}
    \end{block}
    \begin{example}
        For a feature with a minimum value of 10 and a maximum of 100, a raw value of 55 would be scaled as follows:
        \begin{equation}
            X' = \frac{55 - 10}{100 - 10} = \frac{45}{90} = 0.5
        \end{equation}
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Handling Missing Values in Python}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load dataset
data = pd.read_csv('housing_data.csv')

# Impute missing values with median
data['square_footage'].fillna(data['square_footage'].median(), inplace=True)
    \end{lstlisting}
    \begin{block}{Conclusion}
        Understanding and applying data preprocessing techniques can transform complex data into actionable insights, crucial for solving real-world machine learning challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Preprocessing}
    \begin{block}{Introduction to Ethical Considerations}
        Data preprocessing is a crucial step in the machine learning pipeline. 
        It involves preparing raw data for modeling by cleaning, transforming, and selecting appropriate features.
        Decisions during preprocessing can significantly affect model performance and introduce ethical implications related to bias.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Preprocessing Decisions on Model Bias}
    \begin{itemize}
        \item \textbf{Definition of Bias}:
        \begin{itemize}
            \item Bias occurs when predictions systematically favor one group over another, due to unbalanced training data or preprocessing techniques.
        \end{itemize}
        
        \item \textbf{Sources of Bias from Preprocessing}:
        \begin{itemize}
            \item \textbf{Under-sampling Majority Class}:
            \begin{itemize}
                \item Removing instances from the majority class may lead to loss of important information.
                \item \textit{Example}: Excluding approved applications in a credit scoring dataset neglects factors leading to denials.
            \end{itemize}

            \item \textbf{Feature Selection and Removal}:
            \begin{itemize}
                \item Omitting features correlated with sensitive attributes can perpetuate underlying biases.
                \item \textit{Example}: Removing 'zip code' from a housing model may affect socio-economic variables.
            \end{itemize}

            \item \textbf{Data Imputation Methods}:
            \begin{itemize}
                \item Different strategies for missing values can introduce/ amplify bias (e.g., mean imputation).
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications}
    \begin{itemize}
        \item \textbf{Fairness}:
        \begin{itemize}
            \item Ensure model predictions do not unfairly disadvantage any group.
            \item Use fairness-aware models to help mitigate bias.
        \end{itemize}

        \item \textbf{Transparency}:
        \begin{itemize}
            \item Document preprocessing steps for stakeholder understanding.
            \item Implement data governance frameworks.
        \end{itemize}

        \item \textbf{Accountability}:
        \begin{itemize}
            \item Individuals must take responsibility for preprocessing decisions.
            \item Establish protocols for reviewing these choices.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Preprocessing Decisions Matter}:
        \begin{itemize}
            \item Decision impacts model fairness and effectiveness.
        \end{itemize}

        \item \textbf{Data Diversity}:
        \begin{itemize}
            \item A diverse dataset minimizes bias risks.
        \end{itemize}

        \item \textbf{Ethical Frameworks}:
        \begin{itemize}
            \item Implement frameworks prioritizing fairness, accountability, and transparency.
        \end{itemize}
    \end{itemize}

    \begin{block}{Conclusion}
        The intersection of data preprocessing and ethics is crucial in developing fair and unbiased machine learning models. Conscious analysis of preprocessing techniques helps build ethical systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Handling Missing Values}
    \begin{lstlisting}[language=Python]
from sklearn.impute import SimpleImputer

# Using median to handle missing values
imputer = SimpleImputer(strategy='median')
data_imputed = imputer.fit_transform(data)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 1}
    
    \begin{block}{Importance of Data Preprocessing}
        \begin{enumerate}
            \item \textbf{Definition:} 
            Data preprocessing is the process of cleaning and transforming raw data into a usable format for analysis and modeling. It is crucial for ensuring accuracy and efficiency in machine learning models.
            
            \item \textbf{Why It's Important:}
                \begin{itemize}
                    \item \textbf{Quality of Data:} High-quality data leads to more reliable models. Incomplete or noisy data can mislead conclusions.
                    \item \textbf{Time Efficiency:} Effective preprocessing can significantly reduce the time spent on training models by ensuring only relevant features are used.
                \end{itemize}
            
            \item \textbf{Common Preprocessing Steps:}
                \begin{itemize}
                    \item Handling Missing Values (e.g., imputation, removal)
                    \item Scaling and Normalization (e.g., standardization)
                    \item Encoding Categorical Variables (e.g., one-hot encoding)
                \end{itemize}
        \end{enumerate}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 2}
    
    \begin{block}{Role of Feature Engineering}
        \begin{enumerate}
            \item \textbf{Definition:} 
            Feature engineering involves creating new features or modifying existing ones to improve model performance.
            
            \item \textbf{Why It Matters:}
                \begin{itemize}
                    \item \textbf{Increased Model Performance:} Original features may not capture important patterns. Derived features can reveal complex relationships.
                    \item \textbf{Enhanced Interpretability:} Well-constructed features can improve the understanding and explanations of model predictions.
                \end{itemize}
            
            \item \textbf{Techniques:}
                \begin{itemize}
                    \item Polynomial Features (e.g., interaction features)
                    \item Domain-Specific Features (e.g., seasonal indexes in sales forecasting)
                \end{itemize}
        \end{enumerate}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 3}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item \textbf{Data Quality Affects Outcomes:} Always prioritize data quality to avoid bias and enhance model performance.
            \item \textbf{Iterative Process:} Data preprocessing and feature engineering should be revisited throughout the model lifecycle for continuous improvement.
            \item \textbf{Ethical Implications:} Consider the ethical aspects of preprocessing as they can introduce biases that affect model results.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Data preprocessing and feature engineering are foundational steps in the machine learning process that directly impact the effectiveness and interpretability of the final model. Prioritizing these steps not only leads to better models but also ensures ethical modeling practices.
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Further Reading and Resources}
    \begin{block}{Overview}
        To deepen your understanding of data preprocessing and feature engineering, explore the following resources:
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Books}
    \begin{enumerate}
        \item \textbf{"Data Science from Scratch" by Joel Grus}
            \begin{itemize}
                \item Covers basics including preprocessing techniques and feature engineering.
                \item Approachable for beginners with Python examples.
            \end{itemize}
        \item \textbf{"Feature Engineering for Machine Learning" by Alice Zheng and Amanda Casari}
            \begin{itemize}
                \item A practical guide focusing on feature engineering with real datasets.
            \end{itemize}
        \item \textbf{"Python for Data Analysis" by Wes McKinney}
            \begin{itemize}
                \item Excellent introduction to data manipulation and analysis using pandas.
                \item Includes chapters on data cleaning and transformation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Online Courses and Websites}
    \begin{block}{Online Courses}
        \begin{enumerate}
            \item \textbf{Coursera: "Data Science Specialization" by Johns Hopkins University}
                \begin{itemize}
                    \item Learn about data cleaning and feature engineering as part of the workflow.
                \end{itemize}
            \item \textbf{edX: "Data Science MicroMasters" by UC San Diego}
                \begin{itemize}
                    \item Specialized course on data preparation.
                \end{itemize}
            \item \textbf{Kaggle Courses: "Data Cleaning"}
                \begin{itemize}
                    \item Free interactive course on data cleaning techniques.
                \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Websites and Blogs}
        \begin{enumerate}
            \item \textbf{Towards Data Science (Medium)}: Articles and tutorials on preprocessing and feature engineering.
            \item \textbf{KDnuggets}: Offers articles, tutorials, and resources related to data preprocessing.
            \item \textbf{DataCamp Community}: Tutorials and practical guides on data science topics.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Importance of Preprocessing:} Clean data is crucial for robust model performance.
        \item \textbf{Feature Engineering Techniques:} Explore techniques such as normalization and encoding categorical variables.
        \item \textbf{Iterative Process:} View preprocessing and feature engineering as iterative processes requiring refinement.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    Here's a simple illustration of data normalization using \texttt{sklearn}:
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler
import numpy as np

# Sample data (features)
data = np.array([[1, 2], [3, 4], [5, 6]])

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform the data
normalized_data = scaler.fit_transform(data)

print(normalized_data)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Introduction}
    \begin{block}{Introduction to Q\&A}
        This session provides an opportunity for you to clarify any doubts and deepen your understanding of data preprocessing and feature engineering concepts covered in this chapter.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Key Topics Covered}
    \begin{block}{Key Topics Covered in Chapter 2}
        \begin{enumerate}
            \item \textbf{Data Preprocessing Techniques}
            \begin{itemize}
                \item \textbf{Cleaning:} Handling missing values, outliers, and noise in the data.
                \item \textbf{Transformation:} Normalization, standardization, and encoding categorical variables.
                \item \textbf{Reduction:} Dimensionality reduction techniques like PCA.
            \end{itemize}
            \item \textbf{Feature Engineering}
            \begin{itemize}
                \item \textbf{Creation:} Generating new features from existing ones (e.g., combining date components).
                \item \textbf{Selection:} Choosing the most relevant features using techniques like correlation analysis, recursive feature elimination, or feature importance scores.
                \item \textbf{Extraction:} Utilizing methods to derive important information from raw data (e.g., text vectorization).
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Encouraged Questions and Key Points}
    \begin{block}{Encouraged Questions}
        \begin{itemize}
            \item What are the best strategies to handle missing data in a dataset?
            \item How do different normalization techniques affect model performance?
            \item Can you explain the role of feature importance in machine learning models?
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Importance of Clean Data:} Quality data is crucial for effective analysis and model training.
            \item \textbf{Iterative Nature:} Data preprocessing and feature engineering is often an iterative process, requiring multiple rounds of analysis and tweaking.
            \item \textbf{Context Matters:} The choice of preprocessing and feature engineering techniques can depend significantly on the specific dataset and the problem at hand.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Final Note and Closing Statement}
    \begin{block}{Final Note}
        Feel free to ask any questions, whether they pertain to specific techniques, tools, or broader concepts within data preprocessing and feature engineering. This session is designed to help you solidify your understanding and prepare for practical applications in data analysis and machine learning projects.
    \end{block}
    
    \begin{block}{Closing Statement}
        "Remember, there are no silly questions! Your inquiries not only aid your learning but can also benefit your peers."
    \end{block}
\end{frame}


\end{document}