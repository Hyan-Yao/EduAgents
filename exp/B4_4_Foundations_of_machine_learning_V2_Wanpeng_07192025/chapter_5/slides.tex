\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Supervised Learning: Logistic Regression]{Chapter 5: Supervised Learning: Logistic Regression}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Logistic Regression}
    \begin{itemize}
        \item Overview of logistic regression
        \item Significance in machine learning
        \item General purpose in classification tasks
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Logistic Regression}
    \begin{block}{Definition}
        Logistic Regression is a statistical method used for binary classification tasks in supervised learning. Its primary role is to predict the probability of a categorical dependent variable based on one or more independent variables.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Machine Learning}
    \begin{itemize}
        \item Logistic regression is one of the fundamental algorithms for classification tasks.
        \item It helps understand relationships between variables and makes predictions based on learned patterns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{General Purpose in Classification Tasks}
    \begin{block}{Goal}
        The goal of logistic regression is to estimate the probability that an instance belongs to a particular class.
    \end{block}
    \begin{example}
        Consider a medical diagnosis scenario where we need to predict whether a patient has a disease (Yes) or does not (No) based on features such as age, blood pressure, and cholesterol levels.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Binary Outcome:} Used when the outcome variable is binary (two possible outcomes).
        \item \textbf{Logit Function:}
        \begin{equation}
            \text{Logit}(P) = \ln\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
        \end{equation}
        where: 
        \begin{itemize}
            \item \(P\) = probability of the positive class
            \item \(X_1, X_2, \ldots, X_n\) = independent variables
            \item \(\beta_0, \beta_1, \ldots, \beta_n\) = coefficients estimated during model training.
        \end{itemize}
        \item \textbf{Sigmoid Function:} The prediction output is transformed using the sigmoid function:
        \begin{equation}
            P = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_n X_n)}}
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Applications}
    \begin{itemize}
        \item \textbf{Spam Detection:} Classifying emails as spam or not spam.
        \item \textbf{Credit Scoring:} Evaluating whether an applicant is likely to default on a loan.
        \item \textbf{Health Predictions:} Predicting whether a patient has a medical condition based on health metrics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Logistic regression is not limited to binary outcomes; it can be extended to multi-class problems using techniques like One-vs-Rest.
        \item While simple, logistic regression serves as a benchmark for more complex models, demonstrating its importance in the machine learning landscape.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning}
    Supervised learning is a type of machine learning where the model learns from labeled training data. 
    The goal is to learn a mapping from inputs to outputs that can be used to make predictions on unseen data.

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Labeled Data:} Each instance comes with an associated label (e.g., "spam" or "not spam").
            \item \textbf{Feedback Mechanism:} The model receives feedback on its predictions, allowing adjustments over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Labeled Data}
    Labeled data is essential in supervised learning because it serves as the ground truth that guides the learning process.

    \begin{block}{Importance of Labels}
        \begin{itemize}
            \item Define the expected output for the input data.
            \item Allow the model to evaluate performance and make adjustments through loss functions.
        \end{itemize}
    \end{block}

    \textbf{Example:} In a dataset of images, each image needs a label (e.g., cat, dog, horse) for appropriate learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Classification vs. Regression Tasks}
    Supervised learning can be divided into two primary categories: \textbf{classification} and \textbf{regression.}

    \begin{enumerate}
        \item \textbf{Classification:}
            \begin{itemize}
                \item \textbf{Definition:} Predicting a discrete label or category.
                \item \textbf{Examples:}
                    \begin{itemize}
                        \item Email Classification: "spam" or "not spam."
                        \item Medical Diagnosis: Categories like "healthy," "diseased," or "at risk."
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Regression:}
            \begin{itemize}
                \item \textbf{Definition:} Predicting a continuous value.
                \item \textbf{Examples:}
                    \begin{itemize}
                        \item House Price Prediction: Estimating price based on features.
                        \item Temperature Forecasting: Predicting tomorrow's temperature.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression: Definition - Part 1}
    \begin{block}{What is Logistic Regression?}
        Logistic regression is a statistical method used in supervised learning for modeling the probability of a binary (two-class) outcome variable. It is particularly useful when the response we want to predict is categorical, taking on values such as 0 or 1, Yes or No, True or False.
    \end{block}

    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Binary Outcome Variables}: Focuses on scenarios where the dependent variable has two possible outcomes (e.g., Success (1) or Failure (0)).
            \item \textbf{Probability Estimation}: Predicts the probability that the outcome belongs to the positive class, represented as \( P(Y=1|X) \).
            \item \textbf{Sigmoid Function}: Maps real-valued numbers into the range [0, 1] using:
            \[
            P(Y=1|X) = \sigma(z) = \frac{1}{1 + e^{-z}}
            \]
            where \( z = \beta_0 + \beta_1X_1 + \ldots + \beta_nX_n \).
            \item \textbf{Decision Boundary}: The boundary separating classes, typically defined where the predicted probability equals 0.5.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression: Definition - Part 2}
    \begin{block}{Example}
        Consider a dataset of patient attributes to predict heart disease (1) or not (0). Features could include:
        \begin{itemize}
            \item Age: [45, 50, 35]
            \item Cholesterol: [200, 250, 180]
            \item Blood Pressure: [120, 140, 110]
            \item Outcome: [1, 1, 0]
        \end{itemize}
        The model might output probabilities like:
        \begin{itemize}
            \item Patient 1: 0.7 (likely has heart disease)
            \item Patient 2: 0.6 (likely has heart disease)
            \item Patient 3: 0.3 (unlikely to have heart disease)
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Logistic regression is designed for binary classification problems.
            \item Outputs a probability score requiring a threshold for class determination.
            \item The logistic function is crucial for relating features to binary outcomes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression: Definition - Part 3}
    \begin{block}{Application}
        Logistic regression is widely used in various fields, such as:
        \begin{itemize}
            \item Medicine: for predicting diseases.
            \item Finance: for credit scoring.
            \item Marketing: for customer response analysis.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Logistic regression is a fundamental concept in supervised learning that allows effective prediction of binary outcomes using predictors. Next, we will delve into the mathematical foundations that support this powerful modeling technique.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation - Overview}
    \begin{block}{Overview of Logistic Regression}
        Logistic regression is a statistical method for modeling binary outcomes. It uses the logistic function to link the input variables to the probability of a particular outcome.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation - The Logistic Function}
    \begin{block}{The Logistic Function}
        The logistic function is defined as:
        \[
        f(z) = \frac{1}{1 + e^{-z}}
        \]
        \begin{itemize}
            \item \( z \) is a linear combination of independent variables (features).
            \item \( e \) is approximately equal to 2.71828.
        \end{itemize}
        \begin{itemize}
            \item Ranges between 0 and 1, ideal for probabilities.
            \item S-shaped curve (sigmoid function) smoothly transitions between two classes.
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        Given: 
        \[
        z = \beta_0 + \beta_1X_1 + \beta_2X_2
        \]
        The predicted probability:
        \[
        P(Y=1 \mid X) = f(z) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2)}}
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation - Odds and Odds Ratios}
    \begin{block}{Odds and Odds Ratios}
        \begin{itemize}
            \item \textbf{Odds}: Ratio of the probability of an event to the probability of it not occurring:
            \[
            \text{Odds} = \frac{P(Y=1)}{P(Y=0)} = \frac{P(Y=1)}{1 - P(Y=1)}
            \]
            \item \textbf{Odds Ratio}: Compares odds for different groups/conditions:
            \[
            \text{Odds Ratio} = e^{\beta_1}
            \]
        \end{itemize}
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Logistic function transforms input into a probability score.
            \item Analyzing odds ratios interprets predictors' effects on binary outcomes.
            \item Positive coefficient increases odds; negative decreases.
        \end{itemize}
    \end{block}
    \begin{block}{Summary}
        Logistic regression utilizes the logistic function for binary outcome prediction. Understanding odds and odds ratios is vital for interpretation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function Graph - Overview}
    \begin{itemize}
        \item The logistic function is fundamental in logistic regression for modeling binary outcomes.
        \item Maps real-valued numbers to between 0 and 1, making it ideal for probability predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function Graph - Mathematical Definition}
    The logistic function is defined by the formula:
    \begin{equation}
        f(z) = \frac{1}{1 + e^{-z}}
    \end{equation}
    Where:
    \begin{itemize}
        \item \( z \) is the input variable (often a linear combination of features):
        \begin{equation}
            z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n
        \end{equation}
        \item \( e \) is the base of the natural logarithm (approx. 2.71828).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function Graph - S-Shaped Curve and Key Properties}
    \begin{block}{S-Shaped Nature}
        The curve, known as the sigmoid function, approaches:
        \begin{itemize}
            \item 0 as \( z \to -\infty \)
            \item 1 as \( z \to +\infty \)
        \end{itemize}
        At \( z = 0 \), the output probability is 0.5.
    \end{block}
    
    \begin{itemize}
        \item **Range**: Output values range from 0 to 1.
        \item **Interpretation**: Output represents the probability of the dependent variable being equal to 1.
        \item **Derivative**:
        \begin{equation}
            f'(z) = f(z)(1 - f(z))
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function Graph - Importance of Properties}
    \begin{enumerate}
        \item **Non-linearity**: Models complex relationships, unlike linear regression.
        \item **Thresholding**:
        \begin{itemize}
            \item If \( f(z) > 0.5 \), classify as Class 1.
            \item If \( f(z) \leq 0.5 \), classify as Class 0.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function Graph - Example Application}
    \begin{itemize}
        \item **Medical Diagnosis**: Used in predicting outcomes of medical tests (positive or negative).
        \item Logistic regression provides probabilities for disease presence based on test results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Function Graph - Key Points}
    \begin{itemize}
        \item The logistic function transitions smoothly between 0 and 1.
        \item Ideal for binary classification due to its mathematical properties.
        \item Highlights differences between logistic regression and linear regression.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differences from Linear Regression - Overview}
    
    \begin{itemize}
        \item \textbf{Linear Regression}:
            \begin{itemize}
                \item Models the relationship between a continuous dependent variable and independent variables.
                \item \textbf{Goal}: Predict a continuous outcome (e.g., salary, height).
                \item \textbf{Model}: 
                \[
                Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
                \]
                \item \textbf{Output}: A linear equation producing values ranging from negative to positive infinity.
            \end{itemize}

        \item \textbf{Logistic Regression}:
            \begin{itemize}
                \item Used for categorical outcomes, particularly binary ones (e.g., yes/no).
                \item \textbf{Goal}: Estimate the probability of a particular category.
                \item \textbf{Model}:
                \[
                P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
                \]
                \item \textbf{Output}: Probability values between 0 and 1.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differences from Linear Regression - Key Differences}
    
    \begin{enumerate}
        \item \textbf{Nature of Dependent Variable}:
            \begin{itemize}
                \item Linear Regression: Continuous outcomes.
                \item Logistic Regression: Categorical (binary) outcomes.
            \end{itemize}

        \item \textbf{Prediction and Interpretation}:
            \begin{itemize}
                \item Linear Regression: Final predictions not limited to [0, 1].
                \item Logistic Regression: Predictions as probabilities constrained to [0, 1].
            \end{itemize}

        \item \textbf{Error Metrics}:
            \begin{itemize}
                \item Linear Regression: Mean Squared Error (MSE), R-squared.
                \item Logistic Regression: Accuracy, Precision, Recall, AUC-ROC curve.
            \end{itemize}
    
        \item \textbf{Model Assumptions}:
            \begin{itemize}
                \item Linear Regression: Linear relationship, normally distributed errors.
                \item Logistic Regression: No assumption of linear relationship with the dependent variable; linear between predictors and log-odds.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differences from Linear Regression - Examples}
    
    \begin{itemize}
        \item \textbf{Linear Regression Example}:
            \begin{itemize}
                \item Predicting house prices based on square footage and number of bedrooms.
            \end{itemize}

        \item \textbf{Logistic Regression Example}:
            \begin{itemize}
                \item Predicting disease occurrence (yes/no) based on health metrics like age and cholesterol levels.
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Choose the right model based on the outcome variable.
            \item Understand how logistic regression coefficients influence predicted odds.
            \item Logistic regression is crucial in fields such as healthcare and economics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Cases of Logistic Regression}
    \begin{itemize}
        \item Logistic regression is used for binary classification problems.
        \item It has applications across various fields, including healthcare, finance, marketing, and social sciences.
        \item The method supports data-driven decision making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Applications - Healthcare}
    \begin{block}{Healthcare}
        Logistic regression is utilized in:
        \begin{itemize}
            \item **Disease Diagnosis**: Predicting the likelihood of patients having diseases (e.g., heart disease) based on various factors.
        \end{itemize}
        \begin{exampleblock}{Example}
            A model might predict heart disease using patient data (1 = disease, 0 = no disease).
            The logistic function outputs probabilities that can be classified.
        \end{exampleblock}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Applications - Finance and Marketing}
    \begin{block}{Finance}
        Logistic regression aids in:
        \begin{itemize}
            \item **Credit Scoring**: Assessing the risk of default on loans with past borrower data.
        \end{itemize}
        \begin{exampleblock}{Example}
            Variables include credit score, income, loan amount, and employment status.
            Outputs indicate the likelihood of default (1 = yes, 0 = no).
        \end{exampleblock}
    \end{block}
    
    \begin{block}{Marketing}
        Logistic regression helps in:
        \begin{itemize}
            \item **Customer Retention**: Predicting customer loyalty or likelihood to churn based on behavior and interactions.
        \end{itemize}
        \begin{exampleblock}{Example}
            Factors like purchase size and service interactions are analyzed to classify customers.
        \end{exampleblock}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Applications - Social Sciences}
    \begin{block}{Social Sciences}
        Logistic regression is applied in:
        \begin{itemize}
            \item **Voting Behavior**: Modeling the likelihood of individuals voting based on demographics and socioeconomic factors.
        \end{itemize}
        \begin{exampleblock}{Example}
            Variables may include age, education, income, and voting history, predicting voting likelihood (1 = vote, 0 = no vote).
        \end{exampleblock}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Formula}
    \begin{itemize}
        \item **Binary Outcome Focus**: Designed for two outcome categories.
        \item **Probability Interpretation**: Outputs as probabilities for risk assessment.
        \item **Flexibility**: Capable of handling continuous and categorical predictors.
        \item **Foundation of Other Models**: Serves as a base for more complex models.
    \end{itemize}

    \begin{block}{Logistic Regression Formula}
        \[
        P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + ... + \beta_nX_n)}}
        \]
        where:
        \begin{itemize}
            \item $P(Y=1 | X)$ = Probability of the event occurring.
            \item $\beta_0$ = Intercept.
            \item $\beta_1, \beta_2, \ldots, \beta_n$ = Coefficients of independent variables $X_1, X_2, \ldots, X_n$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Logistic regression is a key analytical tool in multiple domains. 
    It enables informed decisions based on predicted probabilities, bridging the gap between theoretical statistics and practical data analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Logistic Regression - Overview}
    \begin{block}{Introduction}
        Logistic Regression is a powerful statistical method widely utilized for binary classification problems. To ensure its validity and the accuracy of its predictions, certain assumptions must be met.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Logistic Regression - Key Assumptions}
    \begin{enumerate}
        \item \textbf{Binary Outcome Variable}
        \item \textbf{Independence of Observations}
        \item \textbf{Linearity of Logits}
        \item \textbf{No Multicollinearity among Predictors}
        \item \textbf{Large Sample Size}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 1: Binary Outcome Variable}
    \begin{itemize}
        \item \textbf{Definition:} The dependent (outcome) variable must be binary (i.e., it can take only two possible outcomes).
        \item \textbf{Example:} Predicting whether a patient has a disease (1) or not (0).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 2: Independence of Observations}
    \begin{itemize}
        \item \textbf{Definition:} The observations must be independent of each other.
        \item \textbf{Example:} In a survey about health behaviors, individual participants' response patterns should not influence each other.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 3: Linearity of Logits}
    \begin{itemize}
        \item \textbf{Definition:} The log-odds of the outcome should be linearly related to the independent variables.
        \item \textbf{Example:} For predictors like age and income, the relationship between log-odds and predictors should be linear.
        \item \textbf{Formula:}
        \[
        \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 4: No Multicollinearity among Predictors}
    \begin{itemize}
        \item \textbf{Definition:} Independent variables should not be too highly correlated with each other.
        \item \textbf{Example:} Predictors like “height” and “weight” may be correlated, affecting the model's reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumption 5: Large Sample Size}
    \begin{itemize}
        \item \textbf{Definition:} Logistic regression requires a sufficiently large sample size for stable predictions.
        \item \textbf{Rule of Thumb:} At least 10 events per predictor variable is recommended.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Confirm that your outcome variable is binary.
        \item Ensure observations are independent.
        \item Check for linear relationships between predictors and log-odds.
        \item Assess multicollinearity using Variance Inflation Factor (VIF) analysis.
        \item Use a larger sample size for reliable model estimation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Tools}
    \begin{itemize}
        \item Residual plots for visualizing the linearity of logits.
        \item Correlation matrices to identify multicollinearity.
        \item VIF values to quantify multicollinearity (VIF > 10 indicates problematic multicollinearity).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Understanding and verifying these assumptions improve logistic regression models and enhance the validity of conclusions derived from analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Introduction}
    \begin{block}{Introduction}
        Model evaluation metrics are crucial for assessing how well a logistic regression model performs. 
        Understanding these metrics allows us to quantify the performance, guide improvements, and interpret the results appropriately.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Confusion Matrix}
    \begin{block}{1. Confusion Matrix}
        A confusion matrix is a table used to describe the performance of a classification model. It shows the actual vs. predicted classifications, detailing true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).
        
        \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            Actual \textbackslash Predicted & Positive (1) & Negative (0) \\
            \hline
            Positive (1) & TP & FN \\
            \hline
            Negative (0) & FP & TN \\
            \hline
        \end{tabular}
        \end{center}

        \textbf{Example:}
        \begin{itemize}
            \item TP: 50 (correctly predicted as positive)
            \item FP: 10 (incorrectly predicted as positive)
            \item TN: 30 (correctly predicted as negative)
            \item FN: 5 (incorrectly predicted as negative)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Performance Metrics}
    \begin{block}{2. Accuracy}
        \[
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \]
        \textbf{Example Calculation:} Given the values from above,
        \[
        \text{Accuracy} = \frac{50 + 30}{50 + 30 + 10 + 5} = \frac{80}{95} \approx 0.84 \text{ or } 84\%
        \]
    \end{block}
    
    \begin{block}{3. Precision}
        \[
        \text{Precision} = \frac{TP}{TP + FP}
        \]
        \textbf{Example Calculation:}
        \[
        \text{Precision} = \frac{50}{50 + 10} = \frac{50}{60} \approx 0.83 \text{ or } 83\%
        \]
    \end{block}
    
    \begin{block}{4. Recall (Sensitivity)}
        \[
        \text{Recall} = \frac{TP}{TP + FN}
        \]
        \textbf{Example Calculation:}
        \[
        \text{Recall} = \frac{50}{50 + 5} = \frac{50}{55} \approx 0.91 \text{ or } 91\%
        \]
    \end{block}
    
    \begin{block}{5. F1 Score}
        \[
        \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
        \]
        \textbf{Example Calculation:}
        \[
        \text{F1 Score} = 2 \cdot \frac{0.83 \cdot 0.91}{0.83 + 0.91} \approx 0.87
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Use a \textbf{confusion matrix} for a comprehensive view of model performance.
        \item \textbf{Accuracy} provides a general measure but can be misleading in cases of imbalanced datasets.
        \item \textbf{Precision} and \textbf{Recall} offer deeper insights into the model's effectiveness in identifying positives.
        \item \textbf{F1 Score} is important for a balanced assessment, emphasizing both precision and recall.
    \end{itemize}
    
    By understanding these metrics, you can critically evaluate the logistic regression model’s performance and make informed decisions about model tuning and deployment.
\end{frame}

\begin{frame}
    \frametitle{Training the Logistic Regression Model}
    
    \begin{block}{Introduction}
        Logistic regression is a statistical method for predicting binary classes. It estimates the probability that a given input point belongs to a particular category using the logistic function.
    \end{block}
    
    \begin{itemize}
        \item Steps to implement:
            \begin{itemize}
                \item Data preparation
                \item Model fitting
                \item Parameter estimation
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{1. Data Preparation}

    \begin{enumerate}
        \item \textbf{Data Collection}: Gather relevant data with independent variables and a binary response variable.
        \item \textbf{Data Cleaning}:
            \begin{itemize}
                \item Handle missing values (impute or remove).
                \item Convert categorical variables into numeric format (one-hot encoding).
            \end{itemize}
        \item \textbf{Feature Scaling}: (optional) Standardize or normalize features for equal contribution in distance calculations.
    \end{enumerate}

    \begin{block}{Example of Data Preparation}
        \begin{tabular}{|c|c|c|}
            \hline
            Hours\_Studied & Attendance (0 = No, 1 = Yes) & Pass (Target) \\
            \hline
            10 & 1 & 1 \\
            5 & 0 & 0 \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Model Fitting}
    
    \begin{itemize}
        \item \textbf{Logistic Function}: The logistic regression model uses the logistic function:
        \begin{equation}
        P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
        Where:
        \begin{itemize}
            \item $P$ = probability of the positive class.
            \item $\beta_0$ = intercept.
            \item $\beta_n$ = coefficients for the features $X_n$.
        \end{itemize}
        
        \item \textbf{Training the Model}: Fit the logistic regression model using optimization techniques such as Maximum Likelihood Estimation (MLE) or gradient descent.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    
    \begin{block}{Python Code (using Scikit-learn)}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Load the dataset
data = pd.read_csv('student_data.csv')

# Clean and prepare the data
X = data[['Hours_Studied', 'Attendance']]
y = data['Pass']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create and fit the model
model = LogisticRegression()
model.fit(X_train, y_train)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{3. Parameter Estimation}

    \begin{enumerate}
        \item \textbf{Coefficient Estimation}: The model learns coefficients ($\beta$) representing the relationship between predictors and response, indicating how log-odds of passing change with one unit increase in predictor.
        
        \item \textbf{Output Interpretation}: Evaluate model outputs (coefficients and intercept) to understand each feature's impact.
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Data preparation is crucial for accurate predictions.
            \item Model fitting uses the logistic function to predict probabilities.
            \item Parameter estimation aids in interpreting the significance of features.
            \item Validate model performance using evaluation metrics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Logistic Regression Outputs - Introduction}
    When a logistic regression model is fitted to data, it produces key outputs that are critical to understanding the independent variables and the dependent binary outcome, including:
    \begin{itemize}
        \item Coefficients
        \item Predicted probabilities
        \item Metrics for model performance
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Logistic Regression Outputs - Coefficients}
    \begin{block}{Coefficients Interpretation}
        Each coefficient (\( \beta \)) represents the change in the log-odds of the dependent variable for a one-unit increase in the predictor variable, holding all other variables constant:
        \begin{itemize}
            \item Positive coefficient: Increases likelihood of the outcome
            \item Negative coefficient: Decreases likelihood of the outcome
        \end{itemize}
    \end{block}
    
    \textbf{Example:}
    \begin{itemize}
        \item Coefficient for "Age": \( \beta = 0.03 \)
        \item Interpretation: Each additional year of age increases odds by a factor of \( e^{0.03} \approx 1.03 \) (or 3\%).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Logistic Regression Outputs - Predicted Probabilities}
    \begin{block}{Probability Calculation}
        The logistic regression output can be transformed to probabilities using the logistic function:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k)}}
        \end{equation}
    \end{block}

    \textbf{Example:}
    \begin{itemize}
        \item For a model with \( \beta_0 = -2 \) and \( \beta_1 = 0.5 \):
        \item If \( X_1 = 4 \):
        \begin{equation}
            P(Y=1) = \frac{1}{1 + e^{-(-2 + 0.5 \times 4)}} = \frac{1}{1 + e^{-0}} = 0.5
        \end{equation}
        \item This suggests a 50\% likelihood of the outcome when \( X_1 = 4 \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Logistic Regression Outputs - Model Evaluation}
    \begin{block}{Key Metrics for Model Evaluation}
        \begin{itemize}
            \item Confusion Matrix: A table describing model performance (True Positives, False Positives, etc.)
            \item Accuracy: Proportion of correct predictions over total predictions
            \item Area Under the Curve (AUC-ROC): Measures model's discriminative ability
        \end{itemize}
    \end{block}

    \begin{block}{Practical Considerations}
        \begin{itemize}
            \item **Multicollinearity**: Check for correlation between predictors
            \item **Odds Ratio**: More interpretable measure of effect size
        \end{itemize}
        
        \textbf{Odds Ratio Calculation:}
        \begin{equation}
            \text{Odds Ratio} = e^{\beta}
        \end{equation}

        For \( \beta = 0.5 \), the odds ratio would be \( e^{0.5} \approx 1.65 \), indicating a 65\% increase in odds for a one-unit increase in the predictor.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Logistic Regression Outputs - Key Takeaways}
    \begin{itemize}
        \item Understand how coefficients relate to log-odds.
        \item Use logistic function to convert log-odds to probabilities.
        \item Apply evaluation metrics to gauge model performance—enhancing model reliability and interpretability.
    \end{itemize}

    \textbf{Conclusion:} Interpreting logistic regression outputs is crucial for deriving actionable insights and making informed decisions based on data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting in Logistic Regression}
    \begin{itemize}
        \item Understanding overfitting and underfitting
        \item Key indicators of these issues
        \item Strategies to mitigate them
        \item Summary of key points
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Understanding Overfitting and Underfitting}
    \begin{block}{Overfitting}
        \begin{itemize}
            \item Capturing noise and details in training data.
            \item High training accuracy but poor validation/test accuracy.
        \end{itemize}
        \textbf{Example:} A model predicting each training data point's outcome perfectly, but failing on new data.
    \end{block}

    \begin{block}{Underfitting}
        \begin{itemize}
            \item Model is too simple to capture data trends.
            \item Low accuracy on both training and testing data.
        \end{itemize}
        \textbf{Example:} A linear model applied to a non-linear dataset mispredicting outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Key Indicators}
    \begin{itemize}
        \item \textbf{Diagnostic Measures}:
        \begin{itemize}
            \item Training vs. Validation Loss: Indicates overfitting when training loss decreases and validation loss increases.
            \item Accuracy Scores: Disparities between training and test set accuracies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Strategies to Mitigate Overfitting and Underfitting}
    \begin{itemize}
        \item \textbf{Cross-Validation}
        \begin{itemize}
            \item k-fold cross-validation to assess model generalization.
        \end{itemize}
        
        \item \textbf{Regularization Techniques}
        \begin{itemize}
            \item Add regularization parameters to the loss function.
            \begin{itemize}
                \item L1 Regularization (Lasso)
                \item L2 Regularization (Ridge)
            \end{itemize}

            \begin{equation}
            J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right) + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
            \end{equation}
        \end{itemize}

        \item \textbf{Feature Selection}
        \begin{itemize}
            \item Remove irrelevant features to lessen noise and complexity.
        \end{itemize}

        \item \textbf{Increase Training Data}
        \begin{itemize}
            \item More data can help the model learn and reduce overfitting.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Summary of Key Points}
    \begin{itemize}
        \item Balance model complexity and data accuracy.
        \item Regularization encourages simpler models.
        \item Continuous evaluation with cross-validation is crucial.
    \end{itemize}
    \textbf{Conclusion:} Implementing these strategies improves the generalization of logistic regression models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - Overview}
    \begin{block}{Overview of Regularization in Logistic Regression}
        Regularization is a technique used to prevent overfitting, where a model learns noise in the data instead of the underlying patterns. 
        In logistic regression, we primarily use:
        \begin{itemize}
            \item \textbf{Lasso Regression} (L1 regularization)
            \item \textbf{Ridge Regression} (L2 regularization)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - Lasso Regression}
    \begin{block}{Lasso Regression (L1 Regularization)}
        \begin{itemize}
            \item \textbf{Definition}: Adds a penalty equal to the absolute value of the coefficients to the loss function.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Loss Function} = -\sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right) + \lambda \sum_{j=1}^{p} | \beta_j |
            \end{equation}
            \item \textbf{Key Points}: 
                \begin{itemize}
                    \item Encourages sparsity; some coefficients become exactly zero.
                    \item Useful for feature selection: non-zero coefficients indicate significant features.
                \end{itemize}
            \item \textbf{Example}: In a dataset with 10 features, Lasso may retain only 4, showing other features are not contributing to predictions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - Ridge Regression}
    \begin{block}{Ridge Regression (L2 Regularization)}
        \begin{itemize}
            \item \textbf{Definition}: Adds a penalty equal to the square of the coefficients to the loss function.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Loss Function} = -\sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right) + \lambda \sum_{j=1}^{p} \beta_j^2
            \end{equation}
            \item \textbf{Key Points}:
                \begin{itemize}
                    \item Does not set coefficients to zero but shrinks them towards zero.
                    \item Effective when many features are relevant, particularly with multicollinearity.
                \end{itemize}
            \item \textbf{Example}: In datasets with correlated features, Ridge maintains all features but reduces their influence, stabilizing predictions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Regularization in Logistic Regression}
    \begin{block}{Benefits of Regularization}
        \begin{itemize}
            \item \textbf{Combats Overfitting}: Ensures that models generalize well to unseen data.
            \item \textbf{Improves Interpretability}: Reduces complexity, making it easier to interpret feature effects.
            \item \textbf{Model Selection}: Helps in balancing bias and variance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Chapter 5: Supervised Learning: Logistic Regression - Slide 14}
    \frametitle{Practical Implementation Example}
    \begin{block}{Overview}
        A practical walkthrough of implementing logistic regression using Python and Scikit-learn, covering dataset selection and model training.
    \end{block}
\end{frame}

\begin{frame}{Dataset Selection}
    \frametitle{1. Dataset Selection}
    \begin{itemize}
        \item \textbf{Example Dataset:} Iris Dataset
            \begin{itemize}
                \item Objective: Classify whether a flower belongs to the species "Iris-setosa" (binary classification).
            \end{itemize}
        \item \textbf{Loading the Dataset:}
    \end{itemize}
    \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
import pandas as pd

# Load the dataset
iris = load_iris()
X = iris.data[:, :2]  # Only using two features for simplicity
y = (iris.target == 0).astype(int)  # Target: 1 if Iris-setosa, else 0
    \end{lstlisting}
    \begin{block}{Key Point}
        Always understand your dataset, including the features and target.
    \end{block}
\end{frame}

\begin{frame}{Data Preparation and Model Training}
    \frametitle{2. Data Preparation}
    \begin{itemize}
        \item \textbf{Train-Test Split:}
        It’s crucial to split the data into training and testing sets to evaluate model performance.
    \end{itemize}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    \end{lstlisting}
    \begin{block}{Key Consideration}
        Set a random seed for reproducibility.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Model Training:} Create and train the Logistic Regression model.
    \end{itemize}
    \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression

# Initialize and fit the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)
    \end{lstlisting}
\end{frame}

\begin{frame}{Model Evaluation and Summary}
    \frametitle{3. Model Evaluation}
    \begin{itemize}
        \item \textbf{Making Predictions:}
    \end{itemize}
    \begin{lstlisting}[language=Python]
y_pred = model.predict(X_test)
    \end{lstlisting}
    \begin{itemize}
        \item \textbf{Calculating Accuracy:}
    \end{itemize}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")
    \end{lstlisting}
    \begin{block}{Key Point}
        Accuracy is a simple yet effective metric to evaluate binary classification models.
    \end{block}
    
    \begin{block}{Summary}
        \begin{itemize}
            \item Logistic Regression is widely used for binary classification.
            \item Implementation steps: dataset selection, train-test split, model training, and evaluation.
            \item Visualization and understanding of data are crucial.
        \end{itemize}
    \end{block}
    
    \begin{block}{Next Step}
        In the following slide, we will discuss the ethical considerations surrounding the use of logistic regression.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Overview}
    \begin{itemize}
        \item Ethical implications must be addressed when deploying logistic regression models.
        \item Key areas impacted: healthcare, finance, criminal justice.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Bias in Data}
    \begin{block}{Bias in Data}
        \begin{itemize}
            \item Models depend on the quality of training data.
            \item Historical biases (racial, gender, socioeconomic) prompt unfair treatment.
            \item \textbf{Example:} Credit scoring models may disadvantage certain demographics, perpetuating inequality.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Interpretability and Accountability}
    \begin{itemize}
        \item \textbf{Interpretability:} Ensure stakeholders understand model predictions.
            \begin{itemize}
                \item Provide clear documentation of model features.
            \end{itemize}
        \item \textbf{Accountability:}
            \begin{itemize}
                \item Establish accountability frameworks for harmful model outcomes.
                \item Illustrative point: AWS's AI Ethics Guidelines.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Informed Consent and Monitoring}
    \begin{itemize}
        \item \textbf{Informed Consent:}
            \begin{itemize}
                \item Obtain consent for sensitive data usage (like healthcare data).
                \item \textbf{Example:} Ensure patients understand data usage in model training.
            \end{itemize}
        \item \textbf{Ongoing Monitoring:}
            \begin{itemize}
                \item Monitor models post-deployment for emerging biases.
                \item Conduct regular audits for model performance and equity.
            \end{itemize}
        \item \textbf{Conclusion:}
            \begin{itemize}
                \item Responsible logistic regression usage necessitates addressing key ethical aspects.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Formula and Code Snippet}
    \begin{block}{Logistic Regression Formula}
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \ldots + \beta_nX_n)}}
        \end{equation}
    \end{block}
    
    \begin{block}{Sample Code for Bias Detection}
        \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
cm = confusion_matrix(y_test, predictions)
print("Confusion Matrix:", cm)
        \end{lstlisting}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 1}
    \begin{block}{Conclusion}
        In this chapter, we explored the fundamentals of \textbf{Logistic Regression}, a pivotal algorithm in the realm of \textbf{Supervised Learning}. 
        Logistic Regression is utilized primarily for binary classification tasks, where the output is categorical (e.g., Yes/No, True/False).
        The model predicts the probability that a given input belongs to a particular category, making it especially valuable in applications like:
    \end{block}
    \begin{itemize}
        \item Medical diagnostics
        \item Credit scoring
        \item Marketing analysis
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 2}
    \begin{block}{Key Concepts Covered}
        \begin{enumerate}
            \item \textbf{Logistic Function}
                \begin{equation}
                P(Y=1|X) = \frac{1}{1 + e^{-\beta_0 - \beta_1 X_1 - \beta_2 X_2 - \ldots - \beta_n X_n}}
                \end{equation}
                Outputs values between 0 and 1, representing probabilities.
                
            \item \textbf{Model Interpretation}
                \begin{itemize}
                    \item Coefficients (\(\beta\)): Each coefficient quantifies the impact of the feature on the log-odds of the outcome.
                    \item Odds ratio helps in understanding changes in odds due to feature increases.
                \end{itemize}

            \item \textbf{Training the Model}
                \begin{itemize}
                    \item Maximized using \textbf{Maximum Likelihood Estimation (MLE)}.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 3}
    \begin{block}{Model Evaluation Metrics}
        Assessing the model's performance includes metrics like:
        \begin{itemize}
            \item \textbf{Accuracy}
            \item \textbf{Precision}
            \item \textbf{Recall}
            \item \textbf{ROC-AUC Score}
        \end{itemize}
    \end{block}
    
    \begin{block}{Assumptions of Logistic Regression}
        Logistic Regression assumes that:
        \begin{itemize}
            \item The dependent variable is categorical.
            \item Observations are independent.
            \item There's a linear relationship between the log-odds of the outcome and predictor variables.
        \end{itemize}
    \end{block}

    \begin{block}{Significance of Logistic Regression}
        \begin{itemize}
            \item \textbf{Simple and Interpretable}
            \item \textbf{Computationally Efficient}
            \item \textbf{Widely Applicable}
        \end{itemize}
    \end{block}
\end{frame}


\end{document}