\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \maketitle
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Unsupervised Learning}
    \begin{itemize}
        \item Unsupervised learning involves training a model on data without labeled outcomes.
        \item The model learns patterns and structures by identifying relationships among features.
        \item There is no prior knowledge of results or labels.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics of Unsupervised Learning}
    \begin{itemize}
        \item \textbf{No Labeled Data:} No outputs or targets are provided during training.
        \item \textbf{Pattern Discovery:} Main goal is to find hidden structures in the data.
        \item \textbf{Dimensionality Reduction:} Reduces number of features while retaining essential information.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Focus on Clustering Methods}
    \begin{itemize}
        \item Clustering groups similar data points based on features.
    \end{itemize}
    
    \begin{block}{Key Points About Clustering}
        \begin{itemize}
            \item \textbf{Definition:} Divides objects into groups (clusters) where members within a group are more similar.
            \item \textbf{Applications:} Used in market segmentation, social network analysis, anomaly detection, etc.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Clustering Algorithms}
    \begin{itemize}
        \item \textbf{K-Means Clustering:} 
            \begin{itemize}
                \item Partitions n observations into k clusters.
                \item Objective formula:
                \begin{equation}
                    J = \sum_{i=1}^k \sum_{x \in C_i} || x - \mu_i ||^2
                \end{equation}
                Where \( \mu_i \) is the centroid of cluster \( C_i \).
            \end{itemize}
        
        \item \textbf{Hierarchical Clustering:}
            \begin{itemize}
                \item Builds a hierarchy of clusters via merging or dividing.
                \item Example: Dendrograms visualize clusters at various levels.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Clustering in Data Analysis}
    \begin{itemize}
        \item \textbf{Insight Generation:} Helps understand underlying structure for informed decision-making.
        \item \textbf{Feature Engineering:} Assists in creating features for supervised learning tasks by forming informative groups.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Remarks}
    \begin{itemize}
        \item Unsupervised learning, especially clustering, is a powerful tool for data analysis.
        \item It uncovers hidden structures without requiring labeled data.
    \end{itemize}
    \begin{block}{Next Steps}
        \begin{itemize}
            \item We will delve deeper into specific clustering techniques and their definitions in subsequent slides.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Definition}
    Clustering is a machine learning technique used in \textbf{unsupervised learning}, where the goal is to group a set of objects into clusters based on their similarities. Unlike supervised learning, clustering does not rely on labeled datasets; instead, it identifies patterns and structures in unlabelled data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Purpose}
    The primary purpose of clustering is to simplify data analysis and provide insights by organizing similar data points into categories. Clustering helps in:
    \begin{itemize}
        \item \textbf{Identifying natural groupings:} Highlights inherent groupings within data, making patterns more visible.
        \item \textbf{Reducing noise:} Groups similar data points together to mitigate the impact of outliers.
        \item \textbf{Facilitating further analysis:} Serves as a precursor for complex tasks, such as classification or dimensionality reduction.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Key Points and Techniques}
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Useful when no prior knowledge exists about data structure (unsupervised context).
        \item Objective is to discover unknown patterns or groupings.
    \end{itemize}

    \textbf{Common Applications include:}
    \begin{itemize}
        \item Customer segmentation in marketing.
        \item Image compression.
        \item Anomaly detection in network security.
    \end{itemize}

    \textbf{Examples of Clustering Techniques:}
    \begin{enumerate}
        \item \textbf{K-means Clustering}
        \begin{itemize}
            \item Partitions the data into k distinct clusters by minimizing distances to centroids.
        \end{itemize}
        \item \textbf{Hierarchical Clustering}
        \begin{itemize}
            \item Builds a hierarchy of clusters, useful for organizing documents based on content similarity.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Clustering}
    Clustering is a fundamental technique in unsupervised learning that groups similar data points together while distinguishing them from dissimilar ones. 
    Its importance extends across various domains and applications due to its ability to uncover hidden patterns and relationships in data without prior labeling.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Clustering Essential?}
    \begin{enumerate}
        \item \textbf{Data Exploration}
        \begin{itemize}
            \item Clustering aids in the exploration of large datasets, helping to identify significant trends.
            \item \textit{Example:} In marketing, clustering can reveal customer segments based on purchasing behavior.
        \end{itemize}
        
        \item \textbf{Anomaly Detection}
        \begin{itemize}
            \item Unusual data points can be quickly identified within a cluster, which is crucial for detecting fraud.
            \item \textit{Example:} Credit card fraud detection systems monitor transactions to identify anomalies.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Clustering Essential? (continued)}
    \begin{enumerate}
        \setcounter{enumi}{2}  % Continue from the previous frame
        \item \textbf{Preprocessing for Other Algorithms}
        \begin{itemize}
            \item Clustering serves as a preprocessing step, reducing dataset complexity.
            \item \textit{Example:} In image processing, clustering simplifies color palettes for recognition tasks.
        \end{itemize}
        
        \item \textbf{Facilitates Recommendations}
        \begin{itemize}
            \item Clustering enhances recommendation systems by grouping similar users or items.
            \item \textit{Example:} Streaming services recommend shows based on user viewing patterns.
        \end{itemize}
        
        \item \textbf{Dimensionality Reduction}
        \begin{itemize}
            \item Clustering condenses high-dimensional data for easier interpretation.
            \item \textit{Example:} In genomics, clustering gene expression data helps researchers understand gene relationships.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Clustering}
    \begin{itemize}
        \item \textbf{Market Segmentation:} Dividing a market into distinct consumer groups based on preferences.
        
        \item \textbf{Social Network Analysis:} Identifying communities based on user interactions.
        
        \item \textbf{Medical Diagnosis:} Grouping patients with similar symptoms to identify treatment paths.
        
        \item \textbf{Image Segmentation:} Partitioning images for easier analysis in computer vision.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Code Snippet}
    Clustering is an indispensable tool in data analysis, providing insights across various applications from business intelligence to scientific research.
    
    \textbf{K-Means Clustering Pseudocode:}
    \begin{block}{}
    \begin{lstlisting}
Initialize k random centroids
Repeat until centroids do not change:
    For each data point:
        Assign it to the nearest centroid
    For each centroid:
        Update it to the mean of its assigned points
    \end{lstlisting}
    \end{block}
    
    By applying clustering techniques, significant insights can be achieved in data-driven fields.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Algorithms}
    
    \begin{block}{Overview of Clustering Algorithms}
        Clustering is a fundamental part of unsupervised learning that involves grouping objects such that objects in the same group (or cluster) are more similar to each other than to those in other groups. The choice of clustering algorithm depends on data type, size, and desired outcome.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Algorithms - Hierarchical Clustering}

    \begin{itemize}
        \item \textbf{Description}: Creates a tree-like structure (dendrogram) to represent nested clusters.
        \item \textbf{Types}:
        \begin{itemize}
            \item Agglomerative (bottom-up): Each data point starts as its own cluster, and pairs of clusters are merged.
            \item Divisive (top-down): A single cluster is divided into smaller clusters.
        \end{itemize}
        \item \textbf{Example}: Customer segmentation based on purchasing patterns, yielding a dendrogram of clusters.
        \item \textbf{Key Point}: Useful for data with inherent hierarchies but can be computationally intensive for large datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Algorithms - Partitioning, Density-Based, and Grid-Based Clustering}

    \begin{itemize}
        \item \textbf{Partitioning Clustering}:
        \begin{itemize}
            \item \textbf{Description}: Divides the dataset into a predefined number of clusters (k). 
            \item \textbf{Example}: K-Means clustering.
            \item \textbf{Algorithm Steps}:
            \begin{enumerate}
                \item Choose the number of clusters, k.
                \item Initialize k centroids randomly.
                \item Assign each point to the nearest centroid.
                \item Recalculate centroids.
                \item Repeat until convergence.
            \end{enumerate}
            \item \textbf{Key Point}: Efficient for large datasets but sensitive to the choice of k.
        \end{itemize}

        \item \textbf{Density-Based Clustering}:
        \begin{itemize}
            \item \textbf{Description}: Identifies clusters as areas of high density.
            \item \textbf{Example}: DBSCAN detects clusters in spatial data, e.g., crime hotspots.
            \item \textbf{Key Point}: Effective in noisy large datasets but requires careful parameter tuning.
        \end{itemize}

        \item \textbf{Grid-Based Clustering}:
        \begin{itemize}
            \item \textbf{Description}: Data space is divided into a grid, forming clusters based on data point density in cells.
            \item \textbf{Example}: Useful in geographical data analysis based on population density.
            \item \textbf{Key Point}: Performs well with large datasets and supports spatial queries efficiently.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}

    \begin{block}{Conclusion}
        Understanding different types of clustering algorithms allows for more informed decisions when analyzing complex data. The choice of algorithm impacts the effectiveness of the clustering outcome and should align with data characteristics and analysis objectives.
    \end{block}

    \begin{block}{Next Steps}
        Next, we will delve into one of the most popular partitioning algorithms: K-Means Clustering. Ensure to grasp the foundational differences between these algorithms to appreciate their various applications fully.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering - Overview}
    K-Means Clustering is a popular unsupervised learning algorithm used for partitioning a dataset into distinct groups or clusters based on feature similarity. 
    The main objective of K-Means is to group similar data points together while ensuring that different groups are as distinct as possible.
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering - Key Concepts}
    \begin{block}{1. Centroids}
        \begin{itemize}
            \item \textbf{Definition}: A centroid is the center of a cluster, defined as the mean position of all points in the cluster.
            \item \textbf{Initialization}: Centroids are initially chosen at random from the dataset.
        \end{itemize}
    \end{block}

    \begin{block}{2. Distance Metrics}
        \begin{itemize}
            \item K-Means typically uses \textbf{Euclidean distance} as the distance metric, calculating the straight-line distance between two points.
            \item \textbf{Alternatives}: Other distance metrics such as Manhattan distance or cosine similarity can be used depending on the data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering - Algorithm Steps}
    \begin{enumerate}
        \item \textbf{Choose K}: Decide on the number of clusters (K) to identify.
        \item \textbf{Initialize Centroids}: Randomly select K data points as initial centroids.
        \item \textbf{Assign Clusters}: Calculate distances from each centroid and assign data points to the nearest centroid.
        \item \textbf{Update Centroids}: Calculate new centroids by finding the mean of all data points in each cluster.
        \item \textbf{Repeat}: Continue steps 3 and 4 until centroids stabilize or a maximum iteration is reached.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering - Example}
    Consider data points: A(1, 2), B(2, 3), C(8, 8), D(9, 9) with K=2:
    \begin{enumerate}
        \item \textbf{Initialize Centroids}: Randomly choose A and C.
            \begin{itemize}
                \item Centroid 1: A(1, 2)
                \item Centroid 2: C(8, 8)
            \end{itemize}
        \item \textbf{Assign Clusters}:
            \begin{itemize}
                \item B(2, 3) is closer to A
                \item D(9, 9) is closer to C
            \end{itemize}
        \item \textbf{Update Centroids}:
            \begin{itemize}
                \item New Centroid 1: Mean of A and B = (1.5, 2.5)
                \item New Centroid 2: Mean of C and D = (8.5, 8.5)
            \end{itemize}
        \item \textbf{Repeat Assignment} until there are no changes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering - Key Points}
    \begin{itemize}
        \item \textbf{Choice of K}: Crucial parameter; can be determined using the Elbow Method or Silhouette Score.
        \item \textbf{Scalability}: K-Means is computationally efficient and scales well to large datasets.
        \item \textbf{Limitations}: May converge to a local minimum; running multiple times with different initializations can improve results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering - Formulas}
    \begin{block}{Euclidean Distance}
        The distance between two points \( p_1 (x_1, y_1) \) and \( p_2 (x_2, y_2) \):
        \[
        d(p_1, p_2) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
        \]
    \end{block}
    
    \begin{block}{Centroid Calculation}
        The centroid \( C_k \) of cluster \( k \):
        \[
        C_k = \frac{1}{N_k} \sum_{i \in Cluster_k} X_i
        \]
        where \( N_k \) is the number of points in cluster \( k \), and \( X_i \) are the data points in cluster \( k \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Overview}
    \begin{block}{Overview of Hierarchical Clustering}
        Hierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters. 
        This approach is particularly useful when:
        \begin{itemize}
            \item The data has a nested structure.
            \item There is no predefined number of clusters.
        \end{itemize}
        It produces a dendrogram, a tree-like diagram that illustrates how clusters are merged or split.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Types}
    \begin{block}{Types of Hierarchical Clustering}
        Hierarchical clustering can be categorized into two main methods:
        \begin{enumerate}
            \item \textbf{Agglomerative Clustering}
                \begin{itemize}
                    \item Bottom-up approach.
                    \item Each data point starts as an individual cluster; clusters are merged as one moves up the hierarchy.
                    \item Steps:
                        \begin{enumerate}
                            \item Start with \( n \) clusters (each point is a cluster).
                            \item Calculate distances between each pair of clusters.
                            \item Merge the two closest clusters.
                            \item Repeat until one cluster remains.
                        \end{enumerate}
                    \item Common Distance Metrics:
                        \begin{itemize}
                            \item Single Linkage: Minimum distance between clusters.
                            \item Complete Linkage: Maximum distance between clusters.
                            \item Average Linkage: Average distance between clusters.
                        \end{itemize}
                \end{itemize}
            \item \textbf{Divisive Clustering}
                \begin{itemize}
                    \item Top-down approach.
                    \item Start with all data points in one cluster and recursively split.
                    \item Steps:
                        \begin{enumerate}
                            \item Begin with a single cluster of all data points.
                            \item Identify the cluster with the highest variance.
                            \item Split this cluster into two subclusters.
                            \item Repeat until each cluster contains a single data point or meets a stopping criterion.
                        \end{enumerate}
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Dendrograms & Summary}
    \begin{block}{Dendrogram Representation}
        A dendrogram is a visual representation of the cluster hierarchy that shows:
        \begin{itemize}
            \item Clusters and their sub-clusters at various hierarchy levels.
            \item The distances at which clusters merge.
        \end{itemize}
        The height of branches illustrates the distance of merging; shorter branches indicate a stronger similarity.
    \end{block}
    
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item Hierarchical clustering can be either agglomerative or divisive.
            \item There is no need to predefine the number of clusters.
            \item The output is a dendrogram that visualizes cluster relationships.
            \item Different linkage criteria define cluster formation.
            \item Useful for exploratory data analysis.
        \end{itemize}
        Use hierarchical clustering to explore potential groupings without a predefined structure.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Density-Based Clustering}
    \begin{block}{Introduction}
        Density-based clustering is an unsupervised machine learning technique that identifies clusters based on the density of data points in a feature space. 
        \begin{itemize}
            \item Unlike traditional clustering methods like K-Means, it identifies clusters of arbitrary shapes and sizes.
            \item It effectively handles noise and outliers.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithm: DBSCAN}
    \begin{block}{Core Concepts}
        \begin{itemize}
            \item \textbf{Epsilon ($\epsilon$)}: A radius to search for neighboring points.
            \item \textbf{MinPts}: Minimum number of points to form a dense region.
        \end{itemize}
    \end{block}
    \begin{itemize}
        \item \textbf{Core Points}: At least MinPts neighbors within $\epsilon$.
        \item \textbf{Border Points}: Within $\epsilon$ of a core point, but not core points themselves.
        \item \textbf{Noise Points}: Not core or border points, considered outliers.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DBSCAN Algorithm Steps}
    \begin{enumerate}
        \item Select an arbitrary point in the dataset.
        \item Retrieve all neighbors density-reachable from the point within $\epsilon$.
        \item If the point is a core point, a new cluster is formed.
        \item Recursively apply the process to all core points found.
        \item Mark remaining points as noise or assign them to existing clusters.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustration of DBSCAN}
    \begin{center}
        \begin{verbatim}
            Core Point (P)
          ./ \    | \  
         /   \   |  \
     (B) Border (N)  \
      /  =>   (C)    (D)
   (Noise)
        \end{verbatim}
    \end{center}
    \begin{itemize}
        \item (P) is a core point surrounded by sufficient neighbors.
        \item (B) is a border point; (C) and (N) are noise points.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Density-Based Clustering}
    \begin{itemize}
        \item \textbf{Cluster Shape Flexibility:} Can identify complex shapes.
        \item \textbf{Robustness to Outliers:} Effectively distinguishes noise.
        \item \textbf{Automatic Cluster Count:} No need for pre-specifying the number of clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Density-Based Clustering}
    \begin{itemize}
        \item \textbf{Geospatial Data Analysis:} Identify regions with high density of features (e.g., crime hotspots).
        \item \textbf{Image Processing:} Clustering of pixel intensity for object detection.
        \item \textbf{Market Segmentation:} Identifying customer segments based on behaviors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{itemize}
        \item Density-based clustering, like DBSCAN, focuses on the density of data points.
        \item It handles noise well and does not require prior cluster count knowledge.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas to Remember}
    \begin{equation}
        \text{Density}(p) = \frac{\text{Number of points within a radius } \epsilon}{\text{Volume of the neighborhood}}
        \label{eq:density}
    \end{equation}
    \begin{equation}
        d(p, q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2}
        \label{eq:distance}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import numpy as np

# Sample data
X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])

# Scaling data
X_scaled = StandardScaler().fit_transform(X)

# DBSCAN
db = DBSCAN(eps=0.5, min_samples=2).fit(X_scaled)
labels = db.labels_

print("Cluster labels:", labels)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Clustering Models - Introduction}
    \begin{block}{Introduction to Clustering Evaluation}
        Evaluating the effectiveness of clustering models is crucial to understanding how well these algorithms are performing in organizing data into meaningful groups.
        
        Unlike supervised learning, where metrics like accuracy and F1-score apply, clustering evaluation employs metrics that account for the intrinsic structures within data. Two widely used metrics for this purpose are:
        \begin{itemize}
            \item \textbf{Silhouette Score}
            \item \textbf{Davies-Bouldin Index}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Clustering Models - Silhouette Score}
    \begin{block}{Silhouette Score}
        \textbf{Definition:} The silhouette score measures how similar an object is to its own cluster compared to other clusters. Values range from -1 to 1:
        \begin{itemize}
            \item \textbf{1}: Well-clustered data point.
            \item \textbf{0}: Point is on the decision boundary between clusters.
            \item \textbf{-1}: Data point may be improperly clustered.
        \end{itemize}

        \textbf{Formula:}
        \begin{equation}
            s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
        \end{equation}
        Where:
        \begin{itemize}
            \item $a(i)$: Average distance from point $i$ to points in its own cluster.
            \item $b(i)$: Average distance from point $i$ to points in the nearest cluster.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Clustering Models - Davies-Bouldin Index}
    \begin{block}{Davies-Bouldin Index}
        \textbf{Definition:} The Davies-Bouldin Index evaluates the average similarity ratio of each cluster with the most similar cluster. A lower index indicates better clustering.

        \textbf{Formula:}
        \begin{equation}
            DB = \frac{1}{n} \sum_{i=1}^{n} \max_{j \neq i} \left( \frac{s_i + s_j}{d(c_i, c_j)} \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $s_i$: Average distance of points in cluster $i$ to the centroid of $i$.
            \item $d(c_i, c_j)$: Distance between centroids $c_i$ and $c_j$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Clustering Models - Key Points & Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Clustering evaluation metrics clarify the quality of the clustering model.
            \item A high silhouette score indicates well-defined clusters.
            \item A lower Davies-Bouldin Index signifies distinct and well-separated clusters.
            \item Visualizations, such as silhouette plots or cluster centroid plots, should complement numerical evaluations.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Clustering evaluation metrics like the Silhouette Score and Davies-Bouldin Index provide essential insights into the effectiveness of clustering algorithms. Selecting the appropriate metric is vital for meaningful groupings in various applications, from customer segmentation to image processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Clustering - Introduction}
    \begin{itemize}
        \item Clustering is a technique in unsupervised learning for grouping similar data points.
        \item Several challenges can impact the effectiveness and accuracy of clustering results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Clustering - Part 1: Scaling Algorithms}
    \begin{block}{Scaling Clustering Algorithms}
        \begin{itemize}
            \item \textbf{High Dimensionality}: Increasing features make data sparse, complicating pattern identification.
            \item \textbf{Data Size}: Larger datasets require more resources, leading to computational inefficiencies.
            \item \textbf{Example}: In high-dimensional data (like text), traditional distance measures become ineffective.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Clustering - Part 2: Selecting the Right Number of Clusters}
    \begin{block}{Selecting the Right Number of Clusters}
        \begin{itemize}
            \item \textbf{Description}: Determining the optimal number of clusters (K) is crucial.
            \item \textbf{Challenges}:
                \begin{itemize}
                    \item Overfitting: Too many clusters can reflect noise.
                    \item Underfitting: Too few clusters may miss important patterns.
                \end{itemize}
            \item \textbf{Methods}:
                \begin{enumerate}
                    \item Elbow Method: Finds an optimal point by plotting explained variance against K.
                    \item Silhouette Score: $S = \frac{b - a}{\max(a, b)}$ 
                    \begin{itemize}
                        \item $a$: average distance to points in the same cluster.
                        \item $b$: average distance to points in the nearest cluster.
                    \end{itemize}
                \end{enumerate}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Clustering - Part 3: Performance of Algorithms}
    \begin{block}{Performance of Clustering Algorithms}
        \begin{itemize}
            \item \textbf{Description}: Evaluating clustering algorithm performance can be subjective.
            \item \textbf{Challenges}:
                \begin{itemize}
                    \item Interpretability: Results vary across algorithms (e.g., K-Means, DBSCAN).
                    \item Stability: Different runs may yield different results, especially in stochastic algorithms.
                \end{itemize}
            \item \textbf{Example}: With K-Means, the initial selection of centroids can significantly influence outcomes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Clustering - Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Key Points}:
            \begin{itemize}
                \item Consider data scaling and normalization.
                \item Use statistical methods for cluster number optimization.
                \item Understand data characteristics and algorithms for meaningful interpretations.
            \end{itemize}
        \item \textbf{Conclusion}: Addressing these challenges is crucial for effective clustering in real-world applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Introduction}
    \begin{itemize}
        \item Clustering is an unsupervised learning technique used to group similar items or data points into clusters based on defined features.
        \item It enables organizations across various industries to:
        \begin{itemize}
            \item Uncover patterns within data
            \item Derive insights for better decision-making
            \item Enhance operational efficiency and strategic planning
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Real-World Uses}

    \begin{enumerate}
        \item \textbf{Marketing}
        \begin{itemize}
            \item \textbf{Customer Segmentation}: Group customers by behavior, demographics, or preferences.
              \begin{itemize}
                  \item \textit{Example:} Grocery stores can create personalized promotions for different customer segments.
              \end{itemize}
            \item \textbf{Targeted Advertising}: Identify which clusters respond better to specific advertisements.
        \end{itemize}
        
        \item \textbf{Biology}
        \begin{itemize}
            \item \textbf{Genetic Research}: Categorize genes based on expression profiles for identifying functions and potential drug targets.
              \begin{itemize}
                  \item \textit{Example:} Hierarchical clustering can uncover groups of genes with similar behaviors under diverse conditions.
              \end{itemize}
            \item \textbf{Ecology and Environmental Studies}: Group similar species or habitats to facilitate biodiversity assessments.
        \end{itemize}
        
        \item \textbf{Image Processing}
        \begin{itemize}
            \item \textbf{Image Compression}: Reduce the number of colors in an image using clustering algorithms.
            \item \textbf{Object Recognition}: Group similar pixels to identify objects within images.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Key Points}
    \begin{itemize}
        \item \textbf{Diverse Applications}: Useful in various fields like marketing, biology, and image processing.
        \item \textbf{Insight Generation}: Helps organizations extract meaningful insights for strategic advantages.
        \item \textbf{Methodological Flexibility}: Different algorithms like K-means, hierarchical clustering, and DBSCAN can be tailored for specific needs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Conclusion}
    \begin{itemize}
        \item The versatility of clustering techniques showcases their importance in modern data analysis.
        \item By leveraging these applications, organizations can:
        \begin{itemize}
            \item Enhance strategies
            \item Improve operational efficiency
            \item Achieve better outcomes
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Code Snippet}
    \begin{block}{Example: Basic K-means Clustering}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

# Sample data points (2D array)
data = np.array([[1, 2], [1, 4], [1, 0],
                 [4, 2], [4, 4], [4, 0]])

# Create KMeans object
kmeans = KMeans(n_clusters=2, random_state=0)

# Fit the model
kmeans.fit(data)

# Get cluster labels and centroids
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

print("Labels:", labels)
print("Centroids:", centroids)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Customer Segmentation}
    \begin{block}{Introduction to Customer Segmentation}
        Customer segmentation is the process of dividing a customer base into distinct groups with similar characteristics. Clustering plays a vital role by identifying patterns without pre-labeled outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Clustering for Customer Segmentation?}
    \begin{itemize}
        \item \textbf{Personalized Marketing:} Tailoring strategies based on customer preferences and behaviors.
        \item \textbf{Targeted Communications:} Crafting specific messages for different groups to enhance engagement.
        \item \textbf{Resource Optimization:} Efficiently allocating resources to high-potential segments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Clustering Works}
    \begin{enumerate}
        \item \textbf{Data Collection:} Gather data such as purchase history and demographics.
        \item \textbf{Feature Selection:} Identify relevant features (e.g., age, income).
        \item \textbf{Clustering Algorithm:} Use algorithms like K-Means to group customers.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: K-Means Clustering in Action}
    \begin{enumerate}
        \item \textbf{Step 1:} Choose the number of clusters (e.g., \(k = 3\)).
        \item \textbf{Step 2:} Initialize cluster centroids randomly.
        \item \textbf{Step 3:} Assign each customer to the nearest centroid.
        \item \textbf{Step 4:} Update centroids based on the mean of cluster points.
        \item \textbf{Step 5:} Repeat steps until assignments no longer change.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Customer Segments Identified}
    \begin{itemize}
        \item \textbf{High-Value Customers:} Frequent purchasers with high spending.
        \item \textbf{Loyal Customers:} Repeat buyers with moderate spending.
        \item \textbf{Occasional Shoppers:} Rare buyers with lower engagement.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Data-Driven Decisions:} Makes informed decisions based on behavior, not assumptions.
        \item \textbf{Dynamic Adaptation:} Clusters evolve over time, requiring re-evaluation.
        \item \textbf{Real-World Impact:} Effective segmentation can lead to increased sales and improved customer retention.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Clustering provides insights into customer preferences, enabling businesses to implement targeted strategies that drive growth and profitability.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Example}
    To calculate the distance between a customer point \(X\) and a cluster centroid \(C\):
    \begin{equation}
    d(X, C) = \sqrt{\sum_{i=1}^{n}(X_i - C_i)^2}
    \end{equation}
    Where:
    \begin{itemize}
        \item \(n\) = number of features,
        \item \(X_i\) = feature values of customer \(X\),
        \item \(C_i\) = feature values of centroid \(C\).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for K-Means Clustering (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import pandas as pd

# Load customer data
data = pd.read_csv('customer_data.csv')

# Select features for clustering
features = data[['age', 'annual_income', 'spending_score']]

# Initialize KMeans
kmeans = KMeans(n_clusters=3)
data['Cluster'] = kmeans.fit_predict(features)

# Display segmented customers
print(data.groupby('Cluster').mean())
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Ethical Considerations in Clustering}
    Clustering algorithms are powerful tools for identifying patterns and groupings within data. However, their use carries important ethical implications, particularly regarding potential biases that can affect outcomes, leading to unfair treatment and discrimination.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points of Ethical Considerations}
    \begin{itemize}
        \item \textbf{Potential Biases in Clustering}:
        \begin{itemize}
            \item \textbf{Data Bias}: Unrepresentative training datasets can perpetuate historical biases. 
            \item \textbf{Feature Selection}: Choice of features can introduce bias, e.g., socio-economic status can lead to discrimination against lower-income populations.
        \end{itemize}
        \item \textbf{Ethical Implications}:
        \begin{itemize}
            \item \textbf{Discrimination}: Clustering may lead to exclusionary practices based on sensitive attributes.
            \item \textbf{Transparency}: Opaque algorithms risk mistrust in the clustering process.
            \item \textbf{Accountability}: Organizations must bear responsibility for clustering outcomes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    \textbf{Use Case:} A retail company applying clustering for customer segmentation may inadvertently create clusters that isolate minority groups due to biased data.

    \textbf{Potential Cluster Example:}
    \begin{itemize}
        \item Cluster 1: High income, predominantly over 50 years old
        \item Cluster 2: Low income, predominantly under 30 years old
    \end{itemize}
    
    Tailored marketing strategies intended for these clusters may not account for the needs of intermediate groups.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Mitigate Bias}
    \begin{enumerate}
        \item \textbf{Diverse Data Collection}: Ensure the data represents a variety of backgrounds and demographics.
        \item \textbf{Regular Audits}: Conduct routine evaluations on clustering outputs to identify biases.
        \item \textbf{Inclusive Algorithm Design}: Include diverse stakeholder perspectives in the algorithm design process.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the ethical dimensions of clustering algorithms is essential for machine learning practitioners. Addressing biases enhances fairness and contributes to more equitable systems in society.

    \textbf{Remember:} Ethics in data science is a shared responsibility. Aim for transparency, inclusiveness, and accountability in every phase of your project.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future of Clustering Techniques}
    \begin{block}{Overview of Emerging Trends}
        This presentation covers the advancements and future directions in clustering research.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advancements in Algorithmic Development}
    \begin{itemize}
        \item \textbf{Improved Scalability:} Development of algorithms like Approximate Nearest Neighbors (ANN) and Mini-Batch K-Means aimed at efficiently handling larger datasets.
        \item \textbf{Hybrid Approaches:} Combining various clustering techniques (e.g., hierarchical and density-based clustering) to enhance clustering quality.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration with Deep Learning}
    \begin{itemize}
        \item \textbf{Deep Clustering:} Emergence of methods like Deep-Embedded Clustering (DEC) that utilize neural networks for feature extraction prior to clustering.
        \item \textbf{Example:} Clustering image datasets using features from convolutional neural networks (CNNs).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling High-Dimensional Data and Ethical Considerations}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item \textbf{Dimensionality Reduction:} Techniques like t-SNE and UMAP that help visualize and cluster high-dimensional data effectively.
        \end{itemize}
        
        \column{0.5\textwidth}
        \begin{itemize}
            \item \textbf{Bias Mitigation:} Focus on ensuring fairness in clustering algorithms with techniques for bias detection and correction, especially in sensitive areas like social media and criminal justice.
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-time Clustering and Interpretability}
    \begin{itemize}
        \item \textbf{Streaming Data Clustering:} Necessity for clustering algorithms designed for streaming data—e.g., CluStream and DenStream for dynamic clustering.
        \item \textbf{Making Clusters Human-Readable:} Emphasis on interpretability in clustering outcomes, such as prototype examples and rules that can be easily explained to end-users.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithm Robustness and Domain-Specific Innovations}
    \begin{itemize}
        \item \textbf{Probabilistic Clustering Models:} Refinement of models like Gaussian Mixture Models (GMM) for more stable results in uncertain scenarios.
        \item \textbf{Tailored Algorithms:} Adaptation of clustering solutions for domain-specific applications such as genomics, finance, and social networks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Clustering techniques are evolving, especially in big data and deep learning contexts.
        \item Major focus on addressing ethical concerns and ensuring cluster interpretability.
        \item Future research aims for algorithm robustness while catering to specific domain needs.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical Session: Implementing K-Means}
    \begin{block}{Overview of K-Means Clustering}
        K-Means is an unsupervised learning algorithm that partitions a dataset into K distinct clusters based on feature similarities. It's widely used for market segmentation, image compression, and pattern recognition.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{How K-Means Works}
    \begin{enumerate}
        \item \textbf{Initialization:} Select K initial cluster centroids (randomly or using methods like K-Means++).
        \item \textbf{Assignment Step:} Assign each data point to the nearest centroid, creating K clusters.
        \item \textbf{Update Step:} Recalculate centroids by taking the mean of all points in each cluster.
        \item \textbf{Repeat:} Repeat steps 2 and 3 until the centroids no longer change or a specified number of iterations is reached.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Formula for K-Means}
    The cost function (objective function) for K-Means is: 
    \begin{equation}
        J = \sum_{k=1}^K \sum_{x_i \in C_k} \| x_i - \mu_k \|^2
    \end{equation}
    where:
    \begin{itemize}
        \item \( J \): total cost
        \item \( C_k \): cluster k
        \item \( \mu_k \): centroid of cluster k
        \item \( x_i \): data point in cluster k
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Dataset}
    Consider the following 2D dataset for clustering:
    \begin{itemize}
        \item Points: (1, 2), (1, 4), (1, 0), (4, 2), (4, 4), (4, 0)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Implementing K-Means in Python}
    Here’s a hands-on implementation using Python and the \texttt{scikit-learn} library.
    \begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Sample data
X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])

# Implementing K-Means
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
y_kmeans = kmeans.predict(X)

# Visualizing the results
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)
plt.title("K-Means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Choosing K:} Experiment with various values of K; methods like the elbow method or silhouette score can help determine the optimal number.
        \item \textbf{Distance Metrics:} K-Means typically uses Euclidean distance, but alternatives (like Manhattan distance) could be used for different types of data distributions.
        \item \textbf{Scalability:} While efficient, K-Means does not perform well with non-globular or high-dimensional data.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Hands-On Task}
    \begin{enumerate}
        \item Load a dataset (e.g., the Iris dataset).
        \item Apply K-Means with different values of K.
        \item Visualize the clusters and evaluate their coherence.
    \end{enumerate}
    This session will build your understanding of clustering algorithms and provide practical skills in implementing K-Means in real-world data analysis scenarios.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrap-up and Key Takeaways}

    \begin{block}{Key Points from Chapter 10: Unsupervised Learning: Clustering}
        \begin{itemize}
            \item Definition of Unsupervised Learning
            \item Clustering Overview
            \item Common Clustering Algorithms
            \item Applications and Relevance
            \item Challenges in Clustering
            \item Key Takeaway
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definition and Overview}

    \begin{itemize}
        \item \textbf{Definition of Unsupervised Learning}:
        \begin{itemize}
            \item Unsupervised learning involves training models on unlabeled data to identify patterns.
        \end{itemize}
        
        \item \textbf{Clustering Overview}:
        \begin{itemize}
            \item Groups objects such that similar objects are in the same cluster.
            \item \textit{Example:} Grouping customers by purchase behavior.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Clustering Algorithms}

    \begin{enumerate}
        \item \textbf{K-Means}:
        \begin{itemize}
            \item Partitions data into K clusters by minimizing variance.
            \item \textit{Key Steps}:
            \begin{itemize}
                \item Choose K
                \item Initialize K centroids
                \item Assign points to nearest centroid
                \item Recalculate centroids
                \item Repeat until convergence
            \end{itemize}
            \item \textit{Example Code}:
            \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

data = np.array([[1, 2], [1, 4], [1, 0],
                 [4, 2], [4, 4], [4, 0]])
kmeans = KMeans(n_clusters=2, random_state=0).fit(data)
print(kmeans.labels_)  # Clustering results
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Hierarchical Clustering}:
        \begin{itemize}
            \item Builds a dendrogram based on similarity of data points.
        \end{itemize}

        \item \textbf{DBSCAN}:
        \begin{itemize}
            \item Groups close points based on a distance measurement, handling noise effectively.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications, Challenges, and Key Takeaway}

    \begin{itemize}
        \item \textbf{Applications and Relevance}:
        \begin{itemize}
            \item Market Segmentation
            \item Image Compression
            \item Anomaly Detection
        \end{itemize}

        \item \textbf{Challenges in Clustering}:
        \begin{itemize}
            \item Determining optimal K
            \item Sensitivity to initialization
            \item Validation complexity without labels
        \end{itemize}

        \item \textbf{Key Takeaway}:
        \begin{itemize}
            \item Clustering techniques are crucial for data exploration and feature engineering.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps}

    \begin{itemize}
        \item \textbf{Summary}:
        \begin{itemize}
            \item Clustering identifies patterns within complex datasets.
        \end{itemize}
        
        \item \textbf{Next Steps}:
        \begin{itemize}
            \item Prepare for the Q\&A Session to clarify uncertainties.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Q\&A Session - Overview}
  
  \begin{block}{Objective}
    To create an interactive space for students to ask questions and clarify their doubts regarding unsupervised learning, specifically focusing on clustering techniques.
  \end{block}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Q\&A Session - Key Concepts to Review}
  
  \begin{itemize}
    \item \textbf{Unsupervised Learning:}
      \begin{itemize}
        \item A type of machine learning where the algorithm learns patterns from unlabelled data.
        \item Key focus: Discovering underlying structures.
      \end{itemize}
    
    \item \textbf{Clustering:}
      \begin{itemize}
        \item A method of grouping data points based on similarity.
        \item Common Algorithms:
          \begin{itemize}
            \item \textbf{K-Means Clustering:} Partitions data into K clusters by minimizing variance.
            \item \textbf{Hierarchical Clustering:} Builds a tree of clusters based on similarity.
            \item \textbf{DBSCAN:} Groups points based on density.
          \end{itemize}
      \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Q\&A Session - Common Questions}
  
  \begin{enumerate}
    \item \textbf{How do I choose the right number of clusters in K-Means?}
      \begin{itemize}
        \item Consider using the Elbow Method, which suggests plotting the WCSS (Within-Cluster Sum of Squares) against the number of clusters to find a ‘knee’ point.
      \end{itemize}
      
    \item \textbf{What metrics can be used to evaluate clustering effectiveness?}
      \begin{itemize}
        \item Common metrics include Silhouette Score, Davies-Bouldin Index, or visualizing clusters using a scatter plot.
      \end{itemize}
      
    \item \textbf{What are the limitations of clustering algorithms?}
      \begin{itemize}
        \item Algorithms may not work well with clusters of varying sizes or densities. It’s important to understand the data distribution and choose the appropriate algorithm accordingly.
      \end{itemize}
  \end{enumerate}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Q\&A Session - Engagement Tips and Conclusion}
  
  \begin{block}{Engagement Tips}
    \begin{itemize}
      \item Encourage students to come prepared with specific scenarios or datasets they are interested in clustering.
      \item Suggest they think of practical applications of clustering in various fields, such as:
        \begin{itemize}
          \item Marketing (customer segmentation)
          \item Biology (species classification)
          \item Image processing
        \end{itemize}
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    This session is your opportunity for deeper understanding — no question is too small or trivial! Let's clarify concepts and explore clustering in-depth! Feel free to reach out with questions around the metrics, algorithm selection, or practical implementations of clustering.
  \end{block}

\end{frame}


\end{document}