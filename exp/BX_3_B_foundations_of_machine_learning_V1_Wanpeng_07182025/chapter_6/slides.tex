\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Feature Engineering]{Chapter 6: Feature Engineering}
\author[J. Smith]{John Smith, Ph.D.}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Feature Engineering}
    \begin{block}{What is Feature Engineering?}
        Feature engineering is the process of using domain knowledge to extract features (input variables) from raw data. This transformation helps improve model performance and predictive accuracy.
    \end{block}

    \begin{block}{Why is Feature Engineering Important?}
        \begin{itemize}
            \item \textbf{Enhanced Model Performance}: Improves prediction accuracy with meaningful, relevant information.
            \item \textbf{Reduction of Overfitting}: Helps models generalize better on unseen data.
            \item \textbf{Improved Interpretability}: Creates clearer insights for stakeholders on feature impacts.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Feature Engineering}
    \begin{enumerate}
        \item \textbf{Feature Creation}
            \begin{itemize}
                \item Combining features (e.g., BMI from height and weight).
                \item Extracting date/time features (e.g., day, month, year from timestamps).
            \end{itemize}
        \item \textbf{Feature Transformation}
            \begin{itemize}
                \item Scaling: Normalizing features to a standard range (e.g., Min-Max Scaling).
                \item Encoding: Converting categorical variables to numerical formats (e.g., One-Hot Encoding).
            \end{itemize}
        \item \textbf{Feature Selection}
            \begin{itemize}
                \item Identifying the most relevant features using methods like Recursive Feature Elimination and LASSO regression.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example and Conclusion}
    \begin{block}{Example: Predicting House Prices}
        Consider a dataset with variables such as:
        \begin{itemize}
            \item Square footage
            \item Number of bedrooms
            \item Year built
        \end{itemize}
        Feature engineering could derive:
        \begin{itemize}
            \item Age of the house = Current year - Year built
            \item Price per square foot = Price / Square footage
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Feature engineering is critical in machine learning, requiring creativity and an understanding of data to generate informative features that enhance model predictions.
    \end{block}

    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Enhances model performance and reduces overfitting.
            \item Involves creation, transformation, and selection of features.
            \item Effective engineering promotes both accuracy and interpretability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Scaling Example}
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler

# Sample data
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]

# Create a scaler
scaler = MinMaxScaler()

# Fit and transform the data
scaled_data = scaler.fit_transform(data)
print(scaled_data)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Features - Definition}
    \begin{block}{Definition of Features}
        In the context of machine learning, \textbf{features} are individual measurable properties or characteristics of the phenomenon being observed. Each feature represents an input variable in a dataset that the machine learning model uses to learn patterns and make predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Features - Role in Machine Learning}
    \begin{block}{Role of Features in Machine Learning Algorithms}
        \begin{enumerate}
            \item \textbf{Input Representation}: Features serve as the input to machine learning algorithms, transforming raw data into a structured format for analysis.
            \item \textbf{Influence on Model Performance}: The selection and representation of features impact the model's accuracy and effectiveness, enhancing the model’s ability to generalize.
            \item \textbf{Feature Space}: Each feature corresponds to a dimension in the feature space, allowing algorithms to navigate complex relationships.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Features - Key Points and Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Importance of Feature Selection}: Selecting the right features is crucial for achieving high model performance.
            \item \textbf{Feature Transformation}: Raw features may require transformation, such as normalization or encoding categorical variables.
        \end{itemize}
    \end{block}

    \begin{block}{Example Scenario: Predicting Fruit Quality}
        \begin{itemize}
            \item \textbf{Features}:
                \begin{itemize}
                    \item \textbf{Weight}: Measure of the fruit's weight in grams.
                    \item \textbf{Color}: Categorical feature indicating the fruit's color.
                    \item \textbf{Sweetness Level}: A score measuring sweetness on a scale of 1 to 10.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Features - Code Snippet}
    \begin{block}{Simple Code Snippet (Python)}
        \begin{lstlisting}[language=Python]
import pandas as pd

# Creating a DataFrame with features
data = {
    'Weight': [150, 200, 120, 180],
    'Color': ['Red', 'Green', 'Yellow', 'Red'],
    'Sweetness_Level': [8, 6, 9, 7]
}

df = pd.DataFrame(data)

# Viewing the DataFrame
print(df)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Features - Introduction}
    \begin{block}{Introduction to Feature Types}
        In machine learning, features are the individual measurable properties or characteristics of the data used by algorithms to make predictions. 
        Understanding the types of features is crucial as it influences:
        \begin{itemize}
            \item Model performance
            \item Interpretability
            \item Strategy for data preprocessing
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Features - Categorization}
    \begin{block}{Categorization of Features}
        \begin{itemize}
            \item **Raw Features**: Original data collected without modifications.
            \item **Engineered Features**: Derived from raw features to improve model performance.
            \item **Categorical Features**: Discrete values that fall into distinct categories.
            \item **Numerical Features**: Continuous or discrete measurable values.
            \item **Text Features**: Derived from textual data and transformed into usable formats for algorithms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Features - Detailed Examples}
    \begin{block}{Detailed Examples}
        \begin{itemize}
            \item \textbf{Raw Features:} 
                \begin{itemize}
                    \item Example: Square footage, number of bedrooms.
                \end{itemize}
            \item \textbf{Engineered Features:} 
                \begin{itemize}
                    \item Example: Price per square foot.
                \end{itemize}
            \item \textbf{Categorical Features:} 
                \begin{itemize}
                    \item Example: Gender, Country, Car Type.
                \end{itemize}
            \item \textbf{Numerical Features:} 
                \begin{itemize}
                    \item Example: Age, Number of Products Sold.
                \end{itemize}
            \item \textbf{Text Features:} 
                \begin{itemize}
                    \item Example: Customer reviews or tweets, transformed using NLP techniques.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Features - Summary and Practical Considerations}
    \begin{block}{Summary}
        Understanding the different types of features is essential for effective feature engineering, enhancing model's predictive power and interpretability.
    \end{block}
    
    \begin{block}{Practical Considerations}
        \begin{itemize}
            \item Examine your dataset to identify existing feature types.
            \item Use domain knowledge to guide feature engineering efforts.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Features - Code Example}
    \begin{block}{Code Snippet Example}
    Here is an example of converting a categorical feature into a numerical format using Python with \texttt{pandas}.
    \begin{lstlisting}[language=Python]
import pandas as pd

# Sample DataFrame
data = pd.DataFrame({
    'Country': ['USA', 'Canada', 'UK', 'USA', 'Canada'],
    'Sales': [100, 200, 150, 300, 250]
})

# One-hot encoding for categorical feature
encoded_data = pd.get_dummies(data, columns=['Country'], drop_first=True)
print(encoded_data)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Feature Selection - Introduction and Key Concepts}
    \begin{block}{Introduction}
        Feature selection is critical in building effective machine learning models. It involves selecting a subset of relevant features to enhance:
        \begin{itemize}
            \item Model accuracy
            \item Interpretability
            \item Performance
        \end{itemize}
    \end{block}

    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Model Accuracy}
            \begin{itemize}
                \item Irrelevant features can introduce noise, leading to overfitting and decreased accuracy.
                \item \textit{Example:} In predicting house prices, unrelated features can mislead the model.
            \end{itemize}
            \item \textbf{Interpretability}
            \begin{itemize}
                \item Simpler models are easier to interpret and trusted by stakeholders.
                \item \textit{Example:} A model with fewer features is easier to understand than one with many.
            \end{itemize}
            \item \textbf{Performance}
            \begin{itemize}
                \item Reducing features decreases computation costs and improves efficiency.
                \item \textit{Example:} Image recognition tasks are faster with selected features rather than high-dimensional data.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Feature Selection - Key Points and Techniques}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Avoid curse of dimensionality.
            \item Improve generalization and reduce overfitting.
            \item Facilitate hypothesis testing.
        \end{itemize}
    \end{block}

    \begin{block}{Techniques for Feature Selection}
        \begin{itemize}
            \item \textbf{Filter Methods:} Evaluate features based on statistical measures (e.g., correlation).
            \item \textbf{Wrapper Methods:} Use model performance feedback for feature selection (e.g., forward selection).
            \item \textbf{Embedded Methods:} Integrate feature selection within model training (e.g., Lasso regression).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Feature Selection - Conclusion and Diagram}
    \begin{block}{Conclusion}
        Effective feature selection is crucial in the data preprocessing phase of machine learning. By refining features, practitioners can build models that are:
        \begin{itemize}
            \item Accurate
            \item Interpretable
            \item High-performing
        \end{itemize}
        This ultimately leads to improved decision-making.
    \end{block}

    \begin{block}{Diagram: Feature Selection Process}
        \begin{itemize}
            \item Start with Full Dataset
            \item Evaluate Feature Importance
            \item Select Relevant Features
            \item Train Final Model
            \item Evaluate Model Performance
        \end{itemize}
        This process ensures robust models tailored to specific problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques}
    \begin{block}{Overview}
        Feature selection is crucial for data preprocessing. It involves choosing a relevant subset of features to enhance model performance, reduce overfitting, improve accuracy, and decrease computation time. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Filter Methods}
    \begin{block}{Description}
        Filter methods use statistical measures to assess feature relevance without machine learning algorithms. They rank features based on criteria and select top features.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Correlation Coefficients}:
        \begin{itemize}
            \item Measures linear relationships using Pearson or Spearman correlations.
            \item \textit{Formula}: 
            \begin{equation}
            r = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}
            \end{equation}
        \end{itemize}
        
        \item \textbf{Chi-Squared Test}:
            \begin{itemize}
                \item Tests independence of categorical features from the target variable.
            \end{itemize}
    \end{itemize}

    \begin{block}{Key Point}
        Fast and computationally efficient, but can ignore feature interactions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Wrapper Methods}
    \begin{block}{Description}
        Wrapper methods evaluate feature subsets based on a specific model's performance, using search algorithms to find optimal feature sets.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Recursive Feature Elimination (RFE)}:
            \begin{itemize}
                \item Starts with all features and recursively removes the least significant ones based on performance.
            \end{itemize}
        
        \item \textbf{Genetic Algorithms}:
            \begin{itemize}
                \item Evolve feature sets via natural selection processes.
            \end{itemize}
    \end{itemize}

    \begin{block}{Key Point}
        More accurate but computationally expensive, as models are trained for each subset.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Embedded Methods}
    \begin{block}{Description}
        Embedded methods perform feature selection as part of model training, integrating the process within the learning algorithm itself.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Lasso Regression}:
            \begin{itemize}
                \item A linear regression model with L1 regularization that selects features by forcing some coefficients to be zero.
                \item \textit{Lasso Loss Function}:
                \begin{equation}
                L(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
                \end{equation}
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Point}
        Combines advantages of filter and wrapper methods, being efficient while considering feature interactions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools for Feature Selection}
    \begin{itemize}
        \item \textbf{Correlation Matrix}:
            \begin{itemize}
                \item Visual representation of correlation coefficients among features.
                \item Identifies redundant features that are highly correlated.
            \end{itemize}
        
        \item \textbf{Lasso Regression}:
            \begin{itemize}
                \item Practical for feature selection and regularization in regression models.
            \end{itemize}
    \end{itemize}

    \begin{block}{Conclusion}
        Feature selection techniques are essential for effective machine learning models. Utilizing filter, wrapper, and embedded methods, together with tools like correlation matrices and Lasso regression, leads to improved model performance and interpretability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering Process - Overview}
    \begin{block}{Introduction}
        Feature engineering is a crucial step in the machine learning pipeline that involves creating, transforming, and selecting input variables to enhance model performance.
        This systematic approach breaks feature engineering into three core phases:
        \begin{itemize}
            \item Identification
            \item Transformation
            \item Evaluation
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering Process - Identification}
    \begin{block}{Identification of Features}
        \begin{itemize}
            \item \textbf{Definition}: Identify relevant features that may influence the output variable.
            \item \textbf{Techniques}:
            \begin{itemize}
                \item \textbf{Domain Knowledge}: Leverage expertise to recognize significant features.
                \item \textbf{Exploratory Data Analysis (EDA)}: Use visualizations like histograms and scatter plots to identify patterns.
                \item \textbf{Statistical Tests}: Perform tests like Chi-square to check associations with the target variable.
            \end{itemize}
            \item \textbf{Example}: For a house price prediction model, features like location, size, number of bedrooms, and age of the property are significant.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering Process - Transformation and Evaluation}
    \begin{block}{Transformation of Features}
        \begin{itemize}
            \item \textbf{Definition}: Modify features to optimize their representation in the model.
            \item \textbf{Common Techniques}:
            \begin{itemize}
                \item \textbf{Normalization}:
                \[
                X' = \frac{X - X_{min}}{X_{max} - X_{min}}
                \]
                \item \textbf{Standardization}:
                \[
                X' = \frac{X - \mu}{\sigma}
                \]
                \item \textbf{Encoding}: Convert categorical variables into numerical values (e.g., one-hot encoding).
            \end{itemize}
            \item \textbf{Example}: Transforming 'City' into three binary features (is\_NY, is\_LA, is\_Chicago).
        \end{itemize}
    \end{block}

    \begin{block}{Evaluation of Features}
        \begin{itemize}
            \item \textbf{Definition}: Assess the effectiveness of features through evaluation metrics and model performance.
            \item \textbf{Methods}:
            \begin{itemize}
                \item \textbf{Feature Importance}: Use algorithms to gauge importance scores.
                \item \textbf{Cross-validation}: Analyze model performance with selected features.
                \item \textbf{Visualization}: Use correlation matrices to determine relationships.
            \end{itemize}
            \item \textbf{Example}: If 'square footage' shows high importance while 'age of property' shows low, it may benefit to exclude the latter.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transforming Features - Introduction}
    \begin{block}{Overview}
        Feature transformation is essential in the feature engineering process. It enhances model performance by preparing data through methods such as:
        \begin{enumerate}
            \item Normalization
            \item Standardization
            \item Encoding for categorical data
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transforming Features - Normalization}
    \begin{block}{Normalization}
        Normalization rescales numeric feature values to a common range, typically [0, 1].
        \begin{itemize}
            \item \textbf{Formula:} 
            \[
            X' = \frac{X - \text{min}(X)}{\text{max}(X) - \text{min}(X)}
            \]
            \item \textbf{Example:} 
                \begin{itemize}
                    \item Feature A: [10, 20, 30, 40, 50]
                    \item Normalized values: [0, 0.25, 0.5, 0.75, 1]
                \end{itemize}
            \item \textbf{Key Point:} 
                Normalization is beneficial for algorithms relying on distance metrics (e.g., KNN, Neural Networks).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transforming Features - Standardization}
    \begin{block}{Standardization}
        Standardization transforms features to have a mean of 0 and a standard deviation of 1.
        \begin{itemize}
            \item \textbf{Formula:} 
            \[
            X' = \frac{X - \mu}{\sigma}
            \]
            where $\mu$ is the mean and $\sigma$ is the standard deviation.
            \item \textbf{Example:} 
                \begin{itemize}
                    \item Feature B: [50, 60, 70, 80, 90]
                    \item Mean ($\mu$): 70, Standard Deviation ($\sigma$): 14.14 
                    \item Standardized values: [-1.41, -0.71, 0, 0.71, 1.41]
                \end{itemize}
            \item \textbf{Key Point:} 
                Standardization is best for algorithms assuming Gaussian distribution (e.g., Linear Regression).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transforming Features - Encoding Categorical Data}
    \begin{block}{Encoding Techniques}
        Many algorithms require numerical input, necessitating the transformation of categorical features.
        \begin{itemize}
            \item \textbf{Label Encoding:} Assigns integers to categories.
                \begin{itemize}
                    \item Example: ["Red", "Green", "Blue"] → [0, 1, 2]
                \end{itemize}
            \item \textbf{One-Hot Encoding:} Creates binary columns for each category.
                \begin{itemize}
                    \item Example for "Color":
                        \begin{itemize}
                            \item Red → [1, 0, 0]
                            \item Green → [0, 1, 0]
                            \item Blue → [0, 0, 1]
                        \end{itemize}
                \end{itemize}
            \item \textbf{Key Point:} 
                One-hot encoding prevents models from treating category values as ordinal.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transforming Features - Conclusion}
    \begin{block}{Conclusion}
        Proper feature transformation enhances model learning and performance. Consider the data distribution and model requirements while selecting a transformation method.
    \end{block}
    \begin{block}{Next Steps}
        In the next slide, we will explore techniques for creating new features from existing data to further enhance our model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating New Features}
    \begin{block}{Overview of Feature Creation}
        Feature engineering is a critical step in the machine learning pipeline. It involves creating new features from existing data to enhance model performance and improve interpretability. The right features can capture underlying patterns and relationships that raw data cannot.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating New Features - Polynomial Features}
    \begin{itemize}
        \item Polynomial features allow for non-linearity in data.
        \item Transform a feature \(x\) into \(x, x^2, x^3, \ldots, x^n\).
    \end{itemize}
    
    \begin{block}{Example}
        Original Feature: 
        \[
        x = [1, 2, 3]
        \] 
        Polynomial Features (up to degree 2):
        \[
        x^2 = [1, 4, 9]
        \end{block}
    
    \begin{block}{Python Code}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating New Features - Interaction Terms}
    \begin{itemize}
        \item Interaction terms consider the relationship between features.
        \item Useful when the effect of one feature depends on another.
    \end{itemize}
    
    \begin{block}{Example}
        Original Features:
        \begin{itemize}
            \item \(A = [1, 2, 3]\)
            \item \(B = [4, 5, 6]\)
        \end{itemize}
        Interaction Feature:
        \[
        A \times B = [4, 10, 18]
        \end{block}
    
    \begin{block}{Python Code}
        \begin{lstlisting}[language=Python]
import pandas as pd

df['A_B_interaction'] = df['A'] * df['B']
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating New Features - Domain-Specific Knowledge}
    \begin{itemize}
        \item Domain knowledge guides feature construction.
        \item Essential for capturing data nuances not apparent in raw data.
    \end{itemize}
    
    \begin{block}{Example}
        In a real estate dataset:
        \begin{itemize}
            \item Transform “size in square feet” into “Size Category”: 
            \[
            \text{{\{'small': 0, 'medium': 1, 'large': 2\}}}
            \end{block}
        \end{itemize}
        
        \begin{block}{Key Points}
            \begin{itemize}
                \item Polynomial and interaction terms model complex relationships effectively.
                \item Use domain knowledge to enhance feature design.
                \item Thoughtfully engineered features improve model accuracy and interpretability.
            \end{itemize}
        \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Notes on Feature Creation}
    \begin{block}{Considerations}
        Building new features is not a one-size-fits-all approach; it requires experimentation and validation. 
        \begin{itemize}
            \item Monitor model performance as new features are introduced.
            \item Not all features will enhance model capabilities.
        \end{itemize}
    \end{block}
    
    By applying these techniques, you can enrich your dataset and potentially improve your machine learning models significantly!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Feature Impact}
    \begin{block}{Overview}
        Evaluating the impact of features on model performance is crucial in the machine learning pipeline. 
        Understanding how each feature contributes to the predictive power of a model allows refined feature sets, leading to enhanced performance and interpretability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Feature Importance}
    \begin{itemize}
        \item \textbf{Feature Importance}:
        \begin{itemize}
            \item Measures the contribution of each feature to the prediction.
            \item Techniques include:
            \begin{itemize}
                \item \textbf{Tree-Based Methods}: Automatically calculate feature importance based on impurity reduction in trees (e.g., Random Forest, XGBoost).
                \item \textbf{Permutation Importance}: Randomly permute feature values and assess how model accuracy is affected.
            \end{itemize}
        \end{itemize}
        \item \textbf{Example}: In a Random Forest model, if the feature "age" significantly reduces impurity, it may be deemed important for predicting the target variable.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Metrics and Validation Strategies}
    \begin{block}{Metrics for Evaluation}
        \begin{itemize}
            \item \textbf{Accuracy}: Proportion of correct predictions; useful for classification.
            \item \textbf{F1 Score}: Harmonic mean of precision and recall; important for imbalanced datasets.
            \item \textbf{Mean Squared Error (MSE)}: Average of squared errors; used for regression tasks.
            \item \textbf{Area Under the Curve (AUC)}: Measures model ability to distinguish between classes in binary classification.
        \end{itemize}
    \end{block}

    \begin{block}{Validation Strategies}
        \begin{itemize}
            \item \textbf{Cross-Validation}: Splitting dataset into training and testing sets to assess model stability (e.g., K-fold CV).
            \item \textbf{Train/Test Split}: Basic method for evaluating model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Example}
    \begin{block}{Scenario}
        You have a dataset of houses with features such as size, number of bedrooms, and location to predict house prices.
        \begin{itemize}
            \item Evaluate the importance of each feature (e.g., size may be highly important while "number of bedrooms" may not).
            \item Monitor model performance using metrics like MSE.
            \item Validate findings using cross-validation for reliability.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Feature impact evaluation is essential for effective feature selection and model building.
            \item Choose metrics based on the type of problem (classification vs. regression).
            \item Employ validation strategies to ensure model generalization and reliability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies in Feature Engineering}
    % Introducing feature engineering and its importance in different industries
    \begin{block}{Introduction to Feature Engineering}
        Feature engineering is a critical step in the machine learning pipeline that involves:
        \begin{itemize}
            \item Creating, selecting, and transforming input variables (features).
            \item Improving model performance across various industries.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Healthcare}
    % Discussing the case study in healthcare regarding patient readmissions
    \textbf{1. Healthcare: Predicting Patient Readmissions} \\
    \textit{Industry Context:} 
    Hospitals aim to reduce patient readmissions which incur significant costs.
    
    \begin{block}{Feature Engineering Practices}
        \begin{itemize}
            \item \textbf{Data Sources:} Integrated electronic health records (EHRs).
            \item \textbf{Feature Creation:} 
            \begin{itemize}
                \item Comorbidity Count: Number of chronic conditions (e.g., diabetes).
                \item Time Since Last Admission: Measures urgency of care needed.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \textbf{Outcome Achieved:} 
    Improved prediction accuracy by 15\%, reducing overall readmissions by 20\%.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies - Finance and Retail}
    % Discussing case studies in finance and retail
    \textbf{2. Finance: Credit Scoring Models} \\
    \textit{Industry Context:} 
    Assessing the creditworthiness of loan applicants.

    \begin{block}{Feature Engineering Practices}
        \begin{itemize}
            \item \textbf{Data Sources:} Customer credit history and employment records.
            \item \textbf{Feature Transformation:} 
            \begin{itemize}
                \item Credit Utilization Ratio: Current credit balances/total limits.
                \item Duration of Credit History: Time since first credit line opened.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \textbf{Outcome Achieved:} 
    Improved applicant quality, increasing loan approvals and reducing defaults by 10\%.

    \vspace{0.5cm}

    \textbf{3. Retail: Customer Segmentation for Targeted Marketing} \\
    \textit{Industry Context:} 
    Maximizing marketing efforts through targeted campaigns.

    \begin{block}{Feature Engineering Practices}
        \begin{itemize}
            \item \textbf{Data Sources:} Transaction records and online behavior.
            \item \textbf{Feature Creation:}
            \begin{itemize}
                \item Monthly Purchase Frequency: Gauges customer loyalty.
                \item Average Basket Size: Insight into buying behavior.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \textbf{Outcome Achieved:} 
    Personalized campaigns increased conversion rates by 30\%.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    % Summarizing the key points and conclusion of the case studies
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Importance of Context: Understanding industry challenges is essential.
            \item Data Variety: Multiple data sources enrich features and model effectiveness.
            \item Impact on Outcomes: Well-engineered features improve accuracy and drive business savings.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Case studies demonstrate the tangible benefits of feature engineering:
        By thoughtfully crafting features, organizations can enhance model performance and achieve strategic goals.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Feature Engineering}
    \begin{block}{Introduction}
        Feature engineering is a crucial step in the machine learning workflow, as it directly influences the performance of models. However, several challenges may arise during this process that can impede the development of effective predictive models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in Feature Engineering - Overfitting}
    \begin{enumerate}
        \item \textbf{Overfitting}
        \begin{itemize}
            \item \textbf{Definition}: Occurs when a model learns the training data too well, capturing noise instead of the underlying pattern.
            \item \textbf{Example}: A model using 100 features for predicting house prices may perform well on the training data but poorly on unseen data due to irrelevant features.
            \item \textbf{Solution}:
            \begin{itemize}
                \item Use cross-validation to ensure robustness.
                \item Implement regularization methods (Lasso, Ridge regression).
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in Feature Engineering - Underfitting and Missing Values}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Underfitting}
        \begin{itemize}
            \item \textbf{Definition}: Occurs when a model is too simple to capture the underlying trend, leading to poor performance on both training and test datasets.
            \item \textbf{Example}: Using a linear regression model to predict a non-linear relationship (e.g., quadratic function).
            \item \textbf{Solution}:
            \begin{itemize}
                \item Increase model complexity (e.g., polynomial features).
                \item Explore more complex algorithms (Random Forest, Neural Networks).
            \end{itemize}
        \end{itemize}

        \item \textbf{Dealing with Missing Values}
        \begin{itemize}
            \item \textbf{Definition}: Missing data can hinder many machine learning algorithms, leading to unreliable predictions.
            \item \textbf{Example}: Missing ages in customer transaction datasets can result in an incomplete understanding of demographic influences.
            \item \textbf{Solution}:
            \begin{itemize}
                \item \textbf{Imputation}: Fill missing values using statistical methods or machine learning (k-NN).
                \item \textbf{Omission}: Remove records with missing data if they represent a small portion of the dataset.
                \item \textbf{Flagging}: Create a binary feature to indicate whether a value was missing.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item Monitor for overfitting and underfitting to enhance model generalization.
        \item Address missing values carefully to maintain information integrity and avoid bias.
        \item Effective feature engineering requires striking a balance between capturing complexity and ensuring model interpretability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet for Handling Missing Values}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.impute import SimpleImputer

# Load dataset
data = pd.read_csv('customers.csv')

# Initialize imputer to fill missing values with mean
imputer = SimpleImputer(strategy='mean')
data['age'] = imputer.fit_transform(data['age'].values.reshape(-1,1))
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Best Practices in Feature Engineering - Part 1}
  
  \begin{block}{Key Takeaways}
    \begin{enumerate}
      \item \textbf{Feature Importance:}
        \begin{itemize}
          \item Select features that significantly influence the target variable.
          \item Use techniques such as correlation matrices and feature importance from models (e.g., Random Forests).
        \end{itemize}
      
      \item \textbf{Handling Missing Values:}
        \begin{itemize}
          \item Common strategies include imputation (mean, median, mode) and indicator variables.
        \end{itemize}
      
      \item \textbf{Feature Scalability and Normalization:}
        \begin{itemize}
          \item Standardize or normalize features to enhance model performance.
          \item Techniques: Standardization and Min-Max Scaling.
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Best Practices in Feature Engineering - Part 2}
  
  \begin{block}{Key Takeaways (Continued)}
    \begin{enumerate}
      \setcounter{enumi}{3} % Continue numbering from the previous frame
      \item \textbf{Creating Interaction Variables:}
        \begin{itemize}
          \item Capture relationships by creating interaction features (e.g., 'Income-to-Expense Ratio').
        \end{itemize}
      
      \item \textbf{Feature Encoding:}
        \begin{itemize}
          \item Convert categorical features into numerical formats suitable for models:
            \begin{itemize}
              \item One-Hot Encoding
              \item Label Encoding
            \end{itemize}
        \end{itemize}
      
      \item \textbf{Regularization Techniques:}
        \begin{itemize}
          \item Apply L1 (Lasso) and L2 (Ridge) regularization to minimize overfitting.
        \end{itemize}
      
      \item \textbf{Cross-Validation for Feature Evaluation:}
        \begin{itemize}
          \item Use k-fold cross-validation to assess feature changes' impact on model performance.
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Best Practices in Feature Engineering - Part 3}
  
  \begin{block}{Best Practices}
    \begin{itemize}
      \item \textbf{Iterative Process:} Feature engineering is continuous. Iterate based on model performance.
      \item \textbf{Domain Knowledge:} Leverage expertise to identify relevant features.
      \item \textbf{Assess Model Performance:} Regularly evaluate feature impacts using appropriate metrics (e.g., RMSE, accuracy).
      \item \textbf{Documentation and Reproducibility:} Maintain records of feature engineering steps for transparency and reproducibility.
    \end{itemize}
  \end{block}
  
  \begin{block}{Final Thought}
    By applying these principles, significantly enhance your machine learning model performance through effective feature engineering.
  \end{block}
\end{frame}


\end{document}