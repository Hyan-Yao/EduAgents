\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation}
    \textbf{Overview of the Importance of Evaluating Model Performance in Machine Learning}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Model Evaluation?}
    \begin{block}{Definition}
        Model evaluation is the process of assessing how well a predictive model performs on a dataset. 
        This involves using specific metrics to quantify its accuracy, reliability, and capability to generalize beyond the training data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Model Evaluation Crucial?}
    \begin{itemize}
        \item \textbf{Ensures Model Effectiveness:} 
        Evaluating a model helps determine if it's useful for making predictions.
        
        \item \textbf{Guides Model Selection:}
        Evaluation allows us to choose the best-performing model based on objective criteria.
        
        \item \textbf{Identifies Improvements:}
        Highlights aspects that need enhancement, informing decisions about feature engineering or algorithm selection.
        
        \item \textbf{Informs Stakeholders:}
        Clear evaluation results foster trust and facilitate evidence-driven decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics in Model Evaluation}
    \begin{itemize}
        \item \textbf{Accuracy:} 
        \[
        \text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}} \times 100 
        \]
        
        \item \textbf{Precision:} 
        \[
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}} 
        \]
        
        \item \textbf{Recall:} 
        \[
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}} 
        \]
        
        \item \textbf{F1 Score:} 
        \[
        F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario: Evaluating a Classification Model}
    Consider a model predicting whether an email is spam (positive) or not (negative). The confusion matrix is:

    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|}
        \hline
        & Predicted Spam & Predicted Not Spam \\
        \hline
        Actual Spam & 70 (TP) & 30 (FN) \\
        \hline
        Actual Not Spam & 10 (FP) & 90 (TN) \\
        \hline
        \end{tabular}
    \end{table}

    \textbf{Metrics Calculated:}
    \begin{itemize}
        \item \textbf{Accuracy:} \( 0.8 \text{ (or 80%)} \)
        \item \textbf{Precision:} \( 0.875 \text{ (or 87.5%)} \)
        \item \textbf{Recall:} \( 0.7 \text{ (or 70%)} \)
        \item \textbf{F1 Score:} \( \approx 0.785 \)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Evaluation is essential for verifying the success of a model.
        \item Different metrics serve various needs depending on the problem context.
        \item Understanding model performance helps in refining the model development process.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By effectively evaluating machine learning models, we ensure they are both reliable and actionable in real-world applications. In the next slide, we will explore the specific objectives of model evaluation, focusing on accuracy, generalization, and performance comparison.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objectives of Model Evaluation}
    
    \begin{block}{Understanding the Purposes}
        Evaluating machine learning models is a crucial step in the data science workflow. This slide focuses on three primary objectives of model evaluation:
        \begin{itemize}
            \item Accuracy
            \item Generalization
            \item Performance Comparison
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objective 1: Accuracy}
    
    \begin{block}{Definition}
        Accuracy is the ratio of correctly predicted observations to the total observations. It measures how often the model makes the right prediction.
    \end{block}
    
    \begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    
    \begin{itemize}
        \item TP = True Positives
        \item TN = True Negatives
        \item FP = False Positives
        \item FN = False Negatives
    \end{itemize}

    \begin{block}{Example}
        In a binary classification problem (e.g., spam detection), if our model correctly identifies 90 out of 100 emails, the accuracy is 90\%.
    \end{block}
    
    \begin{block}{Key Point}
        While useful, accuracy can be misleading in imbalanced datasets, where one class is much more frequent than the other.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objective 2: Generalization}
    
    \begin{block}{Definition}
        Generalization refers to a model's ability to perform well on unseen data, not just the data it was trained on.
    \end{block}
    
    \begin{block}{Example}
        If a model trained on a small dataset of images of cats and dogs can accurately classify new images not in the training set, it demonstrates good generalization.
    \end{block}
    
    \begin{block}{Key Point}
        Overfitting occurs when a model learns the noise in the training data rather than the actual patterns. Regularization techniques or simplifying the model can help improve generalization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objective 3: Performance Comparison}
    
    \begin{block}{Definition}
        Performance comparison involves assessing multiple models to identify which one performs best under certain conditions or metrics.
    \end{block}
    
    \begin{itemize}
        \item Comparing different algorithms (e.g., Decision Trees vs. SVM) based on accuracy, precision, recall, or other metrics.
        \item Cross-validation can provide insights into models’ robustness and consistency across different subsets of the data.
    \end{itemize}

    \begin{block}{Key Point}
        It is important to use the same evaluation criteria when comparing models to ensure a fair assessment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    
    Understanding the objectives of model evaluation—accuracy, generalization, and performance comparison—is vital for selecting and improving machine learning models. Each objective highlights different aspects of a model's effectiveness and informs future enhancements and field applications. 
    This foundation will support our upcoming discussions on specific evaluation metrics in the next slide.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Model Evaluation Metrics - Overview}
    \begin{block}{Introduction}
        In the realm of machine learning and data science, model evaluation is vital for understanding how well our predictive models are performing. 
        Evaluating models requires specific metrics that provide insight into their accuracy, effectiveness, and real-world applicability. 
        Here, we will discuss five essential metrics.
    \end{block}
    \begin{itemize}
        \item Accuracy
        \item Precision
        \item Recall
        \item F1 Score
        \item AUC-ROC
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Model Evaluation Metrics - Accuracy}
    \begin{block}{Accuracy}
        \textbf{Definition:} Accuracy measures the proportion of correctly predicted instances out of the total instances.
        
        \textbf{Formula:}
        \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Observations}}
        \end{equation}
        
        \textbf{Example:} If out of 100 predictions, 90 are correct (70 true positives and 20 true negatives), then:
        \begin{equation}
            \text{Accuracy} = \frac{70 + 20}{100} = 0.90 \, \text{or} \, 90\%
        \end{equation}
        
        \textbf{Key Point:} Accuracy is best used with balanced class distributions; it can be misleading in skewed datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Model Evaluation Metrics - Precision, Recall, F1 Score, AUC-ROC}
    \begin{block}{Precision}
        \textbf{Definition:} Indicates the proportion of true positive predictions relative to the total predicted positives.
        
        \textbf{Formula:}
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \end{equation}
        
        \textbf{Example:} If a model predicts 50 positive cases, with 40 being actual positives:
        \begin{equation}
            \text{Precision} = \frac{40}{40 + 10} = 0.80 \, \text{or} \, 80\%
        \end{equation}
        
        \textbf{Key Point:} High precision indicates a low false positive rate, crucial in applications like spam detection.
    \end{block}

    \begin{block}{Recall}
        \textbf{Definition:} Measures the proportion of actual positives captured correctly. 
        \textbf{Formula:}
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}
        \textbf{Example:} If there are 60 actual positive cases and the model identifies 40:
        \begin{equation}
            \text{Recall} = \frac{40}{40 + 20} = 0.67 \, \text{or} \, 67\%
        \end{equation}
        \textbf{Key Point:} High recall is critical in disease diagnosis scenarios.
    \end{block}
    
    \begin{block}{F1 Score}
        \textbf{Definition:} Harmonic mean of precision and recall.
        \textbf{Formula:}
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
        \textbf{Example:} 
        Using precision (80%) and recall (67%):
        \begin{equation}
            \text{F1 Score} \approx 0.73 \, \text{or} \, 73\%
        \end{equation}
        \textbf{Key Point:} Ideal for balancing precision and recall, especially in uneven class distributions.
    \end{block}

    \begin{block}{AUC-ROC}
        \textbf{Definition:} Quantifies the model's ability to distinguish between classes across all classification thresholds.
        \begin{itemize}
            \item \textbf{AUC Values:} 
                \begin{itemize}
                    \item 0.5: No discrimination
                    \item 1.0: Perfect discrimination
                \end{itemize}
            \item \textbf{Usage:} Particularly useful in binary classification problems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Confusion Matrix - Part 1}
    \begin{block}{What is a Confusion Matrix?}
        A confusion matrix is a table used to evaluate the performance of a classification algorithm. 
        It compares the predicted classifications of the model to the actual classifications in the data.
    \end{block}
    
    \begin{block}{Structure of a Confusion Matrix}
        A confusion matrix consists of four components:
        \begin{itemize}
            \item \textbf{True Positive (TP)}: The number of cases where the model correctly predicted the positive class.
            \item \textbf{True Negative (TN)}: The number of cases where the model correctly predicted the negative class.
            \item \textbf{False Positive (FP)}: The number of cases where the model incorrectly predicted the positive class (Type I error).
            \item \textbf{False Negative (FN)}: The number of cases where the model incorrectly predicted the negative class (Type II error).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Confusion Matrix - Part 2}
    \begin{block}{Example}
        Imagine a medical test that determines whether a patient has a disease:
        \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
            \hline
            \textbf{Actual Positive} & TP = 70 & FN = 5 \\
            \hline
            \textbf{Actual Negative} & FP = 10 & TN = 15 \\
            \hline
        \end{tabular}
        \end{center}
    \end{block}
    
    \begin{block}{Interpreting the Confusion Matrix}
        Key metrics include:
        \begin{itemize}
            \item \textbf{Accuracy}
            \item \textbf{Precision}
            \item \textbf{Recall (Sensitivity)}
            \item \textbf{F1 Score}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Confusion Matrix - Part 3}
    \begin{block}{Key Metrics}
        \begin{enumerate}
            \item \textbf{Accuracy}:
            \[
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{70 + 15}{70 + 15 + 10 + 5} = 0.85 \text{ or } 85\%
            \]
            \item \textbf{Precision}:
            \[
            \text{Precision} = \frac{TP}{TP + FP} = \frac{70}{70 + 10} = 0.875 \text{ or } 87.5\%
            \]
            \item \textbf{Recall (Sensitivity)}:
            \[
            \text{Recall} = \frac{TP}{TP + FN} = \frac{70}{70 + 5} = 0.933 \text{ or } 93.3\%
            \]
            \item \textbf{F1 Score}:
            \[
            F1 = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = 0.903
            \]
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Confusion Matrix - Conclusion and Tips}
    \begin{block}{Conclusion}
        Understanding the confusion matrix is crucial for evaluating classification models.
        Analyzing its components helps in making informed decisions about model improvements.
    \end{block}

    \begin{block}{Tips for Practical Application}
        \begin{itemize}
            \item Visualize the confusion matrix using libraries like \textbf{Scikit-learn} in Python.
            \item Adjust your classification threshold based on the problem context (e.g., prioritizing recall in medical diagnoses).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation Techniques}
    \begin{block}{Overview of Cross-Validation}
        Cross-validation is a technique used to assess how well a model performs on unseen data. It helps in estimating the skill of the model when applied to a different dataset. The primary goal is to ensure that the model maintains its predictive capability across different subsets of the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques - Part 1}
    \begin{itemize}
        \item \textbf{1. k-Fold Cross-Validation}
        \begin{itemize}
            \item \textbf{Concept}: The dataset is divided into 'k' equally sized folds. The model is trained on 'k-1' folds and validated on the remaining fold. This process is repeated 'k' times.
            \item \textbf{Advantages}:
            \begin{itemize}
                \item Provides a comprehensive insight into model performance.
                \item Reduces variance by averaging results across folds.
            \end{itemize}
            \item \textbf{Example}:
            \begin{itemize}
                \item If you have 100 samples and choose $k=5$, the model trains on 80 samples and validates on 20 samples, rotating through each fold.
            \end{itemize}
            \item \textbf{Formula}:
            \begin{equation}
                \text{Accuracy}_i = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
            \end{equation}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques - Part 2}
    \begin{itemize}
        \item \textbf{2. Stratified Sampling}
        \begin{itemize}
            \item \textbf{Concept}: Ensures each fold has the same proportion of classes as the entire dataset.
            \item \textbf{Advantages}:
            \begin{itemize}
                \item Maintains class distribution within each fold.
                \item Reduces bias by ensuring minority classes are represented.
            \end{itemize}
            \item \textbf{Example}:
            \begin{itemize}
                \item For a dataset with 70\% samples of Class A and 30\% of Class B, each fold will reflect this distribution.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{3. Leave-One-Out Cross-Validation (LOOCV)}
        \begin{itemize}
            \item \textbf{Concept}: An extreme case of k-fold where 'k' equals the number of data points. The model is trained on all but one point.
            \item \textbf{Advantages}:
            \begin{itemize}
                \item Utilizes almost the entire dataset for training, leading to reliable performance estimates.
            \end{itemize}
            \item \textbf{Example}:
            \begin{itemize}
                \item For 10 samples, train on 9 and test on the remaining one, repeating this 10 times.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting vs Underfitting - Introduction}
    \begin{block}{Overview}
        In machine learning, understanding the concepts of \textbf{overfitting} and \textbf{underfitting} is crucial for building models that generalize well to new data. These two issues can severely affect model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting}
    \begin{block}{Definition}
        Overfitting occurs when a model learns the training data too well, capturing noise and outliers instead of general patterns. This leads to poor performance on unseen data due to excessive model complexity.
    \end{block}
    
    \begin{block}{Impact on Performance}
        \begin{itemize}
            \item \textbf{High} accuracy on training data
            \item \textbf{Low} accuracy on validation/test data
        \end{itemize}
    \end{block}

    \begin{block}{Visual Example}
        \includegraphics[width=0.8\linewidth]{overfitting_example.png} % Placeholder for the visual
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Underfitting}
    \begin{block}{Definition}
        Underfitting occurs when a model is too simple to capture the underlying structure of the data, resulting in poor performance on both training and validation datasets.
    \end{block}
    
    \begin{block}{Impact on Performance}
        \begin{itemize}
            \item \textbf{Low} accuracy on training data
            \item \textbf{Low} accuracy on validation/test data
        \end{itemize}
    \end{block}

    \begin{block}{Visual Example}
        \includegraphics[width=0.8\linewidth]{underfitting_example.png} % Placeholder for the visual
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Balance is Key}: Aim for a model complexity that avoids both overfitting and underfitting.
            \item \textbf{Model Complexity}: Complex models can overfit; simpler models may underfit important trends.
            \item \textbf{Evaluation Methods}: Techniques like cross-validation help detect and assess model performance issues.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Overfitting and underfitting are crucial in model evaluation. Understanding these will help in selecting appropriate model architectures and tuning strategies for optimal performance on unseen data.
    \end{block}

    \begin{block}{Transition}
        Next, we will explore \textbf{Hyperparameter Tuning}, a key method for adjusting your model to mitigate overfitting and underfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Overview}
    \begin{block}{Hyperparameter Tuning Explained}
        Hyperparameter tuning is the process of optimizing the hyperparameters of a machine learning model to enhance its performance. Unlike model parameters, which the model learns from the training data, hyperparameters are set before the learning process begins and significantly affect how the model learns.
    \end{block}

    \begin{block}{Importance of Hyperparameter Tuning}
        \begin{itemize}
            \item \textbf{Avoid Overfitting/Underfitting:} Proper tuning can help prevent overfitting (model learns noise) and underfitting (model is too simple).
            \item \textbf{Increased Accuracy:} Well-tuned models generally achieve higher accuracy and better generalization on unseen data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Methods for Hyperparameter Tuning - Grid Search}
    \begin{block}{Grid Search}
        \begin{itemize}
            \item \textbf{Definition:} A comprehensive method that exhaustively considers all hyperparameter combinations from a user-specified grid.
            \item \textbf{Process:}
            \begin{enumerate}
                \item Define the hyperparameter grid (e.g., learning rate: [0.01, 0.1, 1]).
                \item Train the model for every combination of hyperparameters.
                \item Evaluate model performance using cross-validation.
            \end{enumerate}
            \item \textbf{Advantages:} Guarantees finding the best combination within the specified grid.
            \item \textbf{Disadvantages:} Computationally expensive with large hyperparameter grids.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20]
}

# Model
rf = RandomForestClassifier()

# Grid search
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Best parameters
print("Best parameters found: ", grid_search.best_params_)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Methods for Hyperparameter Tuning - Random Search}
    \begin{block}{Random Search}
        \begin{itemize}
            \item \textbf{Definition:} Instead of trying all combinations, it selects random combinations of hyperparameters for more efficient exploration.
            \item \textbf{Process:}
            \begin{enumerate}
                \item Define the hyperparameter distributions (e.g., learning rate can be sampled from Uniform(0.001, 0.1)).
                \item Sample a fixed number of random combinations and evaluate each.
            \end{enumerate}
            \item \textbf{Advantages:} 
            \begin{itemize}
                \item Often faster than grid search.
                \item May outperform grid search by discovering optimal parameters outside the grid.
            \end{itemize}
            \item \textbf{Disadvantages:} No guarantee of finding the best combination since it's based on random sampling.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import RandomizedSearchCV

# Hyperparameter distribution
param_dist = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Random search
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=10, cv=5)
random_search.fit(X_train, y_train)

# Best parameters
print("Best parameters found: ", random_search.best_params_)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Model Comparison}
    \begin{block}{Overview}
        Model comparison is a vital step in the machine learning workflow. It involves evaluating and contrasting various models to identify the best performers on a specific dataset, supported by statistical tests and performance metrics.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts: Performance Metrics}
    \begin{itemize}
        \item \textbf{Accuracy}: Proportion of correct predictions.
        \item \textbf{Precision}: Ratio of true positives to the sum of true positives and false positives (PPV).
        \item \textbf{Recall (Sensitivity)}: Ratio of true positives to the sum of true positives and false negatives (TPR).
        \item \textbf{F1 Score}: Harmonic mean of precision and recall, useful for imbalanced datasets:
        \begin{equation}
            F1 = 2 \times \frac{(Precision \times Recall)}{(Precision + Recall)}
        \end{equation}
        \item \textbf{ROC-AUC}: Area under the receiver operating characteristic curve.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts: Statistical Tests}
    \begin{itemize}
        \item \textbf{t-test}: Compares means of two models based on performance metrics.
        \item \textbf{Wilcoxon Signed-Rank Test}: Non-parametric test for comparing two related samples.
        \item \textbf{ANOVA}: Compares performance among three or more models.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Model Comparison}
    \begin{block}{Example 1: Comparing Model Accuracy}
        - Model A Accuracy: 85\% \\
        - Model B Accuracy: 80\% \\
        Interpretation: Model A performs better based on accuracy, but further evaluations (precision, recall) are essential.
    \end{block}
    \begin{block}{Example 2: Statistical Testing}
        - Hypotheses for t-test: \\
        \textbf{H0}: No significant difference between Model A and Model B accuracies. \\
        \textbf{H1}: Significant difference between the accuracies. \\
        If p-value < 0.05, reject H0, indicating a significant performance difference.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from scipy.stats import ttest_ind

# Actual and predicted values
y_true = [0, 1, 0, 1, 1, 0, 0, 1]
y_pred_A = [0, 1, 0, 1, 1, 0, 1, 1]  
y_pred_B = [0, 0, 0, 1, 1, 0, 1, 1]  

# Evaluation metrics
accuracy_A = accuracy_score(y_true, y_pred_A)
accuracy_B = accuracy_score(y_true, y_pred_B)
f1_A = f1_score(y_true, y_pred_A)
f1_B = f1_score(y_true, y_pred_B)

# Statistical testing
t_stat, p_value = ttest_ind(y_pred_A, y_pred_B)

print(f"Model A - Accuracy: {accuracy_A}, F1 Score: {f1_A}")
print(f"Model B - Accuracy: {accuracy_B}, F1 Score: {f1_B}")
print(f"t-Statistic: {t_stat}, p-value: {p_value}")
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{block}{}
        Model comparison is essential for selecting the best model for your data. By utilizing performance metrics and statistical tests, you can substantiate your choices and drive insightful machine learning results.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Application of Model Evaluation}
    \begin{block}{Introduction to Model Evaluation}
        Model evaluation is a critical process in machine learning and data science that ensures models generalize well to unseen data.
        Effective model evaluation leads to:
        \begin{itemize}
            \item Better decision-making
            \item Improved operational efficiency
            \item Enhanced customer satisfaction
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Example: Healthcare}
    \begin{block}{Healthcare: Predicting Patient Readmissions}
        \begin{itemize}
            \item \textbf{Context:} Hospitals aim to minimize patient readmissions.
            \item \textbf{Model Evaluation Application:}
                \begin{itemize}
                    \item Performance Metrics: Precision, Recall, F1 Score
                    \item Example: Predicting readmission based on patient data
                \end{itemize}
            \item \textbf{Result:} Targeted interventions reduced readmissions by 12\%.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Example: Finance and Retail}
    \begin{block}{Finance: Fraud Detection}
        \begin{itemize}
            \item \textbf{Context:} Financial institutions detect fraudulent activities.
            \item \textbf{Model Evaluation Application:}
                \begin{itemize}
                    \item Performance Metrics: ROC-AUC, Confusion Matrix
                    \item Example: Monitoring transaction patterns
                \end{itemize}
            \item \textbf{Result:} Reduced false positives by 30\% and improved customer trust.
        \end{itemize}
    \end{block}

    \begin{block}{Retail: Forecasting Demand}
        \begin{itemize}
            \item \textbf{Context:} Retailers manage inventory effectively.
            \item \textbf{Model Evaluation Application:}
                \begin{itemize}
                    \item Performance Metric: Mean Absolute Percentage Error (MAPE)
                    \item Example: Regression models to predict sales
                \end{itemize}
            \item \textbf{Result:} Achieved a 15\% increase in forecasting accuracy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Importance of context for evaluation metrics
            \item Iterative nature of model evaluation
            \item Trade-offs between different metrics (e.g., precision vs recall)
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Effective model evaluation plays a pivotal role across industries, aiding in informed decision-making and operational optimization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Model Evaluation}
    Model evaluation is crucial in determining how well machine learning models perform. However, various challenges can obscure the true effectiveness of a model. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation Challenges}
    \begin{itemize}
        \item Understanding model evaluation is essential for improving evaluation strategies.
        \item Several challenges can impact the effectiveness of machine learning models.
    \end{itemize}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Model Evaluation}
    \begin{enumerate}
        \item \textbf{Dataset Bias}
            \begin{itemize}
                \item \textbf{Definition:} Training data not representative of real-world scenarios.
                \item \textbf{Example:} Facial recognition systems biased towards lighter skin tones.
                \item \textbf{Impact:} Can lead to ethical and operational issues in model deployment.
            \end{itemize}
        
        \item \textbf{Varying Data Distributions}
            \begin{itemize}
                \item \textbf{Definition:} Training on data distributions that differ from production data.
                \item \textbf{Example:} Retail models trained on regional data may mispredict in different areas.
                \item \textbf{Impact:} Diminished accuracy and reliability on new, unseen data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Understanding Biases:} Recognize selection and confirmation biases.
        \item \textbf{Testing Across Distributions:} Use domain adaptation techniques for better performance on diverse data sources.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods to Address These Challenges}
    \begin{enumerate}
        \item \textbf{Data Augmentation:} Increase training dataset diversity synthetically.
        \item \textbf{Stratified Sampling:} Ensure adequate representation of categories to combat bias.
        \item \textbf{Regularization Techniques:} Use L1/L2 regularization to enhance model generalization.
        \item \textbf{Continual Learning:} Develop adaptable models for changing data distributions.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Overcoming evaluation challenges is essential for robust model performance. Continuous assessment and adaptation of evaluation strategies ensure machine learning solutions are effective and fair. 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion - Overview}
  \begin{block}{Recap of Key Learnings on Model Evaluation}
    Model evaluation is essential for developing robust machine learning solutions. It ensures models perform well on unseen data and meet predefined performance metrics. Key points include:
  \end{block}
  
  \begin{itemize}
    \item Importance of Model Evaluation
    \item Key Evaluation Metrics
    \item Real-World Application
    \item Addressing Challenges
    \item Continuous Evaluation
    \item Importance of Validation Techniques
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Importance of Model Evaluation}
  \begin{itemize}
    \item Ensures performance on unseen data.
    \item Prevents overfitting and underfitting.
    \item Critical for trust in machine learning applications.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Evaluation Metrics}
  \begin{itemize}
    \item \textbf{Accuracy:}
      \begin{equation}
        \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
      \end{equation}
    \item \textbf{Precision:}
      \begin{equation}
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
      \end{equation}
    \item \textbf{Recall:}
      \begin{equation}
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
      \end{equation}
    \item \textbf{F1 Score:}
      \begin{equation}
        \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
      \end{equation}
    \item \textbf{ROC AUC:} Measures the model's ability to distinguish between classes.
  \end{itemize}
\end{frame}


\end{document}