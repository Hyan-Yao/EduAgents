\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands for LaTeX
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Week 8: Probabilistic Reasoning and Bayes' Theorem]{Week 8: Probabilistic Reasoning and Bayes' Theorem}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Probabilistic Reasoning}
    \begin{block}{Overview}
        Probabilistic reasoning is a method of drawing conclusions based on uncertainty and incomplete knowledge. 
        In AI, this approach allows systems to make predictions and decisions based on the likelihood of certain outcomes, rather than relying on deterministic logic alone.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in AI - Part 1}
    \begin{enumerate}
        \item \textbf{Handling Uncertainty:}
        \begin{itemize}
            \item AI systems often face uncertain environments. 
            \item Probabilistic reasoning helps manage uncertainty by quantifying it with probabilities.
            \item \textbf{Example:} A weather prediction system that estimates a 70\% chance of rain reflects uncertainty rather than a definitive forecast.
        \end{itemize}
        
        \item \textbf{Decision Making:}
        \begin{itemize}
            \item In complex scenarios, probabilistic methods assist AI in making better-informed decisions.
            \item \textbf{Example:} Autonomous vehicles use probabilistic reasoning to interpret sensor data and predict movements of pedestrians and other vehicles.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in AI - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Learning from Data:}
        \begin{itemize}
            \item Probabilistic models can be adapted via machine learning.
            \item AI systems learn from experiences and update beliefs with new data.
            \item \textbf{Example:} Naive Bayes classifiers predict class membership based on prior probabilities and likelihood of feature occurrences.
        \end{itemize}

        \item \textbf{Bayesian Inference:}
        \begin{itemize}
            \item A central concept in probabilistic reasoning, combining prior knowledge with new evidence.
            \item \textbf{Formula:} 
            \begin{equation}
                P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
            \end{equation}
            where:
            \begin{itemize}
                \item $P(H|E)$ = Posterior probability (after evidence)
                \item $P(E|H)$ = Likelihood (probability of evidence given the hypothesis)
                \item $P(H)$ = Prior probability (before evidence)
                \item $P(E)$ = Evidence probability (total probability of evidence)
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Probabilistic reasoning provides a framework for reasoning under uncertainty, crucial for robust AI applications.
        \item Real-world applications span various domains including finance, healthcare, and robotics.
        \item Understanding probabilistic models empowers AI practitioners to create more adaptable and intelligent systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By mastering probabilistic reasoning, you enhance your ability to create AI that can function effectively in the unpredictable landscape of real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Probabilistic Reasoning?}
    \textbf{Definition:}\\
    Probabilistic reasoning is a type of logical reasoning that utilizes the mathematical framework of probability to manage uncertainty in decision-making. In artificial intelligence (AI), it helps systems to make informed decisions based on incomplete, noisy, or uncertain information.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Uncertainty:} Real-world scenarios often involve uncertainty due to incomplete data, diverse interpretations, and randomness. Probabilistic reasoning structures the approach to quantify and reason under this uncertainty.
        
        \item \textbf{Probabilities:} A probability is a numerical value (between 0 and 1) that represents the likelihood of an event occurring.
        \begin{itemize}
            \item \textit{Example:} The probability of rain tomorrow might be 0.7, indicating a 70\% chance of rainfall.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in AI}
    \begin{enumerate}
        \item \textbf{Machine Learning:} Probabilistic models such as Bayesian networks enable systems to learn from data while accommodating uncertainty.
        \begin{itemize}
            \item \textit{Example:} In spam detection, a model calculates the probability that an email is spam based on various features like keywords, sender information, etc.
        \end{itemize}
        
        \item \textbf{Natural Language Processing (NLP):} Helps in understanding spoken or written language where meanings can vary widely.
        \begin{itemize}
            \item \textit{Example:} Sentiment analysis uses probabilistic models to determine the sentiment of a text based on the probability of certain words appearing together.
        \end{itemize}
        
        \item \textbf{Robotics:} Assists robots in making decisions in uncertain environments.
        \begin{itemize}
            \item \textit{Example:} A robot navigating a room can estimate its position while knowing that sensor readings might be noisy.
        \end{itemize}
        
        \item \textbf{Medical Diagnosis:} AI systems assess patient data to help in diagnosing diseases by determining the likelihood of various conditions.
        \begin{itemize}
            \item \textit{Example:} Given symptoms and test results, an AI can compute the probability of different illnesses, aiding doctors in decision-making.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Bayes' Theorem - Overview}
    \begin{itemize}
        \item Bayes' Theorem: Fundamental concept in probability and statistics
        \item Updates the probability of a hypothesis based on new evidence
        \item Incorporates prior knowledge with new data for informed decisions
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Formula of Bayes' Theorem}
    \begin{block}{Mathematical Representation}
        \[
        P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
        \]
    \end{block}
    \begin{itemize}
        \item \(P(H|E)\): Posterior Probability - Probability of hypothesis \(H\) given evidence \(E\)
        \item \(P(E|H)\): Likelihood - Probability of evidence \(E\) given that \(H\) is true
        \item \(P(H)\): Prior Probability - Initial probability of hypothesis \(H\)
        \item \(P(E)\): Marginal Probability - Total probability of evidence \(E\)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem in Practice}
    \begin{itemize}
        \item Critical role in probabilistic reasoning with uncertainty 
        \item Applications in:
            \begin{itemize}
                \item Machine learning
                \item Medical diagnosis
                \item Risk assessment
            \end{itemize}
        \item Example: Medical diagnosis of a disease using a test result
        \begin{itemize}
            \item \(P(H) = 0.01\), \(P(E|H) = 0.9\), \(P(E) = 0.05\)
            \item Calculation: \[
                P(H|E) = \frac{0.9 \cdot 0.01}{0.05} = 0.18
            \]
            \item Conclusion: 18% chance of having the disease after a positive test result
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points of Bayes' Theorem}
    \begin{itemize}
        \item Visual relationship between prior knowledge and new evidence
        \item Importance of understanding prior and posterior probabilities
        \item Applicability in real-world problems (medical diagnosis, spam detection)
        \item Enhances rational decision-making amidst uncertainty
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Prior and Posterior Probabilities}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Define and differentiate between prior and posterior probabilities.
            \item Understand the practical applications of Bayes' Theorem using real-world examples.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Prior Probability}
    \begin{block}{Prior Probability (Prior)}
        \begin{itemize}
            \item \textbf{Definition:} The initial assessment of the probability of an event before any new evidence.
            \item \textbf{Formula:} 
            \[
            P(A)
            \]
        \end{itemize}
    \end{block}
    \begin{example}
        \textbf{Example:} Probability of a student passing a statistics exam.
        \begin{itemize}
            \item Historically, 70\% of students pass:
            \[
            P(\text{Pass}) = 0.7
            \]
        \end{itemize}
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Posterior Probability}
    \begin{block}{Posterior Probability (Posterior)}
        \begin{itemize}
            \item \textbf{Definition:} The updated probability after considering new evidence.
            \item \textbf{Formula:} 
            \[
            P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
            \]
        \end{itemize}
    \end{block}
    \begin{example}
        \textbf{Example:} Given that a student studied for the exam.
        \begin{itemize}
            \item If the student's pass rate is 85\% when studying:
            \[
            P(\text{Pass} | \text{Studied}) = \frac{P(\text{Studied} | \text{Pass}) \cdot P(\text{Pass})}{P(\text{Studied})}
            \]
            \item Here, assume \(P(\text{Studied} | \text{Pass}) = 0.9\).
        \end{itemize}
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing the Concepts}
    \begin{block}{Diagram}
        Consider using a Venn diagram to depict how prior probabilities evolve into posterior probabilities with new information.
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Importance of the Prior:} The accuracy of the posterior probability depends on the prior.
            \item \textbf{Dynamic Nature of Probability:} Probability changes with new evidence.
            \item \textbf{Applications:} Used in medical diagnosis, finance, and AI (e.g., spam detection).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Formula Recap}
    \begin{block}{Summary}
        Understanding prior and posterior probabilities is essential for applying Bayes' Theorem effectively. 
        The prior serves as our starting point, while the posterior reflects our updated beliefs after evaluating new information.
    \end{block}
    \begin{block}{Bayes' Theorem Recap}
        \[
        P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
        \]
    \end{block}
    \begin{block}{Final Note}
        Understanding these concepts enhances probabilistic reasoning capabilities!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Likelihood in Bayes' Theorem - Learning Objectives}
    \begin{itemize}
        \item Understand the concept of likelihood in Bayesian analysis.
        \item Recognize the role of likelihood in updating beliefs.
        \item Apply the concept of likelihood to solve simple problems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Likelihood in Bayes' Theorem - What is Likelihood?}
    \begin{block}{Definition}
        Likelihood refers to the probability of the observed data given a specific hypothesis. In the context of Bayes' Theorem, it quantifies how probable the observed evidence is, assuming that the hypothesis is true.
    \end{block}

    \begin{block}{Mathematical Expression}
    In Bayes' Theorem:
        \[
        P(H | E) = \frac{P(E | H) \cdot P(H)}{P(E)}
        \]
        where \( P(E | H) \) is the likelihood, with:
        \begin{itemize}
            \item \( H \) = Hypothesis
            \item \( E \) = Evidence or observed data
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Likelihood in Bayes' Theorem - Example: Coin Toss}
    \begin{block}{Scenario}
        Testing if a coin is biased towards heads.
    \end{block}

    \begin{block}{Hypotheses}
        \begin{itemize}
            \item \( H_1 \): The coin is fair (50\% heads).
            \item \( H_2 \): The coin is biased (70\% heads).
        \end{itemize}
    \end{block}

    \begin{block}{Observed Data}
        You flip the coin 10 times and observe 8 heads.
    \end{block}

    \begin{block}{Calculating Likelihood}
        For \( H_1 \) (fair coin):
        \[
        P(E | H_1) = \binom{10}{8} (0.5)^8 (0.5)^2 = 0.0439
        \]
        
        For \( H_2 \) (biased coin):
        \[
        P(E | H_2) = \binom{10}{8} (0.7)^8 (0.3)^2 = 0.2335
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Likelihood in Bayes' Theorem - Key Points}
    \begin{itemize}
        \item Likelihood is a measure of how well a hypothesis explains the observed data.
        \item It plays a central role in the Bayesian framework, enabling the update of prior beliefs.
        \item Comparing likelihoods helps determine the most probable hypothesis given the evidence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Likelihood in Bayes' Theorem - Conclusion}
    Understanding likelihood is vital for applying Bayes' Theorem effectively. It transforms your prior knowledge into updated beliefs using observed data, allowing for more informed decision-making in the presence of uncertainty.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayesian Networks - Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the structure and components of Bayesian networks.
            \item Identify the role of nodes and edges in representing probabilistic relationships.
            \item Learn how Bayesian networks can be used for inference.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Bayesian Networks?}
    A Bayesian network is a graphical model that represents a set of variables and their probabilistic dependencies. It allows us to apply Bayes' theorem for inference in complex systems, making it easier to visualize and compute relationships between variables.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of Bayesian Networks}
    \begin{enumerate}
        \item \textbf{Nodes}: Each node represents a variable (e.g., weather, disease, test results). Nodes can be discrete (e.g., yes/no) or continuous (e.g., age).
        \item \textbf{Directed Edges}: Arrows between nodes represent conditional dependencies. An edge from node A to node B implies that A influences B.
        \item \textbf{Conditional Probability Tables (CPTs)}: Each node has an associated CPT quantifying the effects of the parent nodes. For example, if node A influences node B, the CPT will provide probabilities of B given different states of A.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Bayesian Network}
    Consider a simple Bayesian network involving three variables:
    \begin{itemize}
        \item \textbf{Rain (R)}: whether it rains or not
        \item \textbf{Traffic Jam (T)}: whether there is a traffic jam or not
        \item \textbf{Accident (A)}: whether an accident occurs or not
    \end{itemize}
    
    The graphical representation would look like this:
    \begin{center}
    \begin{verbatim}
         R
         ↓
         T
         ↓
         A
    \end{verbatim}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conditional Probability Tables}
    \begin{itemize}
        \item \( P(R) \): probability of rain (e.g., 0.2)
        \item \( P(T | R) \): probability of traffic jam given it rains (e.g., 0.8)
        \item \( P(T | \neg R) \): probability of traffic jam given it does not rain (e.g., 0.1)
        \item \( P(A | T) \): probability of an accident given there is a traffic jam (e.g., 0.5)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Modularity}: Bayesian networks allow for easy updates of probabilities if new information becomes available.
        \item \textbf{Inference}: They can be used to compute the probabilities of unknown variables based on known variables.
        \item \textbf{Real-World Applications}: Used in fields like medical diagnosis, risk analysis, and decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem in Bayesian Networks}
    Given a network, use Bayes' theorem to calculate the posterior probabilities of nodes based on observed evidence:
    \begin{equation}
    P(X | E) = \frac{P(E | X) P(X)}{P(E)}
    \end{equation}
    Where:
    \begin{itemize}
        \item \( P(X | E) \): posterior probability,
        \item \( P(E | X) \): likelihood,
        \item \( P(X) \): prior probability,
        \item \( P(E) \): marginal likelihood.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Bayesian networks serve as powerful tools for modeling and reasoning under uncertainty, making them invaluable in domains requiring decision-making based on incomplete information.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    In our following slide, we will delve into practical applications of Bayesian networks to demonstrate their utility in real-world scenarios.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayesian Networks}
    Bayesian networks are powerful probabilistic graphical models that allow for reasoning about uncertainty in various fields, including:
    \begin{itemize}
        \item Medical Diagnosis
        \item Risk Assessment
        \item Natural Language Processing
        \item Predictive Maintenance
        \item Environmental Monitoring
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayesian Networks - Overview}
    \begin{block}{Overview}
        Bayesian networks consist of nodes (variables) and directed edges (dependencies) enabling the representation of uncertain knowledge. Their applications are impactful across different domains by solving complex real-world problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Application 1: Medical Diagnosis}
    \textbf{Example:} Predicting diseases based on symptoms.
    \begin{itemize}
        \item \textbf{Concept:} Model relationships between symptoms and diseases aiding diagnosis.
        \item \textbf{Illustration:}
        \begin{itemize}
            \item Nodes: Fever, Cough, Flu, Cold.
            \item Connections: Fever and Cough linked to Flu and Cold.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Application 2: Risk Assessment}
    \textbf{Example:} Evaluating financial risks in investment.
    \begin{itemize}
        \item \textbf{Concept:} Assess risks associated with portfolios based on market conditions.
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Investors can input market conditions and observe influences on risk.
            \item Update probabilities with new data to refine assessments.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Application 3: Natural Language Processing}
    \textbf{Example:} Spam detection in emails.
    \begin{itemize}
        \item \textbf{Concept:} Classify emails as 'spam' or 'not spam' based on features.
        \item \textbf{Illustration:}
        \begin{itemize}
            \item Nodes: "Contains Urgent", "Contains Offer", "Spam".
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Application 4: Predictive Maintenance}
    \textbf{Example:} Equipment failure prediction in manufacturing.
    \begin{itemize}
        \item \textbf{Concept:} Predict equipment failure from sensor data.
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Proactive maintenance reduces costs and downtime.
            \item Updates to predictions with ongoing sensor data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Application 5: Environmental Monitoring}
    \textbf{Example:} Modeling ecological systems.
    \begin{itemize}
        \item \textbf{Concept:} Model ecological relationship predictions, such as species dynamics.
        \item \textbf{Illustration:}
        \begin{itemize}
            \item Nodes: Rainfall, Temperature, Species Population.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Bayesian networks provide a flexible structure for modeling complex relationships.
        \item They can continuously update based on new information, enhancing decision-making.
        \item Applications span healthcare, finance, and environmental science, showcasing versatility.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Through the applications of Bayesian networks outlined, we see their critical role in solving real-world challenges by reasoning about uncertainty, assisting in better-informed decision-making across varied sectors.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Bayes' Theorem in Decision-Making}
    \begin{block}{Clear Explanation of Concepts}
        Bayes' Theorem is a statistical method that updates the probability of a hypothesis as more evidence becomes available. It provides a systematic approach to incorporate new information into existing beliefs.
    \end{block}
    The theorem is mathematically expressed as:
    \begin{equation}
    P(H | E) = \frac{P(E | H) \cdot P(H)}{P(E)}
    \end{equation}
    Where:
    \begin{itemize}
        \item \(P(H | E)\) = Probability of \(H\) given evidence \(E\) (posterior)
        \item \(P(E | H)\) = Probability of \(E\) given \(H\) (likelihood)
        \item \(P(H)\) = Probability of \(H\) before observing \(E\) (prior)
        \item \(P(E)\) = Total probability of observing \(E\) (marginal likelihood)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem Example: Medical Diagnosis}
    \begin{block}{Example: Medical Diagnosis}
        Consider a doctor determining if a patient has a disease (Disease D) with the following known values:
    \end{block}
    \begin{itemize}
        \item Prevalence of Disease D, \(P(D) = 0.01\)
        \item Probability of a positive test if the patient has the disease, \(P(Pos | D) = 0.9\)
        \item Probability of a positive test if the patient does not have the disease, \(P(Pos | \neg D) = 0.05\)
    \end{itemize}
    To update the probability using Bayes' theorem:
    \begin{equation}
    P(Pos) = P(Pos | D) \cdot P(D) + P(Pos | \neg D) \cdot P(\neg D)
    \end{equation}
    \begin{block}{Result}
        Thus, after a positive test, \(P(D | Pos) \approx 0.154\) (15.4\%).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem Example: Spam Email Classification}
    \begin{block}{Example: Spam Email Classification}
        Let's classify an email as spam (S) or not spam (\(\neg S\)):
    \end{block}
    Given:
    \begin{itemize}
        \item \(P(S) = 0.2\)
        \item \(P(Keyword | S) = 0.8\)
        \item \(P(Keyword | \neg S) = 0.1\)
    \end{itemize}
    To calculate \(P(Keyword)\):
    \begin{equation}
    P(Keyword) = P(Keyword | S) \cdot P(S) + P(Keyword | \neg S) \cdot P(\neg S)
    \end{equation}
    \begin{block}{Result}
        Bayes' theorem gives \(P(S | Keyword) \approx 0.667\) (66.7\% chance it's spam).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probabilistic Inference}
    \begin{block}{Overview of Probabilistic Inference}
        \begin{itemize}
            \item **Definition:** Probabilistic inference is the process of deducing new information from known probabilities in a model.
            \item It involves reasoning about uncertain events and updating beliefs based on new evidence.
            \item This is a cornerstone of Bayesian statistics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Probabilistic Inference}
    \begin{enumerate}
        \item **Bayesian Inference**
        \begin{itemize}
            \item Updates prior beliefs using new evidence to obtain posterior probability.
            \item **Formula:**
            \begin{equation}
            P(H | E) = \frac{P(E | H) \cdot P(H)}{P(E)}
            \end{equation}
            \item \textbf{Where:}
            \begin{itemize}
                \item $P(H | E)$ = Posterior probability (updated belief)
                \item $P(E | H)$ = Likelihood (how probable the evidence is given the hypothesis)
                \item $P(H)$ = Prior probability (initial belief)
                \item $P(E)$ = Marginal probability of evidence
            \end{itemize}
        \end{itemize}

        \item **Maximum Likelihood Estimation (MLE)**
        \begin{itemize}
            \item Estimates parameters by maximizing the likelihood function.
            \item Example: For a biased coin, the likelihood function for observing 7 heads out of 10 flips is:
            \begin{equation}
            L(p) = p^7 (1-p)^3
            \end{equation}
            \item Maximizing this provides the best estimate of $p$.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{More Techniques of Probabilistic Inference}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continues numbering from previous frame
        \item **Markov Chain Monte Carlo (MCMC)**
        \begin{itemize}
            \item A class of algorithms for sampling from probability distributions using a Markov chain.
            \item Useful for complex models where direct sampling is challenging.
        \end{itemize}

        \item **Variational Inference**
        \begin{itemize}
            \item Approximates complex distributions by optimizing a simpler one.
            \item Converts inference into an optimization problem.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Misconceptions about Bayes' Theorem - Overview}
    \begin{itemize}
        \item Bayes' theorem updates beliefs based on new evidence.
        \item Misconceptions can cloud understanding and application.
        \item This presentation clarifies common myths about Bayes' theorem.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Misconceptions about Bayes' Theorem - Misconceptions and Clarifications}

    \begin{block}{1. Bayes' Theorem is Just About Conditional Probability}
        \textbf{Misconception:} It is solely about calculating conditional probabilities.\\
        \textbf{Clarification:} It is a principle for updating knowledge with evidence.
    \end{block}
    
    \begin{equation}
        P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
    \end{equation}
    \begin{itemize}
        \item \(P(H|E)\): posterior probability of hypothesis \(H\) given evidence \(E\).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Misconceptions about Bayes' Theorem - More Misconceptions}

    \begin{block}{2. Bayes' Theorem Guarantees Accurate Predictions}
        \textbf{Misconception:} It will always yield correct outcomes.\\
        \textbf{Clarification:} The accuracy relies on quality of priors and likelihoods.
    \end{block}

    \begin{block}{3. Independence of Events Means Ignoring Bayes' Theorem}
        \textbf{Misconception:} It's not applicable if events are independent.\\
        \textbf{Clarification:} It remains valid; independence simplifies calculations.
    \end{block}

    \begin{block}{4. Bayesian Reasoning is Only for Statisticians}
        \textbf{Misconception:} Too complex for non-statisticians.\\
        \textbf{Clarification:} It has broad applications in various fields.
    \end{block}

    \begin{block}{5. Bayes' Theorem is "Always Possible" in Practice}
        \textbf{Misconception:} It can always be applied.\\
        \textbf{Clarification:} Accurate priors and likelihoods may be difficult to obtain.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}

    \begin{itemize}
        \item Bayes' theorem focuses on updating beliefs.
        \item Predictions depend on the quality of input data.
        \item Applicable in multiple fields beyond statistics.
        \item Awareness of practical limitations is crucial.
    \end{itemize}
    
    \begin{block}{Summary}
        Bayes' theorem is foundational for rational decision-making under uncertainty. Keep these points in mind for deeper implementation.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Bayes' Theorem}
    Bayes' Theorem is a powerful tool in probability and statistics that describes how to update the probability of a hypothesis as more evidence becomes available. 

    \begin{block}{Theorem Expression}
        \begin{equation}
        P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
        \end{equation}
    \end{block}
    Where:
    \begin{itemize}
        \item \( P(H|E) \) = Posterior Probability
        \item \( P(E|H) \) = Likelihood
        \item \( P(H) \) = Prior Probability
        \item \( P(E) \) = Marginal Likelihood
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Step-by-Step Implementation}
    Here is a structured approach to implement Bayes' Theorem in a programming environment using Python:

    \begin{enumerate}
        \item \textbf{Define the Probabilities}
            \begin{itemize}
                \item \( P(H) \) = 0.01
                \item \( P(E|H) \) = 0.9
                \item \( P(E|\neg H) \) = 0.05
            \end{itemize}
        \item \textbf{Calculate the Marginal Likelihood} \( P(E) \)
            \begin{equation}
            P(E) = P(E|H) \cdot P(H) + P(E|\neg H) \cdot P(\neg H)
            \end{equation}
        \item \textbf{Implement the Formulas in Code}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Code Implementation}
    Using Python, you can implement the calculations as follows:
    
    \begin{lstlisting}[language=Python]
# Step 1: Define the probabilities
P_H = 0.01  # Probability of having the disease
P_E_given_H = 0.9  # Likelihood
P_E_given_not_H = 0.05  # Likelihood of false positive
P_not_H = 1 - P_H  # Probability of not having the disease

# Step 2: Calculate marginal likelihood P(E)
P_E = (P_E_given_H * P_H) + (P_E_given_not_H * P_not_H)

# Step 3: Calculate posterior probability P(H|E)
P_H_given_E = (P_E_given_H * P_H) / P_E

print(f"Posterior Probability (P(H|E)): {P_H_given_E:.4f}")
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Interpreting the Results}
    After running the code, interpret the value of \( P(H|E) \):
    \begin{itemize}
        \item \( P(H|E) > 0.7 \): Strong likelihood of having the disease
        \item \( P(H|E) < 0.5 \): Test result is not definitive
    \end{itemize}

    \textbf{Key Points to Remember:}
    \begin{itemize}
        \item Prior Probability sets the stage for updates.
        \item Accurate Likelihoods are crucial (sensitivity and specificity).
        \item Understanding Marginal Likelihood is key for accurate calculations.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Implementing Bayes' Theorem involves understanding hypotheses, calculating probabilities, and using programming to derive conclusions. Hands-on practice enhances understanding and reinforces learned concepts.

    Remember, proper interpretation of probabilities is essential for informed decision-making based on Bayes' Theorem!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Example of Bayes' Theorem - Overview}
    \begin{block}{Bayes' Theorem}
        Bayes' Theorem is a powerful tool for quantitative reasoning about uncertainty and is expressed mathematically as:
        \[
        P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
        \]
    \end{block}
    \begin{itemize}
        \item \( P(A|B) \): Posterior probability (probability of hypothesis \( A \) given evidence \( B \)).
        \item \( P(B|A) \): Likelihood (probability of observing evidence \( B \) under the assumption that \( A \) is true).
        \item \( P(A) \): Prior probability (initial probability of hypothesis \( A \) before seeing evidence \( B \)).
        \item \( P(B) \): Marginal likelihood (total probability of evidence \( B \)).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Example of Bayes' Theorem - Medical Diagnosis}
    \begin{block}{Scenario}
        Assess the probability that a patient has a particular disease based on a positive test result.
    \end{block}
    \begin{itemize}
        \item Prevalence of the disease \( P(Disease) = 0.01 \) (1\%).
        \item Probability of testing positive if diseased \( P(Pos|Disease) = 0.90 \) (90\%).
        \item Probability of testing positive if not diseased \( P(Pos|No Disease) = 0.05 \) (5\%).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Application of Bayes' Theorem}
    \begin{enumerate}
        \item \textbf{Identify Variables:}
            \begin{itemize}
                \item Let \(A\) = Patient has the disease.
                \item Let \(B\) = Test result is positive.
            \end{itemize}

        \item \textbf{Apply the Formula:}
            \[
            P(B) = P(B|A) \cdot P(A) + P(B|Not A) \cdot P(Not A)
            \]
            Substitute known values:
            \[
            P(B) = (0.90 \times 0.01) + (0.05 \times 0.99) = 0.009 + 0.0495 = 0.0585
            \]

        \item \textbf{Calculate \(P(Disease|Pos)\):}
            \[
            P(Disease|Pos) = \frac{P(Pos|Disease) \cdot P(Disease)}{P(Pos)} = \frac{0.90 \times 0.01}{0.0585} \approx 0.1538
            \]
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item The calculated probability \(P(Disease|Pos) \approx 15.38\%\) illustrates that only about 15\% of patients with a positive test actually have the disease, due to the disease's low prevalence and the existence of false positives.
        \item \textbf{Importance of Prior Knowledge:} The difference between the likelihood of the evidence and the actual case underscores the role of prior probability in Bayesian reasoning.
    \end{itemize}
    \begin{block}{Conclusion}
        This example demonstrates the application of Bayes' Theorem in medical diagnostics, enabling data-informed decisions for effective healthcare planning and intervention.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Learning Objectives - Overview}
    \begin{block}{Overview}
        In this chapter, we explored the foundations of probabilistic reasoning and how it is elegantly encapsulated in Bayes' Theorem. Our learning objectives provided a structured approach to understanding these critical concepts, their applications, and their significance in decision-making processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Learning Objectives - Key Concepts}
    \begin{enumerate}
        \item \textbf{Understanding Probabilistic Reasoning}
        \begin{itemize}
            \item Probabilistic reasoning involves drawing conclusions based on uncertain information.
            \item Helps in making predictions about future events based on prior knowledge.
            \item \textit{Example:} Predicting the weather. If it rains 30% of the time, we prepare accordingly.
        \end{itemize}
        
        \item \textbf{Introduction to Bayes' Theorem}
        \begin{itemize}
            \item Provides a mathematical framework for updating beliefs with new evidence.
            \item Relates conditional and marginal probabilities.
            \item \textbf{Formula:} 
            \begin{equation}
            P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Learning Objectives - Applications and Implications}
    \begin{enumerate}
        \item \textbf{Application of Bayes’ Theorem}
        \begin{itemize}
            \item \textit{Example:} Medical Diagnosis
            \item Disease prevalence of 1% with a test sensitivity of 90% and specificity of 95%.
            \item Bayes' theorem calculates the actual probability of having the disease after a positive test result.
        \end{itemize}
        
        \item \textbf{Implications for Decision Making}
        \begin{itemize}
            \item Bayesian reasoning allows continual updating of beliefs with new data.
            \item Significant improvement in decision outcomes when integrating probabilistic reasoning.
        \end{itemize}

        \item \textbf{Conclusion}
        \begin{itemize}
            \item Chapter reinforces how probabilistic reasoning and Bayes' Theorem aid in managing uncertainty.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Probabilistic Reasoning}
    \begin{block}{Introduction}
        Probabilistic reasoning is a vital aspect of AI that enables decision-making under uncertainty. The future promises advancements that will transform this field.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends and Advancements}
    \begin{enumerate}
        \item Improved Algorithms and Models
        \item Enhanced Data Integration
        \item Explainable AI (XAI)
    \end{enumerate}
    \begin{block}{Conclusion}
        Trends in probabilistic reasoning are set to enhance model accuracy, data fusion, explainability, and more.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Improved Algorithms and Models}
    \begin{itemize}
        \item \textbf{Advancements in Bayesian Models}: Integrating Bayesian inference with deep learning.
        \item \textbf{Example}: Bayesian Neural Networks reduce overfitting through model averages.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Enhanced Data Integration}
    \begin{itemize}
        \item Combining structured and unstructured data leads to robust models.
        \item \textbf{Example}: Healthcare applications improve diagnostic accuracy by integrating clinical and genomic data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Explainable AI (XAI)}
    \begin{itemize}
        \item \textbf{Transparent Decision-Making}: Ensuring that probabilistic models are interpretable.
        \item \textbf{Key Point}: Tools like Shapley values and LIME clarify how probabilities impact outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Complex Systems Modeling}
    \begin{itemize}
        \item Need for advanced probabilistic graphical models to represent complex interactions.
        \item \textbf{Example}: In climate modeling, consider multifactor interactions for various outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Bayesian Optimization in Machine Learning}
    \begin{itemize}
        \item Use of probabilistic reasoning for hyperparameter optimization.
        \item \textbf{Key Point}: Gaussian Processes enhance performance tuning by predicting outcomes from previous evaluations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{6. Applications in Robotics and Autonomous Systems}
    \begin{itemize}
        \item Future robots will account for uncertainties in their environments.
        \item \textbf{Example}: Autonomous vehicles applying sensor fusion to make decisions under noisy conditions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        The evolution of probabilistic reasoning is paving the way for innovations in AI, enhancing its capacity to manage real-world complexities.
    \end{block}
    \begin{itemize}
        \item Integration of Bayesian Methods: Enhances model accuracy.
        \item Data Fusion: Merges multiple data types for better insights.
        \item Explainability: Builds trust in AI systems.
        \item Optimized Decision-Making: Increases learning algorithm efficiency.
        \item Robotics Advances: Improves autonomous operations in uncertain environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Q\&A Session - Overview}
  \begin{itemize}
    \item This slide is an open floor for questions and clarifications on content from Week 8.
    \item Focus on:
    \begin{itemize}
      \item Probabilistic Reasoning
      \item Bayes' Theorem
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Learning Objectives}
  \begin{enumerate}
    \item \textbf{Understand Probabilistic Reasoning:} 
      \begin{itemize}
        \item Drawing conclusions based on outcome likelihood.
        \item Essential in AI and machine learning.
      \end{itemize}
    \item \textbf{Interpret Bayes' Theorem:}
      \begin{itemize}
        \item Framework for updating probabilities with new evidence.
      \end{itemize}
    \item \textbf{Apply Concepts to Real-World Problems:}
      \begin{itemize}
        \item Utilize these methods for effective problem-solving.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts and Examples}
  \begin{block}{Bayes' Theorem Formula}
    \begin{equation}
    P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
    \end{equation}
    \begin{itemize}
      \item Where:
      \begin{itemize}
        \item $P(A|B)$ = Probability of hypothesis $A$ given evidence $B$.
        \item $P(B|A)$ = Probability of evidence $B$ given hypothesis $A$.
        \item $P(A)$ = Probability of hypothesis $A$.
        \item $P(B)$ = Probability of evidence $B$.
      \end{itemize}
    \end{itemize}
  \end{block}

  \begin{itemize}
    \item \textbf{Example 1:} Medical Diagnosis
    \item \textbf{Example 2:} Weather Prediction
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Introduction}
    Understanding probabilistic reasoning and Bayes' theorem is essential for making informed decisions and predictions under uncertainty. This slide presents additional resources to deepen your knowledge and application of these concepts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Recommended Reading}
    \begin{block}{Books}
        \begin{itemize}
            \item \textbf{"Bayesian Reasoning and Machine Learning" by David Barber}
            \begin{itemize}
                \item \textit{Overview:} A comprehensive introduction to Bayesian inference and its applications in machine learning.
                \item \textit{Key Concepts:} Covers Bayesian networks, learning algorithms, and practical applications.
            \end{itemize}
         
            \item \textbf{"Probabilistic Graphical Models: Principles and Techniques" by Daphne Koller and Nir Friedman}
            \begin{itemize}
                \item \textit{Overview:} Focuses on the role of probabilistic models in reasoning and decision making.
                \item \textit{Key Concepts:} In-depth coverage of graphical models, inference methods, and model learning.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Research Papers}
        \begin{itemize}
            \item \textbf{"A Few Useful Things to Know About Machine Learning" by Pedro Domingos}
            \begin{itemize}
                \item \textit{Overview:} Discusses the role of Bayes' theorem in machine learning.
                \item \textit{Key Concepts:} Highlights common pitfalls and practical applications of probabilistic reasoning.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Online Courses & Tools}
    \begin{block}{Online Courses}
        \begin{itemize}
            \item \textbf{Coursera: "Probabilistic Graphical Models" by Stanford University}
            \begin{itemize}
                \item \textit{Highlights:} Provides a solid foundation in probabilistic reasoning through video lectures and hands-on assignments.
            \end{itemize}
            \item \textbf{edX: "Bayesian Statistics: From Concept to Data Analysis" by University of California, Santa Cruz}
            \begin{itemize}
                \item \textit{Highlights:} Focuses on the practical application of Bayesian concepts in data analysis.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Online Resources}
        \begin{itemize}
            \item \textbf{Bayes’ Theorem Interactive Tool}
            \begin{itemize}
                \item \textit{Description:} An online tool that allows users to manipulate different variables to see how they impact probabilities.
                \item \textit{Link:} \url{https://www.bayestheoremcalculator.com}
            \end{itemize}
            \item \textbf{"Understanding Bayes' Theorem with Effective Examples" - Towards Data Science}
            \begin{itemize}
                \item \textit{Description:} Simplifies the concept of Bayes' theorem with relatable examples and illustrations.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}


\end{document}