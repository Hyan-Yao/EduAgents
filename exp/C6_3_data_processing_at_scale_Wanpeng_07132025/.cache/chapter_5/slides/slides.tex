\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{caption}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Week 5: Exploring Hadoop Ecosystem]{Week 5: Exploring Hadoop Ecosystem}
\author[J. Smith]{Your Name}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Hadoop Ecosystem}
    \begin{block}{Overview of Hadoop}
        Hadoop is an open-source framework designed for distributed processing of large datasets across clusters of computers using simple programming models. 
        It scales from a single server to thousands of machines, providing high throughput access to application data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Hadoop Ecosystem}
    \begin{enumerate}
        \item \textbf{Hadoop Distributed File System (HDFS)}
        \begin{itemize}
            \item Storage for vast amounts of data across multiple machines.
            \item Breaks large files into smaller blocks (usually 128 MB) and distributes them for redundancy and high availability.
            \item \textit{Example:} Store terabytes of user data from a website reliably.
        \end{itemize}
        
        \item \textbf{Yet Another Resource Negotiator (YARN)}
        \begin{itemize}
            \item Resource management layer that schedules resources across applications.
            \item Allows multiple processing engines (MapReduce, Spark) to securely share data.
            \item \textit{Example:} A MapReduce job can run alongside a Spark job on the same data.
        \end{itemize}
        
        \item \textbf{MapReduce}
        \begin{itemize}
            \item Programming model for processing large data sets through distributed algorithms.
            \item Consists of Map (sorting) and Reduce (aggregating results) phases.
            \item \textit{Example:} Count occurrences of each word in a document.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Hadoop in Big Data Processing}
    \begin{itemize}
        \item \textbf{Scalability}
        \begin{itemize}
            \item Can process petabytes of data by adding more nodes.
        \end{itemize}
        
        \item \textbf{Cost-effectiveness}
        \begin{itemize}
            \item Runs on commodity hardware, minimizing infrastructure costs.
        \end{itemize}
        
        \item \textbf{Fault Tolerance}
        \begin{itemize}
            \item Data replication across nodes ensures continuous operation despite failures.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Facilitates efficient storage, processing, and analysis of large datasets.
            \item Versatile integration with other tools enriches analytics capabilities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Ecosystem Architecture}
    \begin{center}
        \includegraphics[width=0.8\linewidth]{hadoop_ecosystem_diagram.png} % Include your diagram here
    \end{center}
    \begin{itemize}
        \item Illustration of the components and their interactions in the Hadoop ecosystem.
        \item Highlights the interconnected nature of HDFS, YARN, and MapReduce.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Components of the Hadoop Ecosystem}
    \begin{block}{Introduction}
        The Hadoop ecosystem is essential for processing and analyzing vast amounts of data. Understanding its core components—Hadoop Distributed File System (HDFS), Yet Another Resource Negotiator (YARN), and MapReduce—is crucial for leveraging the full power of Hadoop.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Distributed File System (HDFS)}
    \begin{itemize}
        \item \textbf{Definition}: HDFS is the primary storage system of Hadoop, designed to hold large datasets reliably and to stream those data efficiently.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Distributed Storage}: Splits data into blocks (default size is 128 MB) and distributes them across a cluster.
            \item \textbf{Fault Tolerance}: Each block is replicated (default is 3 copies), ensuring no data loss in case of node failure.
        \end{itemize}
        \item \textbf{Example}: In a retail environment, HDFS could store millions of transactions from multiple outlets, enabling analysis of sales trends.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Yet Another Resource Negotiator (YARN)}
    \begin{itemize}
        \item \textbf{Definition}: YARN is the resource management layer of Hadoop. It allocates cluster resources to different applications, balancing system resource usage.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Resource Management}: Dynamically allocates resources based on application demand.
            \item \textbf{Job Scheduling}: Manages multiple jobs in a cluster, allowing efficient multi-tenancy.
        \end{itemize}
        \item \textbf{Illustration}: YARN can manage a mix of data processing jobs like MapReduce and Spark, optimizing resources based on workload.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MapReduce Overview}
    \begin{itemize}
        \item \textbf{Definition}: MapReduce is a programming model and processing engine that allows for the distributed processing of large data sets using parallel algorithms.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Map Function}: Transforms input data into key-value pairs.
            \item \textbf{Reduce Function}: Aggregates these key-value pairs to produce final results.
        \end{itemize}
        \item \textbf{Example}: Analyzing user reviews.
        \begin{itemize}
            \item \textbf{Map}: Generate pairs (word, count) from each review.
            \item \textbf{Reduce}: Sum up counts for each unique word across all reviews.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MapReduce Code Example}
    \begin{lstlisting}[language=Java]
    public class WordCount {
        public static class Mapper extends MapReduceBase 
              implements Mapper<LongWritable, Text, Text, IntWritable> {
            public void map(LongWritable key, Text value, 
                            OutputCollector<Text, IntWritable> output, 
                            Reporter reporter) throws IOException {
                String[] words = value.toString().split(" ");
                for (String word : words) {
                    output.collect(new Text(word), new IntWritable(1));
                }
            }
        }
    }
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item HDFS provides reliable, distributed storage essential for big data.
            \item YARN is crucial for effective resource management and scheduling in a multi-application environment.
            \item MapReduce is foundational for processing large datasets through a simple, parallelizable model.
        \end{itemize}
        \item \textbf{Conclusion}: These core components of the Hadoop ecosystem work together to handle diverse data storage and processing needs, making Hadoop a powerful tool for big data analytics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Distributed File System (HDFS) - Overview}
    \begin{itemize}
        \item HDFS is a centralized storage system in the Hadoop ecosystem.
        \item Designed for storing vast amounts of data across multiple machines.
        \item Optimized for high-throughput access and fault tolerance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{HDFS Architecture}
    HDFS follows a master/slave architecture consisting of:
    \begin{enumerate}
        \item \textbf{NameNode (Master)}
            \begin{itemize}
                \item Manages file system namespace and metadata.
                \item Tracks the location of data blocks.
                \item Frequent communication with DataNodes is required.
                \item Critical: If it fails, access to data is lost.
            \end{itemize}
        \item \textbf{DataNodes (Slaves)}
            \begin{itemize}
                \item Store actual data and serve read/write requests.
                \item Send heartbeat signals to report their health status to NameNode.
                \item Data is divided into blocks (default size: 128 MB).
            \end{itemize}
    \end{enumerate}
    \begin{block}{Diagram of HDFS Architecture}
    \centering
    \includegraphics[width=0.8\textwidth]{hdfs_architecture_diagram} % Place the corresponding diagram image here
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Storage and Replication in HDFS}
    \textbf{Data Storage:}
    \begin{itemize}
        \item Large files are split into smaller blocks (default block size = 128 MB).
        \item Blocks are distributed across the cluster to utilize storage and enhance parallel processing.
        \item Example: A 300MB file is split into three blocks.
    \end{itemize}

    \textbf{Data Replication:}
    \begin{itemize}
        \item Default replication factor is three.
        \item Each block is replicated:
            \begin{itemize}
                \item One on the same rack.
                \item One on a different rack within the data center.
                \item One on another rack.
            \end{itemize}
        \item Benefits:
            \begin{itemize}
                \item Fault tolerance and data availability.
            \end{itemize}
    \end{itemize}

    \textbf{Key Commands:}
    \begin{lstlisting}
    hdfs dfsadmin -report    % Check HDFS Status
    hdfs dfs -ls /           % List files in HDFS
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Yet Another Resource Negotiator (YARN) - Introduction}
    \begin{block}{What is YARN?}
        YARN (Yet Another Resource Negotiator) is a crucial component of the Hadoop ecosystem that improves the scalability and resource management capabilities of a Hadoop cluster.
    \end{block}
    \begin{itemize}
        \item Serves as a central resource management layer.
        \item Manages computing resources and job scheduling.
        \item Enables multiple data processing frameworks to run simultaneously.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{YARN - Key Functions}
    \begin{enumerate}
        \item \textbf{Resource Management:}
            \begin{itemize}
                \item Allocates memory and CPU resources.
                \item Facilitates efficient workload distribution.
                \item Avoids resource contention for effective application performance.
            \end{itemize}
        
        \item \textbf{Job Scheduling:}
            \begin{itemize}
                \item Assigns tasks based on available resources.
                \item Supports dynamic and adaptive scheduling.
                \item Allows jobs to start as resources become available.
            \end{itemize}
        
        \item \textbf{Multi-Tenancy:}
            \begin{itemize}
                \item Supports multiple applications (e.g., Spark, Flink).
                \item Ensures isolated resource usage for different applications.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{YARN Architecture and Use Case}
    \begin{block}{Architecture Components}
        \begin{itemize}
            \item \textbf{ResourceManager (RM):}
                \begin{itemize}
                    \item Master daemon managing cluster resources.
                    \item Contains Scheduler and ApplicationManager subcomponents.
                \end{itemize}
            \item \textbf{NodeManager (NM):}
                \begin{itemize}
                    \item Runs on each node, managing allocated resources.
                    \item Monitors resource usage and executes containers.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Example Use Case}
        A company runs multiple big data jobs (e.g., Spark, MapReduce, Flink) on a single cluster, maximizing resource utilization as YARN reallocates resources between jobs dynamically.
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{MapReduce Framework}
  \begin{block}{Overview of MapReduce}
    MapReduce is a programming model widely used for processing large data sets in a distributed computing environment. It simplifies data processing across vast numbers of machines by breaking tasks into smaller, manageable pieces.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Key Stages of the MapReduce Model}
  \begin{enumerate}
    \item \textbf{Map Stage}
    \begin{itemize}
      \item \textbf{Concept:} Input data is divided into segments, which are processed to extract key-value pairs.
      \item \textbf{Functionality:} Each mapper takes input data, processes it, and produces an intermediate set of key-value pairs.
      \item \textbf{Example:} Input: A log file with user activities.
      \begin{lstlisting}[language=Python]
def mapper(line):
    # Splitting the line and generating key-value pairs
    for word in line.split():
        yield (word, 1)  # Each word is a key, and its count is a value
      \end{lstlisting}
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Key Stages of the MapReduce Model (cont'd)}
  \begin{enumerate}
    \setcounter{enumi}{1}
    \item \textbf{Shuffle and Sort}
    \begin{itemize}
      \item \textbf{Concept:} Occurs between Map and Reduce stages, grouping output of mappers by key.
      \item \textbf{Functionality:} Prepares data for the Reduce phase by sorting.
      \item \textbf{Example:} Input like [('apple', 1), ('banana', 1), ('apple', 1)] becomes [('apple', 2), ('banana', 1)].
    \end{itemize}

    \item \textbf{Reduce Stage}
    \begin{itemize}
      \item \textbf{Concept:} Receives sorted intermediate data to compute the final output.
      \item \textbf{Functionality:} Reducers perform summary operations on aggregated input.
      \item \textbf{Example:} 
      \begin{lstlisting}[language=Python]
def reducer(word, counts):
    return (word, sum(counts))  # Summing counts for each word
      \end{lstlisting}
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{MapReduce Workflow}
  \begin{block}{Flow of MapReduce}
    \includegraphics[width=\linewidth]{your_diagram_path_here} % Placeholder for a diagram
    % Replace with the actual path to your workflow diagram image
    \newline
    \textbf{Input Data} $\rightarrow$ \textbf{Map} $\rightarrow$ \textbf{Shuffle and Sort} $\rightarrow$ \textbf{Reduce} $\rightarrow$ \textbf{Output Data}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Key Points and Conclusion}
  \begin{itemize}
    \item \textbf{Scalability:} Handles petabytes of data through distributed processing.
    
    \item \textbf{Abstraction:} Developers focus on data processing logic without needing to manage infrastructure.
    
    \item \textbf{Fault Tolerance:} Framework automatically handles failures by re-running failed tasks.
  \end{itemize}
  \begin{block}{Conclusion}
    The MapReduce framework is essential in the Hadoop ecosystem, facilitating efficient data processing crucial for scalable big data applications.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Pig}
    \begin{block}{Introduction to Apache Pig}
        \begin{itemize}
            \item Apache Pig is a high-level platform for creating programs that run on Apache Hadoop.
            \item It simplifies processing of large datasets by abstracting the complexity of writing MapReduce programs.
        \end{itemize}
    \end{block}
    
    \begin{block}{Purpose of Apache Pig}
        \begin{itemize}
            \item Designed for analyzing large datasets in a distributed environment.
            \item Allows for complex data transformations without the need for expert knowledge in MapReduce.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Pig's Scripting Language: Pig Latin}
    
    \begin{block}{What is Pig Latin?}
        \begin{itemize}
            \item Pig Latin is the language used to write scripts in Apache Pig.
            \item It is designed to be easy to read and write, enhancing accessibility for non-programmers.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Features of Pig Latin}
        \begin{enumerate}
            \item Data Flow Language: Resembles SQL, intuitive for data analysis tasks.
            \item Built-in Functions: Operators for filtering, grouping, and joining datasets.
            \item Extensibility: Allows User Defined Functions (UDFs) for specialized operations.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Pig Latin Script}
    
    \begin{lstlisting}[language=Pig]
    -- Load data from a CSV file
    data = LOAD 'data.csv' USING PigStorage(',') AS (name:chararray, age:int, salary:double);
    
    -- Filter records for individuals with a salary greater than 50000
    filtered_data = FILTER data BY salary > 50000;
    
    -- Group data by age
    grouped_data = GROUP filtered_data BY age;
    
    -- Calculate the average salary for each age group
    average_salary = FOREACH grouped_data GENERATE group AS age, AVG(filtered_data.salary) AS avg_salary;
    
    -- Store the result
    STORE average_salary INTO 'output/average_salary' USING PigStorage(',');
    \end{lstlisting}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Simplicity: Allows for complex tasks to be expressed simply.
            \item Integration with Hadoop: Seamless operation within the Hadoop ecosystem.
            \item Versatility: Great for various data processing tasks like ETL and analysis.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Pig for Data Transformation - Introduction}
    \begin{block}{Apache Pig Overview}
        Apache Pig is a high-level platform for creating programs that run on Apache Hadoop. It simplifies the complexities of writing complex MapReduce programs by offering a scripting language called Pig Latin, making it easier to perform data transformation tasks such as:
    \end{block}
    \begin{itemize}
        \item \textbf{Extract}: Retrieve data from various sources.
        \item \textbf{Transform}: Clean and prepare the data for analysis.
        \item \textbf{Load}: Store the processed data into a database or other storage systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Pig for Data Transformation - Key Functions}
    \begin{block}{Key Functions in Pig}
        1. \textbf{LOAD}: To load data from the filesystem. \\
        2. \textbf{FILTER}: To remove unwanted data from the dataset. \\
        3. \textbf{FOREACH...GENERATE}: To transform and project data, allowing you to manipulate fields. \\
        4. \textbf{GROUP}: To group data by specific fields. \\
        5. \textbf{JOIN}: To combine datasets based on a common field.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Pig for Data Transformation - Example Scripts}
    \begin{block}{Example 1: Basic ETL Process}
        \begin{lstlisting}[language=Pig]
-- Load the data
transactions = LOAD 'transactions.txt' USING PigStorage(',') AS (user_id:int, amount:double, date:chararray);

-- Filter transactions greater than $100
filtered_transactions = FILTER transactions BY amount > 100;

-- Transform by projecting only user_id and date
result = FOREACH filtered_transactions GENERATE user_id, date;

-- Store the result into a new text file
STORE result INTO 'filtered_transactions' USING PigStorage(',');
        \end{lstlisting}
    \end{block}

    \begin{block}{Example 2: Data Analysis with Grouping}
        \begin{lstlisting}[language=Pig]
-- Load data
transactions = LOAD 'transactions.txt' USING PigStorage(',') AS (user_id:int, amount:double, date:chararray);

-- Group by user_id
grouped_transactions = GROUP transactions BY user_id;

-- Calculate total amount per user
total_per_user = FOREACH grouped_transactions GENERATE group AS user_id, SUM(transactions.amount) AS total_amount;

-- Store results
STORE total_per_user INTO 'total_per_user' USING PigStorage(',');
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Hive - Overview}
    \begin{itemize}
        \item Apache Hive is a data warehousing solution built on top of Hadoop.
        \item Facilitates querying and managing large datasets in distributed storage.
        \item Uses HiveQL, a SQL-like language, to simplify data analysis.
        \item Allows users to perform data analysis without complex MapReduce programming.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Hive - Key Features}
    \begin{enumerate}
        \item \textbf{SQL-Like Query Language (HiveQL):}
            \begin{itemize}
                \item Similar to ANSI SQL, making it accessible for users.
                \item Supports querying, inserting, updating, and deleting data in HDFS.
            \end{itemize}
        \item \textbf{Schema on Read:}
            \begin{itemize}
                \item Data stored without predefined schema; interpreted during queries.
                \item Provides flexibility in data ingestion.
            \end{itemize}
        \item \textbf{Extensibility:}
            \begin{itemize}
                \item Users can create User Defined Functions (UDFs) for custom operations.
            \end{itemize}
        \item \textbf{Support for Various File Formats:}
            \begin{itemize}
                \item Compatible with text, JSON, Avro, Parquet, and ORC.
            \end{itemize}
        \item \textbf{Partitioning and Bucketing:}
            \begin{itemize}
                \item Enhances query performance by managing large datasets effectively.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Hive - Example Query}
    \begin{block}{Example Query in HiveQL}
        \begin{lstlisting}[language=SQL]
SELECT product_name, SUM(sales)
FROM sales_data
WHERE sale_date >= '2023-01-01'
GROUP BY product_name
HAVING SUM(sales) > 1000
ORDER BY SUM(sales) DESC;
        \end{lstlisting}
    \end{block}
    \begin{itemize}
        \item This query retrieves product names and their total sales exceeding 1000 since January 1, 2023.
        \item Demonstrates Hive's simplification of complex data inquiries compared to MapReduce coding.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Creating Hive Tables}
    \textbf{Overview of Hive Tables}
    \begin{itemize}
        \item Apache Hive is used for managing and querying large datasets in a Hadoop ecosystem.
        \item A fundamental aspect is its capability to create and manage tables tailored to specific data needs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Create Hive Tables - Part 1}
    \textbf{1. Create a Basic Table}
    \begin{block}{Syntax}
        \begin{lstlisting}
CREATE TABLE table_name (
    column1_name column1_type,
    column2_name column2_type,
    ...
);
        \end{lstlisting}
    \end{block}
    
    \textbf{Example:}
    \begin{lstlisting}
CREATE TABLE employees (
    id INT,
    name STRING,
    department STRING
);
    \end{lstlisting}
    
    \textbf{2. Specify File Format}
    \begin{lstlisting}
CREATE TABLE employees (
    id INT,
    name STRING,
    department STRING
)
STORED AS PARQUET;
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Create Hive Tables - Part 2}
    \textbf{3. Insert Data}
    \begin{lstlisting}
INSERT INTO TABLE employees VALUES (1, 'Alice', 'Sales');
    \end{lstlisting}
    
    \textbf{4. Partitioning}
    \begin{itemize}
        \item Helps manage large datasets by dividing them into smaller pieces.
        \item Creating a Partitioned Table:
        \begin{lstlisting}
CREATE TABLE employees (
    id INT,
    name STRING
)
PARTITIONED BY (department STRING);
        \end{lstlisting}
    \end{itemize}
    
    \textbf{Inserting Data into Partitions:}
    \begin{lstlisting}
INSERT INTO TABLE employees PARTITION (department='Sales') VALUES (1, 'Alice');
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Create Hive Tables - Part 3}
    \textbf{5. Bucketing}
    \begin{itemize}
        \item Organizes the data into a fixed number of files.
        \item Creating a Bucketed Table:
        \begin{lstlisting}
CREATE TABLE employees (
    id INT,
    name STRING
)
CLUSTERED BY (id) INTO 4 BUCKETS;
        \end{lstlisting}
    \end{itemize}

    \textbf{Summary Points:}
    \begin{itemize}
        \item Bucketing helps optimize queries that aggregate data.
        \item Provides the benefits of both partitioning and improved join performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Quick Reference}
    \begin{lstlisting}
CREATE TABLE employees (
    id INT,
    name STRING
)
PARTITIONED BY (department STRING)
CLUSTERED BY (id) INTO 4 BUCKETS
STORED AS PARQUET;
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Diagram Suggestion}
    \textbf{Consider adding a diagram illustrating the concept of partitioning and bucketing in Hive tables.}
    
    \begin{itemize}
        \item Show how data is structured and accessed in partitioned and bucketed tables.
    \end{itemize}
    
    \textbf{Conclusion:}
    \begin{itemize}
        \item Mastering these concepts will enhance your ability to work effectively with large datasets in a Hadoop environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache HBase - Introduction}
  \begin{block}{What is HBase?}
    HBase is an open-source, distributed, and scalable NoSQL database built on top of the Hadoop ecosystem. It is modeled after Google’s Bigtable and designed to handle large amounts of data in real time.
  \end{block}

  \begin{itemize}
    \item \textbf{Non-relational Database}: Stores data in flexible formats without fixed schemas.
    \item \textbf{Distributed Architecture}: Automatically shards large tables across servers for high availability and fault tolerance.
    \item \textbf{Real-Time Read/Write Access}: Provides low-latency access to large volumes of data.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache HBase - Use Cases}
  \begin{enumerate}
    \item \textbf{Real-Time Analytics:} 
      \begin{itemize}
        \item Example: Social media apps providing real-time user insights and activity feeds.
      \end{itemize}
    \item \textbf{Data Warehousing:} 
      \begin{itemize}
        \item Example: Retail companies storing historical transaction data for analysis.
      \end{itemize}
    \item \textbf{Time-Series Data:} 
      \begin{itemize}
        \item Example: IoT applications monitoring sensor data over time.
      \end{itemize}
    \item \textbf{Content Management Systems:} 
      \begin{itemize}
        \item Example: News websites managing articles and user comments dynamically.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache HBase - Key Points}
  \begin{itemize}
    \item \textbf{Scalability:} HBase scales horizontally, accommodating increasing data volumes without significant downtime.
    \item \textbf{Column-Family Data Structure:} Efficient for read and write operations on specific columns.
    \item \textbf{Integration with Hadoop:} Works seamlessly with Hadoop's MapReduce for batch processing.
  \end{itemize}
  
  \begin{block}{Diagram: HBase Data Model}
    \begin{verbatim}
                    +-------------------+
                    |      HBase Table  |
                    +-------------------+
                          /       |       \
                       Col1    Col2 ...   ColN
                       (CF1)   (CF2)      (CFN)
                   +-------+ +-------+ +-------+
                   | Row 1 | | Row 2 | | Row 3 |
                   +-------+ +-------+ +-------+
                   | ...   | | ...   | | ...   |
                   +-------+ +-------+ +-------+
    \end{verbatim}
    *Note: Each row can have a variable number of columns, contributing to HBase’s flexibility.*
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache HBase - Conclusion}
  HBase serves as an essential component within the Hadoop ecosystem, particularly for applications that require quick access to large volumes of data with low latency. By utilizing a column-oriented storage model, it provides a robust platform for real-time data processing.
\end{frame}

\begin{frame}[fragile]
    \frametitle{HBase Architecture}
    \begin{block}{Overview}
        HBase is a distributed, scalable, NoSQL database integrated with the Hadoop ecosystem, enabling real-time access to vast datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding HBase Architecture}
    \begin{itemize}
        \item Designed for real-time read/write access
        \item Handles massive datasets across commodity hardware
        \item Key focus on scalability and low-latency access
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of HBase Architecture}
    \begin{block}{HMaster}
        \begin{itemize}
            \item Central coordinator
            \item Manages region servers and load balancing
        \end{itemize}
    \end{block}

    \begin{block}{Region Servers}
        \begin{itemize}
            \item Handle read/write requests
            \item Manage regions (subset of data rows)
        \end{itemize}
    \end{block}

    \begin{block}{Regions}
        \begin{itemize}
            \item Basic scalability unit
            \item Dynamic splitting and distribution
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage and Coordination}
    \begin{block}{HFile}
        \begin{itemize}
            \item Storage format on HDFS
            \item Optimized for fast read access
        \end{itemize}
    \end{block}
    
    \begin{block}{Zookeeper}
        \begin{itemize}
            \item Coordination service for HBase
            \item Keeps track of region servers and configurations
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration with Hadoop Ecosystem}
    \begin{itemize}
        \item Relies on HDFS for scalable and fault-tolerant storage
        \item Integration with MapReduce for batch processing
        \item Compatibility with tools like Apache Spark, Hive, and Pig
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case}
    \begin{block}{Retail Analysis}
        A retail company leveraging HBase can:
        \begin{itemize}
            \item Store large transaction volumes
            \item Retrieve user data quickly for real-time analysis
            \item Provide recommendations based on current buying patterns
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item HBase architecture enables scalability and flexibility
        \item Supports a variety of workloads and use cases
        \item Essential for understanding the broader Hadoop ecosystem
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Choosing the Right Tool}
    \begin{block}{Overview of Hadoop Tools}
        The Hadoop ecosystem includes various tools for big data processing:
        \begin{itemize}
            \item \textbf{Apache Pig}
            \item \textbf{Apache Hive}
            \item \textbf{Apache HBase}
        \end{itemize}
        Each tool is suited for specific use cases based on user needs and data characteristics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Pig}
    \begin{block}{Purpose}
        A high-level scripting language for processing large datasets.
    \end{block}
    \begin{block}{Use Cases}
        \begin{itemize}
            \item Complex data flows requiring processing and analysis.
            \item Data transformation tasks and ETL processes.
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        Processing raw log data to find user behavior patterns:
        \begin{lstlisting}[language=Pig, style=custom]
        A = LOAD 'logs.txt' USING PigStorage(',') AS (userid:int, timestamp:long, action:chararray);
        B = GROUP A BY userid;
        C = FOREACH B GENERATE COUNT(A) AS action_count;
        DUMP C;
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Hive}
    \begin{block}{Purpose}
        A data warehousing tool for querying and managing large datasets with a SQL-like language.
    \end{block}
    \begin{block}{Use Cases}
        \begin{itemize}
            \item Executing ad-hoc queries.
            \item Summarizing, aggregating, or analyzing large datasets.
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        Extracting insights from sales data:
        \begin{lstlisting}[language=SQL, style=custom]
        SELECT COUNT(*), SUM(sales_amount)
        FROM sales
        WHERE region = 'West'
        GROUP BY product_id;
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache HBase}
    \begin{block}{Purpose}
        A NoSQL database providing real-time read/write access to large datasets stored in HDFS.
    \end{block}
    \begin{block}{Use Cases}
        \begin{itemize}
            \item Fast access to sparse datasets and real-time operations.
            \item Ideal for time-series data or user profiles.
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        Storing user activity in a social media application:
        \begin{lstlisting}[language=Java, style=custom]
// Sample HBase code to add a record
Table table = connection.getTable(TableName.valueOf("user_actions"));
Put put = new Put(Bytes.toBytes("user1"));
put.addColumn(Bytes.toBytes("profile"), Bytes.toBytes("action"), Bytes.toBytes("login"));
table.put(put);
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Use \textbf{Pig} for data transformations with complex workflows.
            \item Use \textbf{Hive} for large dataset queries and analysis.
            \item Use \textbf{HBase} for real-time data processing and scalable performance.
        \end{itemize}
    \end{block}
    \begin{block}{Summary}
        Choosing the right tool depends on data nature and project requirements. Understanding the strengths of Pig, Hive, and HBase leads to effective big data processing.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Hands-on Lab: Hadoop Ecosystem}
    \begin{block}{Overview}
        This hands-on lab aims to give students practical experience with three key components of the Hadoop ecosystem:
        \begin{itemize}
            \item \textbf{Pig}
            \item \textbf{Hive}
            \item \textbf{HBase}
        \end{itemize}
        Each tool serves unique purposes and provides different functionalities for managing and analyzing big data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Pig}
    \begin{block}{Description}
        Pig is a high-level platform for creating programs that run on Apache Hadoop. The language for Pig is called Pig Latin,
        which simplifies the complexity of writing MapReduce jobs.
    \end{block}
    
    \begin{block}{Exercise}
        \begin{itemize}
            \item \textbf{Task}: Write a Pig script to analyze a dataset of user logs.
            \item \textbf{File}: \texttt{user\_logs.csv} (contains fields like user\_id, action, timestamp)
        \end{itemize}
    \end{block}

    \begin{lstlisting}[language=Pig]
        -- Load the data
        logs = LOAD 'user_logs.csv' USING PigStorage(',') AS (user_id:chararray, action:chararray, timestamp:chararray);

        -- Filter actions for "login"
        logins = FILTER logs BY action == 'login';

        -- Group by user_id
        grouped_logins = GROUP logins BY user_id;

        -- Count logins per user
        login_counts = FOREACH grouped_logins GENERATE group, COUNT(logins);

        -- Store the results
        STORE login_counts INTO 'user_login_counts.csv' USING PigStorage(',');
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Hive \& HBase}
    \begin{block}{Apache Hive}
        Hive is a data warehouse infrastructure built on top of Hadoop. It facilitates querying and managing large datasets using a SQL-like language called HiveQL.
    \end{block}

    \begin{block}{Exercise}
        \begin{itemize}
            \item \textbf{Task}: Create a table and perform a query on a dataset of sales transactions.
            \item \textbf{Data Source}: Assume you have a dataset of sales transactions.
        \end{itemize}
    \end{block}

    \begin{lstlisting}[language=SQL]
        -- Create a table
        CREATE TABLE sales (
            transaction_id STRING,
            user_id STRING,
            amount FLOAT,
            transaction_date DATE
        ) ROW FORMAT DELIMITED 
        FIELDS TERMINATED BY ',' 
        STORED AS TEXTFILE;

        -- Load data into the table
        LOAD DATA INPATH '/path/to/sales_data.csv' INTO TABLE sales;

        -- Query for total sales amount by user
        SELECT user_id, SUM(amount) AS total_sales 
        FROM sales 
        GROUP BY user_id 
        ORDER BY total_sales DESC;
    \end{lstlisting}

    \begin{block}{Apache HBase}
        HBase is a non-relational (NoSQL) database that runs on top of HDFS. It is designed to handle large amounts of data in a column-oriented way.
    \end{block}

    \begin{block}{Exercise}
        \begin{itemize}
            \item \textbf{Task}: Insert data into HBase and retrieve it.
            \item \textbf{Data Example}: User data with user\_id, name, and age.
        \end{itemize}
    \end{block}

    \begin{lstlisting}[language=bash]
        # Create a table
        create 'users', 'personal_info'

        # Insert data
        put 'users', 'user1', 'personal_info:name', 'Alice'
        put 'users', 'user1', 'personal_info:age', '30'
        put 'users', 'user2', 'personal_info:name', 'Bob'
        put 'users', 'user2', 'personal_info:age', '25'

        # Retrieve data
        get 'users', 'user1'
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - Introduction}
    \begin{block}{Introduction to Ethical Considerations}
        Ethical considerations in data processing, especially within the Hadoop ecosystem, involve principles that guide how data is collected, stored, analyzed, and shared. 
        As organizations increasingly rely on data-driven decisions, it is essential to balance the benefits of big data analytics with ethical responsibilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - Key Issues}
    \begin{enumerate}
        \item \textbf{Data Privacy} 
            \begin{itemize}
                \item Personal data, including sensitive information, must be handled with care.
                \item \textit{Example}: Organizations must ensure that personally identifiable information (PII) is anonymized when using Hadoop to analyze customer data.
            \end{itemize}
        
        \item \textbf{Data Security} 
            \begin{itemize}
                \item Protecting data from unauthorized access is critical.
                \item \textit{Illustration}: Strong encryption standards and secure network configurations are essential.
            \end{itemize}
        
        \item \textbf{Bias in Data} 
            \begin{itemize}
                \item Algorithms trained on biased data can lead to discriminatory outcomes.
                \item \textit{Example}: Skewed analysis results from datasets featuring predominant demographics can perpetuate inequality.
            \end{itemize}

        \item \textbf{Data Ownership and Consent} 
            \begin{itemize}
                \item Clear policies about data ownership and usage are vital.
                \item Organizations should obtain informed consent before data collection.
            \end{itemize}

        \item \textbf{Accountability} 
            \begin{itemize}
                \item Organizations must be accountable for data processing decisions and their implications.
                \item Establishing an ethics board can help guide these decisions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - Conclusion and Next Steps}
    \begin{block}{Ethical Governance in Hadoop}
        \begin{itemize}
            \item \textbf{Frameworks and Standards}: Adopt ethical frameworks, like GDPR, integrated into Hadoop workflows.
            \item \textbf{Data Lifecycles}: Implement governance throughout the data lifecycle from acquisition to sharing.
        \end{itemize}
    \end{block}
    
    \begin{block}{Summary}
        Ethical data processing in the Hadoop ecosystem is crucial for ensuring privacy, security, fairness, and accountability. Organizations must uphold ethical standards to maintain stakeholder trust.
    \end{block}
    
    \begin{block}{Conclusion}
        In the age of big data, addressing ethical considerations is essential for compliance and building a socially responsible data ecosystem.
    \end{block}
    
    \textbf{Next Steps:} In our next session, we will summarize key concepts of the Hadoop ecosystem and its relevance in today's data landscape.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Overview}
    \begin{itemize}
        \item The Hadoop ecosystem is essential for managing large datasets in a distributed environment.
        \item Key principles: Scalability, fault tolerance, and cost-efficiency.
        \item Unmatched relevance in today's data landscape where big data analytics is crucial.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Core Components}
    \begin{itemize}
        \item \textbf{Hadoop Distributed File System (HDFS)}: 
        \begin{itemize}
            \item Storage layer for large files, distributed across multiple nodes.
        \end{itemize}
        \item \textbf{MapReduce}: 
        \begin{itemize}
            \item Programming model for parallel processing of large datasets.
        \end{itemize}
        \item \textbf{YARN (Yet Another Resource Negotiator)}: 
        \begin{itemize}
            \item Manages resources and schedules tasks across applications.
        \end{itemize}
        \item \textbf{Hadoop Common}: 
        \begin{itemize}
            \item Shared utilities for other Hadoop modules.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Ecosystem Tools and Relevance}
    \begin{itemize}
        \item \textbf{Ecosystem Tools}:
        \begin{itemize}
            \item \textbf{Hive}: SQL-like querying interface for HDFS.
            \item \textbf{Pig}: High-level platform using Pig Latin for data processing.
            \item \textbf{HBase}: NoSQL database for real-time access to large datasets.
            \item \textbf{Spark}: Fast, in-memory processing framework for analytics.
        \end{itemize}
        
        \item \textbf{Relevance}:
        \begin{itemize}
            \item Big data analytics enhances decision-making and improves customer service.
            \item Scalability allows for handling growing data volumes.
            \item Cost-effective, leveraging commodity hardware for big data management.
        \end{itemize}
    \end{itemize}
\end{frame}


\end{document}