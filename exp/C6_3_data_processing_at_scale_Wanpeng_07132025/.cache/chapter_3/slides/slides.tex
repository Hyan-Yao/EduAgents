\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Hadoop and Spark]{Week 3: Setting Up Hadoop and Spark}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Hadoop and Spark}
    \begin{block}{Overview of Key Big Data Frameworks}
        \begin{itemize}
            \item \textbf{Hadoop}: An open-source framework for distributed storage and processing of large datasets using the MapReduce programming model.
            \item \textbf{Spark}: An open-source unified analytics engine known for speed and ease of use, utilizing in-memory computing for faster data processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Data Processing Landscape}
    \begin{enumerate}
        \item \textbf{Scalability}
            \begin{itemize}
                \item Hadoop processes petabytes of data across clusters.
                \item \textit{Example:} Retailer analyzes transaction data from millions of customers.
            \end{itemize}
        \item \textbf{Speed}
            \begin{itemize}
                \item Spark processes data up to 100 times faster than Hadoop MapReduce.
                \item \textit{Example:} Banking institution uses Spark for real-time fraud detection.
            \end{itemize}
        \item \textbf{Flexibility}
            \begin{itemize}
                \item Both frameworks support structured and unstructured data.
                \item \textit{Example:} Health organization uses Hadoop for patient records and Spark for clinical trial analytics.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Comparison Points}
    \begin{center}
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Feature} & \textbf{Hadoop} & \textbf{Spark} \\
            \hline
            Data Processing Type & Batch Processing & Batch \& Real-Time Processing \\
            \hline
            Storage & HDFS (Hadoop Distributed File System) & Supports HDFS and other storage systems \\
            \hline
            Programming Model & MapReduce & Resilient Distributed Dataset (RDD) \\
            \hline
            Processing Speed & Slower due to disk I/O & Fast due to in-memory computation \\
            \hline
        \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Points and Illustrative Examples}
    \begin{block}{Summary Points}
        \begin{itemize}
            \item Hadoop is ideal for large-scale batch processing.
            \item Spark is suited for applications requiring quick, interactive data processing.
            \item Both frameworks are integral to the big data landscape, fulfilling different roles.
        \end{itemize}
    \end{block}
    \begin{block}{Illustrative Example}
        \textbf{Retail Case Example:}
        \begin{itemize}
            \item Data Source: Customer transactions
            \item Framework Used: Hadoop
            \item Analysis Goal: Identify top-selling products each month.
        \end{itemize}
        
        \textbf{Banking Use Case:}
        \begin{itemize}
            \item Data Source: Credit card transactions
            \item Framework Used: Spark
            \item Analysis Goal: Detect fraudulent transactions in real-time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Core Characteristics of Big Data - Introduction}
    \begin{block}{Brief Summary}
        Big Data refers to extremely large datasets characterized by volume, velocity, variety, and veracity. Understanding these characteristics is essential for implementing effective data processing strategies.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Definition of Big Data}
    \begin{block}{Definition}
        Big Data refers to extremely large datasets that traditional data processing applications are inadequate to deal with. These datasets are often characterized by their complexity and the various forms they take. As the volume of data increases, the methods to analyze, store, and visualize these datasets need to evolve.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Characteristics of Big Data - Volume and Velocity}
    \begin{block}{1. Volume}
        \begin{itemize}
            \item \textbf{Explanation:} The sheer amount of data generated is a fundamental characteristic.
            \item \textbf{Example:} Social media platforms like Facebook generate over 4 petabytes of data daily.
            \item \textbf{Key Point:} Volume necessitates distributed computing solutions like Hadoop.
        \end{itemize}
    \end{block}

    \begin{block}{2. Velocity}
        \begin{itemize}
            \item \textbf{Explanation:} Refers to the speed of data generation and processing.
            \item \textbf{Example:} Real-time data streams from stock markets and website analytics.
            \item \textbf{Key Point:} The rapid flow of data requires frameworks like Apache Spark for real-time processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Characteristics of Big Data - Variety and Veracity}
    \begin{block}{3. Variety}
        \begin{itemize}
            \item \textbf{Explanation:} Big Data comes in multiple formats: structured, semi-structured, and unstructured.
            \item \textbf{Example:} Healthcare data includes structured patient records and unstructured doctor's notes.
            \item \textbf{Key Point:} Analyzing diverse data types necessitates effective data integration technologies.
        \end{itemize}
    \end{block}

    \begin{block}{4. Veracity}
        \begin{itemize}
            \item \textbf{Explanation:} Refers to accuracy, trustworthiness, and validity of data.
            \item \textbf{Example:} Distinguishing between genuine customer reviews and fake bot reviews.
            \item \textbf{Key Point:} Data veracity improves decision-making through trusted insights and requires data cleansing techniques.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Conclusion and Visual Aid}
    \begin{block}{Conclusion}
        Understanding the core characteristics of Big Data is crucial as they dictate the approach to data storage and processing. The scalability and flexibility of tools like Hadoop and Spark enable organizations to manage these characteristics effectively and extract valuable insights.
    \end{block}
    
    \begin{block}{Visual Aid Suggestion}
        \begin{itemize}
            \item A Venn diagram showcasing the four Vs (Volume, Velocity, Variety, Veracity) to illustrate how they intersect in defining Big Data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Handling Big Data - Introduction}
    \begin{block}{Overview}
        Handling big data poses significant challenges that can hinder effective data processing and analysis. 
        Understanding these challenges is crucial for the efficient setup of tools like Hadoop and Spark.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Big Data Processing - Key Issues}
    \begin{enumerate}
        \item \textbf{Data Storage}
            \begin{itemize}
                \item The volume of data necessitates large storage capabilities.
                \item Traditional databases struggle with both structured and unstructured data.
                \item \textit{Example:} 2.5 quintillion bytes of data generated daily from social media, IoT devices, etc.
                \item Solutions like Hadoop HDFS are designed to manage large data volumes efficiently.
            \end{itemize}
    
        \item \textbf{Processing Speed}
            \begin{itemize}
                \item Fast and efficient processing is critical for real-time insights.
                \item \textit{Example:} Streaming data from sensors in a smart city enhances traffic management.
                \item Frameworks like Apache Spark provide in-memory processing capabilities for faster performance.
            \end{itemize}

        \item \textbf{Data Governance}
            \begin{itemize}
                \item Ensuring data quality, privacy, and compliance becomes complex with vast data.
                \item \textit{Example:} GDPR regulatory frameworks require responsible personal data management.
                \item Implementing data governance strategies is essential for data integrity and stakeholder trust.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Summary}
        \begin{itemize}
            \item Storage, speed, and governance are the triad of challenges in big data processing.
            \item Tools like Hadoop and Spark help address these challenges.
            \item Understanding the data landscape is key for leveraging big data effectively.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        \begin{itemize}
            \item Addressing these challenges allows organizations to implement big data solutions efficiently.
            \item Harnessing data potential leads to better strategic insights.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Hadoop - Objective}
    \begin{block}{Objective}
        Learn how to install and configure Apache Hadoop on your system to efficiently process large datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Hadoop - Overview}
    \begin{itemize}
        \item What is Hadoop?
        \begin{itemize}
            \item Open-source framework for distributed processing of large data sets.
            \item Scales from a single server to thousands of machines.
        \end{itemize}
        \item Key Components:
        \begin{enumerate}
            \item \textbf{HDFS} - Storage component.
            \item \textbf{MapReduce} - Processing component.
            \item \textbf{YARN} - Resource management layer.
            \item \textbf{Hadoop Common} - Libraries and utilities.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Steps - Prerequisites}
    \begin{itemize}
        \item \textbf{Step 1: Prerequisites}
        \begin{itemize}
            \item \textbf{Java Development Kit (JDK)}: Install JDK (version 8 or later).
            \begin{lstlisting}[language=bash]
# Check if Java is installed
java -version
            \end{lstlisting}
            \item \textbf{SSH Client}: Enable SSH for Hadoop to manage nodes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Steps - Download and Extract}
    \begin{itemize}
        \item \textbf{Step 2: Download Hadoop}
        \begin{enumerate}
            \item Visit the official Apache Hadoop website.
            \item Download the latest stable release (e.g., `hadoop-3.x.x.tar.gz`).
        \end{enumerate}
        \item \textbf{Step 3: Extract Hadoop}
        \begin{lstlisting}[language=bash]
tar -xzvf hadoop-3.x.x.tar.gz
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Steps - Configure Environment Variables}
    \begin{itemize}
        \item \textbf{Step 4: Configure Environment Variables}
        \begin{itemize}
            \item Open `~/.bashrc` or `~/.profile` and add:
            \begin{lstlisting}[language=bash]
export HADOOP_HOME=~/hadoop-3.x.x
export PATH=$PATH:$HADOOP_HOME/bin
            \end{lstlisting}
            \item Reload the profile:
            \begin{lstlisting}[language=bash]
source ~/.bashrc
            \end{lstlisting}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Steps - Configuration Files}
    \begin{itemize}
        \item \textbf{Configure Hadoop:}
        \begin{itemize}
            \item \textbf{Core Configuration (`core-site.xml`)}
            \begin{lstlisting}[language=XML]
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
            \end{lstlisting}

            \item \textbf{HDFS Configuration (`hdfs-site.xml`)}
            \begin{lstlisting}[language=XML]
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/user/hadoopdata/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/user/hadoopdata/datanode</value>
    </property>
</configuration>
            \end{lstlisting}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Steps - Complete Configuration}
    \begin{itemize}
        \item \textbf{MapReduce Configuration (`mapred-site.xml`)}
        \begin{lstlisting}[language=XML]
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
        \end{lstlisting}
        \item \textbf{YARN Configuration (`yarn-site.xml`)}
        \begin{lstlisting}[language=XML]
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
</configuration>
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Steps - Final Steps}
    \begin{itemize}
        \item \textbf{Step 6: Format the Namenode}
        \begin{lstlisting}[language=bash]
hdfs namenode -format
        \end{lstlisting}
        \item \textbf{Step 7: Start Hadoop Services}
        \begin{itemize}
            \item \begin{lstlisting}[language=bash]
start-dfs.sh        # Start HDFS
start-yarn.sh       # Start YARN
            \end{lstlisting}
        \end{itemize}
        \item \textbf{Step 8: Verify Installation}
        \begin{itemize}
            \item NameNode: \texttt{http://localhost:9870}
            \item ResourceManager: \texttt{http://localhost:8088}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Always check Java version compatibility.
        \item Ensure proper permissions for Hadoop directories.
        \item Configure files carefully to avoid runtime errors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        By following these steps, you can successfully install and configure Apache Hadoop, essential for handling large-scale data processing in today's world. Feel free to ask questions or for clarification on any specific step!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Spark - Introduction}
    \begin{block}{What is Apache Spark?}
        Apache Spark is a powerful open-source unified analytics engine designed for large-scale data processing. 
        It is known for its speed, ease of use, and sophisticated analytics capabilities.
        Spark can run standalone or on top of other distributed computing systems, 
        such as Hadoop, making it a versatile choice for big data applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Spark - Steps Overview}
    \begin{enumerate}
        \item Pre-requisites
        \item Download Spark
        \item Extract Spark
        \item Set Environment Variables
        \item Verify Installation
        \item Configuring Spark with Hadoop
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Spark - Step 1: Pre-requisites}
    \begin{itemize}
        \item \textbf{Java JDK}: 
        Ensure you have Java SE Development Kit (JDK) 8 or higher installed.
        \begin{lstlisting}
        java -version
        \end{lstlisting}
        
        \item \textbf{Hadoop}: 
        If using Spark with Hadoop, install and configure Hadoop first.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Spark - Step 2: Download Spark}
    \begin{enumerate}
        \item Go to the \texttt{Apache Spark download page}.
        \item Choose the Spark release (e.g., Spark 3.x.x).
        \item Match the Spark version with your Hadoop version.
        \item Download the pre-built package (e.g., binary for Hadoop).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Spark - Step 3: Extract Spark}
    \begin{block}{Extract Command}
        \begin{lstlisting}
        tar -xzf spark-3.x.x-bin-hadoop3.x.tgz
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Spark - Step 4: Set Environment Variables}
    \begin{itemize}
        \item Edit your \texttt{.bashrc} or \texttt{.bash_profile}:
        \begin{lstlisting}
        export SPARK_HOME=/path/to/spark-3.x.x-bin-hadoop3.x
        export PATH=$PATH:$SPARK_HOME/bin
        \end{lstlisting}
        \item Load new environment variables:
        \begin{lstlisting}
        source ~/.bashrc
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation of Spark - Step 5: Verify Installation}
    \begin{itemize}
        \item Launch the Spark shell to ensure proper installation:
        \begin{lstlisting}
        spark-shell
        \end{lstlisting}
        \item Confirm that you see the Spark welcome message and version number.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Configuring Spark with Hadoop}
    \begin{itemize}
        \item Include Hadoop configuration in Spark.
        \item Create a \texttt{spark-defaults.conf} file in the \texttt{conf} directory and add:
        \begin{lstlisting}
        spark.hadoop.fs.defaultFS=hdfs://<hadoop_master_node>:<port>
        \end{lstlisting}
        \item Adjust Hadoop configurations if necessary.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{block}{Run a Simple Spark Job}
        \begin{lstlisting}[language=Scala]
        val data = Seq(1, 2, 3, 4, 5)
        val rdd = spark.sparkContext.parallelize(data)
        println(rdd.collect().mkString(", "))
        \end{lstlisting}
    \end{block}
    This code initializes an RDD and prints the elements, demonstrating basic Spark functionality.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Apache Spark is essential for distributed data processing.
        \item Compatibility with Hadoop enables extensive data handling.
        \item Proper environment variables are crucial for a successful setup.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - Overview}
    \begin{itemize}
        \item The Hadoop Command Line Interface (CLI) is crucial for managing the Hadoop ecosystem.
        \item Key functionalities include:
        \begin{itemize}
            \item Managing files in HDFS
            \item Monitoring job statuses
            \item Performing data processing
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - Key Concepts}
    \begin{itemize}
        \item \textbf{Hadoop Distributed File System (HDFS)}: 
        The storage system designed for large data sets across multiple machines.
        
        \item \textbf{YARN (Yet Another Resource Negotiator)}:
        The resource management layer that allocates resources to applications running in the cluster.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - Basic Commands}
    \begin{enumerate}
        \item Checking Hadoop Version
        \begin{lstlisting}
hadoop version
        \end{lstlisting}
        
        \item File Management in HDFS
        \begin{itemize}
            \item Creating a Directory
            \begin{lstlisting}
hadoop fs -mkdir /path/to/directory
            \end{lstlisting}
            \item Listing Files
            \begin{lstlisting}
hadoop fs -ls /path/to/directory
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - File Operations}
    \begin{enumerate}[resume]
        \item Copying Files From Local to HDFS
        \begin{lstlisting}
hadoop fs -put localfile.txt /path/in/hdfs/
        \end{lstlisting}
        
        \item Retrieving Files from HDFS
        \begin{lstlisting}
hadoop fs -get /path/in/hdfs/file.txt localpath/
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - Data Processing}
    \begin{itemize}
        \item Running a MapReduce Job
        \begin{lstlisting}
hadoop jar /path/to/hadoop-streaming.jar -input /path/input -output /path/output
        \end{lstlisting}
        \item Example to filter words from a text file:
        \begin{lstlisting}
hadoop jar /usr/lib/hadoop/hadoop-streaming.jar -input /user/data/input.txt -output /user/data/output -mapper "python mapper.py" -reducer "python reducer.py"
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - Key Points}
    \begin{itemize}
        \item Understanding HDFS and the CLI is crucial for managing data effectively.
        \item Familiarity with basic commands enhances operational efficiency in big data projects.
        \item Practical application of these commands is essential in real-world scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - Conclusion}
    \begin{itemize}
        \item Mastering the Hadoop CLI equips users to manage data in the Hadoop ecosystem effectively.
        \item This knowledge serves as a foundation for advanced operations, including integration with tools like Apache Spark.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Command Line Interface - Next Steps}
    \begin{itemize}
        \item Next, we will transition into Spark Basics.
        \item We will explore similar commands and functions essential for data manipulation and processing in Spark.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Spark Basics}
    \begin{block}{Overview}
        This presentation covers the foundational concepts and commands for data manipulation and processing in Apache Spark.
        
        Key topics include:
        \begin{itemize}
            \item Introduction to Apache Spark
            \item Key concepts: RDDs and DataFrames
            \item Basic Spark commands
            \item Example use cases
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction to Apache Spark}
    \begin{itemize}
        \item Apache Spark is an open-source distributed computing system.
        \item Offers an interface for programming entire clusters with:
        \begin{itemize}
            \item Implicit data parallelism
            \item Fault tolerance
        \end{itemize}
        \item Designed for efficient processing of large datasets.
        \item Key abstractions:
        \begin{itemize}
            \item RDD (Resilient Distributed Dataset)
            \item DataFrame
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Spark}
    \begin{enumerate}
        \item \textbf{RDD (Resilient Distributed Dataset)}
        \begin{itemize}
            \item Fundamental data structure for distributed data manipulation.
            \item \textbf{Transformations}: Create new RDDs (e.g., \texttt{map}, \texttt{filter}).
            \item \textbf{Actions}: Compute results (e.g., \texttt{count}, \texttt{collect}).
        \end{itemize}
        
        \item \textbf{DataFrame}
        \begin{itemize}
            \item Distributed collection of data organized into named columns.
            \item Similar to tables in relational databases, with higher-level operations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Spark Commands - Part I}
    \begin{block}{Creating a Spark Session}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Spark Basics Example") \
    .getOrCreate()
    \end{lstlisting}
    \end{block}

    \begin{block}{Reading Data}
    \begin{lstlisting}[language=Python]
# Reading a CSV file into a DataFrame
df = spark.read.csv("data/sample_data.csv", header=True, inferSchema=True)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Spark Commands - Part II}
    \begin{block}{DataFrame Operations}
        \begin{itemize}
            \item \textbf{Selecting Columns}:
            \begin{lstlisting}[language=Python]
df.select("column1", "column2").show()
            \end{lstlisting}
            
            \item \textbf{Filtering Rows}:
            \begin{lstlisting}[language=Python]
df.filter(df['column1'] > 100).show()
            \end{lstlisting}
            
            \item \textbf{Grouping and Aggregating}:
            \begin{lstlisting}[language=Python]
df.groupBy("column2").count().show()
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Transformations are lazy operations; they do not compute until an action is called.
        \item DataFrames optimize query execution via the Catalyst optimizer.
        \item Efficient resource management is crucial when processing large datasets.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Example Use Case}
    \begin{itemize}
        \item Analyzing sales data from a CSV.
        \item Steps:
        \begin{enumerate}
            \item Load the data.
            \item Filter records with sales above a threshold.
            \item Group data by product type to calculate total sales.
        \end{enumerate}
        \item This process leverages Spark's distributed capabilities for enhanced performance.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion and Next Steps}
    \begin{itemize}
        \item Mastering basic Spark commands is essential for working with larger datasets.
        \item Future slides will cover data ingestion and storage techniques in Hadoop.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion and Storage in Hadoop - Overview}
    \begin{block}{Overview}
        In Hadoop, effectively ingesting and storing vast amounts of data is crucial for big data processing. 
        This slide will cover the primary tools and processes for data ingestion into Hadoop, as well as the role of the Hadoop Distributed File System (HDFS) in storage.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion in Hadoop - Key Tools}
    \begin{itemize}
        \item \textbf{Apache Flume}:
        \begin{itemize}
            \item A distributed service for efficiently collecting, aggregating, and moving large amounts of log data.
            \item \textit{Example}: Ingesting logs from web servers into HDFS for analysis.
        \end{itemize}
        
        \item \textbf{Apache Kafka}:
        \begin{itemize}
            \item A distributed messaging system that can process streams of data in real-time.
            \item \textit{Example}: Streaming sensor data from IoT devices directly into Hadoop for storage and analytics.
        \end{itemize}
        
        \item \textbf{Apache NiFi}:
        \begin{itemize}
            \item A powerful data flow tool that provides a web-based interface to design, monitor, and control data flows.
            \item \textit{Example}: Fetching data from databases, transforming it, and loading it into HDFS.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Distributed File System (HDFS)}
    \begin{block}{Key Characteristics of HDFS}
        \begin{itemize}
            \item \textbf{Distributed Storage}: Files are split into blocks and distributed across multiple nodes.
            \item \textbf{Fault Tolerance}: Data is replicated (default is three copies) to prevent loss due to node failure.
            \item \textbf{High Throughput}: Designed for high data transfer rates and optimized for large files.
        \end{itemize}
    \end{block}
    
    \begin{block}{HDFS Architecture}
        \begin{center}
            \includegraphics[width=0.8\linewidth]{hdfs_architecture.png} % Placeholder for the diagram
        \end{center}
        % Diagram Placeholder: Replace with the actual diagram file
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Storage Processes in HDFS}
    \begin{itemize}
        \item \textbf{Write Process}:
        \begin{itemize}
            \item When a file is written to HDFS, it’s divided into blocks, which are written to various DataNodes.
            \item The NameNode maintains metadata about these blocks and their locations.
        \end{itemize}

        \item \textbf{Read Process}:
        \begin{itemize}
            \item When reading a file, the NameNode provides the locations of the blocks, which are retrieved from the DataNodes.
            \item \textit{Example}: A 1GB log file is divided into 128MB blocks. HDFS stores:
                \begin{itemize}
                    \item Block 1 on DataNode 1
                    \item Block 2 on DataNode 2
                    \item Block 3 on DataNode 3
                \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Data ingestion is the first step in big data processing with Hadoop and can utilize various tools based on data sources and use cases.
        \item HDFS underpins the storage infrastructure of Hadoop, providing a robust mechanism for handling large datasets across distributed environments.
        \item Understanding both ingestion and storage processes is critical for utilizing the full potential of Hadoop.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Running Spark Jobs - Overview}
    \begin{block}{Overview}
        Running Spark jobs is essential for processing big data efficiently. This tutorial focuses on executing Spark jobs, detailing the core transformations and actions that enable data manipulation and analysis.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Running Spark Jobs - Key Concepts}
    \begin{itemize}
        \item \textbf{Spark Jobs}: A job in Spark is executed by a driver program that requests tasks to be performed on multiple worker nodes.
        \item \textbf{Transformations vs. Actions}:
        \begin{itemize}
            \item \textbf{Transformations}: Create a new RDD from an existing one; they are lazy operations.
            \begin{itemize}
                \item Example: \texttt{map()}, \texttt{filter()}, \texttt{flatMap()}
            \end{itemize}
            \item \textbf{Actions}: Trigger execution of transformations and return results.
            \begin{itemize}
                \item Example: \texttt{collect()}, \texttt{count()}, \texttt{saveAsTextFile()}
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Running Spark Jobs - Code Snippets}
    \begin{block}{Basic Spark Setup}
    \begin{lstlisting}[language=python]
from pyspark import SparkContext

# Create a SparkContext
sc = SparkContext("local", "Simple App")
    \end{lstlisting}
    \end{block}

    \begin{block}{Basic Transformation: map}
    \begin{lstlisting}[language=python]
# Create an RDD from a list
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Apply transformation: square each number
squared_rdd = rdd.map(lambda x: x ** 2)
    \end{lstlisting}
    \end{block}

    \begin{block}{Running an Action: collect}
    \begin{lstlisting}[language=python]
# Trigger execution and collect results
results = squared_rdd.collect()
print(results)  # Output: [1, 4, 9, 16, 25]
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Running Spark Jobs - Additional Examples}
    \begin{block}{Filter Transformation}
    \begin{lstlisting}[language=python]
# Filter even numbers
even_rdd = rdd.filter(lambda x: x % 2 == 0)
print(even_rdd.collect())  # Output: [2, 4]
    \end{lstlisting}
    \end{block}

    \begin{block}{Count Action}
    \begin{lstlisting}[language=python]
# Count the number of elements
count = rdd.count()
print(count)  # Output: 5
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Running Spark Jobs - Key Points}
    \begin{itemize}
        \item \textbf{Lazy Evaluation}: Spark optimizes the execution plan, executing transformations only upon an action call.
        \item \textbf{Distributed Computing}: Spark handles the distribution of data automatically across the cluster.
        \item \textbf{Performance}: Transformations yield new RDDs without modifying the existing data, promoting efficiency.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Running Spark Jobs - Visual Representation}
    \begin{block}{Diagram}
        % Insert a diagram here illustrating the flow of data through Spark jobs, showing transformations and actions.
        \includegraphics[width=\linewidth]{path_to_your_diagram.png} 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction}
    \begin{block}{Overview}
        Hadoop and Spark are two leading frameworks for processing and analyzing big data. Understanding their differences in architecture, processing models, and use cases is crucial for selecting the right tool for a specific data-driven task.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture}
    \begin{itemize}
        \item \textbf{Hadoop:}
        \begin{itemize}
            \item Components: Hadoop Distributed File System (HDFS) and MapReduce
            \item Data Storage: Encapsulates data in HDFS for storage efficiency
            \item Fault Tolerance: Achieves through data replication across nodes
            \item Processing: Optimized for batch processing; higher latency
        \end{itemize}
        
        \item \textbf{Spark:}
        \begin{itemize}
            \item Components: RDDs, Spark SQL, Spark Streaming, and MLlib
            \item In-Memory Processing: Speeds execution by avoiding disk I/O
            \item Unified Framework: Supports batch and real-time processing
            \item Fault Tolerance: Utilizes lineage information for efficient recovery
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Processing Models}
    \begin{itemize}
        \item \textbf{Hadoop MapReduce:}
        \begin{itemize}
            \item Map Phase: Processes input data into key-value pairs
            \item Reduce Phase: Aggregates key-value pairs to provide output
            \item Latency: High due to disk I/O; suitable for jobs without immediate result needs
        \end{itemize}
        
        \item \textbf{Spark:}
        \begin{itemize}
            \item Transformations: Lazy evaluation queues transformations until an action is performed
            \item Actions: Trigger computation (e.g., \texttt{collect()}, \texttt{count()})
            \item Speed: Up to 100x faster than MapReduce due to in-memory processing
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Cases}
    \begin{itemize}
        \item \textbf{When to Use Hadoop:}
        \begin{itemize}
            \item Long-running batch jobs
            \item Suitable for data lakes
            \item Cost-effective for vast data storage without real-time processing
        \end{itemize}

        \item \textbf{When to Use Spark:}
        \begin{itemize}
            \item Real-time analytics (e.g., fraud detection)
            \item Machine learning with iterative algorithms
            \item Interactive queries for immediate feedback
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    \begin{itemize}
        \item Both frameworks have distinct strengths.
        \item Hadoop is robust for large-scale batch processing.
        \item Spark excels in real-time processing and diverse data analytics workloads.
        \item Choosing the right tool depends on project requirements such as latency, data volume, and type of analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Diagram - Conceptual Structure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{hadoop_vs_spark_diagram.png} % Placeholder for the diagram
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-on Labs Overview}
    \begin{block}{Introduction}
        This section outlines the hands-on lab activities designed to reinforce your understanding and practical skills in the installation and configuration of Hadoop and Spark. These labs are crucial for developing a foundational grasp of big data technologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Lab Activities Breakdown - Part 1}
    \begin{enumerate}
        \item \textbf{Hadoop Installation Lab}
        \begin{itemize}
            \item \textbf{Objective:} Install Hadoop on a single-node or multi-node cluster.
            \item \textbf{Activities:}
            \begin{itemize}
                \item Download Hadoop from the official website.
                \item Set up SSH access for Hadoop.
                \item Configure the configuration files.
            \end{itemize}
            \item \textbf{Example Configuration:}
            \begin{lstlisting}[language=XML]
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Lab Activities Breakdown - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Hadoop HDFS Operations Lab}
        \begin{itemize}
            \item \textbf{Objective:} Familiarize with HDFS commands.
            \item \textbf{Activities:}
            \begin{itemize}
                \item Load sample data into HDFS.
                \item Execute basic commands.
            \end{itemize}
            \item \textbf{Example Command:}
            \begin{lstlisting}[language=bash]
hdfs dfs -put localfile.txt /user/hadoop/
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Spark Installation Lab}
        \begin{itemize}
            \item \textbf{Objective:} Setup Apache Spark with Hadoop support.
            \item \textbf{Activities:}
            \begin{itemize}
                \item Download Spark and configure it.
                \item Set environment variables.
            \end{itemize}
            \item \textbf{Code Snippet:}
            \begin{lstlisting}[language=bash]
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps - Key Takeaways}
    \begin{itemize}
        \item Understanding the \textbf{Ecosystem}:
        \begin{itemize}
            \item Hadoop and Spark are essential frameworks for big data processing.
            \item Hadoop offers a distributed storage system (HDFS) and MapReduce processing, while Spark enhances speed with in-memory computation.
        \end{itemize}
        
        \item \textbf{Installation and Configuration}:
        \begin{itemize}
            \item Step-by-step installation and version compatibility are crucial.
            \item Familiarity with configuration files like \texttt{hdfs-site.xml}, \texttt{mapred-site.xml}, and \texttt{spark-defaults.conf} is essential for efficient cluster setups.
        \end{itemize}
        
        \item \textbf{Cluster Management}:
        \begin{itemize}
            \item Ability to set up clusters on-premises or in cloud services is essential.
            \item YARN (Yet Another Resource Negotiator) optimizes resource management in Hadoop.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps - Practical Experience}
    \begin{itemize}
        \item \textbf{Hands-On Experience}:
        \begin{itemize}
            \item Labs provided practical experience with Hadoop cluster setup and basic Spark jobs.
            \item This strengthens theoretical knowledge with real-world applications.
        \end{itemize}
        
        \item \textbf{Next Steps - Machine Learning}:
        \begin{itemize}
            \item Introduction to machine learning concepts and their integration with Hadoop and Spark.
            \item Key topics to be covered include:
            \begin{itemize}
                \item Supervised vs. unsupervised learning
                \item Common algorithms
                \item Model evaluation metrics
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps - Further Exploration}
    \begin{itemize}
        \item \textbf{Using MLlib}:
        \begin{itemize}
            \item We will learn about MLlib, Spark's machine learning library.
            \item Key functionalities include classification (e.g., Decision Trees) and clustering (e.g., K-Means).
            \item Example: Predicting customer churn using historical data with Spark MLlib.
        \end{itemize}
        
        \item \textbf{Data Preparation}:
        \begin{itemize}
            \item Emphasis on effective data preprocessing for successful machine learning tasks.
            \item Techniques such as data cleaning, normalization, and feature extraction will be explored.
        \end{itemize}
        
        \item \textbf{Real-World Applications}:
        \begin{itemize}
            \item Discussion of practical applications in sectors like healthcare (disease prediction), finance (fraud detection), and e-commerce (recommendation systems).
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Summary}
        In summary, we have established a solid foundation for working with Hadoop and Spark. The focus will now shift towards machine learning applications, utilizing these powerful tools for analysis and decision-making.
    \end{block}
\end{frame}


\end{document}