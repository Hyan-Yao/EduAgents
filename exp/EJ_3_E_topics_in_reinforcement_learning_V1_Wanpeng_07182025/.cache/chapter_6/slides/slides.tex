\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Function Approximation in Reinforcement Learning}
    \begin{block}{What is Function Approximation?}
        \begin{itemize}
            \item Technique to estimate state/action values in RL
            \item Used when it's impractical to represent the entire environment
            \item Commonly involves neural networks or linear functions
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Function Approximation in RL}
    \begin{enumerate}
        \item \textbf{Handling Large State Spaces:}
            \begin{itemize}
                \item Feasibility of storing values for every state-action pair is low
                \item Generalizes learned experiences to unseen states
            \end{itemize}
        
        \item \textbf{Generalization:}
            \begin{itemize}
                \item Enables agents to perform well in unencountered states
                \item Recognizes patterns for informed decision-making
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Function Approximation Techniques}
    \begin{block}{Value Function Approximation}
        \begin{equation}
            V(s; \theta) \approx \text{value function generated by the model}
        \end{equation}
        \begin{itemize}
            \item \( \theta \) are parameters defining the function (e.g., weights in neural networks)
        \end{itemize}
    \end{block}

    \begin{block}{Policy Approximation}
        \begin{equation}
            \pi(a|s; \phi) \approx \text{policy derived from the model}
        \end{equation}
        \begin{itemize}
            \item \( \phi \) are the parameters of the policy function
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Generalization - Overview}
    \begin{block}{Understanding Generalization in Reinforcement Learning (RL)}
        Generalization is the ability of a learning algorithm to perform well on unseen states or situations based on its experience with a limited set of examples. In RL, generalization affects both the performance and stability of algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Generalization - Performance Impact}
    \frametitle{Impact on Performance}
    \begin{enumerate}
        \item \textbf{Broader Applicability}
        \begin{itemize}
            \item \textbf{Explanation}: Generalization enables the agent to apply learned knowledge to new situations effectively.
            \item \textbf{Example}: An agent trained to navigate through a maze must generalize its learned path-finding strategy to similar mazes not encountered during training.
        \end{itemize}
        
        \item \textbf{Sample Efficiency}
        \begin{itemize}
            \item \textbf{Explanation}: Generalization allows the agent to learn effectively from fewer interactions with the environment.
            \item \textbf{Example}: An agent trained using a general strategy for moving towards a goal can adapt that strategy for different locations, reducing the overall number of episodes required for training.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Importance of Generalization - Stability Impact}
    \frametitle{Impact on Stability}
    \begin{enumerate}
        \item \textbf{Stability of Learning}
        \begin{itemize}
            \item \textbf{Explanation}: Effective generalization can lead to more stable learning processes.
            \item \textbf{Potential Issues}: Poor generalization may lead to overfitting, where the agent performs well in training scenarios but poorly in novel situations.
        \end{itemize}

        \item \textbf{Avoiding Catastrophic Forgetting}
        \begin{itemize}
            \item \textbf{Explanation}: Generalization helps the agent retain previously learned behaviors while adapting to new tasks.
            \item \textbf{Example}: A robot trained in a factory setting needs to maintain its ability to handle existing tasks while learning to adapt to new production lines.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Importance of Generalization - Key Points and Conclusion}
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Generalization is vital for transferring knowledge to unseen scenarios in RL.
            \item A balance must be struck between fitting the training data (low bias) and maintaining robustness across varied states (low variance).
            \item Techniques such as function approximation, regularization, and ensemble methods can improve generalization.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        In summary, effective generalization strategies maximize performance and stability in reinforcement learning algorithms, enabling agents to operate robustly in complex environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Importance of Generalization - Further Reading}
    \frametitle{Suggested Further Reading}
    \begin{itemize}
        \item \textbf{Related Algorithms}: Q-Learning, SARSA
        \item \textbf{Techniques for Improving Generalization}: Regularization methods, Cross-Validation
        \item \textbf{Real-World Applications}: Robotics, Game Playing, Dynamic System Control
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Function Approximation - Overview}
    \begin{block}{Definition}
        Linear function approximation is a method used to estimate the relationship between input features and output values in a linear manner. 
    \end{block}
    \begin{itemize}
        \item Simplifies complex mappings into a straight line or hyperplane
        \item Facilitates generalization and prediction
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Function Approximation - Key Concepts}
    \begin{itemize}
        \item \textbf{Weights (w):}
        \begin{itemize}
            \item Determine the importance of each input feature
            \item Adjusted during training to minimize prediction error
        \end{itemize}

        \item \textbf{Bias (b):}
        \begin{itemize}
            \item Allows fitting data with all input features zero
            \item Shifts the regression line up or down
        \end{itemize}

        \item \textbf{Linear Model:}
        \begin{equation}
            y = wx + b 
        \end{equation}
        where \(y\) is the output, \(wx\) describes the weighted input, and \(b\) is the bias.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Function Approximation - Example}
    \begin{block}{Example}
        Predict house prices based on size (in square feet):
        \begin{itemize}
            \item Features: Size (\(x\))
            \item Target: Price (\(y\))
        \end{itemize}
        Best fitting line:
        \begin{equation}
            Price = 300 \times Size + 50,000 
        \end{equation}
        where:
        \begin{itemize}
            \item 300 is the weight (price increase per square foot)
            \item 50,000 is the baseline price when size is zero
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Function Approximation - Implementation}
    \begin{block}{Python Implementation}
        \begin{lstlisting}[language=Python]
import numpy as np
from sklearn.linear_model import LinearRegression

# Sample data
X = np.array([[500], [1500], [2500]])  # Sizes in square feet
y = np.array([150000, 300000, 600000])  # House prices

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Prediction for a new size
predicted_price = model.predict([[2000]])
print(f"Predicted Price for 2000 sqft: ${predicted_price[0]:,.2f}")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Linear Methods - Overview}
    \begin{block}{Overview of Linear Methods in Reinforcement Learning (RL)}
        In Reinforcement Learning, linear methods serve as a foundational approach to function approximation, allowing us to estimate value functions and policy functions. By leveraging the simplicity of linear combinations, these methods can efficiently model relationships in the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Linear Methods - Key Methods}
    \begin{enumerate}
        \item \textbf{Linear Regression}
            \begin{itemize}
                \item \textbf{Definition}: A statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation.
                \item \textbf{Formula}:
                \begin{equation}
                y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon
                \end{equation}
                \item \textbf{Application}: In RL, linear regression can be used to predict the expected rewards for various actions based on state features.
            \end{itemize}

        \item \textbf{Linear Function Approximation}
            \begin{itemize}
                \item Used to approximate value functions or policies as a linear combination of features derived from states and actions.
                \item \textbf{Example}:
                \begin{equation}
                V(s) = \theta^T \phi(s)
                \end{equation}
            \end{itemize}
        \item \textbf{Relative Value Function Estimation}
            \begin{itemize}
                \item Estimates the value of the action taken in a given state relative to the average reward value.
                \item \textbf{Example}:
                \begin{equation}
                Q(s, a) = w_0 + w_1 f_1(s, a) + w_2 f_2(s, a)
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Linear Methods - Policy Gradient}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Policy Gradient Methods with Linear Structures}
            \begin{itemize}
                \item Linear methods can be incorporated in policy gradient methods by parameterizing the policies linearly.
                \item \textbf{Example}:
                \begin{equation}
                \pi(a|s) = \frac{e^{\theta^T \phi(s, a)}}{\sum_{a'} e^{\theta^T \phi(s, a')}}
                \end{equation}
            \end{itemize}
            \item \textbf{Application}: This enables the learning of stochastic policies that choose actions based on a softmax distribution of linear function approximators.
    \end{enumerate}
    
    \vspace{1em}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Simplicity and Interpretability}: Provides a straightforward and interpretable approach.
            \item \textbf{Efficiency}: Computationally feasible and less prone to overfitting.
            \item \textbf{Flexibility}: Can capture non-linear relationships when combined with feature engineering.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Methods - Overview}
    \begin{block}{Overview}
        Linear methods in function approximation, such as linear regression, serve as foundational tools in machine learning and reinforcement learning. 
        However, they come with inherent limitations that can hinder their effectiveness in capturing complex relationships in data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Methods - Key Points}
    \begin{enumerate}
        \item \textbf{Assumption of Linearity}
            \begin{itemize}
                \item Linear methods assume that the relationship between input variables and output is linear.
                \item \textit{Illustration:} Fitting a straight line to non-linear data, like a parabolic shape, leads to poor fits and errors.
            \end{itemize}
        
        \item \textbf{Limited Expressiveness}
            \begin{itemize}
                \item A linear model can only represent a hyperplane in the feature space.
                \item \textit{Example:} A dataset influenced by both the sum and product of inputs cannot be captured by a linear model.
            \end{itemize}

        \item \textbf{Sensitivity to Outliers}
            \begin{itemize}
                \item Linear methods can be heavily influenced by outliers, leading to skewed results.
                \item \textit{Example:} An extreme value can pull the regression line toward it, distorting the model.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Methods - Further Issues}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering
        \item \textbf{Underfitting Issues}
            \begin{itemize}
                \item Linear models may oversimplify complex data, leading to poor performance on training and unseen data.
                \item \textit{Illustration:} A linear approximation for a problem requiring a complex curve misses important data trends.
            \end{itemize}
        
        \item \textbf{Local Minima}
            \begin{itemize}
                \item Linear approximators can get stuck in local minima during optimization, although less so than non-linear methods.
                \item \textit{Example:} Optimizing a constrained linear model may yield a solution that isn’t globally optimal.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Note on Linear Approximators}
    \begin{block}{Mathematical Note}
        The general form of a linear function approximator is given by:
        \begin{equation}
            f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b
        \end{equation}
        Where:
        \begin{itemize}
            \item \( \mathbf{w} \) is the weight vector,
            \item \( \mathbf{x} \) is the input vector,
            \item \( b \) is the bias term.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Discussion Points}
    \begin{block}{Conclusion}
        Understanding the limitations of linear methods is crucial for selecting the right function approximation approach, especially in complex scenarios. Advancing to more sophisticated approximators, like neural networks, can better address these challenges.
    \end{block}
    
    \vspace{10pt}
    
    \textbf{Discussion Points:}
    \begin{itemize}
        \item What are some scenarios in which linear methods may still be applicable?
        \item How can one identify when a linear model is insufficient?
        \item What techniques can be employed to go beyond linear approximations?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Networks as Function Approximators - Introduction}
    \begin{itemize}
        \item Neural networks are computational models inspired by the human brain.
        \item They consist of interconnected nodes (neurons) organized into layers.
        \item Significant in reinforcement learning (RL) for approximating complex functions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Neural Networks in Reinforcement Learning?}
    \begin{itemize}
        \item \textbf{Complexity and Non-linearity:} 
            \begin{itemize}
                \item Neural networks capture non-linear relationships, surpassing linear methods.
            \end{itemize}
        \item \textbf{Generalization:} 
            \begin{itemize}
                \item They learn generalized representations, predicting outcomes for unseen states.
            \end{itemize}
        \item \textbf{Scalability:} 
            \begin{itemize}
                \item Capable of handling large datasets and many input features.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Function Approximation in Reinforcement Learning}
    \begin{itemize}
        \item Crucial for vast or continuous state-action spaces.
        \item Neural networks can approximate:
            \begin{itemize}
                \item Value functions
                \item Policy functions
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Example}
        Consider a robot learning to navigate a maze:
        \begin{itemize}
            \item A linear function approximator may struggle.
            \item A neural network models intricate pathways effectively.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Code Snippet}
    \begin{itemize}
        \item \textbf{Flexibility:} Suitable for regression and classification tasks.
        \item \textbf{Architecture:} Customizable based on problem requirements.
        \item \textbf{Training:} Uses algorithms like backpropagation for error minimization.
    \end{itemize}

    \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow.keras import layers, models

# Define a simple feedforward neural network
model = models.Sequential([
    layers.Input(shape=(input_dim,)),  # Input shape
    layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons
    layers.Dense(32, activation='relu'),  # Another hidden layer
    layers.Dense(output_dim, activation='linear')  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse')  # Using mean squared error
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Neural networks are powerful function approximators in reinforcement learning.
        \item They provide solutions to challenges from non-linearities and high-dimensional spaces.
        \item Their adaptability positions them as crucial tools in modern AI applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Neural Networks - Introduction}
    \begin{block}{Introduction}
        Neural networks are powerful models used for function approximation in various domains, including reinforcement learning. Understanding the key components—layers, activation functions, and training processes—helps to grasp how neural networks learn and perform tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Neural Networks - Layers}
    \begin{itemize}
        \item \textbf{Definition}: The structure of a neural network is composed of layers: input, hidden, and output layers.
        \item \textbf{Types}:
        \begin{itemize}
            \item \textbf{Input Layer}: Accepts input features (e.g., processes pixel values in image recognition).
            \item \textbf{Hidden Layers}: Perform computations and transformations. The complexity and depth affect learning capability.
                \begin{itemize}
                    \item Example: A network for sequential image analysis may have multiple hidden layers to extract hierarchical features.
                \end{itemize}
            \item \textbf{Output Layer}: Produces the final output. Applies the softmax function for classifications to yield probabilities.
        \end{itemize}
        \item \textbf{Key Point}: More layers can capture complex patterns but may lead to overfitting if not managed properly.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Neural Networks - Activation Functions}
    \begin{itemize}
        \item \textbf{Purpose}: Introduces non-linearity into the network, enabling it to learn complex patterns.
        \item \textbf{Common Activation Functions}:
        \begin{itemize}
            \item \textbf{Sigmoid}:
                \begin{equation}
                    \sigma(x) = \frac{1}{1 + e^{-x}}
                \end{equation}
                Maps output to (0, 1); good for binary classification.
            \item \textbf{ReLU (Rectified Linear Unit)}:
                \begin{equation}
                    f(x) = \max(0, x)
                \end{equation}
                Used for hidden layers; outputs zero for negative inputs and linear for positive.
                \begin{itemize}
                    \item \textbf{Advantage}: Helps mitigate the vanishing gradient problem.
                \end{itemize}
            \item \textbf{Softmax}: Converts logits into probabilities; useful for multi-class classification.
        \end{itemize}
        \item \textbf{Key Point}: The choice of activation function can significantly affect the network’s performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Neural Networks - Training Process}
    \begin{itemize}
        \item \textbf{Objective}: Minimize the difference between predicted and actual outputs using a loss function.
        \item \textbf{Phases of Training}:
        \begin{enumerate}
            \item \textbf{Forward Pass}: Input data is passed through the network to generate output.
            \item \textbf{Loss Calculation}: Loss calculated using a chosen loss function (e.g., mean squared error).
            \item \textbf{Backward Pass (Backpropagation)}: Gradients of the loss with respect to each weight are computed.
            \item \textbf{Weight Update}:
                \begin{equation}
                    w = w - \eta \cdot \nabla L(w)
                \end{equation}
                where \( \eta \) is the learning rate, and \( \nabla L(w) \) is the gradient of the loss function.
        \end{enumerate}
        \item \textbf{Key Point}: The training process is iterative and requires careful tuning of parameters like learning rate and batch size.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Neural Networks - Conclusion}
    \begin{block}{Conclusion}
        Understanding the core components—layers, activation functions, and training processes—of neural networks is essential for their effective application in function approximation tasks, particularly in reinforcement learning.
    \end{block}

    \begin{block}{References}
        \begin{itemize}
            \item Hinton, G., et al. (2012). "Deep Learning".
            \item Goodfellow, I., et al. (2016). "Deep Learning".
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks in RL - Introduction}
    \begin{block}{Introduction}
        Neural networks are powerful tools in reinforcement learning (RL) that enhance the ability of agents to learn and make decisions in complex environments.
        By approximating functions, they can represent policies and value functions that guide decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks in RL - Key Applications}
    \begin{enumerate}
        \item \textbf{Policy Learning}
            \begin{itemize}
                \item \textbf{Definition:} Creating a mapping from states to actions.
                \item \textbf{Example:} Robot navigation outputs actions based on current state.
                \item \textbf{Advantage:} Generalization in high-dimensional spaces.
                
                \begin{equation}
                    \pi(a|s) = P(A_t = a | S_t = s) 
                \end{equation}
            \end{itemize}

        \item \textbf{Value Function Approximation}
            \begin{itemize}
                \item \textbf{Definition:} Predicts expected returns from states.
                \item \textbf{Example:} Assessing potential moves in games.
                \item \textbf{Advantage:} Captures complex patterns often missed by simpler models.
                
                \begin{equation}
                    V(s) = \mathbb{E}[R_t + \gamma V(S_{t+1}) | S_t = s]
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks in RL - More Applications}
    \begin{enumerate}[resume]
        \item \textbf{Q-learning with Deep Neural Networks (DQN)}
            \begin{itemize}
                \item \textbf{Concept:} Approximates Q-values using deep neural networks.
                \item \textbf{Example:} Learning from pixel values in Atari games.
                \item \textbf{Advantage:} Handles large state spaces effectively.
                
                \begin{equation}
                    Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
                \end{equation}
            \end{itemize}

        \item \textbf{Actor-Critic Methods}
            \begin{itemize}
                \item \textbf{Definition:} Combines policy and value function learning.
                \item \textbf{Example:} Actor determines joint movements, critic evaluates performance.
                \item \textbf{Advantage:} Balances exploration and exploitation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks in RL - Summary & Further Exploration}
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item Neural networks enhance reinforcement learning by effectively approximating policies and value functions.
            \item Applications like DQN and Actor-Critic methods illustrate their versatility in RL.
            \item Improves scalability and performance of RL algorithms.
        \end{itemize}
    \end{block}
    
    \begin{block}{Further Exploration}
        \begin{itemize}
            \item Implement a simple DQN using TensorFlow or PyTorch.
            \item Investigate the impact of different network architectures on RL performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Network Training - Overview}
    \begin{block}{Overview of Neural Network Training Challenges}
        Training neural networks is a complex process that can often encounter several challenges. Understanding these challenges is crucial for building effective models. The principal difficulties include:
    \end{block}
    \begin{itemize}
        \item Overfitting
        \item Convergence Issues
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Network Training - Overfitting}
    \begin{block}{Overfitting}
        \textbf{Definition:} \\
        Overfitting occurs when a neural network learns the details and noise in the training data to the extent that it negatively impacts the model's performance on new data.
    \end{block}

    \begin{itemize}
        \item High accuracy on training data but poor generalization to validation/test data.
        \item Indicates model complexity is too high relative to training data available.
    \end{itemize}

    \textbf{Example:} \\
    A neural network recognizing handwritten digits (MNIST):
    \begin{itemize}
        \item Training accuracy: 98\%
        \item Validation accuracy: 70\%
    \end{itemize}

    \begin{lstlisting}[language=Python, basicstyle=\small]
# Example: Early Stopping Implementation
from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5)
model.fit(X_train, Y_train, validation_data=(X_val, Y_val), callbacks=[early_stopping])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Network Training - Convergence Issues}
    \begin{block}{Convergence Issues}
        \textbf{Definition:} \\
        Convergence refers to finding the optimal values for the network parameters to minimize the loss function.
    \end{block}

    \begin{itemize}
        \item \textbf{Local Minima:} Gradient descent may lead to local minima instead of global minimum.
        \item \textbf{Saturation:} Activation functions like sigmoid can cause gradients to become small.
        \item \textbf{Learning Rate:} Inappropriate learning rates can cause slow convergence or overshooting minima.
    \end{itemize}

    \textbf{Example:} \\
    A too-large learning rate leads to oscillation, a too-small learning rate leads to slow learning.

    \begin{lstlisting}[language=Python, basicstyle=\small]
# Example of learning rate adjustment in code
from keras.optimizers import Adam

optimizer = Adam(learning_rate=0.001)  # Optimal learning rate
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Linear Methods vs Non-Linear Methods - Introduction}
    \begin{block}{Introduction}
        Function approximation is pivotal in various fields, including:
        \begin{itemize}
            \item Machine Learning
            \item Statistics
            \item Control Systems
        \end{itemize}
        Understanding the distinctions between linear and non-linear methods helps in selecting the right approach for different scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Linear Methods vs Non-Linear Methods - Linear Methods}

    \textbf{Definition:} Linear methods use a straight-line representation to model relationships.
    \begin{equation}
        y = mx + b
    \end{equation}
    Where:
    \begin{itemize}
        \item $y$ is the predicted output,
        \item $m$ is the slope,
        \item $x$ is the input,
        \item $b$ is the intercept.
    \end{itemize}

    \textbf{Key Characteristics:}
    \begin{itemize}
        \item Simplicity: Easy to interpret and understand.
        \item Computational Efficiency: Requires less computational power.
        \item Assumptions: Assumes a linear relationship, may ignore complex data patterns.
    \end{itemize}

    \textbf{Examples:}
    \begin{enumerate}
        \item Linear Regression (e.g., predicting housing prices)
        \item Logistic Regression (e.g., email spam filtering)
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Linear Methods vs Non-Linear Methods - Non-Linear Methods}

    \textbf{Definition:} Non-linear methods, such as neural networks, use layers of interconnected nodes (neurons) to model complex relationships.

    \textbf{Key Characteristics:}
    \begin{itemize}
        \item Flexibility: Can model complex, non-linear relationships.
        \item Capacity: Learns intricate patterns through multiple layers.
        \item Training Complexity: Requires more data and computational resources.
    \end{itemize}

    \textbf{Examples:}
    \begin{enumerate}
        \item Feedforward Neural Networks (e.g., image recognition)
        \item Convolutional Neural Networks (CNNs) (e.g., automated photo tagging)
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Linear Methods vs Non-Linear Methods - When to Use Each}

    \textbf{Use Linear Methods When:}
    \begin{itemize}
        \item The relationship is approximately linear.
        \item Interpretability is crucial (e.g., in finance).
        \item Computational efficiency is required for large datasets.
    \end{itemize}

    \textbf{Use Non-Linear Methods When:}
    \begin{itemize}
        \item The dataset is large and complex.
        \item High accuracy is needed, interpretability can take a back seat.
        \item Working with high-dimensional data (images, audio, text).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Linear Methods vs Non-Linear Methods - Summary}
    \textbf{Summary:}
    
    Choosing between linear and non-linear methods depends on the problem at hand. Understanding strengths and weaknesses is crucial.
    
    \begin{itemize}
        \item Linear methods provide quick, interpretable results for simpler relationships.
        \item Neural networks excel in complex environments, offering flexibility and power at a greater computational cost.
    \end{itemize}
    
    \textbf{Key Takeaways:}
    \begin{itemize}
        \item Use linear models for simplicity and speed.
        \item Use non-linear models for capturing complexity.
        \item The choice of model significantly affects performance and interpretability.
    \end{itemize}
    Consider exploring practical coding implementations to see these differences in action!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Case Studies in Function Approximation}
    \begin{block}{Introduction to Function Approximation in RL}
        Function approximation is a crucial technique used in reinforcement learning (RL) to generalize knowledge from limited data and efficiently estimate the value of states and actions. This slide explores several compelling real-world case studies to demonstrate the practical applications of function approximation in various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Autonomous Driving}
    \begin{itemize}
        \item \textbf{Example:} Self-Driving Cars
        \begin{itemize}
            \item \textbf{Overview:} Self-driving cars utilize RL agents to make complex driving decisions (e.g., lane changing, obstacle avoidance).
            \item \textbf{Function Approximation Role:} Neural networks approximate the driving policy by processing sensor data (like camera images) and outputs safe actions (acceleration, braking).
            \item \textbf{Impact on Performance:} The use of deep learning allows the model to recognize and react to diverse driving scenarios with high accuracy.
        \end{itemize}
        \item \textbf{Key Takeaway:} Function approximation enables self-driving systems to learn from vast datasets, adapting to dynamic environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Robotics \& Case Study 3: Game Playing}
    \begin{itemize}
        \item \textbf{Case Study 2: Robotics}
        \begin{itemize}
            \item \textbf{Example:} Robot Manipulation
            \item \textbf{Overview:} Robots in manufacturing use RL for tasks such as picking and placing objects.
            \item \textbf{Function Approximation Role:} Q-learning optimized with deep neural networks approximates the value of actions based on the robot's current state.
            \item \textbf{Impact on Efficiency:} Robots significantly improve their manipulation strategies over time, often outperforming traditional rule-based approaches.
            \item \textbf{Key Takeaway:} With function approximation, robots can learn complex decision-making tasks without exhaustive programming.
        \end{itemize}

        \item \textbf{Case Study 3: Game Playing}
        \begin{itemize}
            \item \textbf{Example:} AlphaGo
            \item \textbf{Overview:} AlphaGo, developed by DeepMind, is an AI capable of playing the game Go at a world-class level.
            \item \textbf{Function Approximation Role:} A blend of deep neural networks and reinforcement learning approximates state values and policies through massive training on historical games and self-play.
            \item \textbf{Impact on Strategy:} The ability to handle vast state spaces through function approximation allowed AlphaGo to identify novel strategies and significantly improve its gameplay.
            \item \textbf{Key Takeaway:} Function approximation revolutionizes how AI can tackle strategic decision-making in games, illustrating its potential across different problem spaces.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Important Concepts \& Conclusion}
    \begin{itemize}
        \item \textbf{Types of Function Approximators:}
        \begin{itemize}
            \item Linear Models: Simpler, more interpretable, but limited in capturing complex patterns.
            \item Neural Networks: Powerful in representing intricate relationships within data but require careful tuning and larger datasets.
        \end{itemize}

        \item \textbf{Challenges:}
        \begin{itemize}
            \item Overfitting: Models need to generalize well to unseen states without memorizing training examples.
            \item Sample Efficiency: Striking a balance between the amount of training data required and the performance of the model.
        \end{itemize}

        \item \textbf{Conclusion:} Function approximation is not just a theoretical concept but a practical tool powering innovative applications across diverse sectors. Understanding these case studies provides foundational insights into leveraging RL effectively in real-world challenges.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{References for Further Reading}
    \begin{itemize}
        \item \textbf{Books:} ``Reinforcement Learning: An Introduction'' by Sutton \& Barto
        \item \textbf{Papers:} ``Mastering the game of Go with deep neural networks and tree search'' by Silver et al., 2016
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Overview}
    \begin{block}{Overview of Function Approximation in RL}
        Function approximation is crucial in reinforcement learning (RL), enabling agents to generalize learning from limited experiences to broader scenarios. It allows RL agents to estimate the value of states or state-action pairs even when the state space is too large to handle explicitly.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Takeaways}
    \begin{itemize}
        \item \textbf{Importance of Function Approximation:}
        \begin{itemize}
            \item \textbf{Scalability:} Facilitates learning in large or continuous state spaces (e.g., robotics).
            \item \textbf{Generalization:} Enables performance on unseen states by leveraging past experiences.
        \end{itemize}
        
        \item \textbf{Common Approaches:}
        \begin{itemize}
            \item \textbf{Linear Function Approximation:} 
                \begin{equation}
                V(s) \approx w_1 \phi_1(s) + w_2 \phi_2(s) + \ldots + w_n \phi_n(s)
                \end{equation}
            \item \textbf{Non-linear Function Approximation:} Utilizes neural networks, e.g., Deep Q-Networks (DQN).
        \end{itemize}
        
        \item \textbf{Methods of Implementation:}
        \begin{itemize}
            \item Policy Gradient Methods
            \item Value-based Methods
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Challenges and Future Directions}
    \begin{itemize}
        \item \textbf{Challenges Faced:}
        \begin{itemize}
            \item \textbf{Overfitting:} Risk with complex models leading to poor performance on unseen data.
            \item \textbf{Stability and Convergence:} Issues arising from the correlated nature of experiences in RL.
        \end{itemize}
        
        \item \textbf{Future Directions:}
        \begin{itemize}
            \item Advances in Deep Learning: Exploring architectures (e.g., transformers) in RL.
            \item Enhanced Exploration Strategies: Integrating function approximation with improved exploration techniques.
            \item Meta-learning: Developing methods for agents to improve their function approximators over time.
            \item Multi-Agent Environments: Investigations into dynamic and collaborative settings.
        \end{itemize}
    \end{itemize}
\end{frame}


\end{document}