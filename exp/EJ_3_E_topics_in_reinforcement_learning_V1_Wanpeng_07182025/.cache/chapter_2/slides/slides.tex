\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Title Page Information
\title[Week 2: MDPs]{Week 2: Markov Decision Processes}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Markov Decision Processes (MDPs)}
    A \textbf{Markov Decision Process (MDP)} is a mathematical framework used to describe an environment in reinforcement learning where an agent must make decisions to maximize rewards over time. MDPs provide a formalization that helps in algorithm design and theoretical analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs}
    \begin{enumerate}
        \item \textbf{States (S):}
        \begin{itemize}
            \item A finite set of possible states.
            \item Example: In a grid environment, each cell represents a state.
        \end{itemize}
        
        \item \textbf{Actions (A):}
        \begin{itemize}
            \item A finite set of actions available to the agent.
            \item Example: In a grid world, possible actions could be 'up', 'down', 'left', or 'right'.
        \end{itemize}
        
        \item \textbf{Transition Function (P):}
        \begin{itemize}
            \item Defines the probabilities of reaching the next state: $P(s' | s, a)$.
            \item Example: 80\% chance to stay in the current state due to obstacles.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs (Continued)}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue enumeration from previous frame
        \item \textbf{Reward Function (R):}
        \begin{itemize}
            \item Provides immediate feedback as a scalar reward.
            \item Example: Reward of +10 for reaching a goal state, penalty of -5 for a trap.
        \end{itemize}
        
        \item \textbf{Discount Factor ($\gamma$):}
        \begin{itemize}
            \item Determines the importance of future rewards (0 $\leq$ $\gamma$ < 1).
            \item Example: A discount factor of 0.9 implies future rewards are valued at 90\% of their current value.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Reinforcement Learning}
    MDPs are crucial in reinforcement learning for several reasons:
    \begin{itemize}
        \item \textbf{Framework for Modeling:} Structured way to formulate decision-making problems.
        \item \textbf{Optimal Policies:} Allow the derivation of strategies that maximize expected rewards.
        \item \textbf{Algorithm Development:} Underpin many algorithms like Q-Learning and Policy Gradient Methods.
        \item \textbf{Real-world Applications:} Used in robotics, finance, healthcare, and gaming for decision-making under uncertainty.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Formula}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item MDPs formalize sequential decision-making.
            \item Understanding MDP components aids in algorithm design.
            \item The reward concept is central to guiding agent behavior.
        \end{itemize}
    \end{block}

    \begin{equation}
        V(s) = \max_{a} \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V(s')]
    \end{equation}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Markov Decision Processes are foundational to reinforcement learning, providing a framework for modeling environments and guiding agents towards making optimal decisions. Understanding MDPs is critical for delving into advanced topics and implementations in reinforcement learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Markov Decision Processes (MDPs) - Overview}
    \begin{itemize}
        \item States (S)
        \item Actions (A)
        \item Rewards (R)
        \item Transition Probabilities (P)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs - States and Actions}
    \begin{block}{1. States (S)}
        \begin{itemize}
            \item \textbf{Definition:} A state represents the current situation or configuration of the environment at a specific time. 
            \item \textbf{Example:} In a robot navigation task, possible states might include the robot's position (coordinates) and orientation (angle).
        \end{itemize}
    \end{block}

    \begin{block}{2. Actions (A)}
        \begin{itemize}
            \item \textbf{Definition:} An action is a decision made by an agent that affects the state of the environment.
            \item \textbf{Example:} In the robot navigation, possible actions include moving forward, turning left, or turning right.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs - Rewards and Transitions}
    \begin{block}{3. Rewards (R)}
        \begin{itemize}
            \item \textbf{Definition:} A reward is a scalar value received by the agent after taking an action in a specific state.
            \item \textbf{Example:} In a robot task, reaching a target location might provide a reward of +10.
        \end{itemize}
    \end{block}

    \begin{block}{4. Transition Probabilities (P)}
        \begin{itemize}
            \item \textbf{Definition:} They define the likelihood of moving from one state to another upon taking a specific action.
            \item \textbf{Example:} 
            \begin{equation}
            P(S_{t+1} | S_t, A_t) \text{ where } S_t \text{ is the current state and } A_t \text{ is the action taken.}
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States - What Are States in MDPs?}
    \begin{itemize}
        \item In the context of Markov Decision Processes (MDPs), a \textbf{state} represents a specific situation or configuration of the environment at a given point in time.
        \item States capture all relevant information necessary for deciding the next action.
        \item They are crucial for decision-making and forecasting future actions.
        \item State representation can be discrete or continuous, varying by problem domain.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States - Key Characteristics}
    \begin{enumerate}
        \item \textbf{Comprehensive}: A state includes all necessary information for an agent to make a decision, reflecting the Markov property.
        \item \textbf{Observable vs. Hidden}:
        \begin{itemize}
            \item \textbf{Observable States}: Full visibility (e.g., chessboard).
            \item \textbf{Hidden States}: Partial information (e.g., opponent's cards in poker).
        \end{itemize}
        \item \textbf{Static vs. Dynamic}: States may change over time or due to actions taken by agents.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States - Examples and Representation}
    \textbf{Examples of States:}
    \begin{itemize}
        \item \textbf{Game Environment}:
            \begin{itemize}
                \item Chess: Each unique arrangement of pieces.
                \item Pac-Man: Position of Pac-Man, ghosts, and maze layout.
            \end{itemize}
        \item \textbf{Robotics}: Defined by position in a grid, orientation, or battery level.
        \item \textbf{Finance}: Includes stock prices, economic indicators, and market conditions.
    \end{itemize}

    \textbf{State Representation:}
    \begin{itemize}
        \item \textbf{Vector Representation}: States as vectors corresponding to specific features (e.g., $\text{State} = [x, y, battery\_level]$).
        \item \textbf{Matrices or Tensors}: Complex environments may use matrices or tensors.
        \item \textbf{Symbolic Representation}: Natural language or symbols for interpretation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item States are foundational to MDPs; understanding them is crucial for decision-making.
            \item Quality of state representation influences the effectiveness of policies.
            \item Different applications require flexible approaches to state representation.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Grasping the concept of states and their representation in MDPs lays the groundwork for exploring actions, rewards, and decision policies. Next, we will dive into the actions available to the agent and how they influence the trajectory through the state space.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions in MDPs - Overview}
    \begin{itemize}
        \item Actions are fundamental components in Markov Decision Processes (MDPs).
        \item They dictate the behavior of an agent in a given environment.
        \item The role of actions in decision-making influences state transitions and outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions in MDPs - Key Concepts}
    \begin{enumerate}
        \item \textbf{Definition of Actions:}
        \begin{itemize}
            \item Choices that affect the state of the environment.
            \item Each action leads to a new state based on the current state.
        \end{itemize}
        
        \item \textbf{Action Space:}
        \begin{itemize}
            \item Set of all possible actions in a state, denoted as \( A(s) \).
            \item Example: In a board game, actions could include moving pieces and rolling dice.
        \end{itemize}
        
        \item \textbf{Deterministic vs. Stochastic Actions:}
        \begin{itemize}
            \item \textbf{Deterministic:} Predictable outcome (e.g., specific adjacent square).
            \item \textbf{Stochastic:} Probabilistic outcomes (e.g., die roll).
        \end{itemize}
        
        \item \textbf{Action Selection Policies:}
        \begin{itemize}
            \item Defines strategy for action selection.
            \item Can be deterministic (one action per state) or stochastic (actions based on probability).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario and Decision-Making Role}
    \begin{itemize}
        \item \textbf{Agent in a Grid World:}
        \begin{itemize}
            \item Robot navigating a 5x5 grid.
            \item States correspond to each grid cell (e.g., (0,0)).
            \item Available actions: {Up, Down, Left, Right}.
        \end{itemize}
        
        \item \textbf{Decision-Making Role:}
        \begin{itemize}
            \item Actions decide the agent's future direction and success.
            \item Influence state transitions and received rewards.
        \end{itemize}
        
        \item \textbf{Mathematical Representation:}
        \begin{equation}
            P(s'|s,a)
        \end{equation}
        where:
        \begin{itemize}
            \item \( s \): current state
            \item \( a \): action taken
            \item \( s' \): next state
            \item \( P(s'|s,a) \): transition probability
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards and Their Importance}
    
    \begin{block}{Concept Overview}
        In the context of Markov Decision Processes (MDPs), the \textbf{reward function} plays a crucial role in guiding the agent's learning and decision-making process.
    \end{block} 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Function}
    
    The reward is a numerical value received after taking an action in a particular state. It provides immediate feedback on the effectiveness of the agent's actions.
    
    \begin{itemize}
        \item \textbf{Reward Function (R)}: Defines the immediate reward received after executing an action \( a \) in state \( s \):
        
        \[
        R(s, a) \rightarrow \mathbb{R}
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Rewards}
    
    \begin{enumerate}
        \item \textbf{Guiding Behavior}: Serves as the primary feedback mechanism, enabling the agent to judge the value of its actions.
        \item \textbf{Learning}: Agents update their knowledge and improve future actions through repeated interactions with the environment.
        \item \textbf{Encouraging Exploration}: A well-designed reward function balances exploration (trying new actions) and exploitation (choosing known rewarding actions).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    
    \textbf{Scenario: An autonomous robot navigating through a maze.}
    
    \begin{itemize}
        \item \textbf{States}: Various locations in the maze.
        \item \textbf{Actions}: Moving in different directions (up, down, left, right).
    \end{itemize}
    
    \textbf{Reward Function}:
    \begin{itemize}
        \item Reaching the goal: +10 points
        \item Hitting a wall: -5 points
        \item Each step taken: -1 point
    \end{itemize}
    
    \vspace{3mm} % Adding space for clarity
    In this scenario, the rewards encourage the robot to find the quickest path to the goal while discouraging unnecessary movements and penalizing collisions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Short-term vs Long-term Rewards}
    
    Agents must learn to consider long-term rewards over immediate ones. This principle is encapsulated in the concept of \textbf{discounted rewards}:
    
    \begin{equation}
    G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
    \end{equation}
    where \( \gamma \) is the discount factor (0 < \( \gamma \) < 1).
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Reward Shaping}: A technique to design the reward function to improve learning speed and efficiency.
        \item \textbf{Importance of Designing Effective Rewards}: Understanding and crafting an effective reward function is fundamental for successful learning in reinforcement learning contexts.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    In summary, rewards in MDPs are essential for the agent's learning process, guiding behavior and decision-making effectively. An understanding of the reward function is key to achieving optimal learning outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Introduction}
    \begin{block}{Introduction to Value Functions}
        Value functions are essential in Markov Decision Processes (MDPs). They help quantify expected returns associated with states and actions, forming the foundation for effective reinforcement learning algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Key Concepts}
    \begin{enumerate}
        \item \textbf{State Value Function (V)}:
        \begin{itemize}
            \item Represents the expected return starting from state \( s \) and following policy \( \pi \).
            \item \textbf{Formula}:
            \begin{equation}
                V(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_t | S_0 = s \right]
            \end{equation}
            \item Where:
            \begin{itemize}
                \item \( \mathbb{E}_\pi \): expected value given policy \( \pi \)
                \item \( R_t \): reward at time \( t \)
                \item \( \gamma \): discount factor (0 $\leq$ \( \gamma \) < 1)
            \end{itemize}
        \end{itemize}

        \item \textbf{Action Value Function (Q)}:
        \begin{itemize}
            \item Gives the expected return of taking action \( a \) in state \( s \) and following policy \( \pi \).
            \item \textbf{Formula}:
            \begin{equation}
                Q(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_t | S_0 = s, A_0 = a \right]
            \end{equation}
            \item Where \( A_0 = a \) indicates the action taken at time \( 0 \).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Computational Significance}
    \begin{itemize}
        \item \textbf{Decision Making}: 
        Value functions allow agents to evaluate potential future rewards, essential for the principle of optimality in reinforcement learning.

        \item \textbf{Policy Evaluation and Improvement}:
        \begin{itemize}
            \item These functions are critical for evaluating and improving policies by selecting actions with the highest \( Q \)-values.
            \item This leads to improved performance over time.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Example}
    \begin{block}{Example: Simplified Grid World}
        Consider a 3x3 grid world where an agent collects rewards:
        \begin{itemize}
            \item \textbf{Scenario}:
            The agent starts at square A (0,0) and can move to neighboring squares. Rewards are given at specific squares, with high rewards at (2,2) and penalties for traps.
            \item \textbf{State Value Calculation}: 
            Using the reward function, the values for each state are estimated. 
            For instance, \( V((0,0)) \) is lower than \( V((2,2)) \) due to the high reward.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Example Code}
    \begin{block}{Code Snippet Example (Python)}
        Here’s a simplified implementation of a state value function:
        \begin{lstlisting}[language=Python]
def compute_state_value(states, rewards, gamma):
    V = {s: 0 for s in states}  # Initialize value function
    for s in states:
        V[s] = rewards[s] + gamma * sum(V[s_next] for s_next in next_states(s))
    return V

states = ['A', 'B', 'C']
rewards = {'A': 0, 'B': 1, 'C': 10}
gamma = 0.9
value_function = compute_state_value(states, rewards, gamma)
print(value_function)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Property - Overview}
    \begin{block}{Understanding the Markov Property}
        The \textbf{Markov Property} is a fundamental concept in Markov Decision Processes (MDPs). It establishes a "memoryless" characteristic in the decision-making process:
        \begin{itemize}
            \item The future state depends only on the current state.
            \item Past events do not affect the future state.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Property - Key Concepts}
    \begin{block}{Memoryless Property}
        \begin{itemize}
            \item The next state is determined solely based on the current state.
            \item Mathematically:
            \begin{equation}
                P(S_{t+1} | S_t, S_{t-1}, \ldots, S_0) = P(S_{t+1} | S_t)
            \end{equation}
        \end{itemize}
    \end{block}

    \begin{block}{Transition Probabilities}
        In MDPs, transitions occur with probabilities defined as:
        \begin{equation}
            P(s' | s, a)
        \end{equation}
        where \( s \) is the current state, \( a \) is the action taken, and \( s' \) is the next state.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Markov Property - Examples and Applications}
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Simple Game}:
            Rolling a die affects the state based on current conditions, not previous rolls.
            \item \textbf{Weather Prediction}:
            Tomorrow’s rain probability depends only on today's weather, not on past forecasts.
        \end{itemize}
    \end{block}

    \begin{block}{Key Applications}
        \begin{itemize}
            \item \textbf{Reinforcement Learning}:
            Explores environments using the Markov property without historical data.
            \item \textbf{Operations Research}:
            Models systems involving random processes for optimization problems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Solving MDPs - Overview}
    \begin{block}{Markov Decision Processes (MDPs)}
        MDPs are frameworks for modeling decision-making where outcomes are partly random and partly under the control of a decision-maker.
    \end{block}
    \begin{itemize}
        \item Goal: Find an optimal policy to maximize expected cumulative reward.
        \item Methodologies:
            \begin{itemize}
                \item Dynamic Programming
                \item Reinforcement Learning
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming (DP)}
    \begin{block}{Overview}
        Dynamic Programming systematically solves MDPs by breaking them into simpler subproblems.
        It relies on the principle of optimality.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key DP Methods:}
        \begin{itemize}
            \item \textbf{Value Iteration:}
            \begin{equation}
                V_{k+1}(s) = \max_a \left( R(s, a) + \sum_{s'} P(s'|s, a)V_k(s') \right)
            \end{equation}
            \item \textbf{Policy Iteration:}
            \begin{enumerate}
                \item Initialize a policy \( \pi \).
                \item Evaluate the policy.
                \item Improve the policy using the value function.
                \item Repeat until policy stabilizes.
            \end{enumerate}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning (RL)}
    \begin{block}{Overview}
        RL enables agents to learn optimal policies through interaction with environments rather than pre-defined models.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key RL Approaches:}
        \begin{itemize}
            \item \textbf{Q-Learning:}
            \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
            \end{equation}
            \item \textbf{Deep Q-Network (DQN):}
            \begin{itemize}
                \item Combines a neural network with Q-learning.
                \item Approximates Q-values for high-dimensional state spaces.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Next Steps}
    \begin{itemize}
        \item MDPs are structured models for sequential decision-making.
        \item DP methods may struggle with large state spaces.
        \item RL allows learning from experience, useful for complex environments.
        \item Combining DP and RL can enhance agent training in simulations.
    \end{itemize}

    \begin{block}{Next Steps}
        In the upcoming slide, we will explore practical applications of MDPs in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of MDPs - Introduction}
    \begin{block}{Introduction to Markov Decision Processes (MDPs)}
        Markov Decision Processes are mathematical frameworks used for modeling decision-making scenarios where outcomes are partly random and partly under the control of a decision maker. MDPs consist of:
        \begin{itemize}
            \item States
            \item Actions
            \item Rewards
            \item Transition Probabilities
        \end{itemize}
        This structure makes MDPs suitable for a variety of applications across different fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of MDPs - Key Applications}
    \begin{block}{Key Applications}
        \begin{enumerate}
            \item \textbf{Robotics}
                \begin{itemize}
                    \item MDPs are used for path planning, navigation, and robotic control.
                    \item Example: A delivery robot navigating through obstacles.
                \end{itemize}
            \item \textbf{Automated Decision-Making}
                \begin{itemize}
                    \item Enables automation in environments requiring sequential decision-making.
                    \item Example: Financial trading modeling decisions to buy, hold, or sell assets.
                \end{itemize}
            \item \textbf{Inventory Management}
                \begin{itemize}
                    \item Used to manage inventory levels, balancing costs and stockouts.
                    \item Example: A retailer deciding reorder levels based on inventory and demand forecasts.
                \end{itemize}
            \item \textbf{Game Theory and Strategic Decision-Making}
                \begin{itemize}
                    \item Applicable in competitive environments considering opponents' actions.
                    \item Example: Evaluating moves in a board game like chess.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of MDPs - Conclusion and Code}
    \begin{block}{Conclusion}
        MDPs provide powerful tools for tackling real-world problems involving decision-making under uncertainty. Their applications extend beyond theoretical models to practical uses in technology, business, and more.
    \end{block}

    \begin{block}{Python Code Snippet}
    \begin{lstlisting}[language=Python]
import numpy as np

# Define the state space
states = ['S1', 'S2', 'S3']

# Define the action space
actions = ['A1', 'A2']

# Define the transition probabilities
P = {
    'S1': {'A1': {'S1': 0.8, 'S2': 0.2}, 'A2': {'S2': 1.0}},
    'S2': {'A1': {'S1': 0.4, 'S3': 0.6}, 'A2': {'S1': 1.0}},
    'S3': {'A1': {'S3': 1.0}, 'A2': {'S1': 0.3, 'S2': 0.7}},
}

# Define rewards for each state
R = {'S1': 10, 'S2': 5, 'S3': 0}

def policy(state):
    return np.random.choice(actions)  # Simple random policy example

# Function to choose action based on the policy and state
def choose_action(state):
    return policy(state)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: MDPs in Action}
    A case study showcasing the application of Markov Decision Processes (MDPs) in a practical reinforcement learning environment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes (MDPs)}
    \begin{block}{What is an MDP?}
        A Markov Decision Process (MDP) provides a framework for modeling decision-making scenarios where outcomes are partly random and partly under the control of a decision maker. MDPs are widely used in various fields, particularly in reinforcement learning, enabling agents to optimize decision-making through experience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs}
    \begin{itemize}
        \item \textbf{States (S)}: Represents all possible situations an agent can be in.
        \item \textbf{Actions (A)}: The set of all possible actions the agent can take given a state.
        \item \textbf{Transition Model (P)}: Defines the probability of reaching a new state after taking a specific action.
        \item \textbf{Reward Function (R)}: Provides immediate feedback by assigning a value to each state-action pair.
        \item \textbf{Discount Factor ($\gamma$)}: A value between 0 and 1 representing the importance of future rewards compared to immediate ones.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Case Study: Autonomous Driving}
    \begin{block}{Scenario Overview}
        How MDPs are utilized in autonomous driving, where the vehicle (agent) must make real-time decisions based on its environment to reach a destination safely.
    \end{block}
    
    \begin{itemize}
        \item \textbf{States (S)}: 
        \begin{itemize}
            \item Current speed
            \item Distance from obstacles
            \item Lane position
            \item Traffic signal status
        \end{itemize}
        \item \textbf{Actions (A)}: 
        \begin{itemize}
            \item Accelerate
            \item Brake
            \item Turn left or right
            \item Maintain speed
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Case Study: Autonomous Driving (Continued)}
    \begin{itemize}
        \item \textbf{Transition Model (P)}: Probabilities derived from data gathered through simulations or real tests (e.g., slowing down when approaching a stop sign).
        \item \textbf{Reward Function (R)}: Positive rewards for reaching the destination quickly/safely; penalties for collisions or unsafe actions (e.g., running a red light).
        \item \textbf{Discount Factor ($\gamma$)}: Closer to 1 as safety and efficiency over time are crucial.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Simulation}
    \begin{block}{Simulation Process}
        In an MDP simulation for an autonomous vehicle:
        \begin{itemize}
            \item The vehicle starts at its initial state (speed, position).
            \item Evaluates the best action based on its policy (derived from Q-values) at each time step.
            \item Iterates until reaching stopping conditions (arrival at the destination, encountering obstacles).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item MDPs provide a structured way to model decision-making in uncertain environments.
        \item Reinforcement learning uses MDPs to find optimal policies through experience optimization.
        \item Real-time decision-making enabled by MDPs is crucial in high-stakes scenarios like autonomous driving.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        MDPs facilitate the development of algorithms that learn from interactions with dynamic environments, leading to robust solutions in scenarios like autonomous driving.
    \end{block}
    
    \begin{block}{Next Steps}
        We will address the Challenges and Considerations in modeling problems as MDPs, exploring complexities and computational limits in real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Considerations in Markov Decision Processes (MDPs)}
    \begin{itemize}
        \item Understanding the complexity inherent in MDPs
        \item Exponential growth of state space
        \item Curse of dimensionality
        \item Modeling uncertainty
        \item Computational limits
        \item Convergence issues
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding the Complexity of MDPs - Part 1}
    \begin{enumerate}
        \item **Exponential Growth of State Space**
        \begin{itemize}
            \item In real-world applications, state space can grow exponentially.
            \item Example: A grid-world scenario with obstacles can lead to an immense number of states.
        \end{itemize}

        \item **Curse of Dimensionality**
        \begin{itemize}
            \item As states and actions increase, data required to estimate value functions grows.
            \item This makes exploration computationally intensive and slows down finding optimal policies.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding the Complexity of MDPs - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item **Modeling Uncertainty**
        \begin{itemize}
            \item Real-world problems involve uncertainty, complicating transition probabilities.
            \item Example: In robotic navigation, intended destinations may not be reached due to external factors.
        \end{itemize}

        \item **Computational Limits**
        \begin{itemize}
            \item Methods like value iteration require significant computational resources.
            \item Example: Time complexity for value iteration is \( O(n^2) \).
        \end{itemize}
        
        \item **Convergence Issues**
        \begin{itemize}
            \item Some algorithms may have difficulties converging due to local minima or poorly defined rewards.
            \item Strategies such as epsilon-greedy or simulated annealing can enhance exploration.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas}
    \begin{block}{Bellman Equation}
        The agent's optimal value function can be defined recursively as:
        \begin{equation}
            V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a)V^*(s') \right]
        \end{equation}
        where \( R(s, a) \) is the reward function, \( \gamma \) is the discount factor, and \( P(s' | s, a) \) is the transition probability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item **Key Takeaway**: MDPs model decision-making under uncertainty with significant challenges.
        \item **Action**: Engage in hands-on exercises in simulation environments to solidify understanding of computation limits and modeling complexities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in MDP Research}
    \begin{block}{Introduction to MDP Research Trends}
        Markov Decision Processes (MDPs) are vital in reinforcement learning, operations research, and AI. Emerging trends enhance MDPs' capabilities and applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in MDP Research - Part 1}
    \begin{enumerate}
        \item \textbf{Deep Reinforcement Learning (DRL)}
            \begin{itemize}
                \item Combining MDPs with deep learning techniques.
                \item Example: Neural networks approximating value functions, e.g., AlphaGo.
                \item Key Point: Handles large state spaces better than traditional strategies.
            \end{itemize}

        \item \textbf{Model-free vs. Model-based Approaches}
            \begin{itemize}
                \item Research on model-free learning (e.g., Q-learning) vs. model-based methods.
                \item Example: Hybrid systems for faster, more accurate learning.
                \item Key Point: Balance between exploration and exploitation is crucial.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in MDP Research - Part 2}
    \begin{enumerate}
        \item \textbf{Scaling MDPs to Large-Scale Problems}
            \begin{itemize}
                \item Developing algorithms for large-scale MDPs.
                \item Example: Approximate Dynamic Programming, policy gradient methods.
                \item Key Point: Enables applications in complex scenarios like robotics.
            \end{itemize}

        \item \textbf{Multi-Agent MDPs (MMDPs)}
            \begin{itemize}
                \item Investigating cooperation and competition among multiple agents.
                \item Example: Cooperative robotic systems.
                \item Key Point: Insights into decentralized decision-making.
            \end{itemize}

        \item \textbf{Hierarchical Reinforcement Learning}
            \begin{itemize}
                \item Structuring MDPs into hierarchies for complex tasks.
                \item Example: Task breakdown in navigation.
                \item Key Point: Promotes efficiency and reduces complexity.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in MDP Research - Part 3}
    \begin{enumerate}
        \item \textbf{Generalization and Transfer Learning}
            \begin{itemize}
                \item Methods for transferring strategies between MDP environments.
                \item Example: Using skills from one navigation scenario in another.
                \item Key Point: Increases learning speed and adaptability.
            \end{itemize}

        \item \textbf{Explainable AI in MDPs}
            \begin{itemize}
                \item Making MDP decision processes transparent.
                \item Example: Providing explanations for AI decisions.
                \item Key Point: Enhances user trust in AI systems.
            \end{itemize}
        
        \item \textbf{Conclusion}
            \begin{itemize}
                \item MDP research’s future holds advancements improving algorithms and broadening applicability across domains.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engagement Tip}
    \begin{block}{Hands-on Activity}
        Consider exploring a simple DRL framework using OpenAI's gym to build intuition around practical applications of MDPs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Understanding MDPs}
  Markov Decision Processes (MDPs) are foundational for reinforcement learning, providing a framework for decision-making in uncertain environments.

  \begin{itemize}
      \item Core components of MDPs:
      \begin{itemize}
          \item States (S)
          \item Actions (A)
          \item Transition Model (P)
          \item Rewards (R)
          \item Discount Factor ($\gamma$)
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{MDPs - Policies and Value Functions}
  
  \begin{itemize}
      \item Policies:
      \begin{itemize}
          \item Policy ($\pi$): Strategy determining actions based on state.
          \item Can be deterministic or stochastic.
      \end{itemize}
      
      \item Value Functions:
      \begin{itemize}
          \item State Value Function ($V(s)$): Expected return from state $s$.
          \item Action Value Function ($Q(s,a)$): Expected return from taking action $a$ in state $s$.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Theorems and Practical Example}
  
  \begin{itemize}
      \item Key Theorems:
      \begin{itemize}
          \item Bellman Equation for Value Functions:
          \begin{equation}
          V^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V^\pi(s')
          \end{equation}
      \end{itemize}
      
      \item Relevance to Reinforcement Learning:
      \begin{itemize}
          \item MDPs form the basis for algorithms like Q-learning and Policy Gradients.
      \end{itemize}
      
      \item Practical Example:
      \begin{itemize}
          \item In a grid world, an agent navigates to maximize rewards by evaluating states and actions.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  
  \begin{itemize}
      \item Understanding MDPs is crucial for successful reinforcement learning systems.
      \item Interrelationships among states, actions, rewards, and policies are foundational to decision-making under uncertainty.
      \item Real-world applications of MDPs include:
      \begin{itemize}
          \item Robotics
          \item Resource Management
          \item Automated Systems
      \end{itemize}
  \end{itemize}
\end{frame}


\end{document}