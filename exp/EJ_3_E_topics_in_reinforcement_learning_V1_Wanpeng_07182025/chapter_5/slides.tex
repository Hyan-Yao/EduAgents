\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 5: Temporal-Difference Learning]{Chapter Title: Week 5: Temporal-Difference Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Temporal-Difference Learning}
    \begin{block}{What is Temporal-Difference Learning?}
        Temporal-Difference (TD) Learning is a central concept in reinforcement learning that enables agents to learn how to make decisions. It combines ideas from Monte Carlo methods and dynamic programming, allowing agents to learn predictions of future rewards based on past experiences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of TD Learning}
    \begin{enumerate}
        \item \textbf{Learning from Experience}:
            \begin{itemize}
                \item TD Learning updates the value of states based on learned future rewards.
                \item Estimates are updated at each time step, not requiring complete episodes.
            \end{itemize}
        
        \item \textbf{Bootstrapping}:
            \begin{itemize}
                \item Adjusts values based on existing estimates.
                \item Uses current knowledge to improve, rather than waiting for complete data.
            \end{itemize}
        
        \item \textbf{Reward Signal}:
            \begin{itemize}
                \item Feedback in the form of rewards/penalties drives the learning process.
                \item The goal is to maximize cumulative rewards over time.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of TD Learning}
    \begin{itemize}
        \item \textbf{Sample Efficiency}: More efficient than Monte Carlo methods, updating after every step.
        \item \textbf{Continuous Learning}: Capable of learning from ongoing data, adaptable to dynamic environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Types of TD Learning}
    \begin{enumerate}
        \item \textbf{TD(0)}:
            \begin{equation}
                V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
            \end{equation}
        
        \item \textbf{SARSA}:
            \begin{equation}
                Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
            \end{equation}
        
        \item \textbf{Q-Learning}:
            \begin{equation}
                Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, A) - Q(S_t, A_t)]
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    \begin{block}{Robot in a Maze}
        Consider a robot navigating a maze.
        \begin{itemize}
            \item Each position is a state, and moving changes the state.
            \item The robot receives rewards for reaching the goal and penalties for hitting walls.
            \item TD Learning allows the robot to update its expected values based on rewards as it explores, optimizing its path in real-time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Balances exploration and exploitation.
        \item Incorporates past experiences for effective learning in complex environments.
        \item Understanding TD(0), SARSA, and Q-Learning is crucial for mastering reinforcement learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Historical Context}
    \begin{block}{Introduction to Temporal-Difference Learning}
        Temporal-Difference (TD) Learning is a pivotal concept in reinforcement learning, blending ideas from dynamic programming and Monte Carlo methods. Understanding its historical development offers insight into its current applications and future potential.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Early Foundations (1950s - 1980s)}
    \begin{itemize}
        \item \textbf{Markov Decision Processes (MDPs):} The groundwork for TD Learning started with MDPs, where the agent learns optimal policies based on states and rewards.
        \item \textbf{Learning Paradigms:} Early learning methods focused on statistical convergence and were primarily theoretical.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Development of TD Learning (1988)}
    \begin{itemize}
        \item \textbf{Richard Sutton's Breakthrough:} Sutton introduced the concept of updating value estimates based on the difference between predicted and actual rewards. This work coined the term "temporal-difference."
        \begin{itemize}
            \item \textbf{Value Function Updates:} TD methods update value estimates through experience without needing a complete model of the environment.
            \item \textbf{Bootstrapping:} This technique allows for faster learning by updating estimates based on other estimates.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Enhancements and Variants (1990s)}
    \begin{itemize}
        \item \textbf{Q-Learning (1992):} Introduced by Chris Watkins, it enables agents to learn the value of actions in various states, independent of the environment's dynamics.
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
        \end{equation}
        where:
        \begin{itemize}
            \item \(Q(s, a)\) is the action-value function,
            \item \(\alpha\) is the learning rate,
            \item \(r\) is the reward received,
            \item \(\gamma\) is the discount factor,
            \item \(s'\) is the subsequent state.
        \end{itemize}
        
        \item \textbf{SARSA (On-policy TD):} Introduced by Andrew Barto, where the update rule depends on the action taken following the current policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Modern Applications and Deep Learning (2010s - Present)}
    \begin{itemize}
        \item \textbf{Integration with Deep Learning:} The fusion with deep neural networks resulted in Deep Q-Networks (DQN), enabling agents to learn in complex environments like video games (e.g., AlphaGo).
        \item \textbf{Policy Gradient Methods:} These strategies leverage TD learning concepts to optimize policies directly, enhancing learning efficiency in high-dimensional spaces.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item \textbf{Combines Concepts:} TD learning integrates Monte Carlo and dynamic programming principles for efficient learning without a complete environment model.
        \item \textbf{Key Advances:}
            \begin{itemize}
                \item Sutton's introduction of TD methods
                \item Q-Learning and SARSA as foundational algorithms
                \item Recent advancements with Deep Reinforcement Learning.
            \end{itemize}
        \item \textbf{Significance:} TD learning remains essential for developing autonomous systems that learn from temporally structured data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Remarks}
    Understanding the historical context of temporal-difference learning highlights its evolution and underscores its relevance in today's AI-driven applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Temporal-Difference Learning}
    \begin{itemize}
        \item Temporal-Difference (TD) learning combines ideas from Monte Carlo methods and dynamic programming.
        \item It enables agents to learn predictions about future rewards directly from the environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Reinforcement Learning (RL)}: Framework where agents make decisions to maximize cumulative reward.
        \item \textbf{Value Function}: Estimates future rewards, evaluating the desirability of states or actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Direct Policy Evaluation vs. Temporal-Difference Learning}
    \begin{block}{Direct Policy Evaluation}
        \begin{itemize}
            \item \textbf{Definition}: Averages returns after visiting states; relies on complete episodes.
            \item \textbf{Limitations}: Can lead to long wait times as estimates only computed at episode end.
            \item \textbf{Illustration}: Average return is calculated post-episode, disadvantaging quick learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Temporal-Difference Learning}
    \begin{block}{Definition}
        \begin{itemize}
            \item Updates estimates without waiting for episodes to end; more efficient than direct evaluation.
            \item \textbf{Key Features}:
            \begin{itemize}
                \item Bootstrapping: Uses existing value estimates.
                \item On-Policy Learning: Updates based on current policy actions.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of TD Learning Update}
    \begin{equation}
    V(s_t) \leftarrow V(s_t) + \alpha \times (r_{t+1} + \gamma \times V(s_{t+1}) - V(s_t))
    \end{equation}
    \begin{itemize}
        \item Where:
        \begin{itemize}
            \item $\alpha$: Step-size parameter (learning rate).
            \item $\gamma$: Discount factor (0 ≤ $\gamma$ < 1).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Efficiency}: TD learning updates after every time step; promotes faster convergence.
        \item \textbf{Combining Ideas}: Merges concepts from Monte Carlo methods and dynamic programming.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Application Idea}
    \begin{itemize}
        \item Implement a basic TD learning algorithm in a simple environment (e.g., a grid world).
        \item Observe the impact of parameters $\alpha$ and $\gamma$ on the learning process.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Understanding differences between direct policy evaluation and TD learning illustrates TD methods' efficiency.
        \item Continuous knowledge updates in a dynamic environment lead to robust decision-making capabilities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-Learning Overview}
    
    \begin{block}{Introduction to Q-Learning}
        Q-Learning is a model-free reinforcement learning algorithm that allows agents to learn optimal policies through interaction with an environment.
        It is beneficial in situations where the agent lacks a complete model of the environment's dynamics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    
    \begin{itemize}
        \item \textbf{Agent}: The learner or decision-maker interacting with the environment.
        \item \textbf{Environment}: The system with which the agent interacts.
        \item \textbf{State (S)}: The agent's current representation within the environment.
        \item \textbf{Action (A)}: The choice made by the agent that alters the state.
        \item \textbf{Reward (R)}: Feedback received from the environment based on the action taken.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Q-Learning Works}
    
    \begin{enumerate}
        \item \textbf{Q-Values}: The agent maintains Q-values for state-action pairs, \( Q(s, a) \), indicating expected future rewards.
        \item \textbf{Exploration vs. Exploitation}: The agent needs to balance discovering new actions and utilizing known rewarding actions.
        \item \textbf{Update Rule}: The Q-values are updated using:
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
        \end{equation}
        where \( \alpha \) is the learning rate, \( \gamma \) is the discount factor, \( s' \) is the new state after action \( a \), and \( a' \) represents possible actions from the new state.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Q-Learning}
    
    Imagine a grid world where the agent must reach a goal. Each movement incurs a small negative reward until the goal is reached, which yields a positive reward. Q-learning would proceed as follows:
    
    \begin{itemize}
        \item The agent explores the grid and records Q-values for various actions.
        \item Initially, Q-values may be set to zero. 
        \item As the agent interacts with the environment, it updates its Q-values based on received rewards.
        \item Through exploration and exploitation, the agent learns the optimal path to the goal.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Off-Policy Learning}: Q-learning can learn the optimal policy while following a different behavior policy.
        \item \textbf{Convergence}: With sufficient exploration, Q-learning converges to optimal Q-values.
        \item \textbf{Flexibility}: Applicable in various domains, such as games and robotics, without prior knowledge of the environment’s dynamics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    Q-learning is a potent reinforcement learning algorithm that allows agents to make optimal decisions through direct interaction with the environment. By maintaining and updating Q-values, agents navigate complex scenarios and improve their decision-making policies effectively over time.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example}
    
    \begin{lstlisting}[language=Python]
import numpy as np

# Initialize Q-table
Q = np.zeros((state_size, action_size))

def update_Q(state, action, reward, next_state, alpha, gamma):
    best_next_action = np.argmax(Q[next_state])
    Q[state, action] += alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-Learning Algorithm Details - Overview}
    \begin{itemize}
        \item Q-Learning is a model-free reinforcement learning algorithm.
        \item It is used to learn the value of actions in an environment.
        \item The goal is to find the optimal policy to maximize cumulative reward.
        \item Q-values associate with state-action pairs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-Learning Algorithm Details - Key Concepts}
    \begin{enumerate}
        \item \textbf{Q-Value (Action-Value)}:
            \begin{itemize}
                \item Denoted as $Q(s, a)$.
                \item Represents expected utility (cumulative reward) of taking action 'a' in state 's'.
            \end{itemize}
        \item \textbf{Temporal-Difference Learning}:
            \begin{itemize}
                \item Learns from the difference between predicted and actual rewards over time.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-Learning Algorithm Steps}
    \begin{enumerate}
        \item \textbf{Initialize Q-Values}:
            \begin{itemize}
                \item Set all Q-values for state-action pairs to a small random number or zero.
                \item Example: 
                \begin{lstlisting}
Q = np.zeros((num_states, num_actions))
                \end{lstlisting}
            \end{itemize}
        \item \textbf{Choose an Action}:
            \begin{itemize}
                \item Use an exploration strategy (e.g., $\epsilon$-greedy).
                \item Example: 
                \begin{lstlisting}
action = np.random.choice(possible_actions) 
if np.random.random() < epsilon:
    action = np.random.choice(possible_actions)
else:
    action = np.argmax(Q[state])
                \end{lstlisting}
            \end{itemize}
        \item \textbf{Take the Action and Observe the Reward}
        \item \textbf{Update the Q-Value}:
            \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
            \end{equation}
            \item Where:
            \begin{itemize}
                \item $\alpha$ = learning rate
                \item $r$ = immediate reward received
                \item $\gamma$ = discount factor
                \item $s'$ = the state reached after taking action $a$
            \end{itemize}
        \item \textbf{Repeat until convergence}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Q-Learning - Overview}
    Q-Learning is a powerful model-free reinforcement learning algorithm used to train agents for decision-making. 
    Key advantages include:
    \begin{itemize}
        \item Off-policy learning
        \item Model-free approach
        \item Convergence guarantee
        \item Wide range of applications
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Q-Learning - Off-Policy Learning}
    \begin{block}{Definition}
    In Q-Learning, the learning process can occur independently of the policy that the agent is currently following.
    \end{block}
    \begin{itemize}
        \item \textbf{Benefit:}
        \begin{itemize}
            \item Agents can learn from experiences gathered from other policies or human-generated data, enhancing efficiency.
            \item Example: Learning while following an exploratory strategy can improve knowledge about the environment.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Q-Learning - Model-Free Approach}
    \begin{block}{Definition}
    Q-Learning does not require a model of the environment; it learns values directly from actions taken.
    \end{block}
    \begin{itemize}
        \item \textbf{Benefit:}
        \begin{itemize}
            \item Simplifies the learning process, making it efficient in complex environments.
            \item Example: An autonomous robot can navigate unknown terrains without constructing a full environment map.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Q-Learning - Convergence Guarantee}
    \begin{block}{Definition}
    Q-Learning is guaranteed to converge to the optimal action-value function under certain conditions.
    \end{block}
    \begin{itemize}
        \item \textbf{Benefit:}
        \begin{itemize}
            \item Ensures that an optimal policy can eventually be found, despite initial suboptimal behavior.
        \end{itemize}
    \end{itemize}
    \begin{equation}
    \lim_{n \to \infty} Q(s, a) = Q^*(s, a) \quad \text{(optimal action-value function)}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Q-Learning - Applications}
    \begin{itemize}
        \item **Robotics**: Learning tasks like grasping and item sorting.
        \item **Game Playing**: Successfully applied in games such as chess and Go via self-play.
        \item **Finance**: Optimal portfolio management strategies adapting to changing market conditions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Flexibility:} Off-policy learning supports versatile training.
        \item \textbf{Robustness:} Model-free nature is adaptable to various environments.
        \item \textbf{Efficiency:} Convergence guarantees provide reliability for optimal solutions.
        \item \textbf{Versatility:} Extensive applicability across diverse domains.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Potential Code Snippet - Q-Learning}
    \begin{lstlisting}[language=Python]
import numpy as np

def q_learning(env, num_episodes, alpha, gamma, epsilon):
    Q = np.zeros((env.observation_space.n, env.action_space.n))
    for episode in range(num_episodes):
        state = env.reset()
        done = False
        while not done:
            if np.random.rand() < epsilon:
                action = env.action_space.sample()  # Exploration
            else:
                action = np.argmax(Q[state])  # Exploitation
            
            next_state, reward, done, _ = env.step(action)
            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
            state = next_state
    return Q
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Q-Learning's robust, off-policy nature and model-free advantages make it a cornerstone technique in reinforcement learning. 
    By leveraging these strengths, we can effectively train agents to navigate complex environments and tasks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Overview - Introduction to SARSA}
    \begin{itemize}
        \item \textbf{What is SARSA?}
        \begin{itemize}
            \item SARSA stands for \textbf{State-Action-Reward-State-Action}, used in reinforcement learning to learn action values.
            \item It is an \textbf{on-policy algorithm}, learning the value of the policy being followed, not a separate random policy.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Overview - How SARSA Works}
    \begin{enumerate}
        \item \textbf{Agent-Environment Interaction}
        \begin{itemize}
            \item The agent observes the current state \( S_t \).
            \item Selects an action \( A_t \) using a policy (e.g., ε-greedy).
            \item Performs the action, receives reward \( R_t \), and transitions to the next state \( S_{t+1} \).
            \item Selects the next action \( A_{t+1} \) from the new state.
        \end{itemize}
        
        \item \textbf{Updating the Action-Value Function}
        \begin{equation}
        Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_t + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]
        \end{equation}
        Where:
        \begin{itemize}
            \item \( \alpha \): learning rate
            \item \( \gamma \): discount factor
            \item Estimated return: \( R_t + \gamma Q(S_{t+1}, A_{t+1}) \)
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Overview - SARSA vs Q-Learning}
    \begin{itemize}
        \item \textbf{Key Differences:}
        \begin{itemize}
            \item \textbf{On-Policy vs Off-Policy:} 
            \begin{itemize}
                \item SARSA is on-policy; Q-learning is off-policy, learning regardless of the agent's actions.
            \end{itemize}
            \item \textbf{Exploration vs Exploitation:}
            \begin{itemize}
                \item SARSA considers the actual action taken in the next state, while Q-learning uses the maximum estimated action value.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Overview - Example in Practice}
    \begin{itemize}
        \item \textbf{Scenario:} An agent navigating a grid world.
        \begin{itemize}
            \item Starts at (0,0) and can move in four directions.
            \item Follows an ε-greedy policy: 90\% chance of best-known action, 10\% chance to explore.
        \end{itemize}
    \end{itemize}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            Step & State \( S_t \) & Action \( A_t \) & Reward \( R_t \) & Next State \( S_{t+1} \) & Next Action \( A_{t+1} \) \\
            \hline
            1 & (0,0) & Right & 0 & (0,1) & Right \\
            2 & (0,1) & Down & 0 & (1,1) & Down \\
            3 & (1,1) & Down & +10 & (2,1) & Right \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Overview - Key Points}
    \begin{itemize}
        \item SARSA is effective in environments with variable outcomes based on selected actions.
        \item Its on-policy nature provides realistic estimates of returns, suitable for stochastic environments.
        \item Understanding SARSA is crucial for grasping other reinforcement learning algorithms and strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Algorithm Details - What is SARSA?}
    \begin{itemize}
        \item SARSA stands for State-Action-Reward-State-Action.
        \item It is a reinforcement learning algorithm that learns the value of actions in given states based on the following sequence:
        \begin{itemize}
            \item Current State (S)
            \item Action Taken (A)
            \item Resulting Reward (R)
            \item New State (S')
            \item Next Action (A')
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Algorithm Details - Step-by-Step Explanation}
    \begin{enumerate}
        \item \textbf{Initialization}:
            \begin{itemize}
                \item Set initial Q-values to arbitrary values (often zeros).
                \item Choose parameters: Learning rate $\alpha$, Discount factor $\gamma$, Exploration rate $\epsilon$.
            \end{itemize}
        \item \textbf{Choose an Action}: 
            \begin{itemize}
                \item Select action (A) using $\epsilon$-greedy policy.
            \end{itemize}
        \item \textbf{Take the Action}:
            \begin{itemize}
                \item Execute action (A), observe the reward (R) and new state (S').
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Algorithm Details - Step-by-Step Explanation (cont.)}
    \begin{enumerate}[resume]
        \item \textbf{Select Next Action}:
            \begin{itemize}
                \item Choose next action (A') using the same $\epsilon$-greedy policy.
            \end{itemize}
        \item \textbf{Update Q-Values}:
            \begin{equation}
            Q(S, A) \leftarrow Q(S, A) + \alpha \left( R + \gamma Q(S', A') - Q(S, A) \right)
            \end{equation}
            \begin{itemize}
                \item Updates are based on received reward and estimated future rewards.
            \end{itemize}
        \item \textbf{Transition to the Next State}:
            \begin{itemize}
                \item Set new state (S') as current state (S) and A' as current action (A).
                \item Repeat until a terminal state is reached.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA vs. Q-learning - Key Differentiators}
    \begin{itemize}
        \item \textbf{On-policy vs. Off-policy}:
            \begin{itemize}
                \item SARSA is \textbf{on-policy}: it improves the policy being executed.
                \item Q-learning is \textbf{off-policy}: it evaluates the optimal policy.
            \end{itemize}
        \item \textbf{Exploration Strategy}:
            \begin{itemize}
                \item SARSA's updates are affected by the actual actions taken.
                \item Q-learning uses the maximum Q-value for updates regardless of action.
            \end{itemize}
        \item \textbf{Convergence}:
            \begin{itemize}
                \item SARSA converges to the policy it explores, thus being more cautious.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Example Illustration}
    \begin{itemize}
        \item Example: Agent navigating a grid with movements Up, Down, Left, or Right.
        \item If moving from state S1 to S2 with a reward of +1, and moving Right:
        \begin{equation}
            Q(S1, \text{Up}) \leftarrow Q(S1, \text{Up}) + \alpha \left( 1 + \gamma Q(S2, \text{Right}) - Q(S1, \text{Up}) \right)
        \end{equation}
        \item Importance of both immediate rewards and the value of actions in new states.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Summary Points}
    \begin{itemize}
        \item SARSA uses actions taken for updates, focusing on the ongoing policy.
        \item Best suited for environments requiring balanced exploration-exploitation.
        \item Understanding SARSA lays foundational knowledge for advanced reinforcement learning concepts.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Disadvantages of SARSA - Introduction}
    \begin{block}{Introduction to SARSA}
        SARSA (State-Action-Reward-State-Action) is a temporal-difference learning algorithm used in reinforcement learning. 
        It learns the value of the action taken in each state rather than only the optimal actions, making it unique compared to alternatives like Q-learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of SARSA}
    \begin{enumerate}
        \item \textbf{On-Policy Learning}
            \begin{itemize}
                \item SARSA learns from the actions taken, including exploration steps.
                \item Example: If the agent explores a less optimal action, it learns the value of that action directly.
            \end{itemize}
        \item \textbf{Safety in Exploration}
            \begin{itemize}
                \item SARSA tends to be more cautious, potentially leading to safer exploration.
                \item Illustration: A robot exploring uncharted territory can better understand the consequences of its exploratory actions.
            \end{itemize}
        \item \textbf{Adaptability}
            \begin{itemize}
                \item More adaptable to changes in the environment due to reliance on on-policy data.
                \item Example: Quickly adjusts to changes in a rapidly evolving environment.
            \end{itemize}
        \item \textbf{Reduced Variance}
            \begin{itemize}
                \item Incorporates current policies into its learning, often leading to lower variance in updates.
                \item Benefit: More stable learning in resource-constrained scenarios.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Disadvantages of SARSA}
    \begin{enumerate}
        \item \textbf{Suboptimal Policy Learning}
            \begin{itemize}
                \item SARSA can converge to policies that are not optimal due to its on-policy nature.
                \item Example: Reinforces poor decision-making habits if actions taken are based on a suboptimal policy.
            \end{itemize}
        \item \textbf{Slower Convergence}
            \begin{itemize}
                \item Learning can be slower than off-policy methods like Q-learning due to actual actions taken.
                \item Analogy: Climbing a mountain using a single path can take longer than exploring multiple options.
            \end{itemize}
        \item \textbf{Sensitive to Exploration Strategy}
            \begin{itemize}
                \item Performance heavily relies on the exploration strategy, such as $\epsilon$-greedy methods.
                \item Poor exploration can lead to inadequate learning.
            \end{itemize}
        \item \textbf{Limited Information Utilization}
            \begin{itemize}
                \item Does not leverage the theoretical benefit of estimating values from the best possible actions.
                \item Illustration: An agent may miss optimal strategies because it only learns from actions it takes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Key Points}
        \begin{itemize}
            \item SARSA is context-sensitive and suitable for environments where exploration is critical.
            \item Limitations in learning optimal actions and slower convergence can hinder performance.
            \item Understanding SARSA's strengths and weaknesses is essential for selecting the appropriate strategy in reinforcement learning problems.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula Recap}
        The update rule for SARSA is given by:
        \begin{equation}
        Q(s, a) \gets Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
        \end{equation}
        Where:
        \begin{itemize}
            \item $Q(s, a)$: current action-value function
            \item $\alpha$: learning rate
            \item $r$: reward received
            \item $s'$: next state
            \item $a'$: next action chosen
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Q-Learning and SARSA}
    \begin{block}{Key Differences}
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Feature} & \textbf{Q-Learning} & \textbf{SARSA} \\
            \hline
            Algorithm Type & Off-policy & On-policy \\
            \hline
            Update Rule & Uses the maximum expected future reward & Uses the action taken in the update \\
            \hline
            Exploration Strategy & Greedy policy for updates & Selects action based on current policy \\
            \hline
            Convergence Behavior & Generally more stable, needs more data & Responsive to environment dynamics \\
            \hline
            Suitability & Optimal policy sought & Policy adherence needed \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-Learning}
    \begin{itemize}
        \item Off-policy learning algorithm.
        \item Learns the value of the optimal policy regardless of the agent's actions.
        \item Update rule uses the maximum expected reward.
    \end{itemize}
    
    \textbf{Example:} An agent uses an $\epsilon$-greedy strategy to choose actions, but updates Q-values assuming the best actions possible.

    \textbf{Use Case:} Ideal in environments where the agent explores independently of the learned policy, such as robotic navigation tasks seeking the optimal path.
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Explanation}
    \begin{itemize}
        \item On-policy algorithm updating value estimates based on the actual policy followed.
        \item Provides conservative updates leading to more cautious learning.
    \end{itemize}
    
    \textbf{Example:} An agent follows a specific policy affecting interaction and updates Q-values based on the actions it takes.

    \textbf{Use Case:} Suitable for environments that require structured policy adherence, e.g., autonomous driving, where safety is a concern.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Off-Policy vs. On-Policy:} 
        Q-Learning learns strategies irrespective of its actions, while SARSA is more cautious, suited for policy adherence.
        
        \item \textbf{Exploration vs. Exploitation:} 
        Q-Learning maximizes future rewards (greedy), whereas SARSA maintains the current exploration strategy for safer learning.
        
        \item \textbf{Practical Use Cases:} 
        Use Q-Learning for optimal policy learning and SARSA where balancing safety and exploration is critical.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Understanding the differences between Q-Learning and SARSA is essential for task suitability.
        \item Both algorithms cater to different challenges in reinforcement learning despite being in the same learning family.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical Applications of Temporal-Difference Learning}
    \begin{block}{Introduction}
        Temporal-Difference (TD) Learning is a fundamental approach in reinforcement learning, combining Monte Carlo methods and dynamic programming. Real-world applications demonstrate its effectiveness.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Applications}
    \begin{enumerate}
        \item \textbf{Game Playing (Chess and Go)}
            \begin{itemize}
                \item \textbf{Example:} AlphaGo (Google DeepMind)
                \item \textbf{Key Point:} TD methods enhance performance by evaluating game states using past rewards.
            \end{itemize}
        
        \item \textbf{Robotics and Autonomous Navigation}
            \begin{itemize}
                \item \textbf{Example:} Robot Path Planning
                \item \textbf{Key Point:} Enables real-time adaptation to dynamic environments.
            \end{itemize}
        
        \item \textbf{Personalized Recommendations}
            \begin{itemize}
                \item \textbf{Example:} Online Retail (e.g., Amazon)
                \item \textbf{Key Point:} Continuous improvement of recommendations based on user interactions.
            \end{itemize}
        
        \item \textbf{Healthcare Management}
            \begin{itemize}
                \item \textbf{Example:} Treatment Recommendation Systems
                \item \textbf{Key Point:} Supports data-driven insights into treatment effectiveness.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Learning for Value Estimation:} 
        \begin{itemize}
            \item TD methods estimate state values using received rewards and estimated future rewards.
        \end{itemize}
        
        \item \textbf{Exploration vs. Exploitation:}
        \begin{itemize}
            \item Balances discovering new actions with leveraging known high-value strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: TD Learning Example}
    Here’s a simple implementation of TD Learning using Python with NumPy:
    
    \begin{lstlisting}[language=Python]
import numpy as np

# Initialize variables
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
num_states = 5  # Example state space
value_table = np.zeros(num_states)  # Value table initialization

# TD update function
def temporal_difference_update(state, reward, next_state):
    value_table[state] += alpha * (reward + gamma * value_table[next_state] - value_table[state])

# Example usage
state = 0
reward = 1
next_state = 1
temporal_difference_update(state, reward, next_state)
print(value_table)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Temporal-Difference learning techniques have shown significant power across various fields, facilitating real-time learning and adaptation. Their versatility continues to grow as new applications arise, making them integral to modern AI.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Overview}
    \begin{block}{Temporal-Difference (TD) Learning}
        TD Learning is a reinforcement learning technique where agents learn from feedback in the form of rewards or penalties based on their actions. While powerful, its implementation involves several challenges and limitations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in TD Learning - Part 1}
    \begin{enumerate}
        \item \textbf{Sensitivity to Hyperparameters}
        \begin{itemize}
            \item TD learning algorithms are highly sensitive to hyperparameters like learning rate and discount factor.
            \item \textit{Example}: A high learning rate can cause oscillations in value estimates, preventing convergence.
        \end{itemize}

        \item \textbf{Exploration vs. Exploitation Dilemma}
        \begin{itemize}
            \item Balancing exploration of new actions against exploiting known rewarding actions is crucial.
            \item \textit{Example}: In grid-world scenarios, excessive exploration may waste opportunities to optimize known strategies.
        \end{itemize}

        \item \textbf{Credit Assignment Problem}
        \begin{itemize}
            \item Assigning credit to actions that result in rewards can be complex, especially with delayed feedback.
            \item \textit{Example}: In chess, a victory may depend on moves made several turns prior, complicating credit assignment.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in TD Learning - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Function Approximation Issues}
        \begin{itemize}
            \item Using function approximators may lead to overfitting, causing poor performance in unseen states.
            \item \textit{Example}: An agent trained in limited conditions may struggle in different environments due to a rigid value function.
        \end{itemize}

        \item \textbf{Sparse Rewards}
        \begin{itemize}
            \item Real-world applications often present sparse rewards, leading to slow and difficult learning.
            \item \textit{Example}: A robot navigating a space receives feedback only upon task completion.
        \end{itemize}

        \item \textbf{Computational Complexity}
        \begin{itemize}
            \item Certain TD learning methods can be computationally expensive, requiring significant resources for training.
            \item \textit{Example}: Deep Q-Learning needs considerable GPU power for effective training across large state spaces.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Conclusion}
    \begin{itemize}
        \item Proper tuning of hyperparameters is critical for effective TD learning.
        \item A balanced approach to exploration and exploitation optimizes learning efficacy.
        \item Addressing credit assignment is fundamental in environments with delayed rewards.
        \item Caution is needed with function approximation to avoid overfitting.
        \item Overcoming sparse rewards may involve techniques like reward shaping.
        \item Prepare for the computational requirements of TD learning in complex environments.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Despite inherent challenges, TD learning remains a crucial technique in reinforcement learning. Awareness of these limitations aids practitioners in developing more robust TD learning implementations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Temporal-Difference Learning - Introduction}
    \begin{block}{Overview}
        Temporal-Difference (TD) Learning is a cornerstone of Reinforcement Learning (RL) that integrates ideas from Monte Carlo methods and dynamic programming.
    \end{block}
    As the field evolves, key trends and promising research areas in TD learning begin to emerge, including:
    \begin{itemize}
        \item Deep Reinforcement Learning (DRL)
        \item Hierarchical Reinforcement Learning (HRL)
        \item Exploration Strategies
        \item Incorporating Uncertainty in Value Estimates
        \item Multi-Agent and Cooperative Learning
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Temporal-Difference Learning - Key Trends}
    \begin{enumerate}
        \item \textbf{Deep Reinforcement Learning (DRL)}
            \begin{itemize}
                \item Integration with deep neural networks (e.g., DQN) improves performance.
                \item Applications: Game playing (e.g., AlphaGo, Dota 2), robotics.
                \item Future Work: Explore architectures to reduce sample complexity.
            \end{itemize}
        
        \item \textbf{Hierarchical Reinforcement Learning (HRL)}
            \begin{itemize}
                \item Structuring tasks into hierarchies for TD learning.
                \item Example: High-level policies set goals for low-level policies.
                \item Future Work: Automatic hierarchy construction for scalability.
            \end{itemize}

        \item \textbf{Exploration Strategies}
            \begin{itemize}
                \item Innovations can enhance TD's ability to discover optimal policies.
                \item Implement curiosity-driven exploration in dynamic environments.
                \item Future Work: Develop adaptive exploration-exploitation strategies.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Temporal-Difference Learning - Continued}
    \begin{enumerate}[resume]
        \item \textbf{Incorporating Uncertainty in Value Estimates}
            \begin{itemize}
                \item Quantifying uncertainty in TD value estimates for better decision-making.
                \item Use Bayesian methods to update value functions.
                \item Future Work: Research uncertainty-aware TD methods for robustness.
            \end{itemize}

        \item \textbf{Multi-Agent and Cooperative Learning}
            \begin{itemize}
                \item Extending TD learning to multi-agent scenarios.
                \item Example: Learning in competitive gaming environments.
                \item Future Work: Investigate coordination frameworks using TD methods.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Evolution of TD Learning as a foundation for advanced techniques.
            \item Continuous integration with emerging technologies for innovative applications.
            \item Focus on improving data efficiency in constrained environments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary}
    \begin{block}{Chapter: Week 5 - Temporal-Difference Learning}
        A summary of key takeaways from the chapter on temporal-difference learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Part 1}
    \begin{enumerate}
        \item \textbf{Definition and Concept:}   
        Temporal-Difference (TD) Learning is a fundamental technique in reinforcement learning where the learning agent updates its predictions of future rewards based on the difference between predicted and actual rewards.
        
        \item \textbf{Key Principles:}
        \begin{itemize}
            \item \textbf{Bootstrapping:} Updates based on previously estimated values, not waiting for an episode to complete.
            \item \textbf{Temporal-Difference Error ($\delta$):} 
            \begin{equation}
            \delta = r + \gamma V(s') - V(s)
            \end{equation}
            where $r$ is the immediate reward, $V(s)$ is the value of the current state, $V(s')$ is the value of the next state, and $\gamma$ is the discount factor.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Advantages of TD Learning:}
        \begin{itemize}
            \item Real-time learning allows agents to continually refine their estimates.
            \item Efficient for large state spaces, requiring fewer samples to converge.
        \end{itemize}

        \item \textbf{Challenges and Limitations:}
        \begin{itemize}
            \item Sensitive to hyperparameters such as the learning rate ($\alpha$) and discount factor ($\gamma$).
            \item Can converge to suboptimal policies; requires careful tuning.
        \end{itemize}

        \item \textbf{Future Directions:}
        \begin{itemize}
            \item Explore hierarchical and multi-agent frameworks.
            \item Integration with deep learning, leading to advancements like Deep Q-Networks (DQN).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    Temporal-Difference Learning is a pivotal approach in reinforcement learning that enhances the ability of agents to learn from their experiences. This enables the development of robust, adaptive models that perform well in dynamic environments.

    \textbf{Engagement Suggestion:}  
    Consider building small projects using TD learning algorithms, such as developing a simple game where agents learn to maximize scores through interaction. This hands-on experience will deepen the understanding of key concepts and encourage application-oriented thinking.
\end{frame}


\end{document}