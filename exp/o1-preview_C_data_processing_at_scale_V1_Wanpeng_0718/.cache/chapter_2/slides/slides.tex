\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 2: Data Processing Frameworks}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Data Processing Frameworks}
    \begin{block}{Importance}
        In the era of big data, the ability to process and analyze large datasets efficiently and effectively has become crucial. Data processing frameworks are essential tools that provide the architecture and tools needed to handle this data volume and complexity.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What Are Data Processing Frameworks?}
    Data processing frameworks are systems or libraries that streamline the process of collecting, storing, processing, and analyzing data. They allow developers to:
    \begin{itemize}
        \item Write code more easily
        \item Manage resources efficiently
        \item Simplify complex data workflows
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Data Volume}: Refers to the vast amount of information generated every second. This includes structured data (databases) and unstructured data (e.g., social media, videos).
        \item \textbf{Data Velocity}: The speed at which data is generated, collected, and processed. Real-time data processing frameworks can respond immediately to incoming data streams.
        \item \textbf{Data Variety}: The different forms of data – from text and images to logs and transactions. Frameworks can integrate and process this diverse data seamlessly.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Are Data Processing Frameworks Necessary?}
    \begin{itemize}
        \item \textbf{Scalability}: Easily scale to accommodate growing amounts of data without significant changes in the underlying architecture.
        \item \textbf{Efficiency}: Optimized for parallel processing, reducing the time to extract insights from large datasets.
        \item \textbf{Flexibility}: Supports various programming languages and data types, adaptable to different business needs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Examples of Data Processing Frameworks}
    \begin{itemize}
        \item \textbf{Apache Hadoop}: 
        \begin{itemize}
            \item A widely-used framework that allows distributed processing of large datasets across clusters of computers using simple programming models.
            \item Includes components like Hadoop Distributed File System (HDFS) for storage and Yet Another Resource Negotiator (YARN) for resource management.
        \end{itemize}
        \item \textbf{Apache Spark}:
        \begin{itemize}
            \item A fast and general-purpose cluster-computing system.
            \item Provides high-level APIs in Java, Scala, Python, and R, excelling in batch processing, interactive queries, and streaming data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Data processing frameworks are vital for modern data analytics.
        \item They handle large volumes, rapid velocity, and diverse types of data efficiently.
        \item Familiarity with these frameworks is essential for data engineers and analysts.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Understanding data processing frameworks is a foundational step for anyone looking to work in data science and analytics. They not only facilitate advanced data analysis but also empower organizations to make data-driven decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Apache Hadoop - Overview}
    \begin{block}{What is Apache Hadoop?}
        Apache Hadoop is an open-source framework designed for storing and processing large datasets across clusters of computers.
        It scales from a single server to thousands of machines, enabling local computation and storage.
    \end{block}
    
    \begin{block}{Importance in Big Data Processing}
        Hadoop addresses challenges posed by big data, such as volume, velocity, and variety, allowing organizations to handle large amounts of structured and unstructured data efficiently and cost-effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Apache Hadoop}
    \begin{block}{Key Components}
        Hadoop's architecture is built on two primary components:
        \begin{enumerate}
            \item \textbf{Hadoop Distributed File System (HDFS)}
            \begin{itemize}
                \item Designed for storing large files in a distributed manner.
                \item Ensures data redundancy and high availability.
                \item \textbf{Key Features:}
                \begin{itemize}
                    \item Block Storage: Files split into blocks (default 128 MB).
                    \item Data Replication: Blocks replicated (default 3 replicas).
                \end{itemize}
                \item \textbf{Example:} A 512 MB file can be split into 4 blocks of 128 MB stored on different nodes.
            \end{itemize}
        
            \item \textbf{Yet Another Resource Negotiator (YARN)}
            \begin{itemize}
                \item Acts as the resource management layer of Hadoop.
                \item \textbf{Key Features:}
                \begin{itemize}
                    \item Resource Management: Dynamically assigns resources based on demand.
                    \item Job Scheduling: Manages tasks across the cluster.
                \end{itemize}
                \item \textbf{Example:} YARN utilizes nodes with available resources for data processing jobs, minimizing bottlenecks.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop in the Big Data Ecosystem}
    \begin{block}{Role in Big Data}
        Hadoop serves as the foundational layer for many big data applications, such as:
        \begin{itemize}
            \item \textbf{Apache Hive}: Data warehouse software for managing large datasets using SQL-like queries.
            \item \textbf{Apache Pig}: High-level platform for creating data processing programs on Hadoop.
            \item \textbf{Apache HBase}: NoSQL database on top of HDFS for real-time data access.
        \end{itemize}
    \end{block}

    \begin{block}{Summary Points}
        \begin{itemize}
            \item \textbf{Scalability}: Horizontally scales by adding nodes as data grows.
            \item \textbf{Fault Tolerance}: Data redundancy through replication ensures availability even with node failures.
            \item \textbf{Cost-Effectiveness}: Utilizes commodity hardware, reducing costs compared to traditional systems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Practical Example}
    \begin{block}{Conclusion}
        Apache Hadoop is essential for organizations seeking to leverage big data. Its architecture, with HDFS for storage and YARN for resource management, allows for efficient data processing.
    \end{block}

    \begin{block}{Code Examples}
        \begin{lstlisting}[language=bash]
# Command to check HDFS file system
hdfs dfs -ls /

# Command to copy a local file to HDFS
hdfs dfs -put localfile.txt /user/hadoop/
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Apache Spark - Introduction}
    \begin{itemize}
        \item \textbf{Definition}: Apache Spark is an open-source, distributed processing system designed for big data analytics. 
        \item \textbf{Purpose}: It processes large datasets quickly by leveraging in-memory computation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Apache Spark - Architecture}
    \begin{block}{Components}
        \begin{itemize}
            \item \textbf{Driver Program}: Runs the user’s application, converts it into tasks, and distributes them to workers.
            \item \textbf{Cluster Manager}: Allocates resources to Spark applications (examples: standalone cluster, Apache Mesos, or Hadoop YARN).
            \item \textbf{Workers}: Nodes in the cluster that execute tasks assigned by the Driver.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Apache Spark - Core Components}
    \begin{enumerate}
        \item \textbf{Spark Core}
        \begin{itemize}
            \item Provides task scheduling, memory management, fault recovery.
            \item \textbf{Key Concepts:}
            \begin{itemize}
                \item \textbf{RDD (Resilient Distributed Dataset)}: Immutable collections that can be processed in parallel, ensuring resilience and fault tolerance.
                \item \textbf{Transformations and Actions}:
                \begin{itemize}
                    \item \textbf{Transformations}: Create a new RDD from an existing one (e.g., \texttt{map()}, \texttt{filter()}).
                    \item \textbf{Actions}: Trigger computation and return results (e.g., \texttt{count()}, \texttt{collect()}).
                \end{itemize}
            \end{itemize}
        \end{itemize}

        \item \textbf{Spark SQL}
        \begin{itemize}
            \item Allows querying structured data using SQL.
            \item \textbf{Key Features:}
            \begin{itemize}
                \item Supports various data sources (e.g., Hive tables, Avro, Parquet).
                \item DataFrames and Datasets provide a higher-level abstraction over RDDs.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Apache Spark - Comparison with Hadoop}
    \begin{itemize}
        \item \textbf{Speed}: 
        Spark processes data in-memory, significantly faster than Hadoop's MapReduce which writes intermediate results to disk.
        
        \item \textbf{Ease of Use}:
        Spark offers APIs in multiple languages (Java, Scala, Python, R), making it more accessible compared to Hadoop.

        \item \textbf{Capabilities}:
        Spark supports batch processing, real-time processing (via Spark Streaming), graph processing (via GraphX), and machine learning (via MLlib).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Apache Spark - Architecture - Illustration}
    \begin{verbatim}
    +-------------------+                     +----------------------+
    |    Driver Program  |<-- Task Scheduling |     Cluster Manager    |
    +-------------------+                     +----------------------+
                 |                                      |
          +------------------+            +------------+----------------+
          |        RDDs      |            |          Spark Workers     |
          +-------|----------+            +----------------------------+
                  |                                    |
          +--------+--------+    +----------------------+   +----------+
          |     Transformations    |    Actions        |   |  Spark SQL|
          +----------------------+   +------------------+   +----------+
    \end{verbatim}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Apache Spark - Key Points}
    \begin{itemize}
        \item Apache Spark offers improved performance through in-memory processing.
        \item RDDs provide robustness and parallel processing capabilities.
        \item Spark SQL extends functionality, enabling SQL queries on large datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences between Hadoop and Spark - Overview}
    \begin{itemize}
        \item Apache Hadoop and Apache Spark are both frameworks for processing large datasets.
        \item They have distinct characteristics tailored to different use cases.
        \item Key differentiating factors include:
        \begin{itemize}
            \item Speed
            \item Ease of Use
            \item Capabilities
            \item Fault Tolerance
            \item Ecosystem Integration
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences - Speed and Ease of Use}
    \begin{block}{Speed}
        \begin{itemize}
            \item \textbf{Hadoop}: Utilizes the MapReduce model, writing data to disk after each phase, leading to slower processing.
            \item \textbf{Spark}: Operates in-memory, allowing for rapid computations, especially beneficial for iterative algorithms.
        \end{itemize}
        \textbf{Example:} Data scientists may find Spark to complete tasks faster than Hadoop.
    \end{block}
    
    \begin{block}{Ease of Use}
        \begin{itemize}
            \item \textbf{Hadoop}: Requires a deep understanding of Java and MapReduce.
            \item \textbf{Spark}: Offers high-level APIs in Python, Scala, and R, simplifying data manipulation.
        \end{itemize}
        \textbf{Example:} PySpark enables concise data analysis with less code compared to Hadoop.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences - Capabilities and Conclusion}
    \begin{block}{Capabilities}
        \begin{itemize}
            \item \textbf{Hadoop}:
                \begin{itemize}
                    \item Primarily for batch processing.
                    \item Uses HDFS for scalable storage.
                \end{itemize}
            \item \textbf{Spark}:
                \begin{itemize}
                    \item Supports real-time processing and batch processing.
                    \item Offers MLlib, GraphX, and Spark SQL for various data processing needs.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Spark is faster due to in-memory processing.
            \item Spark is more user-friendly with high-level APIs.
            \item Spark handles both batch and real-time data effectively.
        \end{itemize}
    \end{block}
    
    \textbf{Conclusion:} Choosing between Hadoop and Spark depends on your specific requirements like speed, ease of use, and data processing type.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion Techniques}
    \begin{block}{Overview}
        Data ingestion refers to the process of obtaining and importing data for immediate use or storage in a database. In the context of big data frameworks like Apache Hadoop and Apache Spark, efficient data ingestion techniques are crucial for ensuring timely and effective data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Data Ingestion Techniques}
    \begin{enumerate}
        \item \textbf{Batch Ingestion}
            \begin{itemize}
                \item \textbf{Definition}: Data is collected and processed in large blocks or batches at scheduled intervals.
                \item \textbf{Use Case}: Suitable for scenarios where real-time processing is not crucial, such as monthly sales reports.
                \item \textbf{Example}:
                    \begin{lstlisting}[language=bash]
                    sqoop import --connect jdbc:mysql://localhost/sales --table orders --target-dir /user/hadoop/orders
                    \end{lstlisting}
            \end{itemize}
        
        \item \textbf{Streaming Ingestion}
            \begin{itemize}
                \item \textbf{Definition}: Data is collected and processed continuously as it arrives.
                \item \textbf{Use Case}: Ideal for scenarios requiring real-time analysis, such as monitoring social media feeds or sensor data.
                \item \textbf{Example}:
                    \begin{lstlisting}[language=python]
                    from pyspark import SparkContext
                    from pyspark.streaming import StreamingContext

                    sc = SparkContext("local[*]", "NetworkWordCount")
                    ssc = StreamingContext(sc, 1)

                    lines = ssc.socketTextStream("localhost", 9999)
                    words = lines.flatMap(lambda line: line.split(" "))
                    words.pprint()
                    \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Data Ingestion Techniques (continued)}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        \item \textbf{Micro-batch Ingestion}
            \begin{itemize}
                \item \textbf{Definition}: A blend of batch and streaming techniques, where data is processed in small batches at very short intervals.
                \item \textbf{Use Case}: Balances the need for both real-time processing and reduced latency.
            \end{itemize}

        \item \textbf{File-based Ingestion}
            \begin{itemize}
                \item \textbf{Definition}: Periodically uploading data files (such as CSV, JSON) to a data store.
                \item \textbf{Use Case}: Useful for transferring large datasets from one location to another.
            \end{itemize}

        \item \textbf{API-based Ingestion}
            \begin{itemize}
                \item \textbf{Definition}: Data is pulled from external services via APIs.
                \item \textbf{Use Case}: Useful for real-time data feeds from web services.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Choosing the Right Technique: The choice depends on data volume, velocity, and variety.
            \item Integration with Frameworks: Each technique's integration with Hadoop and Spark is vital for optimal performance.
            \item Tools and Utilities: Familiarity with tools like Sqoop, Apache NiFi, Kafka, and Spark Streaming is essential.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Effective data ingestion techniques are critical in leveraging the full capabilities of data processing frameworks like Hadoop and Spark. Understanding these methods helps optimize workflows and adapt to varying data environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Slide Preview}
    Stay tuned for our next discussion on \textbf{Data Processing and Transformation}, where we'll explore the ETL (Extract, Transform, Load) processes within Hadoop and Spark frameworks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Processing and Transformation - Introduction}
    \begin{block}{Introduction}
        Data processing and transformation are critical stages in the data lifecycle, especially within big data frameworks such as Hadoop and Spark. This slide focuses on how these frameworks handle data processing tasks and support ETL (Extract, Transform, Load) processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Processing and Transformation - The Backbone of ETL}
    \begin{enumerate}
        \item \textbf{Extract}: Collection of data from multiple sources (e.g., databases, data lakes, APIs, stream data).
        \item \textbf{Transform}: Processing to filter, aggregate, map, and merge datasets into a usable format.
        \item \textbf{Load}: Loading transformed data into a target data warehouse, database, or visualization tool for analysis.
    \end{enumerate}

    \begin{block}{Example of ETL Process}
        A retail company collects sales data from different branches: 
        \begin{itemize}
            \item \textbf{Extract}: Pull data from daily CSV files generated by each store.
            \item \textbf{Transform}: Clean (remove duplicates), combine (aggregate sales by category), enrich (calculate total sales per store).
            \item \textbf{Load}: Load processed data into a central database for reporting.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Processing Frameworks: Hadoop vs. Spark}
    \begin{itemize}
        \item \textbf{Hadoop MapReduce}:
            \begin{itemize}
                \item Processing Model: Disk-based storage, batch processing.
                \item Example: Analyzing log files to extract user behavior.
            \end{itemize}
        \item \textbf{Apache Spark}:
            \begin{itemize}
                \item Processing Model: In-memory processing, faster execution.
                \item Example: Real-time data streaming for social media sentiment analysis.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Functions in Spark for Data Transformation}
    \begin{itemize}
        \item \textbf{DataFrame API}: Supports SQL-like operations for easy data manipulation.
        
        \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.appName("ETL Example").getOrCreate()
        df = spark.read.csv("sales_data.csv", header=True)
        total_sales = df.groupBy("product_category").agg({"sales": "sum"})
        total_sales.show()
        \end{lstlisting}
        \end{block}
        
        \item \textbf{RDD Operations}: Functions such as map, filter, and reduce provide fine-grained control over transformations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{itemize}
        \item Data processing and transformation are essential for extracting insights from data.
        \item Hadoop MapReduce and Spark offer robust frameworks for ETL processes, each suited to different scenarios.
        \item Understanding these frameworks' nuances prepares for designing scalable data workflows.
    \end{itemize}
    \begin{block}{Final Reminder}
        The choice between Hadoop and Spark should be guided by specific data processing needs, performance requirements, and available resources.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{block}{Overview}
        In this slide, we delve into the design and implementation of scalable data architectures, specifically utilizing Hadoop and Spark. Scalability ensures that a data processing framework can handle increasing amounts of data efficiently and reliably. This session highlights the key aspects of performance and reliability in architectures to optimize data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Scalability}
            \begin{itemize}
                \item \textbf{Definition}: The system's ability to accommodate increased workloads without compromising performance.
                \item \textbf{Types}:
                    \begin{itemize}
                        \item \textbf{Vertical Scalability (Scaling Up)}: Enhancing the power of existing machines (more CPU, RAM).
                        \item \textbf{Horizontal Scalability (Scaling Out)}: Adding more machines to distribute the load.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Data Processing Frameworks}
            \begin{itemize}
                \item \textbf{Hadoop}: 
                    \begin{itemize}
                        \item Open-source framework for distributed storage and processing.
                        \item \textbf{Components}:
                            \begin{itemize}
                                \item HDFS: For distributed storage.
                                \item MapReduce: For distributed data processing.
                            \end{itemize}
                    \end{itemize}
                \item \textbf{Spark}: 
                    \begin{itemize}
                        \item Open-source data processing engine for in-memory data processing.
                        \item \textbf{Components}:
                            \begin{itemize}
                                \item RDDs: Immutable data structures for distributed data management.
                                \item DataFrames and Datasets: Higher abstractions to simplify data manipulation.
                            \end{itemize}
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Optimization Techniques and Reliability}
    \begin{itemize}
        \item \textbf{Performance Optimization Techniques}
            \begin{itemize}
                \item \textbf{Data Partitioning}:
                    \begin{itemize}
                        \item Hadoop: Data is split into blocks (default 128MB) for parallel processing.
                        \item Spark: Custom partitioning optimizes resource utilization for skewed data.
                    \end{itemize}
                \item \textbf{Caching and Persistence}:
                    \begin{itemize}
                        \item Spark allows caching RDDs in memory for faster workflows.
                    \end{itemize}
                \item \textbf{Efficient Data Serialization}:
                    \begin{itemize}
                        \item Choosing data formats (e.g., Avro, Parquet) for reduced I/O operations.
                    \end{itemize}
            \end{itemize}

        \item \textbf{Reliability Considerations}
            \begin{itemize}
                \item \textbf{Fault Tolerance}:
                    \begin{itemize}
                        \item Hadoop: Automatic recovery via data replication in HDFS.
                        \item Spark: RDD lineage allows rerunning only failed transformations.
                    \end{itemize}
                \item \textbf{Data Consistency}: Ensuring data remains consistent across distributed systems.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case: E-Commerce Analytics}
    \begin{itemize}
        \item \textbf{Scenario}: E-commerce platform processes large user data logs to analyze purchase behavior.
        \item \textbf{Implementation}:
            \begin{itemize}
                \item \textbf{Hadoop}: Used for batch processing analytics for customer patterns on a daily basis.
                \item \textbf{Spark}: Used for real-time analytics for inventory management insights.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Points}
    \begin{itemize}
        \item Effective scalable architectures using Hadoop and Spark are vital for handling big data efficiently.
        \item Key aspects include performance optimization strategies, fault tolerance, and data consistency.
        \item Mastering these frameworks helps professionals derive timely insights from vast datasets.
    \end{itemize}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Understand the difference between vertical and horizontal scaling.
            \item Leverage Hadoop for batch processing and Spark for real-time analytics.
            \item Implement optimization techniques to enhance data processing efficiency.
            \item Ensure reliability through fault tolerance and consistent data practices.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example Code Snippet (Spark)}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("EcommerceDataProcessing") \
    .getOrCreate()

# Load data
df = spark.read.csv("ecommerce_data.csv", header=True, inferSchema=True)

# Data transformation
df = df.groupBy("userId").agg({"purchaseAmount": "sum"}) \
       .orderBy("userId")

# Cache DataFrame for performance
df.cache()

# Show results
df.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Governance and Ethical Considerations - Understanding Data Governance}
    \begin{block}{Definition}
        Data governance refers to the overall management of the availability, usability, integrity, and security of the data employed in an organization. 
    \end{block}
    
    \begin{itemize}
        \item Key components:
        \begin{enumerate}
            \item Data Quality: Accuracy, consistency, and reliability of data.
            \item Data Management Policies: Directives for data handling.
            \item Compliance and Control: Adherence to legal standards (e.g., GDPR, HIPAA).
            \item Stewardship: Assigning data management roles and responsibilities.
        \end{enumerate}
    \end{itemize}
    
    \begin{block}{Example}
        In a healthcare organization, a data governance framework ensures patient information is securely stored and accessible only by authorized personnel.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Governance and Ethical Considerations - Ethical Considerations in Data Processing}
    \begin{block}{Definition}
        Ethics in data processing revolves around the moral principles that govern how data is handled, ensuring respect for privacy and responsible usage.
    \end{block}

    \begin{itemize}
        \item Key ethical principles:
        \begin{enumerate}
            \item Informed Consent: Individuals should agree to data usage.
            \item Privacy: Safeguarding personal information.
            \item Transparency: Clear communication of data practices.
            \item Fairness: Avoiding bias in data processing.
        \end{enumerate}
    \end{itemize}

    \begin{block}{Example}
        A social media platform using algorithms must ensure they do not perpetuate misinformation or bias against certain groups.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Governance and Ethical Considerations - Implementing Frameworks}
    \begin{itemize}
        \item Steps to implement data governance and ethics:
        \begin{enumerate}
            \item Establish Governance Policies: Define clear directives for data handling.
            \item Engage Stakeholders: Involve all relevant parties in policy discussions.
            \item Continuous Training: Educate employees on governance and ethical standards.
        \end{enumerate}
        \item Example Frameworks:
        \begin{itemize}
            \item COBIT: Comprehensive framework for managing data.
            \item Fair Information Practice Principles (FIPPs): Guides ethical data practices.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Data governance ensures quality and compliance.
            \item Ethical considerations protect individuals' rights.
            \item Both frameworks are essential for responsible data processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications of Hadoop and Spark - Introduction}
    \begin{block}{Introduction to Data Processing Frameworks}
        Data processing frameworks such as Hadoop and Apache Spark are pivotal in managing and analyzing large datasets across various industries. They enable businesses to gain insights and make data-driven decisions through efficient data processing and analytics capabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop in Action: Case Studies}
    \begin{enumerate}
        \item \textbf{Retail Industry - Walmart}
            \begin{itemize}
                \item \textbf{Challenge:} Handling massive amounts of customer and sales data across numerous stores.
                \item \textbf{Application:} Utilized Hadoop for data storage and batch processing to analyze trends, manage inventory, and predict customer behaviors.
                \item \textbf{Benefits:}
                    \begin{itemize}
                        \item Improved inventory management.
                        \item Enhanced personalized marketing strategies.
                        \item Optimized supply chain logistics.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Healthcare Sector - CERN}
            \begin{itemize}
                \item \textbf{Challenge:} Managing the vast datasets produced during particle physics experiments.
                \item \textbf{Application:} Leveraged Hadoop's distributed processing to analyze terabytes of data from experiments.
                \item \textbf{Benefits:}
                    \begin{itemize}
                        \item Accelerated data processing capabilities.
                        \item Enhanced research outcomes through rapid data analysis and sharing.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Spark in Action: Case Studies}
    \begin{enumerate}
        \item \textbf{Financial Services - UBS}
            \begin{itemize}
                \item \textbf{Challenge:} Real-time fraud detection and analysis of transactional data.
                \item \textbf{Application:} Used Apache Spark’s in-memory processing capabilities to analyze data streams in real-time.
                \item \textbf{Benefits:}
                    \begin{itemize}
                        \item Reduced reaction time to potential fraud.
                        \item Enhanced decision-making through rapid insights.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Social Media - LinkedIn}
            \begin{itemize}
                \item \textbf{Challenge:} Need for real-time analytics and personalized content recommendations.
                \item \textbf{Application:} Applied Spark for processing user behavior data to deliver targeted content and advertisements.
                \item \textbf{Benefits:}
                    \begin{itemize}
                        \item Improved user engagement and interaction.
                        \item Increased advertising revenue through targeted campaigns.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Hadoop excels in batch processing large datasets, while Spark is optimized for real-time data processing.
            \item Both frameworks are scalable, cost-effective, and powerful tools for big data applications.
            \item Real-world applications demonstrate versatile deployment across industries, improving operational efficiencies and data insights.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding the practical applications of Hadoop and Spark not only highlights the importance of these technologies but also inspires critical thinking about how data processing frameworks can transform industries and shape the future of data analytics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading}
    \begin{itemize}
        \item Explore more case studies of companies successfully implementing Hadoop and Spark.
        \item Look into academic articles that analyze the impacts of these frameworks on business intelligence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps - Key Takeaways from Week 2}
    \begin{enumerate}
        \item \textbf{Introduction to Data Processing Frameworks}
        \begin{itemize}
            \item Critical for managing and analyzing large datasets efficiently.
            \item Key frameworks: \textbf{Hadoop} and \textbf{Apache Spark}.
        \end{itemize}
        
        \item \textbf{Hadoop Overview}
        \begin{itemize}
            \item Distributed storage and processing framework.
            \item Utilizes:
            \begin{itemize}
                \item HDFS for distributed large data storage.
                \item MapReduce for parallel data processing.
            \end{itemize}
            \item \textbf{Example:} Retail company analyzing transaction logs.
        \end{itemize}
        
        \item \textbf{Apache Spark Overview}
        \begin{itemize}
            \item Enhances Hadoop capabilities with speed and ease of use.
            \item Key Features:
            \begin{itemize}
                \item In-memory processing for faster data manipulation.
                \item Supports multiple languages: Python, Java, Scala.
            \end{itemize}
            \item \textbf{Example:} Financial institution utilizing Spark for real-time risk assessment.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps - Applications and Comparisons}
    \begin{enumerate}[resume]
        \item \textbf{Real-world Applications}
        \begin{itemize}
            \item Organizations optimize operations via data processing frameworks:
            \begin{itemize}
                \item Data analytics for improved decision-making.
                \item Personalized marketing to enhance customer engagement.
            \end{itemize}
        \end{itemize}

        \item \textbf{Performance Comparison: Hadoop vs Spark}
        \begin{itemize}
            \item \textbf{Processing Speed:} Spark outperforms Hadoop due to in-memory processing.
            \item \textbf{Ease of Use:} Spark's API is more user-friendly for rapid development.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps - Upcoming Topics}
    \begin{enumerate}
        \item \textbf{Advanced Data Processing Techniques}
        \begin{itemize}
            \item Explore frameworks like Apache Flink and Apache Beam.
        \end{itemize}

        \item \textbf{Hands-on Workshops}
        \begin{itemize}
            \item Collaborative sessions for practicing with Hadoop and Spark.
        \end{itemize}

        \item \textbf{Data Pipeline Architectures}
        \begin{itemize}
            \item Designing data pipelines for seamless data flow.
        \end{itemize}

        \item \textbf{Performance Tuning and Optimization}
        \begin{itemize}
            \item Techniques for efficiency, including resource management.
        \end{itemize}

        \item \textbf{Integration with Machine Learning}
        \begin{itemize}
            \item Using frameworks for ML applications with libraries like MLlib.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
    Understanding data processing frameworks such as Hadoop and Spark is essential for modern data analysis. Mastering these tools will empower you to effectively manage large-scale data challenges. Look forward to furthering your knowledge in the upcoming sessions!
    \end{block}
\end{frame}


\end{document}