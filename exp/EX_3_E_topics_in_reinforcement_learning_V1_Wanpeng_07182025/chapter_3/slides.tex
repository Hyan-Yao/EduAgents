\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Dynamic Programming]{Week 3: Dynamic Programming}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Dynamic Programming - Overview}
    \begin{block}{Overview of Dynamic Programming in Reinforcement Learning}
        Dynamic Programming (DP) is a powerful computational method commonly used in reinforcement learning (RL) for solving problems with various states and actions. It involves decision-making in complex environments by breaking larger problems into simpler, manageable subproblems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Dynamic Programming - Key Concepts}
    \begin{itemize}
        \item \textbf{State:} A specific situation in which an agent can find itself. For example, in a chess game, each board configuration is a different state.
        
        \item \textbf{Action:} A choice made by the agent that may change its state. In chess, moving a piece is an action.
        
        \item \textbf{Reward:} Feedback received after taking an action in a given state, indicating the success of the action in terms of reaching a goal.
        
        \item \textbf{Policy ($\pi$):} A strategy employed by the agent defining the actions to take in various states. The goal of RL is to find an optimal policy that maximizes cumulative rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming - Significance and Applications}
    \begin{block}{Significance of Dynamic Programming}
        \begin{itemize}
            \item \textbf{Optimal Solutions:} DP ensures finding the optimal policy using Bellman equations, which relate the value of states and actions.
            
            \item \textbf{Efficiency:} DP algorithms efficiently solve problems with overlapping subproblems, saving resources and time.
            
            \item \textbf{Foundational Techniques:} Many RL algorithms, like Value Iteration and Policy Iteration, are based on DP principles, making it a cornerstone of modern RL.
        \end{itemize}
    \end{block}
    
    \begin{block}{Applications of Dynamic Programming}
        \begin{enumerate}
            \item \textbf{Game Playing:} Applied in AI for games like chess or Go to assess game states and determine optimal strategies.
            \item \textbf{Robotics:} Used for path planning, helping robots navigate complex environments.
            \item \textbf{Finance:} Assists in making optimal investment decisions considering future scenarios.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Formula}
    \begin{block}{Example: The Knapsack Problem}
        Consider an agent deciding which items to pack in a limited-capacity knapsack to maximize value:
        \begin{itemize}
            \item \textbf{States:} Current weight of items in the knapsack.
            \item \textbf{Actions:} Including or excluding each item.
            \item \textbf{Reward:} The value of the items packed.
        \end{itemize}
        The optimal approach involves using DP to evaluate the maximum value for each weight limit iteratively.
    \end{block}
    
    \begin{block}{Key Formula: Bellman Equation}
        The Bellman equation is central to understanding DP in RL:
        \begin{equation}
        V(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]
        \end{equation}
        Where:
        \begin{itemize}
            \item $V(s)$ is the value of state $s$.
            \item $a$ is the action taken.
            \item $R(s,a,s')$ is the reward received after transitioning from state $s$ to state $s'$.
            \item $P(s'|s,a)$ is the probability of reaching state $s'$ given state $s$ and action $a$.
            \item $\gamma$ is the discount factor, balancing immediate and future rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Fundamental Concepts in Dynamic Programming - Overview}
    \begin{block}{Overview of Dynamic Programming}
        Dynamic Programming (DP) is a powerful method for solving complex problems by breaking them down into simpler subproblems. It is widely used in various fields including reinforcement learning, operations research, and computer science.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Fundamental Concepts in Dynamic Programming - Key Concepts}
    \begin{itemize}
        \item \textbf{States} 
            \begin{itemize}
                \item \textbf{Definition}: A state represents a specific situation or configuration within a decision-making process.
                \item \textbf{Example}: In a chess game, each arrangement of pieces represents a distinct state.
            \end{itemize}
        
        \item \textbf{Actions} 
            \begin{itemize}
                \item \textbf{Definition}: An action is a decision made by an agent that can alter the current state.
                \item \textbf{Example}: In chess, legal moves like moving a knight or bishop are actions that change the game state.
            \end{itemize}
            
        \item \textbf{Rewards} 
            \begin{itemize}
                \item \textbf{Definition}: A reward is a numerical value received by an agent after taking an action in a state.
                \item \textbf{Example}: An agent in reinforcement learning receives points for winning but may incur a penalty for losing.
            \end{itemize}

        \item \textbf{Optimal Policies} 
            \begin{itemize}
                \item \textbf{Definition}: An optimal policy defines the best action to take in each state to maximize accumulated reward.
                \item \textbf{Importance}: Finding optimal policies is crucial for effective decision-making in dynamic programming.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Fundamental Concepts in Dynamic Programming - Illustrative Example}
    \begin{block}{Example: Grid World}
        Consider a simple grid world where an agent can move Up, Down, Left, or Right.
        \begin{itemize}
            \item \textbf{States}: Each cell in the grid is a state.
            \item \textbf{Actions}: The agent can take one of four defined actions from any cell.
            \item \textbf{Rewards}: A reward (e.g., +1) for reaching a goal cell and a penalty (e.g., -1) for hitting a wall.
            \item \textbf{Optimal Policy}: The policy that maximizes total expected reward involves navigating through states to reach the goal.
        \end{itemize}
    \end{block}

    \begin{equation}
        V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') \right)
    \end{equation}
    where \( V(s) \) is the value of state \( s \), \( R(s, a) \) is the immediate reward, \( \gamma \) is the discount factor, and \( P(s'|s, a) \) is the transition probability.
\end{frame}

\begin{frame}[fragile]{Fundamental Concepts in Dynamic Programming - Conclusion}
    \begin{block}{Conclusion}
        Understanding states, actions, rewards, and optimal policies lays the foundation for mastering dynamic programming. 
        These concepts are essential for developing efficient algorithms that solve complex decision-making problems across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Evaluation - Overview}
    \begin{block}{What is Policy Evaluation?}
        Policy evaluation is a systematic approach within Dynamic Programming that assesses the effectiveness of a given policy in decision-making within a specific environment. The main goal is to determine the performance of a policy by measuring expected returns or values from states when following that policy.
    \end{block}
    
    \begin{itemize}
        \item **Policy (\(\pi\))**: A strategy dictating actions in various states (deterministic or stochastic).
        \item **Value Function (\(V\))**: Represents the expected return from a state under a given policy, quantifying the desirability of a state.
    \end{itemize}
    
    \begin{block}{Objective}
        The primary objective is to compute the value function for all states under a specific policy, providing insight into its long-term performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Evaluation - Methods}
    \begin{block}{Methods for Policy Evaluation}
        \begin{enumerate}
            \item \textbf{Dynamic Programming Approach:}
            \begin{itemize}
                \item Uses the Bellman equation for iterative updates:
                \begin{equation}
                    V^\pi(s) = \sum_{s'} P(s'|s,\pi(s)) [R(s,a) + \gamma V^\pi(s')]
                \end{equation}
            \end{itemize}
            \item \textbf{Iterative Policy Evaluation:}
            \begin{itemize}
                \item Start with an arbitrary value function \(V_0\) and update it iteratively:
                \begin{equation}
                    V_{k+1}(s) = \sum_{s'} P(s'|s,\pi(s)) [R(s,\pi(s)) + \gamma V_k(s')]
                \end{equation}
            \end{itemize}
            \item \textbf{Monte Carlo Methods:}
            \begin{itemize}
                \item Estimates the value function by averaging returns from sampled episodes of the policy.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Evaluation - Example and Key Points}
    \begin{block}{Example}
        Consider a simple grid world where an agent can move in four directions. If the agent follows a policy \(\pi\) that always chooses "right", we can calculate the value of state \(s\) based on the expected rewards received.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Convergence:} It's critical to ensure that policy evaluation converges to the accurate value function.
        \item \textbf{Role in Policy Improvement:} Results from policy evaluation inform the next steps in improving policies based on their effectiveness.
    \end{itemize}
    
    This evaluation phase is foundational for subsequent steps in reinforcement learning, enhancing policies toward optimality and improving decision-making in dynamic environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Improvement - Overview}
    \begin{block}{What is Policy Improvement?}
        Policy improvement is a crucial step in reinforcement learning, where we refine a given policy based on evaluation results to maximize expected reward.
    \end{block}

    \begin{block}{Relationship to Policy Evaluation}
        - Evaluate the performance of the existing policy.
        - Calculate the value function to guide policy adjustments.
    \end{block}

    \begin{block}{Key Techniques for Policy Improvement}
        \begin{itemize}
            \item Greedy Policy Improvement
            \item Soft Policy Improvement
            \item Iterative Policy Improvement
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Improvement - Techniques}
    \begin{block}{Greedy Policy Improvement}
        Select actions that maximize the expected value from the current value function:
        \begin{equation}
        \pi_{new}(s) = \arg\max_{a} Q(s, a)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( \pi_{new}(s) \): new policy for state \( s \)
            \item \( Q(s, a) \): action-value function for state \( s \) and action \( a \)
        \end{itemize}
    \end{block}
    
    \begin{block}{Soft Policy Improvement}
        Allows exploration with some probability of choosing sub-optimal actions:
        \begin{equation}
        P(a | s) = \frac{e^{Q(s, a)/\tau}}{\sum_{a'} e^{Q(s, a')/\tau}}
        \end{equation}
        Where \( \tau \) controls randomness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Improvement - Iteration Process}
    \begin{enumerate}
        \item Start with an initial policy.
        \item Evaluate the policy to determine its value function.
        \item Improve the policy based on the value function.
        \item Repeat until convergence (no further improvement).
    \end{enumerate}

    \begin{block}{Example of Policy Improvement}
        \begin{itemize}
            \item \textbf{Initial Policy:} Move towards the goal in a grid-world.
            \item \textbf{Evaluation Result:} Calculate returns based on expected rewards.
            \item \textbf{Improvement:} Adjust actions based on higher expected returns.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration}
    \begin{block}{What is Value Iteration?}
        Value Iteration is an algorithm used in Reinforcement Learning and Markov Decision Processes (MDPs) to find the optimal policy. It iteratively updates the value function for each state until convergence, enabling the derivation of the best actions to take in each state.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Value Iteration}
    \begin{enumerate}
        \item \textbf{Initialization:}
            \begin{itemize}
                \item Start with an arbitrary value function \( V(s) \) for all states \( s \). A common choice is \( V(s) = 0 \) for all states.
            \end{itemize}
        
        \item \textbf{Value Update:}
            \begin{equation}
                V_{new}(s) = \max_{a} \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') \right)
            \end{equation}
            where:
            \begin{itemize}
                \item \( R(s, a) \): immediate reward for taking action \( a \) in state \( s \).
                \item \( P(s'|s, a) \): transition probability to state \( s' \) after taking action \( a \).
                \item \( \gamma \): discount factor (0 to 1).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Value Iteration (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Convergence Check:}
            \begin{equation}
                \max_{s} |V_{new}(s) - V(s)| < \epsilon
            \end{equation}
            \begin{itemize}
                \item If values converge, derive the policy.
            \end{itemize}

        \item \textbf{Policy Derivation:}
            \begin{equation}
                \pi(s) = \arg\max_{a} \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') \right)
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: A Simple Grid World}
    \begin{itemize}
        \item **Initial Setup**: Initialize values of all states to 0.
        
        \item **Iterate**: 
            \begin{itemize}
                \item Calculate new values based on possible actions until convergence. 
                \item Observe improving values for positions closer to the goal.
            \end{itemize}
        
        \item **Derive Policy**: 
            \begin{itemize}
                \item Determine which action to take based on the maximum value once values converge.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Efficiency}: Value iteration guarantees finding the optimal policy under the assumption of a complete and well-defined MDP.
        
        \item \textbf{Discount Factor}: The choice of \( \gamma \) affects future reward valuation. 
            \begin{itemize}
                \item \( \gamma \) close to 0 = shortsighted.
                \item \( \gamma \) close to 1 = considers long-term rewards more heavily.
            \end{itemize}
        
        \item \textbf{Convergence}: The algorithm converges to an optimal value function, with speed dependent on problem structure and parameters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    Value iteration is a foundational algorithm in dynamic programming and plays a vital role in solving complex decision-making problems in reinforcement learning. Understanding its mechanics helps in building models that can effectively learn from environments.
\end{frame}

\begin{frame}
    \frametitle{Example of Dynamic Programming}
    \begin{block}{Understanding Dynamic Programming}
        Dynamic Programming (DP) is a technique for solving complex problems by breaking them down into simpler subproblems and storing results to avoid redundant computations. It is effective in optimization problems and decision-making processes, particularly in Reinforcement Learning (RL).
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Real-World Example: Optimizing a Delivery Route}
    \begin{block}{Scenario}
        A delivery service aims to minimize the time for delivering packages to various cities using different routes.
    \end{block}

    \begin{block}{Application of Dynamic Programming}
        \begin{enumerate}
            \item Define the Problem:
                \begin{itemize}
                    \item Let $T(i, j)$ be the travel time between cities $i$ and $j$.
                    \item Define $D(i)$ as the minimum delivery time starting from city $i$.
                \end{itemize}
            \item Recurrence Relation:
                \[
                D(i) = \min_{j \in \text{cities}} (T(i, j) + D(j))
                \]
            \item Base Case:
                \[
                D(i) = 0 \text{ for all terminal cities}
                \]
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming Example in Action}
    \begin{block}{Cities and Travel Times}
        \begin{itemize}
            \item Cities: A, B, C
            \item Travel Times: 
                \begin{itemize}
                    \item $T(A, B) = 2$ hours
                    \item $T(A, C) = 5$ hours
                    \item $T(B, C) = 1$ hour
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{DP Table}
        \begin{tabular}{|c|c|c|c|}
            \hline
            Current City & Remaining Deliveries & Delivery Time & Optimal Delivery Time \\
            \hline
            A & B, C & $T(A, B) + D(B), T(A, C) + D(C)$ & 3 hours (A $\rightarrow$ B $\rightarrow$ C) \\
            \hline
            B & C & $T(B, C) + D(C)$ & 1 hour \\
            \hline
            C & - & 0 hours & 0 hours \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Optimal Substructure:} DP benefits from problems with overlapping subproblems.
            \item \textbf{Memoization:} Storing results of subproblems to enhance efficiency.
            \item \textbf{Time Complexity:} Reduces complexity from exponential to polynomial, enabling tractability.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        DP is a vital tool in RL and optimization, offering systematic approaches to solve complex problems efficiently, exemplified by the delivery route optimization scenario.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet (Python Representation)}
    \begin{lstlisting}[language=Python]
def min_delivery_time(current_city, delivery_map):
    if current_city is terminal:
        return 0
    if current_city in memo:
        return memo[current_city]
    
    optimal_time = float('inf')
    for city in delivery_map[current_city]:
        travel_time = delivery_time(current_city, city) + min_delivery_time(city, delivery_map)
        optimal_time = min(optimal_time, travel_time)
    
    memo[current_city] = optimal_time
    return optimal_time
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Overview}
    \begin{block}{Understanding Dynamic Programming Challenges}
        Dynamic Programming (DP) is effective for solving complex problems, but it poses several challenges and limitations. Exploring these obstacles is crucial for understanding when and how to apply DP effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Computational Complexity}
    \begin{itemize}
        \item \textbf{High Computational Complexity}
        \begin{itemize}
            \item \textbf{Problem Size}: Infeasible as input size grows. Complexity ranges from polynomial ($O(n^2)$) to exponential ($O(2^n)$).
            \item \textbf{Example}: Fibonacci sequence - DP solution $\mathcal{O}(n)$ vs. naive recursive $\mathcal{O}(2^n)$.
  
            \item \textbf{Memory Usage}: Significant memory is required for intermediate results; large states can be a problem.
            \item \textbf{Example}: 2D DP tables for problems like Knapsack or Longest Common Subsequence require considerable space.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Problem Suitability}
    \begin{itemize}
        \item \textbf{Overlapping Subproblems}
        \begin{itemize}
            \item \textbf{Identification}: Problems must exhibit overlapping subproblems to benefit from DP.
            \item \textbf{Example}: Shortest path or Fibonacci numbers are classic examples due to recurring subproblems.
        \end{itemize}
        
        \item \textbf{Optimal Substructure Condition}
        \begin{itemize}
            \item \textbf{Checking Conditions}: Problems need an optimal substructure; optimal solutions must be derived from subproblems.
            \item \textbf{Consider this}: The Traveling Salesman Problem lacks optimal substructure; local solutions may not lead to global optimum.
        \end{itemize}

        \item \textbf{Implementation Complexity}
        \begin{itemize}
            \item \textbf{Algorithm Design}: Designing a DP solution involves careful transitions; poor definitions can lead to inefficiencies.
            \item \textbf{Example}: Misdefining states in a 0/1 knapsack problem may introduce unwanted complexity.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Key Points}
    \begin{itemize}
        \item Analyze problem constraints to check if DP is suitable.
        \item Monitor memory usage; optimize to reduce space complexity.
        \item Understand the problem's structure for effective use (optimal substructure and overlapping subproblems).
        
        \item \textbf{Conclusion:} 
        DP is valuable but requires careful consideration for specific problems. Awareness of these challenges aids in selecting and implementing effective DP solutions in practice.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relation to Other Methods - Overview}
    \begin{itemize}
        \item Dynamic Programming (DP) is fundamental in Reinforcement Learning (RL) for sequential decision-making.
        \item Comparison with other methods: Monte Carlo (MC) and Temporal Difference (TD) Learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming (DP)}
    \begin{itemize}
        \item \textbf{Definition}: Algorithms that decompose problems into simpler subproblems, solving each once and storing the results.
        \item \textbf{Characteristics}:
        \begin{itemize}
            \item Requires a model of the environment (transition probabilities and rewards).
            \item Best for smaller state spaces due to computational complexity.
        \end{itemize}
        \item \textbf{Use Cases}: Includes policy iteration and value iteration for evaluating and improving policies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo (MC) Methods}
    \begin{itemize}
        \item \textbf{Definition}: Learns directly from episodes of experience without needing a model.
        \item \textbf{Characteristics}:
        \begin{itemize}
            \item Learns from complete episodes; updates at the end.
            \item Suitable for large state spaces.
        \end{itemize}
        \item \textbf{Strengths}:
        \begin{itemize}
            \item Avoids convergence issues associated with DP.
            \item Easier implementation for certain tasks.
        \end{itemize}
        \item \textbf{Weaknesses}:
        \begin{itemize}
            \item Requires many episodes for accuracy, leading to high variance.
        \end{itemize}
        \item \textbf{Example}: In a game scenario, MC agents refine strategies through multiple game plays.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Temporal Difference (TD) Learning}
    \begin{itemize}
        \item \textbf{Definition}: Combines DP and MC, learning directly from episodes while using existing value estimates.
        \item \textbf{Characteristics}:
        \begin{itemize}
            \item Updates occur at each time step based on current estimates.
            \item Does not require full episodes to adjust value functions.
        \end{itemize}
        \item \textbf{Strengths}:
        \begin{itemize}
            \item More efficient than MC, needing fewer episodes for effective learning.
            \item Handles continuous state spaces well.
        \end{itemize}
        \item \textbf{Weaknesses}:
        \begin{itemize}
            \item May produce biased estimates due to reliance on other estimates (bootstrapping).
        \end{itemize}
        \item \textbf{Example}: In a robot navigation task, a TD agent continuously updates its estimates as it moves.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Comparisons}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|l|l|l|}
            \hline
            \textbf{Aspect} & \textbf{Dynamic Programming} & \textbf{Monte Carlo} & \textbf{Temporal Difference} \\ \hline
            Model Requirement & Requires model & No model needed & No model needed \\ \hline
            Learning Method & Full episodes/steps & Full episodes & Incremental updates \\ \hline
            Convergence Speed & Fast with model & Slow, many episodes & Fast, fewer episodes \\ \hline
            Variance in Estimates & Low (deterministic) & High (stochastic) & Moderate (bootstrapping) \\ \hline
            Suitability & Small/medium spaces & Large spaces & Large action/state spaces \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item DP, MC, and TD serve unique roles in reinforcement learning.
        \item Choice depends on problem structure, available information, and computational resources.
        \item Consider computational efficiency, convergence properties, and model availability when selecting a learning strategy!
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Conclusion - Key Concepts of Dynamic Programming (DP)}
  
  \begin{itemize}
      \item Dynamic Programming is used to solve complex problems by simplifying them into subproblems.
      \item In reinforcement learning, DP aids in:
      \begin{itemize}
          \item Estimating values
          \item Making optimal decisions
      \end{itemize}
      \item **Principle of Optimality**: An optimal policy results in subsequent decisions that also form an optimal policy.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Conclusion - DP Techniques}
  
  \begin{itemize}
      \item **Policy Evaluation**: Computes value function for a given policy.
      \item **Policy Improvement**: Updates policy to increase expected return.
      \item **Algorithms**:
      \begin{itemize}
          \item **Value Iteration**: Updates value function until convergence.
          \item **Policy Iteration**: Alternates between evaluation and improvement.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Conclusion - Implications and Takeaways}
  
  \begin{itemize}
      \item **Implications for RL**:
      \begin{itemize}
          \item DP is fundamental for advanced methods like Monte Carlo and Temporal Difference learning.
          \item Provides precise value estimates but requires a complete environment model.
          \item Understanding DP aids in handling larger state spaces through function approximation.
      \end{itemize}
      \item **Key Takeaways**:
      \begin{itemize}
          \item Essential for optimal decision-making in RL.
          \item Utilizes the Bellman equation for evaluating and improving policies.
          \item Practical applications are limited by the need for comprehensive environment knowledge.
      \end{itemize}
  \end{itemize}
\end{frame}


\end{document}