\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}

% Title Page Information
\title[Reinforcement Learning]{Week 1: Introduction to Reinforcement Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Reinforcement Learning}
    \begin{block}{What is Reinforcement Learning?}
        Reinforcement Learning (RL) is a fundamental area of machine learning that focuses on how agents ought to take actions in an environment to maximize cumulative reward. In contrast to supervised learning, RL is driven by an agent's interactions with its environment, learning from the consequences of actions rather than pre-existing datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Agent}: The learner or decision-maker that interacts with the environment.
        \item \textbf{Environment}: Everything the agent interacts with, providing feedback in the form of rewards or penalties.
        \item \textbf{Actions}: The set of all possible moves the agent can make to influence the environment.
        \item \textbf{States}: Different situations the agent can find itself in.
        \item \textbf{Rewards}: Feedback signals indicating the desirability of an action, often as numeric values.
        \item \textbf{Policy}: A strategy that determines actions based on the current state.
        \item \textbf{Value Function}: A prediction of future rewards to evaluate the goodness of the current policy.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Reinforcement Learning Works}
    \begin{itemize}
        \item \textbf{Exploration vs. Exploitation}: The agent must balance exploring new actions for better rewards versus utilizing known actions that yield high rewards.
        
        \item \textbf{Learning Feedback Loop}: The agent takes an action, receives a reward, and updates its knowledge or policy based on that experience.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Game Playing}: Successful applications in Chess, Go, and video games, where algorithms learn by receiving rewards for winning or penalties for losing.
        \item \textbf{Robotics}: Robots learn tasks such as walking or manipulating objects by repeatedly trying actions and receiving feedback.
        \item \textbf{Autonomous Vehicles}: Self-driving cars use RL to navigate complex environments while adhering to traffic rules.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance and Relevance}
    \begin{itemize}
        \item \textbf{Adaptability}: RL adapts to changing environments and learns complex behaviors over time, suitable for real-world applications.
        \item \textbf{Efficiency}: Learning optimal policies through trial and error leads to effective solutions in decision-making across various industries.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Reinforcement Learning represents a shift towards more autonomous systems capable of learning from experience. Its significance is growing as challenges become more complex and require intelligent decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Point to Emphasize}
    Understanding the foundational concepts of Reinforcement Learning is critical as they set the stage for exploring algorithms, techniques, and real-world applications in subsequent lessons.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation}
    In RL, the expected future reward can often be expressed using the Bellman equation:
    \begin{equation}
        V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a)V(s') \right)
    \end{equation}
    Where:
    \begin{itemize}
        \item \( V(s) \): value of state \( s \)
        \item \( R(s, a) \): immediate reward for action \( a \)
        \item \( \gamma \): discount factor (0 < \( \gamma \) < 1) determining the importance of future rewards
        \item \( P(s'|s, a) \): probability of transitioning to state \( s' \) from state \( s \) after taking action \( a \)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Historical Context}
    \begin{block}{Overview}
        Reinforcement Learning (RL) has evolved over several decades, growing from theoretical foundations to applications in various fields, including robotics, gaming, and business. Understanding its historical context helps to grasp the significance and current capabilities of RL.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Milestones in Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{1950s - Early Foundations}
            \begin{itemize}
                \item Neuroscience Inspiration
                \item Markov Decision Processes (MDP) by Bellman (1957)
            \end{itemize}
            
        \item \textbf{1970s - Theoretical Development}
            \begin{itemize}
                \item Dynamic Programming by Richard Bellman
                \item Q-Learning (Watkins, 1989)
            \end{itemize}
            
        \item \textbf{1980s - Practical Applications}
            \begin{itemize}
                \item Adaptive Control in engineering
            \end{itemize}
            
        \item \textbf{1990s - Growth in Popularity}
            \begin{itemize}
                \item Neural Networks & RL; advancements in gaming (Tesauro)
            \end{itemize}
            
        \item \textbf{2000s - Consolidation of Ideas}
            \begin{itemize}
                \item Policy Gradient Methods
                \item Multi-Agent Systems
            \end{itemize}
            
        \item \textbf{2010s - Deep Reinforcement Learning}
            \begin{itemize}
                \item Deep Q-Networks (DQN) by Google DeepMind
                \item Real World Applications in various industries
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Interdisciplinary Origins: RL spans psychology, neuroscience, computer science, and mathematics.
            \item Evolution of Algorithms: Transition from simple trial-and-error to sophisticated deep learning-based methods.
            \item Importance of Feedback: Central to RL, emphasized through experiences, rewards, and punishments.
        \end{itemize}
    \end{block}

    \begin{block}{Illustrative Example}
        \textbf{Atari Game Playing with DQN:} 
        An agent using RL trains to play video games, learning strategies through positive (rewards) and negative (loss of lives) feedback.
    \end{block}

    \begin{block}{Conclusion}
        The evolution of reinforcement learning highlights its foundational concepts, key milestones, and transformative impact on artificial intelligence. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminology - Part 1}
    \frametitle{Understanding the Foundations of Reinforcement Learning}
    
    \begin{block}{Key Terms Explained}
        \begin{enumerate}
            \item \textbf{Agent}: 
                \begin{itemize}
                    \item \textbf{Definition}: The "learner" or "decision maker" in a reinforcement learning system.
                    \item \textbf{Example}: A robot navigating a maze or a software program playing chess.
                    \item \textbf{Key Point}: The agent interacts with the environment to achieve a goal.
                \end{itemize}
            \item \textbf{Environment}:
                \begin{itemize}
                    \item \textbf{Definition}: Everything the agent interacts with and where it operates to make decisions.
                    \item \textbf{Example}: The maze for the robot, or the chessboard for the chess program.
                    \item \textbf{Key Point}: The environment provides the context within which the agent functions.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminology - Part 2}
    
    \begin{block}{Key Terms Explained (Continued)}
        \begin{enumerate}
            \setcounter{enumi}{2} % Start numbering from 3
            \item \textbf{State}:
                \begin{itemize}
                    \item \textbf{Definition}: A representation of the current situation or configuration of the environment at a specific time.
                    \item \textbf{Example}: The current position of the robot in the maze or the current configuration of pieces on a chessboard.
                    \item \textbf{Key Point}: States can be discrete (specific positions) or continuous (range of values).
                \end{itemize}
            \item \textbf{Action}:
                \begin{itemize}
                    \item \textbf{Definition}: The choices the agent can make to affect the state of the environment.
                    \item \textbf{Example}: Moving left, right, forward, or backward for the robot; moving a pawn or knight in chess.
                    \item \textbf{Key Point}: Actions are how agents influence their environment to reach their goals.
                \end{itemize}
            \item \textbf{Reward}:
                \begin{itemize}
                    \item \textbf{Definition}: A scalar feedback signal received by the agent after taking an action in a particular state, indicating the success of that action.
                    \item \textbf{Example}: Eating food (positive reward) in a food-hunting scenario, or receiving a penalty for hitting a wall (negative reward).
                    \item \textbf{Key Point}: Rewards drive the learning process, guiding agents toward more successful strategies.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminology - Part 3}
    
    \begin{block}{Visualization Example}
        \begin{center}
            \includegraphics[width=0.6\textwidth]{visualization_example.png}
        \end{center}
        \begin{itemize}
            \item Agent interacts with the environment through actions, receives rewards, and transitions between states.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        Understanding these basic terms is critical for grasping how reinforcement learning systems operate. 
        Each term interconnects to describe the dynamic process of learning through interaction with the environment, based on feedback.
    \end{block}

    \begin{block}{Common Relationship}
        \begin{equation}
            \text{Cumulative Reward (R)} = R_t + R_{t+1} + R_{t+2} + \ldots
        \end{equation}
    \end{block}

    \begin{block}{Engagement}
        \begin{itemize}
            \item Ask students to think of personal examples where they act as an agent in a decision-making environment.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Principles - Overview}
    \begin{block}{Core Concepts}
        Reinforcement Learning (RL) involves an Agent learning how to make decisions by interacting with an Environment. 
        The agent receives feedback in the form of Rewards based on its actions, aiming to maximize cumulative rewards over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Principles - Exploration vs. Exploitation}
    \begin{itemize}
        \item \textbf{Exploration}: Trying new actions to discover their effects and best rewards.\\
              \textit{Example:} A robot exploring different paths in a maze.
              
        \item \textbf{Exploitation}: Choosing known actions that yield the highest rewards based on past experiences.\\
              \textit{Example:} A player consistently using a successful game strategy.
    \end{itemize}
    \begin{block}{Key Point}
        Balancing exploration and exploitation is essential. Too much exploration can waste time, while too much exploitation can hinder discovering better alternatives.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Principles - The Reward Signal}
    The Reward Signal is crucial for guiding the agent's learning:
    \begin{itemize}
        \item \textbf{Positive Rewards}: Indicate successful actions towards the goal (e.g., scoring points in a game).
        \item \textbf{Negative Rewards}: Indicate unfavorable actions, discouraging repetition (e.g., losing points).
    \end{itemize}
    \begin{equation}
        R(s, a) = \text{reward received after taking action } a \text{ in state } s
    \end{equation}
    \begin{block}{Illustration}
        Imagine a child at a playground. 
        If they try the slide (exploration), they experience thrills but risk missing out if they only go to the swings (exploitation).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Principles - Summary}
    \begin{itemize}
        \item RL involves agents learning from environment interactions based on rewards.
        \item Balancing exploration of unknown actions with exploitation of known actions is fundamental.
        \item The reward signal feedback is critical for effective learning.
    \end{itemize}
    \begin{block}{Key Takeaway}
        Mastering the dynamics of exploration vs. exploitation and the impact of the reward signal is essential in reinforcement learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Reinforcement Learning - Overview}
    Reinforcement Learning (RL) is a powerful machine learning paradigm that allows agents to learn optimal behaviors through interactions with their environment. Some key industries where RL applications have made significant strides include:
    \begin{itemize}
        \item Gaming
        \item Robotics
        \item Healthcare
        \item Finance
        \item Transportation
        \item Energy
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Reinforcement Learning - Detailed Examples}
    \begin{block}{Gaming}
        \textbf{Description:} RL has transformed the gaming industry, enabling AI agents to learn complex strategies and enhance user experiences.\\
        \textbf{Example:} AlphaGo, by DeepMind, masters Go and defeats world champions. RL adapts to changing game dynamics.
    \end{block}
    
    \begin{block}{Robotics}
        \textbf{Description:} Robots use RL for trial-and-error learning to improve performance.\\
        \textbf{Example:} Boston Dynamics' Spot robot learns to navigate complex environments autonomously.
    \end{block}

    \begin{block}{Healthcare}
        \textbf{Description:} RL assists in personalized treatment planning and optimizing medications.\\
        \textbf{Example:} RL algorithms optimize therapies for chronic diseases, improving patient outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Reinforcement Learning - Summary and Key Takeaways}
    Reinforcement Learning has diverse applications enhancing functionality, adaptability, and efficiency across industries. The key takeaways are:
    \begin{itemize}
        \item RL allows for adaptive learning and decision-making in complex scenarios.
        \item Applications range widely from gaming to healthcare, showcasing versatility.
        \item Continuous exploration and exploitation is vital for successful RL applications.
    \end{itemize}
    
    \textbf{Illustration Idea:} Consider adding a diagram showing an RL agent interacting with its environment in a loop (Observations $\rightarrow$ Actions $\rightarrow$ Rewards $\rightarrow$ Update Policy).

    \begin{lstlisting}[language=Python, caption=Pseudocode for RL agent updates]
# Pseudocode for RL agent updates
for episode in range(num_episodes):
    state = env.reset()
    done = False
    while not done:
        action = policy(state)
        next_state, reward, done = env.step(action)
        update_q_value(state, action, reward, next_state)
        state = next_state
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Reinforcement Learning - Overview}
    \begin{itemize}
        \item Reinforcement Learning (RL) involves agent-environment interaction.
        \item It has significant applications but also faces key challenges impacting its effectiveness:
        \begin{itemize}
            \item Sample inefficiency
            \item Scalability
            \item Balancing exploration and exploitation
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Reinforcement Learning - Sample Inefficiency}
    \begin{block}{Sample Inefficiency}
        \begin{itemize}
            \item **Definition**: RL often necessitates numerous interactions to learn effective policies.
            \item **Implication**: In real-world scenarios, data acquisition is costly/time-consuming; agents may require thousands/millions of trials.
            \item **Example**: A robotic controller needing repeated actions to navigate a space efficiently.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Reinforcement Learning - Scalability and Exploration}
    \begin{block}{Scalability}
        \begin{itemize}
            \item **Definition**: Complexity of state and action spaces can impede algorithm scalability.
            \item **Implication**: More states/actions lead to exponential growth in computation/time required for training.
            \item **Example**: Open-world games present vast environments with numerous states, complicating agent training.
        \end{itemize}
    \end{block}
    
    \begin{block}{Exploration vs. Exploitation}
        \begin{itemize}
            \item **Definition**: Agents must explore new actions (exploration) while also leveraging known high-reward actions (exploitation).
            \item **Implication**: Balance is crucial; excessive exploration wastes resources, while excessive exploitation risks suboptimal policies.
            \item **Example**: In a multi-armed bandit scenario, an agent must navigate between trying different machines (exploration) and sticking to the best-known machine (exploitation).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Ethical Considerations - Overview}
    \begin{block}{Understanding Ethical Implications in Reinforcement Learning}
        Ethical considerations refer to the moral principles guiding the development and application of technologies. In reinforcement learning (RL), this encompasses issues related to fairness, transparency, and societal impact.
    \end{block}

    \begin{block}{Why Ethical Considerations Matter}
        As RL models are increasingly integrated into real-world applications, they influence decisions in areas such as healthcare, finance, and autonomous systems. Ethical implications can either enhance or detract from the trustworthiness and reliability of these applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Ethical Considerations - Key Concerns}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}:
        \begin{itemize}
            \item RL algorithms may perpetuate existing biases in training data.
            \item Example: A hiring RL model may favor candidates from a certain demographic due to biased historical data.
        \end{itemize}
        
        \item \textbf{Transparency and Explainability}:
        \begin{itemize}
            \item Many RL algorithms operate as "black boxes".
            \item Example: In healthcare, opaque RL recommendations may create distrust among doctors and patients.
        \end{itemize}
        
        \item \textbf{Safety and Unintended Consequences}:
        \begin{itemize}
            \item Trial-and-error learning can lead to harmful actions if not constrained.
            \item Example: An RL model for autonomous vehicles might take unsafe shortcuts.
        \end{itemize}
        
        \item \textbf{Societal Impact}:
        \begin{itemize}
            \item Deployment of RL systems can exacerbate social inequalities.
            \item Example: Automated decision-making in social services might lead to discrimination.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Ethical Considerations - Call to Action}
    \begin{block}{Recommendations}
        \begin{itemize}
            \item \textbf{Integrate Ethical Thinking}: Encourage discussions about ethical implications during development.
            \item \textbf{Diversity in Teams}: Promote diverse teams to address potential biases.
            \item \textbf{Stakeholder Engagement}: Involve affected communities to understand different perspectives and needs.
        \end{itemize}
    \end{block}

    \begin{block}{Final Thoughts}
        Reinforcement learning holds transformative potential; however, navigating its ethical landscape responsibly is crucial. Establishing guidelines and frameworks for ethical practice will ensure the beneficial use of RL technology.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Overview}
    This course is designed to build a robust foundational knowledge of Reinforcement Learning (RL), a subset of machine learning where agents learn to make decisions by taking actions in an environment to maximize cumulative rewards. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Objectives - Part 1}
    \begin{enumerate}
        \item \textbf{Understand Reinforcement Learning Fundamentals}
        \begin{itemize}
            \item Define key concepts: agent, environment, states, actions, and rewards.
            \item Grasp the difference between supervised, unsupervised, and reinforcement learning.
        \end{itemize}

        \item \textbf{Explore Key Components}
        \begin{itemize}
            \item \textbf{Agent and Environment:} Learn how these interact.
            \begin{itemize}
                \item \textit{Example:} An agent (self-driving car) navigates through an environment (road).
            \end{itemize}
            \item \textbf{State and Action Spaces:} Identify possible states and actions in various scenarios.
            \begin{itemize}
                \item \textit{Example:} On a chessboard, each position is a state; moving to another position is an action.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Objectives - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering
        \item \textbf{Learn About Rewards and Policies}
        \begin{itemize}
            \item Understand the concept of reward signals and how they influence agent behavior.
            \item Develop an understanding of policies, which direct the agent's actions.
            \item \textbf{Key Formula:} The objective of the agent is to maximize cumulative reward, represented as:
            \begin{equation}
                R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
            \end{equation}
            where \( r_t \) is the immediate reward and \( \gamma \) (0 < \( \gamma \) < 1) is the discount factor.
        \end{itemize}

        \item \textbf{Study Value Functions and Q-Learning}
        \begin{itemize}
            \item Comprehend how value functions estimate future rewards from a given state or state-action pair.
            \item Examine Q-learning as a model-free RL algorithm, aiming to learn quality of actions directly.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Objectives - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{5} % Continue numbering
        \item \textbf{Evaluate Applications of RL}
        \begin{itemize}
            \item Discuss real-world applications such as robotics, game playing (e.g., AlphaGo), and personalized recommendations.
            \item Explore success stories and the ethical implications of using RL in sensitive domains.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Reinforcement learning involves trial-and-error interactions with the environment.
            \item Ethical considerations play a crucial role in developing RL applications responsibly.
            \item Foundational concepts enable practical implementations and innovations in AI.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By the end of this course, students will have a well-rounded understanding of reinforcement learning principles, technical skills to apply them, and a critical perspective on the ethical ramifications of their use in society.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet for Q-Learning}
    \begin{lstlisting}[language=Python]
import numpy as np

# Initialize Q-table
Q = np.zeros((state_size, action_size))

# Update rule for Q-learning
def update_Q(state, action, reward, next_state, alpha, gamma):
    best_next_action = np.argmax(Q[next_state])    
    Q[state, action] += alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])
    \end{lstlisting}
\end{frame}


\end{document}