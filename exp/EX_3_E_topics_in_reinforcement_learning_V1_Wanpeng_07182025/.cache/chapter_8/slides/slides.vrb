\frametitle{Estimating Gradients - Temporal Difference Methods}
    \begin{block}{Concept}
        TD methods update value estimates based on other learned estimates without waiting for the final outcome of the episode, achieving more immediate updates.
    \end{block}

    \begin{itemize}
        \item It combines ideas from Monte Carlo and dynamic programming.
        \item Updates the value of actions taken using the difference between the estimated value of the current state and the estimated value after taking an action.
    \end{itemize}

    \begin{equation}
        V(s_t) \leftarrow V(s_t) + \alpha \delta_t
    \end{equation}
    Where:
    \begin{equation}
        \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
    \end{equation}
    Here:
    \begin{itemize}
        \item $r_t$ = reward received after taking action
        \item $\gamma$ = discount factor (0 < $\gamma$ < 1)
        \item $\alpha$ = learning rate (0 < $\alpha$ â‰¤ 1)
    \end{itemize}
