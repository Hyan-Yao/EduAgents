\frametitle{Key Points and Bellman Equation}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Decisions are based on the agent's current state.
            \item A complete MDP must define state relationships through actions and rewards.
            \item State representation affects learning efficiency and policy optimization.
        \end{itemize}
    \end{block}

    \begin{block}{Mathematical Formula}
        A common representation in MDPs is the Bellman equation:
        \begin{equation}
            V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( V(s) \): value function for state \( s \)
            \item \( R(s, a) \): expected reward after action \( a \) in state \( s \)
            \item \( P(s' | s, a) \): state transition probability
            \item \( \gamma \): discount factor (0 â‰¤ \( \gamma \) < 1)
        \end{itemize}
    \end{block}
