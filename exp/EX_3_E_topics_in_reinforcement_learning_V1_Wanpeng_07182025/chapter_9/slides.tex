\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Multi-Agent Reinforcement Learning]{Week 9: Multi-Agent Reinforcement Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Multi-Agent Reinforcement Learning (MARL)?}
    Multi-Agent Reinforcement Learning (MARL) extends the traditional reinforcement learning framework to include multiple intelligent agents that interact within a shared environment. Each agent aims to maximize its own cumulative reward while considering the actions and decisions of its peers.
    
    \begin{itemize}
        \item \textbf{Agent:} An entity that makes decisions based on its observations of the environment.
        \item \textbf{Environment:} The surrounding context that provides feedback to the agents based on their actions.
        \item \textbf{Actions:} The choices available to agents to influence their environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of MARL}
    \begin{enumerate}
        \item \textbf{Complex Problem Solving:} MARL addresses complex problems that are infeasible for single-agent systems, such as traffic management, resource allocation, and team sports.
        \item \textbf{Emergent Behaviors:} In multi-agent systems, agents can exhibit complex behaviors that emerge through interaction, contributing to advancements in artificial intelligence.
        \item \textbf{Cooperation vs. Competition:} MARL allows for exploration of both cooperative (working together for mutual benefit) and competitive (striving against each other) interactions among agents.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of MARL Applications}
    \begin{itemize}
        \item \textbf{Robotics:} Multiple robots working together to accomplish tasks like warehouse management or search and rescue missions.
        \item \textbf{Video Games:} Agents competing against each other or teaming up to achieve objectives, leading to realistic scenarios and strategy development.
    \end{itemize}
    
    \textbf{Core Frameworks:}
    \begin{itemize}
        \item \textbf{Decentralized Methods:} Each agent learns independently based on its observations while still being part of a joint environment.
        \item \textbf{Centralized Methods:} A single entity oversees the learning process; agents may share information to improve collective outcomes.
    \end{itemize}
    
    \textbf{Preview of Next Slide:} In the next section, we will delve into the challenges faced in MARL environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Interaction Dynamics:} The presence of other agents alters the state and reward structures, making strategic planning more complex.
        \item \textbf{Learning Algorithms:} Common approaches in MARL include Q-learning, policy gradient methods, and decentralized training.
        \item \textbf{Scalability:} MARL must effectively scale to handle a varying number of agents and diverse types of interactions.
    \end{itemize}
    
    \textbf{Diagram Suggestion:} Consider including a flowchart illustrating the interaction between agents and the environment, highlighting the feedback loop of actions, states, and rewards.
\end{frame}

\begin{frame}[fragile]{Challenges in Multi-Agent Environments - Overview}
    \begin{block}{Key Challenges in MARL}
        In Multi-Agent Reinforcement Learning (MARL), agents interact to optimize goals, leading to several challenges including coordination, competition, and communication.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Challenges in Coordination Among Agents}
    \begin{itemize}
        \item \textbf{Definition:} Coordination involves multiple agents working together toward common goals or individual objectives affected by others' actions.
        \item \textbf{Challenges:}
            \begin{itemize}
                \item \textbf{Non-Stationarity:} Agents' optimal policies change as others learn, complicating stable strategy maintenance.
                \item \textbf{Example:} In soccer, a player's unexpected strategy changes can disrupt team coordination.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Challenges in Competition and Communication}
    \begin{itemize}
        \item \textbf{Competition:}
            \begin{itemize}
                \item \textbf{Definition:} When agents act against each other to maximize individual rewards.
                \item \textbf{Challenges:}
                    \begin{itemize}
                        \item \textbf{Adversarial Behaviors:} Agents may develop strategies to outplay others, creating unpredictable dynamics.
                        \item \textbf{Example:} In poker, strategic deception and adaptation are critical due to unknown opponent actions.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Communication:}
            \begin{itemize}
                \item \textbf{Definition:} Agents sharing information to improve overall performance.
                \item \textbf{Challenges:}
                    \begin{itemize}
                        \item \textbf{Limited Information:} Incomplete or noisy observations complicate decision-making.
                        \item \textbf{Example:} In self-driving cars, limited communication can lead to misunderstandings that may cause accidents.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Interdependence:} Actions of one agent significantly influence others' performance.
        \item \textbf{Dynamic Environments:} Unlike single-agent settings, the presence of multiple learning agents contributes to a dynamic environment.
        \item \textbf{Trade-offs:} Balancing cooperation and competition is complex and context-dependent.
    \end{itemize}
    \begin{block}{Conclusion}
        Addressing coordination, competition, and communication challenges is essential for effective MARL implementations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Multi-Agent Reinforcement Learning - Overview}
    Multi-Agent Reinforcement Learning (MARL) involves multiple agents learning and making decisions within a shared environment. This introduces unique challenges that traditional Reinforcement Learning (RL) does not face. 
    \begin{block}{Key Strategies}
        \begin{itemize}
            \item Centralized Training
            \item Decentralized Execution
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Multi-Agent Reinforcement Learning - Centralized Training}
    Centralized Training involves all agents training together while sharing information about their experiences. This approach allows for effective coordination and improved learning efficiency.

    \begin{block}{Key Features}
        \begin{itemize}
            \item \textbf{Shared Experience Replay:} Agents learn from the experiences of others.
            \item \textbf{Joint Action Learning:} Helps in coordinating behaviors by learning the value of joint actions.
        \end{itemize}
    \end{block}
    
    \begin{exampleblock}{Example}
        In a cooperative scenario like multi-robot navigation, all robots use a shared memory of past actions and rewards to optimize their paths collectively.
    \end{exampleblock}
    
    \begin{equation}
        Q(a_1, a_2, \ldots, a_n) = E[R | a_1, a_2, \ldots, a_n]
    \end{equation}
    Where \( a_i \) denotes the action taken by agent \( i \) and \( R \) is the cumulative reward.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Multi-Agent Reinforcement Learning - Decentralized Execution}
    After training centrally, agents operate independently during execution. Each agent makes decisions based on local information, allowing for scalability.

    \begin{block}{Key Aspects}
        \begin{itemize}
            \item \textbf{Local Policy:} Decisions are made based on own observations.
            \item \textbf{Adaptability:} Agents adapt to environmental changes quickly and efficiently.
        \end{itemize}
    \end{block}

    \begin{exampleblock}{Example}
        In a competitive gaming scenario, each player (agent) uses their learned policy to make decisions based solely on immediate surroundings and game state.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cooperative vs. Competitive Learning - Introduction}
    \begin{block}{Introduction to Cooperative and Competitive Learning in MARL}
        Multi-Agent Reinforcement Learning (MARL) involves multiple agents learning simultaneously within an environment. 
        These can manifest as cooperative or competitive strategies based on their interactions and goals.
        Understanding these distinctions is crucial for designing effective MARL systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cooperative Learning}
    \begin{itemize}
        \item \textbf{Definition:} Agents work collaboratively towards shared goals.
        \item \textbf{Characteristics:}
        \begin{itemize}
            \item Shared Rewards: Agents receive rewards based on joint actions and performance.
            \item Information Sharing: Observations and strategies are shared to improve performance.
            \item Coordination: Requires synchronized behaviors and communication mechanisms.
        \end{itemize}
        \item \textbf{Example:} Multiple drones coordinating for efficient surveillance.
    \end{itemize}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Promotes synergy among agents.
            \item Often leads to more efficient problem-solving.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Competitive Learning}
    \begin{itemize}
        \item \textbf{Definition:} Agents strive to outperform each other, leading to zero-sum or non-zero-sum games.
        \item \textbf{Characteristics:}
        \begin{itemize}
            \item Individual Rewards: Agents maximize their own rewards, often at others' expense.
            \item Strategic Interactions: Anticipation of opponents' behavior is crucial.
        \end{itemize}
        \item \textbf{Example:} Players in competitive games like Chess or Poker.
    \end{itemize}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Fosters individualistic strategies.
            \item Involves complex strategies and counter-strategies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Overview}
    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Aspect} & \textbf{Cooperative Learning} & \textbf{Competitive Learning} \\
            \hline
            Goals & Shared goals & Individual goals \\
            \hline
            Reward Structure & Joint rewards & Individual rewards \\
            \hline
            Interaction & Collaboration & Opposition \\
            \hline
            Learning Dynamics & Enhanced through teamwork & Driven by competition \\
            \hline
            Examples & Multi-robot systems & Board games, video games \\
            \hline
        \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        Understanding whether to implement cooperative or competitive strategies is critical in designing MARL systems. 
        The choice influences agents' learning processes, algorithms, and overall performance.
    \end{block}
    \begin{block}{Next Steps}
        Explore real-world applications of MARL in various fields to see these principles in action and understand how collaboration and competition shape outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: MARL Applications}
    \begin{block}{Introduction to MARL}
        Multi-Agent Reinforcement Learning (MARL) involves multiple agents interacting in an environment, either cooperatively or competitively. Its applications span various fields, including robotics, gaming, and autonomous systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MARL in Robotics}
    \begin{itemize}
        \item \textbf{Example: Swarm Robotics}
        \begin{itemize}
            \item \textbf{Concept:} Mimics social organisms (like ants or bees) to accomplish tasks efficiently.
            \item \textbf{Application:} Multiple robots working together for exploration, object carrying, or search-and-rescue missions.
            \item \textbf{Key Point:} Agents share information and learn collaboratively, leading to improved task completion and adaptability in dynamic environments.
        \end{itemize}
    \end{itemize}
    \begin{block}{Illustration}
        Think of a group of drones coordinating to map an area. They utilize MARL to communicate obstacles and adjust their paths in real-time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MARL in Gaming and Autonomous Systems}
    \begin{itemize}
        \item \textbf{MARL in Gaming}
        \begin{itemize}
            \item \textbf{Example: Multi-Agent Video Games}
            \begin{itemize}
                \item \textbf{Concept:} Simulating complex environments with independent agents (characters, NPCs).
                \item \textbf{Application:} Agents learn strategies through competition and collaboration in multiplayer games.
                \item \textbf{Key Point:} MARL optimizes gameplay experiences and challenges, as agents adapt based on opponents' decisions.
            \end{itemize}
            \begin{block}{Illustration}
                Consider a battle royale game where players learn to strategically work together against others.
            \end{block}
        \end{itemize}

        \item \textbf{MARL in Autonomous Systems}
        \begin{itemize}
            \item \textbf{Example: Autonomous Vehicles}
            \begin{itemize}
                \item \textbf{Concept:} Vehicles interact with each other and the environment to make driving decisions.
                \item \textbf{Application:} Employing MARL for collision avoidance, route optimization, and traffic adaptation.
                \item \textbf{Key Point:} Enhances safety and efficiency, as agents learn cooperative behaviors like merging and yielding.
            \end{itemize}
            \begin{block}{Illustration}
                Visualize self-driving cars coordinating at a busy intersection without traffic signals.
            \end{block}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics in Multi-Agent Settings}
    \begin{block}{Understanding Performance Metrics}
        Performance metrics are critical for evaluating the effectiveness of multi-agent systems in various environments. In multi-agent reinforcement learning (MARL), the complexity introduced by the interactions between agents necessitates specific metrics to assess both individual and collective behaviors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Part 1}
    \begin{itemize}
        \item \textbf{Cumulative Reward}:
            \begin{equation}
                R_t = \sum_{n=0}^{N} r_{t+n}
            \end{equation}
            where \( R_t \) is the cumulative reward from time \( t \), and \( r \) is the immediate reward received at each time step.
        
        \item \textbf{Win Rate}:
            \begin{itemize}
                \item Indicates the proportion of games or scenarios won by the agents.
                \item A higher win rate signifies better performance in adversarial contexts.
            \end{itemize}
        
        \item \textbf{Convergence Speed}:
            \begin{itemize}
                \item Assesses how quickly agents learn optimal strategies.
                \item Faster convergence reflects more efficient learning algorithms or effective cooperation strategies.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - Part 2}
    \begin{itemize}
        \item \textbf{Stability of Learning}:
            \begin{itemize}
                \item Measures how consistently an agent can achieve similar performance over time.
                \item Variance in cumulative rewards provides insights into performance stability.
            \end{itemize}
        
        \item \textbf{Communication Efficiency}:
            \begin{itemize}
                \item Measures effectiveness and efficiency of communication protocols among agents.
                \item Metrics include communication cost (number of messages sent) and information shared (relevance of exchanged information).
            \end{itemize}
        
        \item \textbf{Individual vs Collective Performance}:
            \begin{itemize}
                \item Metrics include individual reward and team reward.
                \item Assess performance of agents in isolation and in concert.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    \begin{block}{Example: Multi-Agent Soccer Game}
        \begin{itemize}
            \item Each agent (player) aims to maximize their own score (individual reward) while working to score for the team (team reward).
            \item Performance can be evaluated using metrics such as:
                \begin{itemize}
                    \item Win rate: percentage of games won
                    \item Cumulative team reward: total goals scored
                    \item Communication efficiency: number of passes leading to goals
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item Performance metrics in MARL should capture both individual and team dynamics.
        \item Cumulative rewards help assess overall agent success.
        \item Metrics like stability and convergence provide insights into learning efficiency and adaptability.
        \item Communication efficiency is vital where agents must collaborate or compete effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in MARL Research}
    \begin{block}{Overview}
        Multi-Agent Reinforcement Learning (MARL) is rapidly evolving, driven by advancements in artificial intelligence and emerging real-world applications. 
        Researchers are now exploring various innovative directions to enhance the effectiveness and efficiency of collaborative agents.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Areas of Research in MARL}
    \begin{enumerate}
        \item \textbf{Scalability and Coordination}
            \begin{itemize}
                \item \textit{Challenge}: Coordination complexity increases with the number of agents.
                \item \textit{Future Direction}: Develop decentralized algorithms for efficient collaboration.
                \item \textit{Example}: Autonomous vehicle fleets coordinating traffic flow independently.
            \end{itemize}

        \item \textbf{Communication Protocols}
            \begin{itemize}
                \item \textit{Challenge}: Need for effective information sharing.
                \item \textit{Future Direction}: Adaptive communication strategies for selective information sharing.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continued Key Areas of Research in MARL}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue from the last frame's enumerated items
        \item \textbf{Safety and Robustness}
            \begin{itemize}
                \item \textit{Challenge}: Resilience to failures and adversarial attacks.
                \item \textit{Future Direction}: Incorporate safety mechanisms in MARL frameworks.
                \item \textit{Example}: Cybersecurity in multi-robot systems defending against attacks.
            \end{itemize}

        \item \textbf{Transfer Learning and Generalization}
            \begin{itemize}
                \item \textit{Challenge}: Difficulty in applying learned knowledge to new tasks.
                \item \textit{Future Direction}: Enhance transferability using meta-learning approaches.
            \end{itemize}
        
        \item \textbf{Interdisciplinary Applications}
            \begin{itemize}
                \item \textit{Challenge}: Bridging the gap between research and practical applications.
                \item \textit{Future Direction}: Collaborate with fields like economics, biology, and social sciences.
                \item \textit{Example}: MARL for wildlife conservation and predator-prey simulations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Decentralization and Collaboration}: Importance of agents working without central authority.
        \item \textbf{Adaptive Strategies}: Flexibility in communication and learning tailored to tasks.
        \item \textbf{Real-World Impact}: Practical applications in robotics, economics, and social dynamics.
    \end{itemize}

    \begin{block}{Conclusion}
        As MARL grows, future research directions will enable revolutionary applications across domains, impacting technology and society significantly through intelligent systems solving complex issues.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Potential Algorithms to Explore}
    \begin{itemize}
        \item Cooperative Deep Q-Learning (CDQL)
        \item Multi-Agent Proximal Policy Optimization (MAPPO)
        \item Communication Graphs for Adaptive Messaging
    \end{itemize}
    \begin{block}{Key Reminder}
        The future of MARL relies not only on technological advancements but also on understanding the interplay between agents and their environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations in MARL - Introduction}
  \begin{itemize}
    \item Multi-Agent Reinforcement Learning (MARL) poses significant societal impacts and ethical challenges.
    \item Addressing these considerations is crucial as MARL systems become more integrated into our daily lives.
    \item We must ensure that MARL contributes positively to society.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations in MARL - Key Points}
  \begin{enumerate}
    \item \textbf{Fairness and Equity}
      \begin{itemize}
        \item MARL can reinforce biases if trained on skewed data.
        \item \textit{Example}: Ride-sharing prioritizing certain neighborhoods can widen socio-economic divides.
      \end{itemize}

    \item \textbf{Accountability}
      \begin{itemize}
        \item The distributed nature of MARL complicates accountability for decisions.
        \item \textit{Example}: Damage caused by drones controlled through MARL raises questions of responsibility.
      \end{itemize}

    \item \textbf{Safety and Security}
      \begin{itemize}
        \item Vital to ensure safe operations in high-stakes environments (e.g., healthcare, autonomous vehicles).
        \item \textit{Example}: Traffic management systems that misinterpret conditions may introduce new hazards.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations in MARL - Privacy and Societal Impacts}
  \begin{enumerate}
    \setcounter{enumi}{3} % Continue numbering from previous frame
    \item \textbf{Privacy Concerns}
      \begin{itemize}
        \item Reliance on vast amounts of data may include sensitive information.
        \item \textit{Example}: Smart cities utilizing surveillance data for optimization may infringe on privacy rights.
      \end{itemize}

    \item \textbf{Societal Impacts}
      \begin{itemize}
        \item \textbf{Economic}: Automation may lead to job displacement but can also create new tech opportunities.
        \item \textbf{Social}: Optimized resources can enhance quality of life but risk exacerbating inequalities.
        \item \textbf{Environmental}: MARL can promote sustainability but potentially lead to over-exploitation of resources.
      \end{itemize}
  \end{enumerate}
\end{frame}


\end{document}