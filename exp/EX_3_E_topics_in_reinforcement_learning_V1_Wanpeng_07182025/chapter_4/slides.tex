\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Monte Carlo]{Week 4: Monte Carlo Methods}
\author[J. Smith]{John Smith, Ph.D.}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Monte Carlo Methods - Overview}
    \begin{block}{Overview of Monte Carlo Methods}
        Monte Carlo methods are a set of statistical techniques that rely on random sampling to solve problems that might be deterministic in nature. 
        Named after the famous Monte Carlo Casino due to their inherent randomness, these methods are particularly useful in various fields, including:
        \begin{itemize}
            \item Physics
            \item Finance
            \item Computer Science
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Monte Carlo Methods - Key Concepts}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Random Sampling}: Generates random samples to estimate properties of a system or process, inferring overall behavior by simulating a large number of outcomes.
            \item \textbf{Estimation}: Uses simple experiments with random inputs to produce results that approximate complex algorithms or analytical solutions.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Methods - Significance in Reinforcement Learning}
    \begin{block}{Significance in Reinforcement Learning}
        Monte Carlo methods are crucial for estimating the value of states and state-action pairs in reinforcement learning. They facilitate:
        \begin{itemize}
            \item \textbf{Exploration vs. Exploitation}: Agents can explore various strategies and learn optimal policies over time using random sampling.
            \item \textbf{Learning from Complete Episodes}: Monte Carlo methods evaluate expected returns from entire episodes, allowing agents to learn from complete experiences.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Monte Carlo Methods in RL}
    \begin{block}{Examples of Monte Carlo Methods in RL}
        \begin{enumerate}
            \item \textbf{Monte Carlo Control}: Evaluates the value of different states under various policies and updates policies based on observed outcomes.
            \item \textbf{Monte Carlo Prediction}: Estimates the value function for a given policy through repeated simulations to refine understanding of long-term rewards.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Monte Carlo Methods}
    \begin{block}{Real-World Applications}
        Monte Carlo methods have applications across various fields:
        \begin{itemize}
            \item \textbf{Finance}: Pricing complex derivatives where closed-form solutions are unavailable.
            \item \textbf{Physics}: Simulating particle interactions and predicting outcomes in complex physical systems.
            \item \textbf{Engineering}: Conducting risk assessments in engineering designs with uncertainty factors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustration of Monte Carlo Simulation}
    \begin{block}{Example: Estimating \(\pi\)}
        To estimate the value of \(\pi\) using Monte Carlo methods:
        \begin{enumerate}
            \item Randomly generate points in a square (with side length 2 units) that circumscribes a circle of radius 1 unit.
            \item Count the proportion of points that fall within the circle compared to the total number of points.
            \item The area of the circle can be estimated as:
            \begin{equation}
                \text{Estimated } \pi = 4 \times \left(\frac{\text{points inside circle}}{\text{total points}}\right)
            \end{equation}
        \end{enumerate}
        This example illustrates the intuitive process behind Monte Carlo methods through random sampling!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion on Monte Carlo Methods}
    \begin{block}{Conclusion}
        Monte Carlo methods form a foundational technique in various disciplines, especially in reinforcement learning, where they enhance decision-making under uncertainty. 
        Understanding their significance and application is essential for effectively leveraging these powerful methods.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Policy Evaluation - Overview}
    Monte Carlo Policy Evaluation is a method in reinforcement learning to assess the quality of a given policy based on sampled experiences. It helps:
    \begin{itemize}
        \item Evaluate how well a policy performs.
        \item Guide improvements or adjustments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Principles of Monte Carlo Policy Evaluation}
    \begin{enumerate}
        \item \textbf{Sample-based Evaluation:} Estimates value functions using sampled episodes.
        \item \textbf{Temporal Episodes:} Evaluates complete sequences from start to terminal state.
        \item \textbf{Discounted Returns:}
            \begin{equation}
                G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
            \end{equation}
        \item \textbf{State Value Estimation:}
            \begin{equation}
                V(s) = \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_t^i
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithms Used in Monte Carlo Policy Evaluation}
    \begin{enumerate}
        \item \textbf{First-Visit Monte Carlo:} Updates value based on first occurrences.
            \begin{equation}
                V(s) \gets V(s) + \alpha (G_n - V(s))
            \end{equation}
        \item \textbf{Every-Visit Monte Carlo:} Updates value every time a state is visited.
            \begin{equation}
                V(s) \gets V(s) + \alpha (G_t - V(s))
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Monte Carlo Policy Evaluation}
    Consider a grid world scenario:
    \begin{itemize}
        \item An agent can move in four directions based on its policy.
        \item It starts at an initial state and follows the policy until a terminal state.
        \item Returns are calculated for all visited states.
        \item State values are updated based on the observed returns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Complete Episodes:} Necessary for accurate evaluations.
        \item \textbf{Value Updates:} Improve with more episodes, leading to convergence.
        \item \textbf{Exploration Requirement:} Sufficient exploration needed for effective policy evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Monte Carlo Policy Evaluation is essential for estimating policy values in reinforcement learning. It:
    \begin{itemize}
        \item Utilizes sampled experiences for evaluations.
        \item Assists in understanding the effectiveness of a policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Control Methods - Overview}
    \begin{block}{Overview of Monte Carlo Control Methods}
        Monte Carlo methods aim to optimize agent behavior in reinforcement learning by learning both the policy and the value function. They utilize random sampling to explore actions and outcomes, enabling the development of effective policies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Control Methods - On-Policy vs. Off-Policy}
    \begin{block}{On-Policy Methods}
        \begin{itemize}
            \item \textbf{Definition}: The policy being evaluated and improved is the same as the policy used to interact with the environment.
            \item \textbf{Key Characteristics}:
                \begin{itemize}
                    \item Data is collected based on the current policy.
                    \item The policy is updated using this data.
                \end{itemize}
            \item \textbf{Example}: SARSA (State-Action-Reward-State-Action) algorithm.
        \end{itemize}
    \end{block}
    
    \begin{block}{Off-Policy Methods}
        \begin{itemize}
            \item \textbf{Definition}: The policy being improved is different from the policy generating the data, allowing learning from experiences of other policies.
            \item \textbf{Key Characteristics}:
                \begin{itemize}
                    \item Learns from historical data or data from another agent.
                    \item Offers flexibility and potential for efficient learning.
                \end{itemize}
            \item \textbf{Example}: Q-Learning algorithm.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Control Methods - Implications and Key Points}
    \begin{block}{Implications of On-Policy and Off-Policy}
        \begin{itemize}
            \item \textbf{Exploration vs. Exploitation}:
                \begin{itemize}
                    \item On-policy promotes exploration based on the current policy.
                    \item Off-policy exploits successful existing policies for efficient learning.
                \end{itemize}
            \item \textbf{Convergence and Stability}:
                \begin{itemize}
                    \item On-policy: slower convergence but stable improvements based on real-time feedback.
                    \item Off-policy: faster convergence under certain conditions, but potential stability issues.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item \textbf{Monte Carlo Control}: Technique for learning optimal policies using random sampling.
            \item \textbf{On-Policy vs. Off-Policy}:
                \begin{itemize}
                    \item On-policy: learns from executed policy (e.g., SARSA).
                    \item Off-policy: learns from any generated data (e.g., Q-Learning).
                \end{itemize}
            \item \textbf{Efficiency}: Off-policy allows for quicker convergence but may compromise stability; on-policy ensures stability with prolonged exploration.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Monte Carlo Methods - Overview}
    \begin{block}{Overview}
        Monte Carlo methods are powerful statistical techniques utilized for solving problems through random sampling and simulations. 
        In reinforcement learning (RL) and artificial intelligence (AI), they are essential for:
        \begin{itemize}
            \item Estimating value functions
            \item Optimizing strategies
            \item Improving decision-making processes
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Monte Carlo Methods - Key Applications}
    \begin{enumerate}
        \item \textbf{Policy Evaluation}
            \begin{itemize}
                \item Evaluates expected return of a policy by simulating outcomes across multiple episodes.
                \item \textit{Example}: Simulating player strategies in a game to find the highest average score.
            \end{itemize}
    
        \item \textbf{Policy Improvement}
            \begin{itemize}
                \item Used to update and enhance policies based on observed returns.
                \item \textit{Example}: Improving an autonomous driving policy by simulating various traffic conditions.
            \end{itemize}

        \item \textbf{Game Playing}
            \begin{itemize}
                \item Monte Carlo Tree Search (MCTS) optimizes decisions in complex games.
                \item \textit{Example}: AI implementing MCTS to evaluate potential moves in Chess or Go.
            \end{itemize}

        \item \textbf{Risk Assessment}
            \begin{itemize}
                \item Models uncertainties in decision-making to evaluate risks of actions.
                \item \textit{Example}: Using Monte Carlo simulations in finance to project future stock prices.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Monte Carlo Methods - Continued}
    \begin{itemize}
        \item \textbf{Robotics and Control}
            \begin{itemize}
                \item Assists decision-making in robotic movements by evaluating action outcomes.
                \item \textit{Example}: A robot using Monte Carlo sampling to navigate efficiently while avoiding obstacles.
            \end{itemize}
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Monte Carlo methods utilize randomness to explore various outcomes in stochastic environments.
            \item They foster a continuous learning cycle through policy evaluation and improvement.
            \item Particularly useful in complex models where analytical solutions are impractical.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Formula}
    \begin{block}{Monte Carlo Return}
        The return at time \( t \) is defined as:
        \begin{equation}
            G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
        \end{equation}
        Where \( G_t \) is the return at time \( t \), \( R \) represents rewards received, and \( \gamma \) is the discount factor.
    \end{block}
    
    \begin{block}{Summary}
        Monte Carlo methods are integral to advancing reinforcement learning and AI applications by enabling effective simulations, evaluations, and strategy improvements. They are indispensable tools for managing uncertainty in various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Limitations of Monte Carlo Methods}
    \begin{block}{Overview of Monte Carlo Methods}
        Monte Carlo methods rely on repeated random sampling to obtain numerical results.
        In reinforcement learning (RL), they are used for estimating value functions,
        policy evaluation, and optimizing strategies. These methods offer unique advantages
        but also present certain limitations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Monte Carlo Methods}
    \begin{enumerate}
        \item \textbf{Model-Free Learning} 
        \begin{itemize}
            \item No model of the environment is required, allowing direct learning from episodes.
            \item \textit{Example:} In chess, agents refine strategies from game outcomes.
        \end{itemize}
        
        \item \textbf{Simplicity} 
        \begin{itemize}
            \item Algorithms are straightforward to implement, especially for discrete actions.
            \item \textit{Example:} Monte Carlo control algorithms use simple loops over episodes.
        \end{itemize}
        
        \item \textbf{Convergence Guarantees} 
        \begin{itemize}
            \item Proven convergence to optimal policy under specific conditions.
            \item \textit{Formula:} If all state-action pairs are explored infinitely, \( V(s) \to V^*(s) \).
        \end{itemize}
        
        \item \textbf{Strong Performance in Complex Environments} 
        \begin{itemize}
            \item Effective in episodic tasks where rewards are delayed.
            \item \textit{Example:} Excels in games like Go due to feedback from final outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Monte Carlo Methods}
    \begin{enumerate}
        \item \textbf{High Variance}
        \begin{itemize}
            \item Learning from complete episodes can lead to high variance.
            \item \textit{Example:} A single episode's extreme reward affects learning disproportionately.
        \end{itemize}
        
        \item \textbf{Data Inefficiency}
        \begin{itemize}
            \item Requires many episodes to gather sufficient data for accurate estimation.
            \item \textit{Challenge:} Time-consuming in environments with sparse rewards.
        \end{itemize}
        
        \item \textbf{Exploration Challenges}
        \begin{itemize}
            \item Risk of convergence to suboptimal policies if exploration does not occur.
            \item \textit{Example:} An agent favoring a specific action might miss better alternatives.
        \end{itemize}
        
        \item \textbf{Episodic Nature}
        \begin{itemize}
            \item Limited use in continuous tasks, designed for environments with clear episodes.
            \item \textit{Example:} Stock trading lacks the episodic structure for traditional methods.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Monte Carlo methods are powerful in reinforcement learning but best suited for problems with clear episodic outcomes.
        \item A thorough understanding of both advantages and limitations aids in selecting the most appropriate method for RL scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies of Monte Carlo Methods in Reinforcement Learning}
    \begin{block}{Introduction to Monte Carlo Methods}
        Monte Carlo methods utilize random sampling to make numerical estimations and predictions. In reinforcement learning (RL), they are particularly useful for problems with unknown dynamics, helping estimate the value of states or actions based on experiences gained from episodes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Playing Atari Games (DQN)}
    \begin{itemize}
        \item \textbf{Overview:} DQN integrates deep learning with Q-learning, evaluating potential actions in gameplay using Monte Carlo methods.
        \item \textbf{How it Works:} Experiences sampled from an experience replay buffer are used to update Q-values based on Monte Carlo estimates of returns.
        \item \textbf{Outcome:} Achieved superhuman performance in Atari games, enabling robust learning through past experiences.
    \end{itemize}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Manages high-dimensional spaces through sampling and approximation.
            \item Randomized experience sampling enhances update diversity and learning robustness.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: AlphaGo and Case Study 3: Portfolio Management}
    \begin{itemize}
        \item \textbf{AlphaGo:}
        \begin{itemize}
            \item \textbf{Overview:} Combined Monte Carlo Tree Search (MCTS) with neural networks to play Go.
            \item \textbf{How it Works:} MCTS uses random simulations for outcome estimation; neural networks predict winning probabilities.
            \item \textbf{Outcome:} Defeated world champions, showcasing Monte Carlo’s effectiveness in large decision spaces.
        \end{itemize}
        
        \item \textbf{Portfolio Management:}
        \begin{itemize}
            \item \textbf{Overview:} Uses Monte Carlo methods to optimize asset allocation in finance.
            \item \textbf{How it Works:} Agents simulate market scenarios to evaluate expected returns and risks, guiding portfolio adjustments.
            \item \textbf{Outcome:} Facilitates improved investment strategies adaptive to market volatility.
        \end{itemize}
    \end{itemize}
    \begin{block}{Key Points}
        \begin{itemize}
            \item MCTS in AlphaGo illustrates recursive application of Monte Carlo methods for outcome evaluations.
            \item Portfolio management demonstrates the real-world applicability of Monte Carlo methods in uncertain environments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Part 1}
    \begin{block}{Key Takeaways from Monte Carlo Methods in Reinforcement Learning}
        \begin{enumerate}
            \item \textbf{Definition Recap:} 
                Monte Carlo methods are stochastic techniques that compute results based on random sampling.
            \item \textbf{Applications in Reinforcement Learning:}
                These methods evaluate and improve strategies based on sampled trajectories, enabling optimal policy learning.
            \item \textbf{Key Concepts:}
                \begin{itemize}
                    \item \textbf{Episodes:} A complete sequence of states, actions, and rewards ending at a terminal state.
                    \item \textbf{Returns:} Total accumulated reward, defined as
                    \begin{equation}
                        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
                    \end{equation}
                    where $\gamma$ is the discount factor.
                    \item \textbf{Policy Evaluation:} Estimating the state-value function through sampling and averaging returns.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Part 2}
    \begin{block}{Advantages and Challenges}
        \begin{itemize}
            \item \textbf{Advantages:}
                \begin{itemize}
                    \item Easy to implement, ideal for beginners.
                    \item Model-free approach that relies on experience.
                \end{itemize}
            \item \textbf{Challenges:}
                \begin{itemize}
                    \item Extensive exploration may lead to slow convergence, especially in large state spaces.
                    \item High variance in returns can affect learning stability; averaging helps.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Final Thoughts}
    \begin{block}{Conclusion}
        Monte Carlo methods provide a robust framework for learning in reinforcement learning. They are essential for developing agents that adapt to complex environments and lay the groundwork for advanced methods such as Temporal-Difference learning.
    \end{block}
    
    \begin{block}{Key Formula}
        The return at time \( t \) can be summarized as:
        \begin{equation}
            G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \\
        \end{equation}
        with \( 0 \leq \gamma < 1 \).
    \end{block}
\end{frame}


\end{document}