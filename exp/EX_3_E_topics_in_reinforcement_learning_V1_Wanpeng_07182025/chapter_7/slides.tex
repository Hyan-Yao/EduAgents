\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Deep Reinforcement Learning]{Week 7: Deep Reinforcement Learning}
\author[]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Deep Reinforcement Learning?}
    Deep Reinforcement Learning (DRL) combines reinforcement learning (RL) principles with deep learning techniques to enable agents to make decisions and learn optimal behaviors in complex environments. It leverages neural networks to approximate the value functions or policies, enhancing RL's capabilities in high-dimensional state spaces.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Definition}: RL is a type of machine learning where an agent learns to make decisions by interacting with an environment, receiving feedback in the form of rewards or punishments.
        \item \textbf{Key Components}:
        \begin{itemize}
            \item \textbf{Agent}: The learner or decision maker.
            \item \textbf{Environment}: Everything the agent interacts with.
            \item \textbf{Actions}: Choices made by the agent.
            \item \textbf{States}: Current situation in the environment.
            \item \textbf{Rewards}: Feedback signals (positive or negative).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in AI}
    \begin{itemize}
        \item \textbf{Scalability}: Handles large and complex datasets, crucial for real-world applications.
        \item \textbf{Autonomy}: Enables machines to perform tasks without explicit programming by learning from experience.
        \item \textbf{Exploration vs. Exploitation}: Core challenge in RL of balancing exploration of new actions with exploitation of known rewarding actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Gaming}:
            \begin{itemize}
                \item Examples: AlphaGo, OpenAI's Dota 2 agent.
                \item Achievements: Surpassed human performance in various games.
            \end{itemize}
        \item \textbf{Robotics}:
            \begin{itemize}
                \item Task Learning: Walking, grasping, and assembly.
                \item Benefits: Adaptable and robust behavior in dynamic environments.
            \end{itemize}
        \item \textbf{Autonomous Vehicles}:
            \begin{itemize}
                \item Navigation and obstacle avoidance.
                \item Impact: Enhances the safety and efficiency of self-driving systems.
            \end{itemize}
        \item \textbf{Healthcare}:
            \begin{itemize}
                \item Applications: Personalizing treatment plans and optimizing resources.
                \item Outcome: Better patient outcomes through improved decision-making.
            \end{itemize}
        \item \textbf{Finance}:
            \begin{itemize}
                \item Functions: Portfolio management and algorithmic trading.
                \item Advantage: Adjusts strategies based on market conditions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item DRL harnesses deep learning for complex decision-making.
        \item Learning from high-dimensional data makes it a revolutionary approach across domains.
        \item Continuous research and development can lead to transformative applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Deep Reinforcement Learning is an innovative subset of AI that empowers agents to learn effective strategies through interactions with complex environments. By intertwining RL learning mechanisms with deep learning capabilities, DRL is poised to solve challenging problems across diverse fields.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Basics - Definition}
    \begin{block}{What is Reinforcement Learning (RL)?}
        Reinforcement Learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. 
        Unlike supervised learning, where the model is trained with labeled data, in RL, the agent learns from the consequences of its actions over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Basics - Key Components}
    \begin{itemize}
        \item \textbf{Agents}:
        \begin{itemize}
            \item The learner or decision-maker that interacts with the environment.
            \item \textit{Example:} A robot navigating a maze.
        \end{itemize}
        
        \item \textbf{Environment}:
        \begin{itemize}
            \item Everything the agent interacts with, providing context for the agent's operation.
            \item \textit{Example:} The maze itself, including walls and pathways for the robot.
        \end{itemize}
        
        \item \textbf{Rewards}:
        \begin{itemize}
            \item Feedback signals received by the agent from the environment after performing an action. 
            \item \textit{Example:} Reaching the exit gives positive reward (+10); running into a wall gives negative reward (-1).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Basics - Terminology}
    \begin{itemize}
        \item \textbf{State (s)}: Current situation representation of the environment.
        \item \textbf{Action (a)}: Choice made by the agent affecting the environment's state.
        \item \textbf{Policy ($\pi$)}: Strategy to determine which action to take from a given state.
        \item \textbf{Value Function (V)}: Predicts expected future rewards from a certain state following a policy.
        \item \textbf{Q-Function (Q)}: Expected utility of taking a specific action in a certain state, following a given policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Basics - Challenges}
    \begin{itemize}
        \item \textbf{Trial and Error Learning}: Optimize actions through exploration and exploitation.
        \item \textbf{Temporal Credit Assignment}: Assigning credit for rewards to actions taken many steps earlier.
        \item \textbf{Exploration vs. Exploitation Dilemma}: Balancing the need to explore new strategies against exploiting known rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Basics - Conclusion}
    \begin{block}{Conclusion}
        Reinforcement Learning is a powerful framework for training agents in dynamic and complex environments. By understanding its basic components—agents, environments, actions, rewards, and policies—students can better appreciate how RL solutions are constructed and optimized to solve real-world problems.
    \end{block}
    
    \begin{block}{Formula Overview}
        \begin{equation}
            R_t = r(s_t, a_t)
        \end{equation}
        \begin{equation}
            V(s) = R + \gamma \sum_{s'} P(s'|s, a)V(s')
        \end{equation}
        \textit{Where $\gamma$ is the discount factor representing the importance of future rewards.}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Learning Integration - Overview}
    \begin{block}{Overview}
        Deep learning enhances reinforcement learning (RL) by providing powerful tools for function approximation and representation learning. Understanding this integration is crucial for building efficient and effective RL agents that can learn from complex environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Learning Integration - Function Approximation}
    \begin{block}{1. Function Approximation}
        \begin{itemize}
            \item \textbf{Definition}: Estimating the value function or policy directly without explicit state representation.
            \item \textbf{Limitation in Traditional RL}: Estimating Q-values for each state-action pair is impractical in large state spaces.
            \item \textbf{Deep Learning Contribution}: Deep neural networks generalize across states, enabling effective estimation in high-dimensional spaces.
        \end{itemize}
    \end{block}

    \textbf{Example: Chess} \\
    \begin{itemize}
        \item \textbf{Without Deep Learning}: Requires explicit storage of state-action values for every configuration (infeasible).
        \item \textbf{With Deep Learning}: A deep network approximates the value of moves by learning from previous games, allowing better generalization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Learning Integration - Representation Learning}
    \begin{block}{2. Representation Learning}
        \begin{itemize}
            \item \textbf{Definition}: Automatically discovering features from raw data to enhance task performance.
            \item \textbf{Importance in RL}: Many RL problems involve high-dimensional sensory inputs. Deep learning transforms them into meaningful lower-dimensional representations.
        \end{itemize}
    \end{block}

    \textbf{Example: Video Game Environments} \\
    \begin{itemize}
        \item \textbf{Raw Input}: A game frame could be a 224x224-pixel image.
        \item \textbf{Feature Extraction}: CNNs (Convolutional Neural Networks) streamline the input, extracting relevant features like player positions and goals.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Mathematical Foundations}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Deep learning provides flexibility and scalability for complex RL tasks.
            \item Various neural architectures (e.g., CNNs, RNNs) can be adapted for specific RL applications.
            \item Enhanced generalization capabilities improve RL agents' performance in unseen states.
        \end{itemize}
    \end{block}

    \begin{block}{Q-Learning Update Rule}
        \begin{equation}
            Q(s, a; \theta) \gets Q(s, a; \theta) + \alpha \left( r + \gamma \max_{a'} Q(s', a'; \theta) - Q(s, a; \theta) \right)
        \end{equation}
        \textbf{where:} 
        \begin{itemize}
            \item $\alpha$ is the learning rate.
            \item $\gamma$ is the discount factor.
            \item $r$ is the reward.
            \item $s'$ is the next state.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Network Pseudocode}
    \textbf{Example Code: Deep Q-Network Pseudocode} \\
    \begin{lstlisting}[language=Python]
# Initialize replay memory
replay_memory = []

# For each episode
for episode in range(num_episodes):
    state = env.reset()
    done = False
    
    while not done:
        action = select_action(state)  # Using epsilon-greedy policy
        next_state, reward, done = env.step(action)  
        
        # Store experience in replay memory
        replay_memory.append((state, action, reward, next_state, done))
        
        # Sample from replay memory and update model
        if len(replay_memory) >= batch_size:
            batch = sample(replay_memory, batch_size)
            train_model(batch)  # Training the network

        state = next_state
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Introduction}
    \begin{block}{Overview}
        Deep Q-Networks (DQN) combine Q-learning with deep learning, enabling agents to learn from high-dimensional sensory inputs, such as images, without requiring hand-crafted features.
    \end{block}
    \begin{itemize}
        \item Traditional Q-learning techniques
        \item Use of deep learning architectures
        \item Enhanced learning capabilities
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - Key Concepts}
    \begin{enumerate}
        \item \textbf{Q-Learning}:
            \begin{itemize}
                \item Value-based reinforcement learning algorithm
                \item Update rule:
                \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
                \end{equation}
            \end{itemize}
        
        \item \textbf{Deep Learning for Function Approximation}:
            \begin{itemize}
                \item Neural networks approximate Q-value functions
                \item Better generalization from complex input spaces
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Q-Networks (DQN) - How They Work}
    \begin{itemize}
        \item \textbf{Experience Replay}:
            \begin{itemize}
                \item Stores past experiences in a replay buffer
                \item Samples from buffer to break correlation
            \end{itemize}
        
        \item \textbf{Target Networks}:
            \begin{itemize}
                \item Uses a separate target network updated less frequently
                \item Stabilizes learning and reduces divergence risks
            \end{itemize}
        
        \item \textbf{Example}:
            \begin{itemize}
                \item Agent plays Atari
                \item Processes raw game pixel input through a neural network
                \item Outputs Q-values for actions like "jump," "move left"
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods - Overview}
    \begin{block}{Overview}
        Policy Gradient Methods optimize policies directly in deep reinforcement learning (RL). 
        Unlike value-based methods, they define a probability distribution over actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Policies and Objectives}
    \begin{itemize}
        \item \textbf{Policy}:
            \begin{itemize}
                \item A mapping from states $s$ to a probability distribution over actions $a$.
                \item Can be stochastic or deterministic.
            \end{itemize}
        \item \textbf{Objective}:
            \begin{equation}
                J(θ) = \mathbb{E}_{τ \sim π_θ} [R(τ)]
            \end{equation}
            where $R(τ)$ is the total return from trajectory $τ$ generated by policy $π_θ$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Optimization and REINFORCE}
    \begin{itemize}
        \item \textbf{Gradient Ascent}:
            \begin{equation}
                θ_{new} = θ_{old} + α \nabla J(θ_{old})
            \end{equation}
            where $α$ is the learning rate.
        \item \textbf{REINFORCE Algorithm}:
            \begin{equation}
                \nabla J(θ) = \mathbb{E}[_{t=0}^{T} [\nabla \log(π_θ(a_t|s_t)) \cdot R_t]]
            \end{equation}
            where $R_t$ is the return after taking action $a_t$ from state $s_t$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration: CartPole Problem}
    \begin{itemize}
        \item The CartPole problem keeps a pole balanced on a moving cart.
        \item Policy gradient method could learn via:
            \begin{itemize}
                \item Using a neural network to approximate the policy.
                \item Adjusting action probabilities based on the pole's stability.
                \item Feedback (rewards) informs the policy adjustments.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Stochastic Policies} allow for exploration of the action space.
        \item \textbf{Variance Reduction} techniques like baseline subtraction enhance learning efficiency.
        \item \textbf{Stable Updates}: Gradient-based updates can lead to stable convergence.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Policy gradient methods provide a powerful alternative to value-based methods in RL,
        enabling direct learning of policies for complex environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Deep Reinforcement Learning Models}
    \begin{block}{Key Techniques for Effective Training}
        \begin{itemize}
            \item Exploration vs. Exploitation Strategies
            \item Reward Shaping
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation Strategies}
    \begin{block}{Definitions}
        \begin{itemize}
            \item \textbf{Exploration}: Trying new actions to discover their effects.
            \item \textbf{Exploitation}: Choosing the best-known action based on past experiences to maximize reward.
        \end{itemize}
    \end{block}
    \begin{block}{Balancing Act}
        Effective agents must balance:
        \begin{itemize}
            \item Exploration (gathering more information)
            \item Exploitation (maximizing rewards)
        \end{itemize}
    \end{block}
    \begin{block}{Common Strategies}
        \begin{itemize}
            \item Epsilon-Greedy
            \item Softmax Action Selection
            \item Upper Confidence Bound (UCB)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Epsilon-Greedy Strategy Example}
    \begin{lstlisting}[language=Python]
import random

def epsilon_greedy_action(Q, state, epsilon):
    if random.random() < epsilon:
        return random.choice(range(len(Q[state])))  # Explore
    else:
        return Q[state].argmax()  # Exploit
    \end{lstlisting}
    \begin{block}{Key Points}
        \begin{itemize}
            \item $\epsilon$ promotes exploration when the random sample is less than $\epsilon$.
            \item Selects actions based on a balance between exploration and exploitation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reward Shaping}
    \begin{block}{Definition}
        \begin{itemize}
            \item Modifying the reward function to provide more frequent and informative feedback.
        \end{itemize}
    \end{block}
    \begin{block}{Purpose}
        \begin{itemize}
            \item Helps agents learn faster and shape desired behaviors effectively.
        \end{itemize}
    \end{block}
    \begin{block}{Implementation}
        \begin{itemize}
            \item Rewards that encourage efficient goal-reaching.
            \item Example: Small rewards for correct actions and penalties for wrong ones (e.g., in a maze).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Balancing exploration and exploitation is crucial to prevent local optima.
        \item Carefully designed reward shaping can significantly enhance learning efficiency.
        \item Misleading rewards can lead to undesirable behaviors in the agent.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Reinforcement Learning - Overview}
    \begin{block}{Overview}
        Deep Reinforcement Learning (DRL) combines reinforcement learning's trial-and-error learning with deep learning's data processing capabilities. This combination enables DRL to be applied across various industries, leading to significant advancements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Reinforcement Learning - Key Applications}
    \begin{itemize}
        \item \textbf{Healthcare}
            \begin{itemize}
                \item Treatment Planning
                \item Drug Discovery
            \end{itemize}
        \item \textbf{Robotics}
            \begin{itemize}
                \item Autonomous Navigation
                \item Manipulation Tasks
            \end{itemize}
        \item \textbf{Finance}
            \begin{itemize}
                \item Algorithmic Trading
                \item Risk Management
            \end{itemize}
        \item \textbf{Gaming}
            \begin{itemize}
                \item Game AI
                \item Game Design
            \end{itemize}
        \item \textbf{Natural Language Processing (NLP)}
            \begin{itemize}
                \item Conversational Agents
                \item Text Summarization
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Reinforcement Learning - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Real-Time Adaptation:} DRL systems effectively adapt to new information or environmental changes.
            \item \textbf{High-Dimensional Data Processing:} DRL leverages deep learning to analyze complex inputs.
            \item \textbf{Self-Improvement Mechanism:} Continuous learning from feedback allows DRL applications to refine models over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Reinforcement Learning - Conclusion}
    \begin{block}{Conclusion}
        Deep Reinforcement Learning is transforming various industries by offering innovative solutions to complex problems. Its capability to make informed decisions with minimal human intervention places it at the forefront of technological advancements. As methodologies continue to be refined, the scope of DRL applications is expected to expand considerably.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Deep Reinforcement Learning}
    Deep Reinforcement Learning (DRL) has shown remarkable success, but it also presents significant challenges:
    \begin{itemize}
        \item Sample Inefficiency
        \item Instability
        \item Overfitting
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenge 1: Sample Inefficiency}
    \begin{block}{Explanation}
        In DRL, the agent often requires a vast amount of data (interactions with the environment) to learn effectively.
    \end{block}
    \begin{exampleblock}{Example}
        A DRL model might need to play millions of chess games to learn optimal strategies, which is impractical in real-world scenarios.
    \end{exampleblock}
    \begin{block}{Key Point}
        Techniques like Experience Replay and Intrinsic Motivation can enhance sample efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenge 2: Instability}
    \begin{block}{Explanation}
        Training DRL models can be unpredictable, leading to fluctuating performance due to rapid updates in deep neural networks.
    \end{block}
    \begin{exampleblock}{Example}
        A robotic agent learning to walk may lose all previous learning if its weights are updated too drastically.
    \end{exampleblock}
    \begin{block}{Key Point}
        Techniques like Target Networks and Dual-Q Learning can stabilize training by providing more consistent updates.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenge 3: Overfitting}
    \begin{block}{Explanation}
        Overfitting occurs when a model learns the training data too well, failing to generalize to new situations.
    \end{block}
    \begin{exampleblock}{Example}
        An agent trained in a virtual environment may excel there but perform poorly in varied real-world conditions.
    \end{exampleblock}
    \begin{block}{Key Point}
        Techniques like Dropout, Regularization, and diverse training scenarios help mitigate overfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding these challenges is essential for developing robust DRL systems. Researchers are focusing on methodologies to:
    \begin{itemize}
        \item Address sample inefficiency
        \item Enhance stability during training
        \item Prevent overfitting
    \end{itemize}
    This work enables DRL to become more applicable in real-world contexts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading}
    \begin{block}{Experience Replay Buffer}
        \begin{lstlisting}[language=python]
class ExperienceReplay:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def store(self, transition):
        self.buffer.append(transition)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Loss Function for Q-Learning}
        \begin{equation}
        L(\theta) = \mathbb{E}_{(s,a,r,s')} \left[ (r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))^2 \right]
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Societal Implications}
    \begin{block}{Discussion Overview}
        This presentation covers the ethical considerations and societal impacts of deep reinforcement learning (DRL) technologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Ethics in Deep Reinforcement Learning (DRL)}
    \begin{itemize}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item DRL models may learn biases from training data.
                \item \textit{Example}: An autonomous hiring system favoring applicants based on skewed data, perpetuating discrimination.
            \end{itemize}
        \item \textbf{Transparency and Accountability}
            \begin{itemize}
                \item DRL decisions can be opaque, complicating user understanding of decision-making.
                \item \textit{Example}: Understanding why an autonomous vehicle chose a specific route is essential for accountability.
            \end{itemize}
        \item \textbf{Autonomy and Control}
            \begin{itemize}
                \item Increasing autonomy reduces human oversight.
                \item \textit{Concern}: Who is responsible for actions by malfunctioning systems?
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Societal Impacts of DRL Technologies}
    \begin{itemize}
        \item \textbf{Job Displacement}
            \begin{itemize}
                \item Automation of tasks can lead to significant job losses.
                \item \textit{Example}: DRL systems replacing human workers in sectors like transportation and manufacturing.
            \end{itemize}
        \item \textbf{Safety and Security}
            \begin{itemize}
                \item DRL in critical applications poses dire consequences if systems fail.
                \item \textit{Example}: Misdiagnosis by a DRL healthcare algorithm could result in severe health issues.
            \end{itemize}
        \item \textbf{Access and Inequities}
            \begin{itemize}
                \item Advanced DRL may only be accessible to wealthy organizations, increasing the digital divide.
                \item \textit{Example}: Only certain hospitals might afford sophisticated DRL systems, leading to unequal healthcare quality.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Ethical Frameworks}
            \begin{itemize}
                \item Establishing responsible guidelines for DRL usage, emphasizing fairness, accountability, and transparency.
            \end{itemize}
        \item \textbf{Stakeholder Involvement}
            \begin{itemize}
                \item Engaging diverse groups to include multifaceted perspectives in DRL development.
            \end{itemize}
        \item \textbf{Ongoing Research}
            \begin{itemize}
                \item Continuous research is essential for understanding the evolving ethical landscape of DRL.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Deep Reinforcement Learning offers significant potential but also poses ethical and societal risks. By promoting responsibility and awareness in its application, we can leverage its benefits while mitigating potential harms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Deep Reinforcement Learning}
    \begin{block}{Overview}
        Emerging trends in Deep Reinforcement Learning (DRL) are set to revolutionize artificial intelligence by enhancing applications and methodologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends and Developments - Part 1}
    \begin{enumerate}
        \item \textbf{Transfer Learning in DRL}
            \begin{itemize}
                \item \textbf{Concept:} Applying knowledge learned in one task to related tasks.
                \item \textbf{Example:} A DRL model trained on one game can be adapted to another similar game.
                \item \textbf{Importance:} Reduces data and computational requirements across various domains.
            \end{itemize}
        
        \item \textbf{Multi-Agent Reinforcement Learning (MARL)}
            \begin{itemize}
                \item \textbf{Concept:} Multiple agents learning and interacting in a shared environment.
                \item \textbf{Example:} Coordination in autonomous vehicles navigating traffic.
                \item \textbf{Benefits:} Develops complex cooperative or competitive behaviors.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends and Developments - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % continue numbering
        \item \textbf{Model-Based Reinforcement Learning}
            \begin{itemize}
                \item \textbf{Concept:} Models the environment to simulate actions instead of direct interaction.
                \item \textbf{Example:} A robot simulating potential actions to enhance learning efficiency.
                \item \textbf{Future Direction:} Aims to reduce sample complexity and improve training efficiency.
            \end{itemize}
        
        \item \textbf{Explainable AI (XAI) in DRL}
            \begin{itemize}
                \item \textbf{Concept:} Enhancing interpretability of DRL systems.
                \item \textbf{Example:} Insights into AI decisions in healthcare or autonomous driving.
                \item \textbf{Importance:} Boosts trust in AI technologies by addressing ethical concerns.
            \end{itemize}
        
        \item \textbf{Safety and Robustness in DRL}
            \begin{itemize}
                \item \textbf{Concept:} Ensuring reliable behavior of DRL agents in unpredictable environments.
                \item \textbf{Example:} An AI for robotic surgery avoiding patient risk.
                \item \textbf{Focus Area:} Researchers are prioritizing safety in agent development.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Potential Applications and Conclusion}
    \begin{block}{Potential Applications}
        \begin{itemize}
            \item \textbf{Healthcare:} Personalized treatment plans and optimized surgical procedures.
            \item \textbf{Finance:} Automated trading using market simulations.
            \item \textbf{Robotics:} Enhanced systems for dynamic workflows in manufacturing and services.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        The future of DRL is evolving, integrating ethical considerations, safety protocols, and generalization capabilities for enhanced real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile, plain]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Transfer learning reduces data requirements.
        \item MARL enhances collaboration in complex environments.
        \item Model-based approaches improve efficiency and depth of learning.
        \item Explainability and safety will shape trust and adoption in DRL technologies.
    \end{itemize}
\end{frame}


\end{document}