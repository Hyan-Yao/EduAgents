\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Policy Gradient Methods]{Week 8: Policy Gradient Methods}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Policy Gradient Methods}
    \begin{itemize}
        \item Overview of policy gradient methods in reinforcement learning.
        \item Their significance in optimizing policies directly.
        \item Differences from other RL approaches.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Policy Gradient Methods?}
    \begin{block}{Definition}
        Policy Gradient Methods are a class of algorithms in Reinforcement Learning (RL) that optimize the policy directly, unlike value-based methods that estimate the value of states or actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Policy Gradient Methods}
    \begin{itemize}
        \item \textbf{Direct Policy Optimization:} 
              - Adjust parameters based on gradients from expected reward.
              - Handles high-dimensional action spaces and stochastic environments effectively.
        \item \textbf{Applicability to Complex Problems:} 
              - Useful for large action spaces (e.g., robotics, game playing).
              - Models a probability distribution over actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Policy Gradients Differ from Other RL Approaches}
    \begin{itemize}
        \item \textbf{Value-Based vs. Policy-Based:}
        \begin{itemize}
            \item Value-Based Methods (e.g., Q-learning): Estimate value functions to determine best actions indirectly.
            \item Policy-Based Methods (Policy Gradients): Make decisions based on the policy itself, updating parameters via gradients of expected returns.
        \end{itemize}
        \item \textbf{Exploration vs. Exploitation:} 
              - Policy gradient methods incorporate built-in exploration strategies, adapting effectively to balance exploration and exploitation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Advantages in environments where:
            \begin{itemize}
                \item Action space is continuous or high-dimensional.
                \item Policies need to be stochastic.
            \end{itemize}
        \item Utilizes:
        \begin{itemize}
            \item \textbf{REINFORCE} Algorithm: Updates weights using the complete return from each episode.
            \item \textbf{Actor-Critic} Methods: Combine policy optimization with value function estimators for stability.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Basic Policy Gradient Update}
    \begin{block}{Update Rule}
        Given a policy parameterized by $\theta$, the update rule can be described as:
        \begin{equation}
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
        \end{equation}
        \begin{equation}
            \theta_{new} = \theta_{old} + \alpha \nabla J(\theta)
        \end{equation}
        where $\alpha$ is the learning rate and $\tau$ represents trajectories sampled from the policy $\pi$.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Policy Gradient Methods are effective tools in deep reinforcement learning.
        \item They enable training of agents in complex, dynamic environments.
        \item Focus on direct policy optimization presents a viable alternative to value-based approaches.
        \item Offer enhanced flexibility and robustness in solving real-world problems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Understanding Policy Functions - Overview}
    \begin{itemize}
        \item Definition of policy functions in reinforcement learning (RL)
        \item Types of policies: deterministic vs stochastic
        \item Roles of policies in decision making and exploration-exploitation dilemma
        \item Key formulas for both types of policies
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Understanding Policy Functions - Definition}
    \begin{block}{Definition of Policy Functions}
        In reinforcement learning (RL), a \textbf{policy} defines the behavior of an agent by mapping states of the environment to actions. It represents a strategy for decision-making.
    \end{block}
    
    \begin{itemize}
        \item Mathematically, a policy is denoted as \( \pi(a|s) \)
            \begin{itemize}
                \item \( s \): current state
                \item \( a \): action taken
                \item \( \pi \): probability distribution over actions given a state
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Understanding Policy Functions - Types of Policies}
    \begin{block}{Types of Policy Functions}
        \begin{enumerate}
            \item \textbf{Deterministic Policies}
                \begin{itemize}
                    \item Maps each state to a specific action.
                    \item Notation: \( a = \pi(s) \)
                    \item Example: Always moving right in a specific position in a grid world.
                    \item Use Case: Useful in environments where the optimal action is clear.
                \end{itemize}
                
            \item \textbf{Stochastic Policies}
                \begin{itemize}
                    \item Assigns probability distribution over actions for each state.
                    \item Notation: \( \pi(a|s) = P(A = a | S = s) \)
                    \item Example: 70% chance to move a specific chess piece, 30% for another.
                    \item Use Case: Useful in uncertain environments requiring exploration.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding Policy Functions - Roles and Formulas}
    \begin{block}{Roles of Policy Functions in Reinforcement Learning}
        \begin{itemize}
            \item \textbf{Decision Making}: Guides the agent on actions, influencing performance.
            \item \textbf{Exploration vs. Exploitation}:
                \begin{itemize}
                    \item Deterministic Policies: Tend to exploit known actions, may lead to suboptimal choices if environments change.
                    \item Stochastic Policies: Encourage exploration, beneficial for discovering better policies.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Formulas}
        \begin{align*}
        \text{Deterministic Policy:} \quad & a = \pi(s) \\
        \text{Stochastic Policy:} \quad & \pi(a|s) = P(A = a | S = s)
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding Policy Functions - Conclusion}
    Recognizing the fundamental nature of policy functions, including both deterministic and stochastic policies, is essential for understanding how agents learn to interact with their environments in reinforcement learning. 
    \\[0.5cm]
    As we progress, we will explore how these policies are optimized using the \textbf{Policy Gradient Theorem}.
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Policy Gradient Theorem - Introduction}
    \begin{block}{Overview}
        The Policy Gradient Theorem is fundamental for optimizing policies in Reinforcement Learning (RL). 
        It focuses on enhancing the policy rather than the action-value function, which is useful in high-dimensional action spaces.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Policy Gradient Theorem - Mathematical Formulation}
    \begin{block}{Key Variables}
        \begin{itemize}
            \item \( \theta \): Policy parameters (weights).
            \item \( \pi_\theta(a|s) \): Policy probability of action \( a \) in state \( s \).
            \item \( J(\theta) \): Objective function, the expected return defined as:
            \begin{equation}
                J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
            \end{equation}
        \end{itemize}
    \end{block}
    
    \begin{block}{Gradient Estimation}
        The policy gradient theorem states:
        \begin{equation}
            \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) R(\tau) \right]
        \end{equation}
        This means to improve the policy, adjust \( \theta \) in the direction of the gradient of \( J \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Policy Gradient Theorem - Implications}
    \begin{block}{Implications for Optimizing Policies}
        \begin{enumerate}
            \item \textbf{Direction of Improvement}: The gradient indicates actions that should be reinforced, leading to higher expected rewards.
            \item \textbf{Stochastic Policies}: Encourages exploration of various actions, reducing the risk of local optima.
            \item \textbf{Reinforcement with Rewards}: The policy is improved based on actual received rewards, linking performance with actions.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Policy Gradient Methods are effective for complex action spaces and partial observability.
            \item Stochastic vs Deterministic Policies: Stochastic policies allow for better exploration.
            \item Exploration-Exploitation Trade-off: Balances exploration of new actions and exploitation of known rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Policy Gradient Theorem - Example Code}
    \begin{block}{Code Snippet Example (Python)}
        \begin{lstlisting}[language=Python]
import numpy as np

def compute_policy_gradient(states, actions, rewards, policy_probs):
    # Calculating the return for each trajectory
    returns = np.array([np.sum(rewards[t:]) for t in range(len(rewards))])
    
    # Updating policy
    policy_gradients = []
    for state, action, return_value in zip(states, actions, returns):
        # Get log probability 
        log_prob = np.log(policy_probs[state, action])
        policy_gradients.append(log_prob * return_value)
    
    return np.mean(policy_gradients)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Policy Gradient Theorem - Conclusion}
    \begin{block}{Conclusion}
        The Policy Gradient Theorem is essential for optimizing policies in reinforcement learning. 
        By manipulating policy parameters according to the gradients, agents can learn optimal strategies effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Estimating Gradients - Overview}
    In reinforcement learning, estimating the gradients of policies is critical for optimizing decision-making strategies. This slide focuses on two primary techniques: 

    \begin{itemize}
        \item **Monte Carlo Methods**
        \item **Temporal Difference (TD) Methods**
    \end{itemize}

    These techniques help calculate the gradients necessary for adjusting the policy to maximize expected rewards.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Estimating Gradients - Monte Carlo Methods}
    \begin{block}{Concept}
        Monte Carlo methods involve simulating entire episodes of experience to estimate the expected returns for actions taken within the policy.
    \end{block}

    \begin{itemize}
        \item Collect a set of episodes by following the current policy.
        \item For each action taken, compute the return (total accumulated reward) from that action onward.
        \item Use these returns to update the policy.
    \end{itemize}

    \begin{equation}
        \nabla J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \nabla \log \pi_{\theta}(a_t | s_t) G_t^{(i)}
    \end{equation}
    Where:
    \begin{itemize}
        \item $N$ = number of episodes
        \item $G_t^{(i)}$ = return starting from time step $t$ in episode $i$
        \item $\nabla \log \pi_{\theta}(a_t | s_t)$ = gradient of the log probability of taking action $a_t$ in state $s_t$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Estimating Gradients - Temporal Difference Methods}
    \begin{block}{Concept}
        TD methods update value estimates based on other learned estimates without waiting for the final outcome of the episode, achieving more immediate updates.
    \end{block}

    \begin{itemize}
        \item It combines ideas from Monte Carlo and dynamic programming.
        \item Updates the value of actions taken using the difference between the estimated value of the current state and the estimated value after taking an action.
    \end{itemize}

    \begin{equation}
        V(s_t) \leftarrow V(s_t) + \alpha \delta_t
    \end{equation}
    Where:
    \begin{equation}
        \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
    \end{equation}
    Here:
    \begin{itemize}
        \item $r_t$ = reward received after taking action
        \item $\gamma$ = discount factor (0 < $\gamma$ < 1)
        \item $\alpha$ = learning rate (0 < $\alpha$ ≤ 1)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Policy Gradient Algorithms - Overview}
    \begin{itemize}
        \item Policy Gradient methods directly learn policies rather than relying on value functions.
        \item Focus on two widely used techniques:
        \begin{itemize}
            \item REINFORCE Algorithm
            \item Actor-Critic Methods
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Policy Gradient Algorithms - REINFORCE}
    \begin{block}{REINFORCE Algorithm}
        \begin{itemize}
            \item Monte Carlo Policy Gradient method.
            \item Updates policy based on returns from actions taken.
        \end{itemize}
    \end{block}
    
    \begin{block}{How It Works}
        \begin{enumerate}
            \item Generate a sequence of actions and rewards for each episode.
            \item Compute total discounted return \( R_t \) from each time step \( t \).
            \item Update policy parameters \( \theta \):
            \begin{equation}
                \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla \log \pi_\theta(a_t | s_t) R_t \right]
            \end{equation}
        \end{enumerate}
    \end{block}

    \begin{block}{Advantages and Disadvantages}
        \begin{itemize}
            \item \textbf{Advantages:}
                \begin{itemize}
                    \item Simplicity and ease of implementation.
                    \item Effective in sparse reward environments.
                \end{itemize}
            \item \textbf{Disadvantages:}
                \begin{itemize}
                    \item High variance in gradient estimates.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Policy Gradient Algorithms - Actor-Critic}
    \begin{block}{Actor-Critic Methods}
        \begin{itemize}
            \item Combines strengths of policy gradients and value function approximation.
            \item The Actor updates the policy, while the Critic evaluates actions.
        \end{itemize}
    \end{block}

    \begin{block}{How It Works}
        \begin{itemize}
            \item Actor's policy gradient:
            \begin{equation}
                \nabla J(\theta) \approx \mathbb{E}_{t} \left[ \nabla \log \pi_\theta(a_t | s_t) A(s_t, a_t) \right]
            \end{equation}
            \item Critic's TD error:
            \begin{equation}
                \text{TD Error} = r_t + \gamma V(s_{t+1}) - V(s_t)
            \end{equation}
        \end{itemize}
    \end{block}

    \begin{block}{Advantages and Disadvantages}
        \begin{itemize}
            \item \textbf{Advantages:}
                \begin{itemize}
                    \item Lower variance in gradient estimates.
                    \item Faster convergence due to the Critic's guidance.
                \end{itemize}
            \item \textbf{Disadvantages:}
                \begin{itemize}
                    \item Increased complexity due to the need for tuning both Actor and Critic.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Data Efficiency: Direct optimization of policies without explicit value functions.
            \item Robustness: Adaptability to environmental changes.
            \item Trade-offs: Bias-variance trade-offs across different algorithms affecting applications.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding REINFORCE and Actor-Critic methods is crucial for creating effective RL agents. 
        \begin{itemize}
            \item REINFORCE: straightforward approach.
            \item Actor-Critic: offers enhanced stability and efficiency.
        \end{itemize}
        Next, we will explore the implementation in Python.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementation of Policy Gradient Methods in Python}
    \begin{block}{Overview}
        Policy Gradient Methods optimize policies directly and are key in reinforcement learning. In this slide, we will implement a simple version of the REINFORCE algorithm, providing example code snippets along the way.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Setting Up the Environment}
    To implement a policy gradient algorithm, set up a Python environment with:
    \begin{itemize}
        \item \textbf{NumPy}: for numerical operations
        \item \textbf{Gym}: for simulating environments
        \item \textbf{Matplotlib}: for plotting results
    \end{itemize}
    \begin{lstlisting}[language=python]
!pip install numpy gym matplotlib
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of the REINFORCE Algorithm}
    \textbf{1. Importing Libraries}
    \begin{lstlisting}[language=python]
import numpy as np
import gym
import matplotlib.pyplot as plt
    \end{lstlisting}

    \textbf{2. Defining the Policy Network}
    \begin{lstlisting}[language=python]
class PolicyNetwork:
    def __init__(self, n_states, n_actions):
        self.n_states = n_states
        self.n_actions = n_actions
        self.weights = np.random.rand(n_states, n_actions)

    def forward(self, state):
        state = np.dot(state, self.weights)
        exp = np.exp(state - np.max(state))  # for numerical stability
        return exp / exp.sum(axis=0)  # softmax
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing the REINFORCE Algorithm}
    \textbf{3. Selecting Actions Based on Policy}
    \begin{lstlisting}[language=python]
def select_action(state, policy_network):
    probabilities = policy_network.forward(state)
    return np.random.choice(range(policy_network.n_actions), p=probabilities)
    \end{lstlisting}

    \textbf{4. Implementing REINFORCE}
    \begin{lstlisting}[language=python]
def reinforce(env, policy_network, episodes, gamma=0.99):
    episodes_rewards = []
    
    for episode in range(episodes):
        states, actions, rewards = [], [], []
        state = env.reset()
        done = False
        while not done:
            action = select_action(state, policy_network)
            next_state, reward, done, _ = env.step(action)

            states.append(state)
            actions.append(action)
            rewards.append(reward)
            state = next_state
            
        # Compute cumulative rewards...
        ...
    return episodes_rewards
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Running the Training Loop and Visualization}
    \textbf{5. Running the Training Loop}
    \begin{lstlisting}[language=python]
env = gym.make('CartPole-v1')
policy_net = PolicyNetwork(n_states=env.observation_space.shape[0], n_actions=env.action_space.n)
reward_history = reinforce(env, policy_net, episodes=1000)
    \end{lstlisting}

    \textbf{6. Visualizing Results}
    \begin{lstlisting}[language=python]
plt.plot(reward_history)
plt.title('Total Reward per Episode')
plt.xlabel('Episodes')
plt.ylabel('Total Reward')
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Policy Gradient Principle}: Directly optimizes the policy function rather than the value function.
        \item \textbf{Reward Signal}: Uses cumulative rewards to update the policy.
        \item \textbf{Exploration vs. Exploitation}: Randomly sampling actions encourages exploration, crucial in reinforcement learning.
    \end{itemize}
    This foundational implementation serves as a starting point for understanding and experimenting with more complex policy gradient methods.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations of Policy Gradient Methods}
    \begin{block}{Introduction}
        Policy gradient methods optimize policies directly in reinforcement learning. However, these techniques face several challenges that can hinder their performance and effectiveness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{High Variance in Gradient Estimates}
    \begin{itemize}
        \item \textbf{Definition:} 
            Policy gradient methods estimate the gradient using sampled trajectories, introducing noise and leading to high variance in estimates.
            
        \item \textbf{Impact:} 
            High variance can result in unstable training, making convergence to an optimal policy challenging.
            
        \item \textbf{Example:} 
            If most samples are from extreme states, the gradient will be skewed, causing erratic updates.
            
        \item \textbf{Solution:} 
            Techniques like \textit{baseline subtraction} reduce variance. A common baseline is the average reward, ensuring more stable updates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Sample Efficiency}
    \begin{itemize}
        \item \textbf{Definition:} 
            Sample efficiency is the ability of an algorithm to learn effective policies using minimal data. Policy gradient methods often need many episodes to converge.
            
        \item \textbf{Impact:} 
            High sample inefficiency is a major drawback, especially in costly or time-consuming sampling environments.
            
        \item \textbf{Example:} 
            In robotic training, requiring thousands of episodes can make the algorithm impractical.
            
        \item \textbf{Solution:} 
            Techniques like \textit{truncated importance sampling} and \textit{actor-critic methods} improve sample efficiency by combining policy gradients with value approximations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Challenges}
    \begin{itemize}
        \item \textbf{High Variance:} Leads to unstable training and convergence issues.
        \item \textbf{Sample Inefficiency:} Requires extensive data for effective learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Equations}
    \begin{equation}
        \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_{\theta} \log \pi_\theta(a_t | s_t) R(\tau) \right]
    \end{equation}
    \begin{itemize}
        \item Here, $ \tau $ represents the trajectory, $ R(\tau) $ is the return, and $ \pi_\theta $ is the policy parameterized by $ \theta $.
    \end{itemize}
    
    \begin{equation}
        \hat{R}(\tau) = \sum_{t=0}^{T} \gamma^t r_t
    \end{equation}
    \begin{itemize}
        \item Utilized to weigh returns by the likelihood of the sampled actions under the target policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the challenges associated with policy gradient methods allows practitioners to design better reinforcement learning solutions. By addressing high variance and sample efficiency, we can improve both the stability of training and the ability to learn from fewer samples.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Note to Students}
    As we move forward, keep these challenges in mind when considering real-world applications of policy gradient methods in the next slide.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Policy Gradient Methods}
    \begin{block}{Introduction}
        Policy Gradient methods are reinforcement learning algorithms that optimize the policy by adjusting parameters of a stochastic policy in the direction of expected rewards. They excel in high-dimensional action spaces and are effective in complex decision-making scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications - Part 1}

    \begin{enumerate}
        \item \textbf{Robotics}
        \begin{itemize}
            \item \textit{Example:} Robotic Arm Control
            \item Learning to optimize movements through trial and error to perform tasks like picking and placing.
        \end{itemize}
        
        \item \textbf{Game Playing}
        \begin{itemize}
            \item \textit{Example:} AlphaGo
            \item Used policy gradients to learn strategies in Go, outperforming human experts.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications - Part 2}

    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Natural Language Processing (NLP)}
        \begin{itemize}
            \item \textit{Example:} Text Generation
            \item Optimizing the next word selection to enhance coherence and relevance in generated sentences.
        \end{itemize}

        \item \textbf{Finance \& Trading}
        \begin{itemize}
            \item \textit{Example:} Algorithmic Trading
            \item Agents learn buy/sell strategies based on market conditions, aiming for profit maximization.
        \end{itemize}

        \item \textbf{Healthcare}
        \begin{itemize}
            \item \textit{Example:} Personalized Treatment Plans
            \item Treatment recommendations are adapted based on historical patient data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item \textbf{Direct Optimization:} Focus on policy improvement allows for tackling complex decision spaces.
        \item \textbf{Versatility:} Applicable in various domains like robotics, games, NLP, finance, and healthcare.
        \item \textbf{Sample Complexity:} While advantageous, policy gradient methods may require substantial data to address high variance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Reference}
    The basic update rule for policy gradient methods can be expressed as:
    \begin{equation}
        \theta_{new} = \theta_{old} + \alpha \nabla J(\theta)
    \end{equation}
    where:
    \begin{itemize}
        \item \( \theta \) = parameters of the policy,
        \item \( \alpha \) = step size,
        \item \( J(\theta) \) = expected reward function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations}
    \begin{block}{Introduction}
        Policy gradient methods present ethical challenges that must be addressed as they are integrated into decision-making systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations - Part 1}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item \textbf{Definition}: Inherit biases from training data, leading to unfair outcomes.
                \item \textbf{Example}: Biased historical hiring data may favor certain demographics.
                \item \textbf{Key Point}: Regular audits and diverse datasets minimize bias.
            \end{itemize}
        
        \item \textbf{Transparency and Explainability}
            \begin{itemize}
                \item \textbf{Definition}: Difficulty in interpreting inner workings leads to "black box" concerns.
                \item \textbf{Example}: Lack of rationale in healthcare treatment recommendations erodes trust.
                \item \textbf{Key Point}: Develop interpretable models and clear decision-making explanations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Accountability and Responsibility}
            \begin{itemize}
                \item \textbf{Definition}: Challenges in determining responsibility for AI decisions.
                \item \textbf{Example}: Accountability in failures of self-driving cars—who is liable?
                \item \textbf{Key Point}: Establish guidelines for accountability in AI deployment.
            \end{itemize}

        \item \textbf{Privacy Concerns}
            \begin{itemize}
                \item \textbf{Definition}: Reliance on large datasets may violate personal privacy.
                \item \textbf{Example}: Personalized advertising using personal data without consent.
                \item \textbf{Key Point}: Implement encryption and data protection measures.
            \end{itemize}

        \item \textbf{Autonomy and Job Displacement}
            \begin{itemize}
                \item \textbf{Definition}: Automation threatens job security across sectors.
                \item \textbf{Example}: Policy gradient methods in manufacturing can replace human workers.
                \item \textbf{Key Point}: Develop workforce transition strategies to mitigate impacts.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Summary}
        Addressing ethical considerations is essential for mitigating potential harms in AI deployment. A collaborative approach involving diverse stakeholders will lead to more responsible strategies.
    \end{block}

    \begin{block}{Conclusion}
        Ethical awareness in policy gradient methods is integral for successful societal integration. Advocating for ethical guidelines is crucial for future practitioners.
    \end{block}

    \begin{block}{Further Discussion Questions}
        \begin{itemize}
            \item 1. How can we ensure fairness in AI decision-making?
            \item 2. What steps can developers take to enhance transparency?
            \item 3. Which policies are necessary for accountability in AI systems?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions}
    \begin{block}{Key Takeaways from Policy Gradient Methods}
        \begin{enumerate}
            \item \textbf{Fundamental Principles}:
                \begin{itemize}
                    \item Optimize the policy directly rather than through the value function or Q-values.
                    \item Update policy parameters by estimating the gradient of expected reward with respect to the policy parameters.
                \end{itemize}
            \item \textbf{Advantages}:
                \begin{itemize}
                    \item Handle high-dimensional action spaces.
                    \item Applicable to continuous action spaces.
                \end{itemize}
            \item \textbf{Challenges}:
                \begin{itemize}
                    \item High variance in gradient estimates, often mitigated using techniques like Baseline and GAE.
                    \item Sample inefficiency, requiring significant experience for effective convergence.
                \end{itemize}
            \item \textbf{Common Variants}:
                \begin{itemize}
                    \item REINFORCE: Monte Carlo policy updates.
                    \item Actor-Critic methods: Balance exploration and exploitation.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Research Directions}
    \begin{block}{Exploration of New Frontiers}
        \begin{enumerate}
            \item \textbf{Variance Reduction Techniques}:
                \begin{itemize}
                    \item Minimize variance in policy gradient estimates.
                \end{itemize}
            \item \textbf{Combining Policy Gradients}:
                \begin{itemize}
                    \item Investigate hybrid approaches with value-based methods.
                \end{itemize}
            \item \textbf{Exploration Strategies}:
                \begin{itemize}
                    \item Develop sophisticated strategies for sparse-reward environments.
                \end{itemize}
            \item \textbf{Robustness and Generalization}:
                \begin{itemize}
                    \item Enhance robustness to environmental changes and adversarial settings.
                \end{itemize}
            \item \textbf{Ethical and Societal Implications}:
                \begin{itemize}
                    \item Consider ethical ramifications in sensitive areas like healthcare and finance.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Code Snippet}
    \begin{block}{Policy Gradient Update}
        \begin{equation}
            \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T} \nabla \log \pi_{\theta}(a_t | s_t) \cdot R_t \right]
        \end{equation}
        Where \( \tau \) is a trajectory, \( R_t \) is the return, and \( \theta \) are the parameters of the policy.
    \end{block}
    
    \begin{block}{Code Example}
        \begin{lstlisting}[language=Python]
for episode in range(num_episodes):
    states, actions, rewards = collect_trajectory(env, policy)
    returns = compute_returns(rewards)
    for t in range(len(returns)):
        loss += -log(policy(states[t], actions[t])) * returns[t]
update_policy(loss)
        \end{lstlisting}
    \end{block}
\end{frame}


\end{document}