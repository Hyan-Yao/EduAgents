\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}

% Title Page Information
\title[Week 2: MDPs]{Week 2: Markov Decision Processes}
\author[John Smith]{John Smith, Ph.D.}
\institute[University Name]{Department of Computer Science \\ University Name \\ Email: email@university.edu}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Overview}
    \begin{block}{Introduction}
        Markov Decision Processes (MDPs) form the foundational framework for modeling decision-making scenarios in reinforcement learning. MDPs provide a mathematical representation of sequential decision problems where outcomes are partly random and partly under the control of a decision-maker.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Definition}
    \begin{block}{What is a Markov Decision Process?}
        An MDP is characterized by a tuple \( (S, A, P, R, \gamma) \):
        \begin{itemize}
            \item \textbf{S:} A set of states representing all possible situations.
            \item \textbf{A:} A set of actions available to the decision-maker.
            \item \textbf{P:} The state transition probability function \( P(s' | s, a) \).
            \item \textbf{R:} The reward function \( R(s, a) \).
            \item \textbf{$\gamma$ (gamma):} The discount factor \( (0 \leq \gamma < 1) \).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Significance}
    \begin{block}{Significance in Reinforcement Learning}
        \begin{itemize}
            \item MDPs provide a formal framework for defining the environment in which an agent operates.
            \item They help in determining the best policy to maximize cumulative rewards.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Sequential Decision Making:} Actions affect future states and rewards.
            \item \textbf{Stochastic vs. Deterministic:} Suitable for applications with uncertainty.
            \item \textbf{Policy Optimization:} Finding the optimal policy is crucial in MDPs.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes - Example}
    \begin{block}{Example: Simple Grid World MDP}
        Imagine a grid where an agent can move up, down, left, or right:
        \begin{itemize}
            \item \textbf{States (S):} Each cell in the grid represents a state.
            \item \textbf{Actions (A):} Moving in one of the four directions.
            \item \textbf{Rewards (R):} +1 for reaching the goal state and -1 for hitting a wall. 
            \item \textbf{Transitions (P):} Remain in the same state if moving out of bounds.
        \end{itemize}
    \end{block}
    \begin{block}{Grid World Layout}
        \begin{verbatim}
        |    |    | G  |
        |    | W  |    |
        |    |    |    |
        \end{verbatim}
        Here, "G" represents the goal and "W" represents a wall.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Understanding MDPs is crucial for developing efficient algorithms in reinforcement learning. By mastering MDP concepts, learners can tackle complex decision-making problems in a structured manner.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Markov Decision Process?}
    \begin{block}{Definition}
        A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under control.
    \end{block}
    \begin{itemize}
        \item MDPs are fundamental in reinforcement learning.
        \item They depend on the current state of the system for decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of a Markov Decision Process}
    \begin{enumerate}
        \item \textbf{States (S)}
        \begin{itemize}
            \item Represents configurations of the environment.
            \item Example: Unique arrangements in a chess game.
        \end{itemize}

        \item \textbf{Actions (A)}
        \begin{itemize}
            \item Choices available at each state influencing the next state.
            \item Example: Legal chess moves like moving a knight.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Rewards (R)}
        \begin{itemize}
            \item Feedback received after transitioning from one state to another due to an action.
            \item Example: +1 for capturing a piece, -1 for losing one.
        \end{itemize}

        \item \textbf{Transitions (P)}
        \begin{itemize}
            \item Describes probabilities of moving from one state to another due to an action.
            \item Example: Probabilities of board positions after rolling a die.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points}
        \begin{itemize}
            \item MDPs formalize decision-making in uncertain environments.
            \item Complete framework: states, actions, rewards, and transitions.
            \item Essential for developing algorithms to solve MDPs.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States in Markov Decision Processes (MDPs)}
    \begin{block}{Understanding States in MDPs}
        \begin{itemize}
            \item \textbf{Definition of State:} A *state* represents a specific situation an agent finds itself in, containing all relevant information for decision-making.
            \item \textbf{State Space (S):} The set of all possible states, denoted as \( S \). Its size and complexity affect the difficulty of solving the MDP.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of State Representation}
    \begin{block}{Capturing Complexity}
        A well-defined state representation ensures that an agent makes informed decisions. Poor representation can lead to suboptimal strategies.
    \end{block}
    
    \begin{block}{Examples of State Representation}
        \begin{itemize}
            \item \textbf{Grid World:} The state can be represented by coordinates (x, y).
                \begin{itemize}
                    \item Example: \( s = (1, 2) \)
                \end{itemize}
            \item \textbf{Game State in Chess:} Needs to represent all piece positions and game history.
                \begin{itemize}
                    \item Example: A string or matrix representation of the board.
                \end{itemize}
            \item \textbf{Discrete vs. Continuous States:} States may be finite (discrete) or infinite (continuous), which requires more complex techniques.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Bellman Equation}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Decisions are based on the agent's current state.
            \item A complete MDP must define state relationships through actions and rewards.
            \item State representation affects learning efficiency and policy optimization.
        \end{itemize}
    \end{block}

    \begin{block}{Mathematical Formula}
        A common representation in MDPs is the Bellman equation:
        \begin{equation}
            V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( V(s) \): value function for state \( s \)
            \item \( R(s, a) \): expected reward after action \( a \) in state \( s \)
            \item \( P(s' | s, a) \): state transition probability
            \item \( \gamma \): discount factor (0 â‰¤ \( \gamma \) < 1)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions in Markov Decision Processes (MDPs)}
    
    \begin{block}{Overview of Actions}
        In MDPs, actions are choices available to an agent that influence its transition from one state to another. Each action can lead to different outcomes characterized by the immediate transition and associated probabilities.
    \end{block}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definition of Actions}
    
    \begin{enumerate}
        \item \textbf{Actions:} The set of decisions or maneuvers an agent can perform in a given state, denoted by \( A \).
        \item \textbf{Action Space:} The collection of all possible actions, which can be:
            \begin{itemize}
                \item \textbf{Discrete:} e.g., move left, right
                \item \textbf{Continuous:} e.g., adjusting a speed
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Actions on State Transitions}
    
    Actions directly affect state transitions, described by the transition function:
    
    \begin{block}{Transition Function}
        \( P(s' | s, a) \) describes the probability of moving to state \( s' \) from state \( s \) after taking action \( a \).
    \end{block}

    \begin{block}{Example Scenario}
        Consider a grid world where an agent can move in four directions:
        \begin{itemize}
            \item \textbf{States:} Positions on the grid represented as (x, y)
            \item \textbf{Actions:} 
                \begin{itemize}
                    \item Up (U)
                    \item Down (D)
                    \item Left (L)
                    \item Right (R)
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Dynamics and Key Points}
    
    \begin{itemize}
        \item If the agent is at (1, 1) and takes Up (U), the next state is (1, 2) with probability 1 (no obstacles).
        \item If Down (D) is taken, next state could be (1, 0) with probability 0.8, or remain at (1, 1) with probability 0.2 due to uncertainty.
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Deterministic vs. Stochastic Actions:} 
                \begin{itemize}
                    \item Deterministic actions lead to predictable transitions.
                    \item Stochastic actions introduce randomness.
                \end{itemize}
            \item \textbf{Policy:} Defines action choice in each state, denoted as \( \pi(a | s) \) for probability of taking action \( a \) in state \( s \).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation}
    
    \begin{block}{Effect of Actions on State Transitions}
        The effect of actions can be encapsulated as follows:
        \begin{equation}
            P(s' | s, a) = \text{Probability of reaching state } s' \text{ from state } s \text{ after action } a
        \end{equation}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding actions in MDPs is crucial for effective policies guiding decision-making in uncertain environments. Actions define exploration and influence long-term outcomes and rewards.
    \end{block}
    
    \begin{block}{Next Topic}
        We will now explore rewards, detailing their influence on decision-making and evaluating the desirability of states.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in Markov Decision Processes}
    \begin{block}{Introduction to Rewards}
        \begin{itemize}
            \item \textbf{Definition}: A reward is a scalar value assigned to each state (or state-action pair) that quantifies the immediate benefit derived from that state or action.
            \item \textbf{Role}: Rewards guide decision-making and influence which actions are chosen based on state desirability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Influence on Decision Making}
    \begin{block}{Objective}
        The goal of an agent within an MDP is to maximize cumulative rewards over time.
    \end{block}
    \begin{itemize}
        \item \textbf{Immediate Reward (R)}: The reward received after taking an action in a state.
        \item \textbf{Expected Cumulative Reward}:
        \begin{equation}
            R_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
        \end{equation}
        where $\gamma$ (0 â‰¤ $\gamma$ < 1) is the discount factor that reflects the present value of future rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Desirability of States}
    \begin{itemize}
        \item \textbf{Value Functions}: Rewards compute value functions, estimating expected returns.
        \begin{equation}
            V(s) = \mathbb{E}[R_t | s_t = s]
        \end{equation}
        \item \textbf{Action Value Function (Q)}: Measures expected cumulative reward for taking action $a$ in state $s$:
        \begin{equation}
            Q(s, a) = \mathbb{E}[R_t | s_t = s, a_t = a]
        \end{equation}
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Rewards shape agents' understanding of preferable states/actions.
            \item Balancing immediate vs. long-term rewards is essential for effective decision-making.
            \item Proper reward structure is vital to avoid unintended behaviors in agents.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transitions - Understanding State Transitions in MDPs}
    \begin{block}{What are Transitions?}
        - Transitions describe how an agent moves from one state to another based on a chosen action.
        - They define the dynamics of the environment, influencing decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transitions - Key Concepts}
    \begin{itemize}
        \item \textbf{States:} 
            - Represent the environment at a given time. 
            - Example: Different arrangements of chess pieces represent different states.

        \item \textbf{Actions:} 
            - Choices available to the agent at a state. 
            - Example: Moving a pawn or capturing a piece in chess.

        \item \textbf{Transition Probabilities:} 
            - Quantify the likelihood of moving from one state to another, given an action.
            - Denoted as \( P(s'|s, a) \) where:
                \begin{itemize}
                    \item \( s \): current state
                    \item \( a \): action taken
                    \item \( s' \): next state
                \end{itemize}
            - Probabilities sum to 1 for all possible next states \( s' \) from state \( s \) when action \( a \) is taken.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transitions - Example and Key Points}
    \begin{block}{Formula}
        \begin{equation}
            P(s'|s, a) = \text{Probability of transitioning to state } s' \text{ from state } s \text{ after action } a
        \end{equation}
    \end{block}
    
    \begin{block}{Example: Gridworld Scenario}
        - In a grid world, if an agent is in state \( (2, 3) \) and chooses to move right:
            \begin{itemize}
                \item \( P((2, 4)|(2, 3), \text{right}) = 0.8 \) (likely to move right)
                \item \( P((2, 3)|(2, 3), \text{right}) = 0.1 \) (stays due to obstacles)
                \item \( P((2, 2)|(2, 3), \text{right}) = 0.1 \) (accidental move back)
            \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Transition dynamics inform optimal strategies for navigating the state space.
            \item Each transition influences an agent's state, affecting rewards.
            \item Distinction between deterministic (one outcome) and stochastic (multiple outcomes with probabilities) scenarios.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Overview}
    \begin{block}{Understanding Policies in MDPs}
        A policy in the context of Markov Decision Processes (MDPs) is a strategy that defines the actions an agent should take in various states of the environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Types of Policies}
    \begin{itemize}
        \item Policies can be classified into two types:
            \begin{itemize}
                \item \textbf{Deterministic Policies}
                \item \textbf{Stochastic Policies}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Deterministic Policies}
    \begin{block}{Deterministic Policies}
        A deterministic policy maps each state to a specific action:
        \[
        \pi: S \rightarrow A
        \]
        \begin{itemize}
            \item If an agent is in state \( s \), it always takes action \( a = \pi(s) \).
            \item \textbf{Example:}
            \begin{itemize}
                \item State: Position of a robot, e.g. (2, 3)
                \item Action: Move up, down, left, or right
                \item Policy: Always move up to (1, 3) when at (2, 3).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Stochastic Policies}
    \begin{block}{Stochastic Policies}
        A stochastic policy defines a probability distribution over actions for each state:
        \[
        \pi(a|s) = P(A_t = a | S_t = s)
        \]
        \begin{itemize}
            \item If an agent is in state \( s \), it may choose action \( a \) with a certain probability.
            \item \textbf{Example:}
            \begin{itemize}
                \item State: Position of a robot
                \item Action: Move up, down, left, or right
                \item Policy: Move up with a probability of 0.7 and down with a probability of 0.3 when at (2, 3).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Definition - Key Points}
    \begin{itemize}
        \item \textbf{Importance of Policies:} Central to decision-making in MDPs.
        \item \textbf{Choice of Policy Type:} Depends on the problem and environmental uncertainty.
        \item \textbf{Learning Policies:} Agents often aim to find an optimal policy to maximize cumulative reward over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Overview}
    In Reinforcement Learning (RL), \textbf{Value Functions} are fundamental concepts that help evaluate the desirability of states or actions in an environment. They provide a mechanism to calculate how good it is to be in a given state or to perform a certain action at a particular state.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - State Value Function (V)}
    \begin{itemize}
        \item \textbf{Definition}: The state value function, denoted as \( V(s) \), measures the expected return from being in a state \( s \) and following a particular policy \( \pi \).
        
        \item \textbf{Formula}:
        \begin{equation}
            V(s) = \mathbb{E}_\pi \left[ R_t \mid S_t = s \right] = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s', r} P(s', r | s, a) [r + \gamma V(s')]
        \end{equation}
        where:
        \begin{itemize}
            \item \( R_t \) is the total reward at time \( t \),
            \item \( \gamma \) is the discount factor (0 < \(\gamma\) < 1),
            \item \( P(s', r | s, a) \) is the probability of transitioning to state \( s' \) and receiving reward \( r \) after taking action \( a \) in state \( s \).
        \end{itemize}
        
        \item \textbf{Example}: In a grid-world scenario, the value of an empty cell might be high if it leads to larger rewards in subsequent states (like reaching the goal) and low if it leads to traps.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Action Value Function (Q)}
    \begin{itemize}
        \item \textbf{Definition}: The action value function, denoted as \( Q(s, a) \), predicts the expected return of taking action \( a \) in state \( s \) and then following policy \( \pi \).

        \item \textbf{Formula}:
        \begin{equation}
            Q(s, a) = \mathbb{E}_\pi \left[ R_t \mid S_t = s, A_t = a \right] = \sum_{s', r} P(s', r | s, a) [r + \gamma V(s')]
        \end{equation}
        
        \item \textbf{Example}: Continuing with the grid-world, if the robot can take actions like 'move up' or 'move down', the \( Q \) value helps determine which action maximizes expected rewards based on previous experiences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Roles of Value Functions in RL}
    \begin{itemize}
        \item Value functions help in \textbf{decision-making}: By comparing \( V(s) \) or \( Q(s, a) \) for different states/actions, the agent can determine the optimal policy \( \pi^* \).
        
        \item They are integral to algorithms like \textbf{Q-learning} and \textbf{Value Iteration}: Both methods aim to find the optimal value functions, which in turn dictate the best policy for the agent.
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Value functions are indispensable in assessing long-term successes of policies in reinforcement learning.
            \item Understanding both state and action value functions is crucial for designing effective learning algorithms.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        In summary, state and action value functions provide a critical evaluation tool for determining the effectiveness of policies in Markov Decision Processes (MDPs). Mastering these concepts paves the way for deeper understanding and application of various RL algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Introduction}
    \begin{block}{Introduction to Bellman Equations}
        Bellman Equations are fundamental to understanding and solving Markov Decision Processes (MDPs). They express the relationship between the value of a state or action and the values of other states or actions that derive from it, forming the backbone of dynamic programming in reinforcement learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Key Concepts}
    \begin{enumerate}
        \item \textbf{State Value Function (V(s))}:
        \begin{itemize}
            \item Represents the expected return starting from state \(s\) and following policy \(\pi\).
            \item \textbf{Bellman Equation}:
            \begin{equation}
                V(s) = \sum_{a \in A} \pi(a|s) \sum_{s', r} P(s', r | s, a) \left[ r + \gamma V(s') \right]
            \end{equation}
            \item Where:
            \begin{itemize}
                \item \(\pi(a|s)\) is the policy probability of action \(a\) in state \(s\).
                \item \(P(s', r | s, a)\) denotes the transition probabilities to state \(s'\) with reward \(r\).
                \item \(\gamma\) is the discount factor.
            \end{itemize}
        \end{itemize}

        \item \textbf{Action Value Function (Q(s, a))}:
        \begin{itemize}
            \item Estimates the expected return by taking action \(a\) in state \(s\), then following policy \(\pi\).
            \item \textbf{Bellman Equation}:
            \begin{equation}
                Q(s, a) = \sum_{s', r} P(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q(s', a') \right]
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Bellman Equations}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item \textbf{Optimal Policy Derivation}: Useful for deriving optimal policies and value functions.
            \item \textbf{Dynamic Programming}: Crucial for methods like Policy Iteration and Value Iteration, allowing computation of optimal policies.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Consider states \( S = \{A, B\} \) and actions \( A = \{a_1, a_2\} \):
        \begin{itemize}
            \item Policy: In State \(A\), \(\pi(a_1|A) = 1\), \(\pi(a_2|A) = 0\); In State \(B\), \(\pi(a_1|B) = 0\), \(\pi(a_2|B) = 1\).
            \item Can compute \(V(A)\) and \(V(B)\) using Bellman Equations, obtaining insights on expected long-term benefits.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Dynamic Programming in MDPs}
  \textbf{Overview:} Dynamic programming techniques for solving MDPs include:
  \begin{itemize}
    \item Policy Evaluation
    \item Policy Improvement
    \item Value Iteration
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Introduction}
  Dynamic Programming (DP) offers a systematic approach to solving Markov Decision Processes (MDPs). MDPs model decision-making scenarios where outcomes are partly random and partly under the control of a decision maker. This slide outlines the key DP techniques utilized in optimizing MDPs.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Policy Evaluation}
  \begin{block}{Definition}
    Computes the value function for a given policy, measuring expected return when following that policy.
  \end{block}
  
  \begin{equation}
  V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')]
  \end{equation}
  
  \begin{itemize}
    \item Where:
    \begin{itemize}
      \item $V^\pi(s)$ is the value of state $s$ under policy $\pi$.
      \item $\gamma$ is the discount factor (0 < $\gamma$ < 1).
    \end{itemize}
    \item Iterative process using this formula until values stabilize.
    \item \textbf{Example:} If a robot follows a policy that favors moving right, the value function indicates the policy's effectiveness in terms of future rewards.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Policy Improvement}
  \begin{block}{Definition}
    Updates the policy by choosing actions that maximize expected value from the value function.
  \end{block}

  \begin{equation}
  \pi'(s) = \arg\max_{a \in \mathcal{A}} \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]
  \end{equation}

  \begin{itemize}
    \item Process: Evaluate current policy, assess actions to form a better one.
    \item \textbf{Example:} If moving left yields better returns, adjust the policy to favor leftward movements.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Value Iteration}
  \begin{block}{Definition}
    Combines policy evaluation and improvement into a single iterative process.
  \end{block}

  \begin{equation}
  V_{k+1}(s) = \max_{a \in \mathcal{A}} \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V_k(s')]
  \end{equation}

  \begin{itemize}
    \item Update the value function until the change is below a threshold, where $k$ is the iteration number.
    \item \textbf{Example:} For a given state, evaluate actions and update value by selecting the action yielding the highest value.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Iterative Nature:} All techniques iterate to refine estimates of value function and policy.
    \item \textbf{Convergence:} Under bounded rewards and discount factor, these methods converge to optimal value function and policy.
    \item \textbf{Applicability:} These DP techniques are foundational in reinforcement learning.
  \end{itemize}
  
  By understanding these key dynamic programming techniques, we can efficiently solve MDPs and apply these concepts to various real-world scenarios, such as robotics and financial modeling.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs}
    \begin{block}{Overview of Markov Decision Processes (MDPs)}
        Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making situations where outcomes are partly determined by chance and partly by actions of a decision-maker. MDPs consist of:
        \begin{itemize}
            \item States
            \item Actions
            \item Transitions
            \item Rewards
            \item Policies
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of MDPs}
    \begin{enumerate}
        \item \textbf{Robotics}
        \begin{itemize}
            \item \textbf{Example: Autonomous Navigation}
            \begin{itemize}
                \item Robots navigate through environments (avoiding obstacles, reaching targets).
                \item Define states (positions), actions (movement), and model state transitions (probabilistic outcomes).
                \item \textbf{Outcome:} Robots learn optimal navigation policies.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Game AI}
        \begin{itemize}
            \item \textbf{Example: Character Behavior in Video Games}
                \begin{itemize}
                    \item Game AI makes decisions based on current states and player influence.
                    \item \textbf{Key Application:} NPCs decide actions (attack, flee, seek power-ups) using MDPs.
                    \item \textbf{Outcome:} Dynamic and responsive game experiences.
                \end{itemize}
        \end{itemize}
        
        \item \textbf{Finance}
        \begin{itemize}
            \item \textbf{Example: Portfolio Management}
            \begin{itemize}
                \item Investors allocate assets (stocks, bonds, etc.).
                \item MDPs define states (portfolio value), actions (buy/sell), and predict rewards (expected ROI).
                \item \textbf{Outcome:} Developing optimal investment strategies.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Flexibility:} MDPs model diverse decision-making scenarios across fields.
            \item \textbf{Efficiency in Learning:} Use dynamic programming and reinforcement learning for optimal strategies.
            \item \textbf{Uncertainty Handling:} MDPs account for probabilistic outcomes in environments.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Markov Decision Processes are foundational for solving complex decision-making problems in various domains, enabling intelligent systems to learn and adapt. Understanding applications highlights their relevance and impact across different fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Notation and Objective}
    \begin{block}{MDP Notation}
        Let \( S \) be the set of states, \( A \) be the set of actions, \( P(s'|s, a) \) be the state transition probabilities, and \( R(s, a) \) be the reward function. 
    \end{block}
    \begin{equation}
        \text{Objective: } \pi: S \rightarrow A \text{ that maximizes } E\left[\sum_{t=0}^\infty \gamma^t R(S_t, A_t)\right]
    \end{equation}
    where \( \gamma \) is the discount factor.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Overview}
    \begin{block}{Recap of Markov Decision Processes (MDPs)}
        \begin{itemize}
            \item MDPs provide a formal framework for defining environments in reinforcement learning.
            \item They consist of states, actions, transition probabilities, rewards, and discount factors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Concepts of MDPs}
    \begin{enumerate}
        \item \textbf{States (S)}: Possible states of the agent (e.g., positions).
        \item \textbf{Actions (A)}: Possible actions for the agent (e.g., move left, right).
        \item \textbf{Transition Probabilities (P)}: Probabilities of state changes after actions, represented as \( P(s'|s,a) \).
        \item \textbf{Rewards (R)}: Immediate payoffs received after state transitions.
        \item \textbf{Discount Factor ($\gamma$)}: A factor (0 $\leq$ $\gamma$ < 1) indicating the importance of future rewards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Role in Reinforcement Learning}
    \begin{block}{Decision Making with MDPs}
        MDPs enable agents to:
        \begin{itemize}
            \item Balance exploration and exploitation in decision-making.
            \item Optimize policies defined as mappings from states to actions.
        \end{itemize}
    \end{block}

    \begin{block}{Value Functions}
        \begin{itemize}
            \item State Value Function ($V$): 
            \[
            V^{\pi}(s) = E_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_t | s_0=s \right]
            \]
            \item Action Value Function ($Q$): 
            \[
            Q^{\pi}(s, a) = E_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_t | s_0=s, a_0=a \right]
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Final Thoughts}
    \begin{block}{Essential Takeaway}
        \begin{itemize}
            \item MDPs are foundational to reinforcement learning, facilitating the development of strategies for intelligent decision-making.
            \item Mastering MDPs provides the groundwork for understanding complex algorithms like Q-learning and deep reinforcement learning.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}