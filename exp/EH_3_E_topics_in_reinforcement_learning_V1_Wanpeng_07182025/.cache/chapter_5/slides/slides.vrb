\frametitle{Q-learning Update Rule}
    \begin{block}{Q-learning Update Rule}
        The core of the Q-learning algorithm is its update rule, which updates the Q-value of a state-action pair based on the reward received and the maximum future rewards obtainable from the next state. The update rule is expressed mathematically as:
        \[
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_a Q(s', a) - Q(s, a) \right]
        \]
    \end{block}

    \begin{itemize}
        \item \textbf{Q(s, a):} Current estimate of the Q-value for taking action \(a\) in state \(s\).
        \item \(\alpha:\) Learning rate (0 < \(\alpha\) ≤ 1), determining how much new information overrides old information.
        \item \(r:\) Immediate reward received after taking action \(a\) in state \(s\).
        \item \(\gamma:\) Discount factor (0 ≤ \(\gamma\) < 1), which determines the importance of future rewards.
        \item \(s':\) The next state reached after taking action \(a\).
        \item \(\max_a Q(s', a):\) The maximum predicted Q-value for the next state \(s'\).
    \end{itemize}
