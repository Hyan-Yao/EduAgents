\frametitle{Core Concepts and Formulations}

    \textbf{Exploration vs. Exploitation:} A core challenge in RL is balancing exploring new actions (exploration) with using known actions that yield high rewards (exploitation).

    \textbf{Mathematical Formulation:}
    \begin{block}{Expected Cumulative Reward}
        \begin{equation}
        R = \sum_{t=0}^{\infty} \gamma^t r_t
        \end{equation}
        where $R$ is the expected cumulative reward, $r_t$ is the reward at time $t$, and $\gamma$ (0 â‰¤ $\gamma$ < 1) is the discount factor.
    \end{block}

    \textbf{Example: Q-Learning Algorithm}
    \begin{equation}
    Q(s, a) \gets Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
    \end{equation}
    where:
    \begin{itemize}
        \item $Q(s, a)$: current Q-value for action $a$ in state $s$
        \item $\alpha$: learning rate
        \item $r$: received reward after taking action $a$
        \item $s'$: new state after action $a$
    \end{itemize}
