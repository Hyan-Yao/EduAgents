\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

\title[Introduction to Reinforcement Learning]{Week 1: Introduction to Reinforcement Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{Department of Computer Science \\ University Name \\ Email: email@university.edu \\ Website: www.university.edu}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Reinforcement Learning}
    
    \textbf{Definition:} Reinforcement Learning (RL) is a subfield of artificial intelligence (AI) focused on how agents should take actions in an environment to maximize cumulative rewards. Unlike supervised learning, RL systems learn through the consequences of their actions.
    
    \textbf{Key Concepts:}
    \begin{itemize}
        \item \textbf{Agent:} The learner or decision-maker.
        \item \textbf{Environment:} Everything the agent interacts with and seeks to influence.
        \item \textbf{Action:} Choices made by the agent to affect the state of the environment.
        \item \textbf{State:} A snapshot of the environment at a given time.
        \item \textbf{Reward:} A feedback signal received after an action, indicating the success in achieving goals.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Artificial Intelligence}

    \begin{enumerate}
        \item \textbf{Decision-Making:} RL empowers machines to learn optimal strategies for decision-making through experience.
        
        \item \textbf{Adaptability:} RL can adjust to changing environments, making it powerful for dynamic tasks in complex scenarios (e.g., robotics, self-driving cars).
        
        \item \textbf{Applications:}
        \begin{itemize}
            \item \textbf{Games:} Training AI in Chess, Go, and video games.
            \item \textbf{Robotics:} Efficient movement patterns and task adaptation.
            \item \textbf{Healthcare:} Optimizing treatment protocols by learning from patient responses.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts and Formulations}
    
    \textbf{Exploration vs. Exploitation:} A core challenge in RL is balancing exploring new actions (exploration) with using known actions that yield high rewards (exploitation).
    
    \textbf{Mathematical Formulation:}
    \begin{block}{Expected Cumulative Reward}
        \begin{equation}
        R = \sum_{t=0}^{\infty} \gamma^t r_t
        \end{equation}
        where $R$ is the expected cumulative reward, $r_t$ is the reward at time $t$, and $\gamma$ (0 ≤ $\gamma$ < 1) is the discount factor.
    \end{block}

    \textbf{Example: Q-Learning Algorithm}
    \begin{equation}
    Q(s, a) \gets Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
    \end{equation}
    where:
    \begin{itemize}
        \item $Q(s, a)$: current Q-value for action $a$ in state $s$
        \item $\alpha$: learning rate
        \item $r$: received reward after taking action $a$
        \item $s'$: new state after action $a$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Presentation Overview}
  \tableofcontents
\end{frame}

\begin{frame}[fragile]{History of Reinforcement Learning - Introduction}
  \begin{block}{Evolution of Reinforcement Learning}
    Reinforcement Learning (RL) is a branch of machine learning focused on how agents should take actions in an environment to maximize cumulative reward. The development of RL has been influenced by:
  \end{block}
  \begin{itemize}
    \item Psychology
    \item Neuroscience
    \item Computer Science
    \item Behavioral Economics
  \end{itemize}
  \begin{block}{Overview}
    Below, we chronologically outline key milestones and contributors in RL's history.
  \end{block}
\end{frame}

\begin{frame}[fragile]{History of Reinforcement Learning - Key Milestones}
  \begin{enumerate}
    \item \textbf{1940s - Early Roots in Psychology}
      \begin{itemize}
        \item B.F. Skinner's operant conditioning concepts laid groundwork for learning through interaction.
      \end{itemize}

    \item \textbf{1950s - Theoretical Foundations}
      \begin{itemize}
        \item Herbert A. Simon and Allen Newell develop decision-making models in AI.
        \item Richard Bellman's Bellman Equation introduces dynamic programming.
      \end{itemize}

    \item \textbf{1970s - Key Learning Algorithms}
      \begin{itemize}
        \item Sutton and Barto develop Temporal-Difference (TD) learning.
        \item Chris Watkins introduces Q-Learning in 1989.
      \end{itemize}

    \item \textbf{1990s - Breakthrough Applications}
      \begin{itemize}
        \item TD-Gammon by Gerald Tesauro showcases RL in backgammon.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{History of Reinforcement Learning - Recent Trends}
  \begin{enumerate}[start=5]
    \item \textbf{2000s - Rise of Combined Techniques}
      \begin{itemize}
        \item Integration of deep learning with RL leads to Deep Reinforcement Learning.
        \item DeepMind's DQN achieves human-level performance on Atari games in 2013.
      \end{itemize}

    \item \textbf{2016 onwards - Modern Innovations and Applications}
      \begin{itemize}
        \item DeepMind's AlphaGo defeats world champion Go players, demonstrating RL's power.
        \item Applications span diverse fields: healthcare, finance, autonomous vehicles.
      \end{itemize}
  \end{enumerate}

  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Interdisciplinary roots from psychology and economics.
      \item Evolution from basic algorithms to complex models like DQN.
      \item Profound real-world impact across multiple industries.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]{Conclusion and Next Steps}
  \begin{block}{Conclusion}
    RL has evolved from foundational psychological theories to sophisticated algorithms with impactful applications. This history contextualizes current practices and innovations.
  \end{block}
  
  \begin{block}{Next Steps}
    In the following slide, we will explore practical applications of Reinforcement Learning across different industries, illustrating real-world problem-solving capabilities.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    % Brief overview of reinforcement learning applications
    Reinforcement Learning (RL) has emerged as a powerful framework for decision-making with practical applications across various industries. Its ability to learn optimal policies through interactions with dynamic environments makes it indispensable in fields like:
    \begin{itemize}
        \item Robotics
        \item Gaming
        \item Finance
        \item Healthcare
        \item Energy Systems
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in Robotics}
    \begin{block}{Application: Autonomous Navigation}
        \begin{itemize}
            \item **Explanation:** RL algorithms enable robots to learn navigation tasks by receiving rewards for successfully reaching a destination or completing a specified task.
            \item **Example:** A drone learning to navigate through a series of obstacles while maximizing its flight efficiency.
            \item **Key Point:** Robots can adapt to new environments and improve performance over time, essential for real-world applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in Gaming and Finance}
    \begin{block}{Gaming: Game AI}
        \begin{itemize}
            \item **Explanation:** RL is widely used in training game agents to play complex games, where agents learn optimal strategies through trial and error.
            \item **Example:** DeepMind's AlphaGo, which mastered the game of Go, beating world champions by learning from millions of simulated games.
            \item **Key Point:** RL's capacity for strategic learning demonstrates its potential in competitive environments.
        \end{itemize}
    \end{block}
    
    \begin{block}{Finance: Algorithmic Trading}
        \begin{itemize}
            \item **Explanation:** Many financial institutions leverage RL to optimize trading strategies by predicting market movements and maximizing profits.
            \item **Example:** An RL agent learns to buy or sell stocks based on historical price data and real-time signals to enhance trading outcomes.
            \item **Key Point:** RL helps financial analysts make more informed decisions in volatile markets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in Healthcare and Energy}
    \begin{block}{Healthcare: Treatment Personalization}
        \begin{itemize}
            \item **Explanation:** RL can help customize treatment plans by learning what interventions yield the best patient outcomes over time.
            \item **Example:** A personalized dosage regimen for patients where the RL model adjusts treatments based on individual responses.
            \item **Key Point:** Personalized medicine can improve patient care and lead to better health outcomes.
        \end{itemize}
    \end{block}
    
    \begin{block}{Energy Systems: Smart Grid Management}
        \begin{itemize}
            \item **Explanation:** RL can optimize energy distribution and consumption in smart grids, balancing supply and demand dynamically.
            \item **Example:** A smart grid controller using RL to manage electricity flow and storage based on usage patterns.
            \item **Key Point:** Enhancements in energy efficiency contribute to sustainability goals.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The versatility of Reinforcement Learning allows it to adapt to various domains, often leading to improved efficiency and innovation. Understanding these applications sets the foundation for exploring core RL concepts in the following slides.
\end{frame}

\begin{frame}[fragile]
    \frametitle{References}
    \begin{enumerate}
        \item Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." \textit{Nature}.
        \item Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." \textit{Nature}.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Suggested Code Snippet}
    \begin{lstlisting}[language=Python]
initialize Q(s,a) arbitrarily
repeat (for each episode):
    initialize s
    repeat (for each step of the episode):
        choose a from s using policy derived from Q (e.g., ε-greedy)
        take action a, observe r, s'
        Q(s, a) ← Q(s, a) + α[r + γ max Q(s', a') - Q(s, a)]
        s ← s'
    until s is terminal
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Reinforcement Learning - Overview}
    \begin{block}{Introduction}
        This presentation covers the core concepts of Reinforcement Learning including:
        \begin{itemize}
            \item Agents
            \item Environments
            \item States
            \item Actions
            \item Rewards
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts - Agent and Environment}
    \begin{enumerate}
        \item \textbf{Agent}
        \begin{itemize}
            \item \textit{Definition:} The learner or decision-maker that interacts with the environment to achieve specific goals.
            \item \textit{Example:} A self-driving car navigating based on its surroundings.
        \end{itemize}
        
        \item \textbf{Environment}
        \begin{itemize}
            \item \textit{Definition:} The context in which the agent operates and makes decisions.
            \item \textit{Example:} In a self-driving car scenario, the environment includes vehicles, pedestrians, traffic signals, and the road.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts - State, Action, and Reward}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{State}
        \begin{itemize}
            \item \textit{Definition:} A specific situation or configuration of the environment capturing relevant information for decision-making.
            \item \textit{Example:} A particular arrangement of pieces on a chess board.
        \end{itemize}
        
        \item \textbf{Action}
        \begin{itemize}
            \item \textit{Definition:} A decision made by the agent that changes the state of the environment (either discrete or continuous).
            \item \textit{Example:} Pressing a button to jump in a video game.
        \end{itemize}
        
        \item \textbf{Reward}
        \begin{itemize}
            \item \textit{Definition:} Feedback received by the agent after taking an action, indicating success or failure.
            \item \textit{Example:} Scoring points after completing an objective in a game.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts - Cumulative Reward}
    \begin{block}{Interactions and Learning Objective}
        The agent continually interacts with the environment, transitioning through states and actions to maximize cumulative rewards. The objective is to learn a policy that determines the best actions in varying states.
    \end{block}
    
    \begin{equation}
        R = r_1 + \gamma r_2 + \gamma^2 r_3 + \ldots + \gamma^{T-1} r_T
    \end{equation}
    where:
    \begin{itemize}
        \item \( R \) is the total reward.
        \item \( r_t \) is the reward received at time \( t \).
        \item \( \gamma \) is the discount factor (0 ≤ $\gamma$ < 1) for future rewards.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Reinforcement Learning revolves around agent-environment interactions, where decisions based on states and actions lead to rewards that inform optimal strategies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agents and Environments - Definitions}
    
    \begin{block}{1. Agent}
        An agent is an entity that makes decisions and takes actions within an environment to achieve specific goals. In reinforcement learning, an agent interacts with the environment to learn a policy that maximizes cumulative rewards over time.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Autonomy:} Agents operate independently, making decisions based on their observations of the environment.
        \item \textbf{Learning Ability:} Agents improve their performance over time through experience, typically using algorithms like Q-learning or policy gradients.
    \end{itemize}
    
    \begin{block}{Example of an Agent}
        \textbf{Autonomous Vehicle:} An autonomous car is a practical example of an RL agent that navigates through roads based on its observations of other vehicles, pedestrians, and traffic signals.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agents and Environments - Environment}
    
    \begin{block}{2. Environment}
        The environment is everything that the agent interacts with, encompassing all aspects that influence the agent’s performance. 
    \end{block}
    
    \begin{itemize}
        \item \textbf{State Space:} The complete set of all possible conditions in which the agent can find itself.
        \item \textbf{Action Space:} The set of all actions available to the agent to influence the state of the environment.
        \item \textbf{Reward Structure:} A system that provides feedback to the agent based on the actions taken in each state.
    \end{itemize}
    
    \begin{block}{Example of an Environment}
        \textbf{Atari Game:} Games like Pong or Space Invaders serve as environments where an agent interacts with game elements to achieve high scores through various actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Additional Concepts}

    \begin{itemize}
        \item The agent's goal is to learn a policy that defines the optimal action to take in each state to maximize rewards.
        \item The environment provides the feedback loop through rewards, helping the agent evaluate the effectiveness of its actions.
        \item The interaction between agents and environments is central to reinforcement learning, driving the learning process.
    \end{itemize}
    
    \begin{block}{Formula}
        To define the relationship between states \( S \), actions \( A \), and rewards \( R \):
        \begin{equation}
            R(t) = f(S(t), A(t)) 
        \end{equation}
        where \( R(t) \) represents the reward received after taking action \( A(t) \) in state \( S(t) \).
    \end{block}

    \begin{block}{Basic Pseudocode for an Agent}
    \begin{lstlisting}
def Agent(environment):
    state = environment.reset()
    while not environment.is_terminal(state):
        action = choose_action(state)
        next_state, reward = environment.step(action)
        update_policy(state, action, reward, next_state)
        state = next_state
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States, Actions, and Rewards}
    \textbf{Understanding the Core Concepts}
    \begin{itemize}
        \item In Reinforcement Learning (RL), the interplay between states, actions, and rewards is fundamental.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts - States}
    \begin{itemize}
        \item \textbf{States (s)}:
        \begin{itemize}
            \item Definition: A specific situation or configuration of the environment at a given time.
            \item Example: In chess, the position of the pieces on the board.
            \item Mathematical Representation: Set \( S \) where \( s_t \in S \) indicates state at time \( t \).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts - Actions and Rewards}
    \begin{itemize}
        \item \textbf{Actions (a)}:
        \begin{itemize}
            \item Definition: Choices made by the agent that alter the current state.
            \item Example: Accelerating or turning in a driving simulator.
            \item Action Space: The set of possible actions, denoted as \( A \).
        \end{itemize}

        \item \textbf{Rewards (r)}:
        \begin{itemize}
            \item Definition: Scalar feedback received after an action is taken.
            \item Example: Scoring points after completing a level in a game.
            \item Reward Function: \( R(s, a) \) indicates the reward received for action \( a \) in state \( s \).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Relationship: A Feedback Loop}
    \begin{itemize}
        \item The agent observes the current \textbf{state} \( s_t \), takes an \textbf{action} \( a_t \), and receives a \textbf{reward} \( r_{t+1} \) along with the new \textbf{state} \( s_{t+1} \).
        
        \item The iterative cycle can be summarized as:
        \[
        s_t \xrightarrow{a_t} (s_{t+1}, r_{t+1})
        \]

        \item The goal is to maximize the cumulative reward:
        \[
        \text{Maximum Cumulative Reward} = \sum_{t=0}^{T} r_t
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustration: Tic-Tac-Toe Example}
    \begin{itemize}
        \item **State**: Current board configuration (e.g., XOX-O-O-XXX).
        \item **Action**: Placing 'X' or 'O' in an empty spot.
        \item **Reward**: +1 for winning, -1 for losing, 0 for a draw.
    \end{itemize}

    \begin{block}{Key Takeaway}
        By mastering the relationship between states, actions, and rewards, students will better understand and implement reinforcement learning algorithms effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model-free vs. Model-based Learning - Overview}
    In reinforcement learning (RL), we distinguish between two primary approaches for learning policies and value functions:

    \begin{itemize}
        \item \textbf{Model-free Learning:} Learns optimal actions directly from experiences without modeling environment dynamics.
        \item \textbf{Model-based Learning:} Constructs an internal model of the environment to make decisions and plan future actions.
    \end{itemize}

    Understanding these distinctions is crucial as they significantly impact how agents operate within their environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model-free Learning}
    
    \textbf{Definition:} 
    Model-free learning directly learns optimal actions from experiences without constructing a model of the environment's dynamics.

    \textbf{Key Concepts:}
    \begin{itemize}
        \item \textbf{Value-Based Methods:} e.g., Q-learning.
        \item \textbf{Policy-Based Methods:} e.g., REINFORCE algorithm.
    \end{itemize}

    \textbf{Advantages:}
    \begin{itemize}
        \item \textbf{Simplicity:} No need for environmental modeling.
        \item \textbf{Robustness:} Effective in complex or unknown environments.
    \end{itemize}

    \textbf{Disadvantages:}
    \begin{itemize}
        \item \textbf{Sample Inefficiency:} Requires more interactions for convergence.
        \item \textbf{Slow Adaptation:} Requires longer to adjust to changing environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model-free Learning - Example}

    \textbf{Example: Q-Learning}
    
    The agent updates its Q-values based on experiences defined by:
    
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    \end{equation}
    
    where:
    \begin{itemize}
        \item $s$ = current state
        \item $a$ = action taken
        \item $r$ = received reward
        \item $\gamma$ = discount factor
        \item $\alpha$ = learning rate
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model-based Learning}
    
    \textbf{Definition:}
    Model-based learning constructs an internal representation (model) of the environment's dynamics.

    \textbf{Key Concepts:}
    \begin{itemize}
        \item \textbf{Model Construction:} Predicts next state and rewards based on current states and actions.
        \item \textbf{Planning:} Utilizes the model for simulating and evaluating potential future actions.
    \end{itemize}

    \textbf{Advantages:}
    \begin{itemize}
        \item \textbf{Sample Efficiency:} Learns effective policies with fewer interactions.
        \item \textbf{Adaptability:} Quickly adapts to changes in dynamic environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model-based Learning - Example}

    \textbf{Example: Dynamic Programming}
    
    Techniques such as Policy Iteration and Value Iteration use a model of the environment for optimal policy finding.

    For instance, Value Iteration updates values based on:
    
    \begin{equation}
        V(s) \leftarrow \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')]
    \end{equation}

    where:
    \begin{itemize}
        \item $p(s', r | s, a)$ = probability of moving to state $s'$ and receiving reward $r$ after action $a$ in state $s$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Model-free methods focus on direct learning from interactions. 
        \item Model-based methods leverage a model for planning.
        \item Consider trade-offs between computational complexity, sample efficiency, and adaptability.
        \item Choosing the right method depends on problem context and environment dynamics.
    \end{itemize}

    Understanding these differences equips students to better select appropriate strategies for reinforcement learning algorithms.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Insights on Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Definition and Importance}
        \begin{itemize}
            \item Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards over time. 
            \item It holds great promise for tasks such as robotics, game playing, and autonomous vehicles.
        \end{itemize}

        \item \textbf{Core Components of RL}
        \begin{itemize}
            \item \textbf{Agent}: The learner or decision maker (e.g., a robot).
            \item \textbf{Environment}: Where the agent operates (e.g., a game arena).
            \item \textbf{Actions}: Choices made by the agent (e.g., moving in various directions).
            \item \textbf{States}: The current situation of the agent (e.g., robot's position).
            \item \textbf{Rewards}: Feedback from the environment after actions (e.g., points or penalties).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Algorithms and Approaches}
    \begin{enumerate}[resume]
        \item \textbf{Two Main Approaches}
        \begin{itemize}
            \item \textbf{Model-free Learning}: Directly learns from interactions without a model (e.g., Q-Learning).
            \item \textbf{Model-based Learning}: Learns a model of the environment for decision making based on future predictions.
        \end{itemize}

        \item \textbf{Significant Algorithms}
        \begin{itemize}
            \item \textbf{Q-Learning}: 
            \begin{equation}
                Q(s, a) \gets Q(s, a) + \alpha \left[ r + \gamma \max_a Q(s', a) - Q(s, a) \right]
            \end{equation}
            where $\alpha$ is the learning rate, $\gamma$ is the discount factor, $r$ is the reward, and $(s', a)$ is the next state and action.
            \item \textbf{Policy Gradient Methods}: Directly optimize the policy the agent follows.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways and Next Steps}
    \begin{enumerate}[1.]
        \item \textbf{Key Takeaways:}
        \begin{itemize}
            \item RL is crucial for intelligent systems learning from environmental interactions.
            \item Understanding core RL components is fundamental for trial and error learning.
            \item Mastery of exploration versus exploitation is vital for efficient learning.
        \end{itemize}
        
        \item \textbf{Next Steps}
        \begin{itemize}
            \item Prepare for deeper exploration into RL techniques, methods, and real-world applications in upcoming sessions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Learning Objectives - Overview}
  \begin{block}{Course Aim}
    This course provides a comprehensive understanding of Reinforcement Learning (RL), focusing on how agents take actions to maximize rewards. By course end, students will understand essential concepts and applications of RL.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Learning Objectives - Part 1}
  \begin{enumerate}
    \item \textbf{Understand the Fundamentals of Reinforcement Learning}
      \begin{itemize}
        \item \textbf{Concept:} Core principles including agents, environments, states, actions, and rewards.
        \item \textbf{Example:} A dog learning tricks receives a treat (reward) for correctly sitting (action).
        \item \textbf{Key Point:} Learning through trial and error aims to maximize cumulative rewards.
      \end{itemize}
  
    \item \textbf{Differentiate Between Learning Types}
      \begin{itemize}
        \item \textbf{Concept:} Recognize unique characteristics of RL versus supervised and unsupervised learning.
        \item \textbf{Example:} In supervised learning, the model learns from labeled data, while in RL, it learns from actions and their consequences.
        \item \textbf{Key Point:} RL relies on environmental feedback rather than pre-defined labels.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Learning Objectives - Part 2}
  \begin{enumerate}
    \setcounter{enumi}{2}
    
    \item \textbf{Explore Key Components of RL Algorithms}
      \begin{itemize}
        \item \textbf{Concept:} Understand critical elements like the reward signal, value functions, and policies.
        \item \textbf{Key Point:} Value functions estimate state value; policies define agent behavior.
        \item \begin{equation}
            G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \quad (0 < \gamma \leq 1)
          \end{equation}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Learning Objectives - Part 3}
  \begin{enumerate}
    \setcounter{enumi}{3}
    
    \item \textbf{Implement Basic RL Algorithms}
      \begin{itemize}
        \item \textbf{Concept:} Learn to implement algorithms like Q-learning and Policy Gradients.
        \item \textbf{Code Snippet:}
          \begin{lstlisting}[language=Python]
import numpy as np

# Simple Q-learning implementation
Q = np.zeros((state_space, action_space))
for episode in range(num_episodes):
    state = environment.reset()
    done = False
    while not done:
        action = np.argmax(Q[state])
        next_state, reward, done, _ = environment.step(action)
        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
        state = next_state
          \end{lstlisting}
      \end{itemize}
      
    \item \textbf{Evaluate and Compare RL Techniques}
      \begin{itemize}
        \item \textbf{Concept:} Assess different RL methods, their advantages, drawbacks, and use cases.
        \item \textbf{Example:} Compare model-free methods (Q-Learning) with model-based approaches (Dynamic Programming).
        \item \textbf{Key Point:} Selecting the right technique depends on environment complexity and data availability.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Learning Objectives - Final Thoughts}
  \begin{enumerate}
    \setcounter{enumi}{5}
    
    \item \textbf{Apply RL in Real-world Applications}
      \begin{itemize}
        \item \textbf{Concept:} Explore RL applications in robotics, game playing, and autonomous driving.
        \item \textbf{Example:} AlphaGo, an RL-based AI, defeated human champions in Go.
        \item \textbf{Key Point:} Understanding applications contextualizes theoretical principles and their impacts.
      \end{itemize}
    
    \item \textbf{Closing Note:}
      \begin{itemize}
        \item By achieving these objectives, students will gain essential theories and practical skills needed for tackling real-world challenges with RL.
      \end{itemize}
  \end{enumerate}
\end{frame}


\end{document}