\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Monte Carlo Methods]{Week 4: Monte Carlo Methods}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Monte Carlo Methods}
    \begin{block}{What are Monte Carlo Methods?}
        Monte Carlo methods are a class of computational algorithms that use random sampling to obtain numerical results, especially when traditional analytical methods are infeasible.
    \end{block}
    In reinforcement learning, they estimate value functions and optimize policies based on sampled experiences.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Monte Carlo Methods}
    \begin{itemize}
        \item \textbf{Random Sampling:} Generation of random samples to approximate mathematical functions or solve problems.
        \item \textbf{Reinforcement Learning Context:} Used for evaluating and improving policies based on episodes of experience.
    \end{itemize}
    
    \begin{block}{Relevance in Reinforcement Learning}
        \begin{enumerate}
            \item \textbf{Policy Evaluation:} Estimating returns of actions based on sampled experiences.
            \item \textbf{Exploration vs. Exploitation:} Balancing exploration of new actions with exploitation of known rewarding actions.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Techniques and Example}
    \begin{block}{Monte Carlo Estimation}
        To estimate the value of a state or state-action pair, the agent plays multiple episodes and calculates the average return:
        \begin{equation}
        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
        \end{equation}
        Where \(R_t\) is the reward received at time \(t\) and \(0 \leq \gamma < 1\) is the discount factor.
    \end{block}
    
    \begin{block}{Example in Action}
        In a grid-world environment, an agent performs 100 episodes, randomly exploring paths and updates value estimates using the return formula.
    \end{block}
    
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Monte Carlo methods are essential for understanding RL environments.
        \item Sufficient exploration is required for accurate estimation.
        \item Can converge to value function approximations with appropriate application.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{What are Monte Carlo Methods? - Definition}
    \begin{block}{Definition}
        Monte Carlo methods are a class of computational algorithms that rely on random sampling to obtain numerical results. These methods are particularly powerful for tasks that involve uncertainty and complex systems, where deterministic approaches may fail or become infeasible.
    \end{block}

    In predictive modeling and control, Monte Carlo methods provide a framework for estimating the behavior of processes by executing simulations and evaluating average outcomes across numerous trials. This variability helps in approximating solutions to problems that may be too complex for analytical or closed-form solutions.
\end{frame}

\begin{frame}[fragile]{What are Monte Carlo Methods? - Key Concepts}
    \begin{enumerate}
        \item \textbf{Random Sampling}: Generate random inputs from defined distributions to simulate a process or model, capturing inherent variability and uncertainty in real-world scenarios.
        
        \item \textbf{Simulation}: Perform a large number of simulations (or trials) to approximate the distribution of outcomes, calculate probabilities, and make informed predictions based on the results.

        \item \textbf{Statistical Analysis}: Aggregate results to derive statistical metrics (means, variances, confidence intervals) that describe the model's behavior.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{What are Monte Carlo Methods? - Applications}
    \begin{block}{Examples of Applications}
        \begin{itemize}
            \item \textbf{Finance}: Estimating the value of complex derivatives using simulations to model different market scenarios.
            \item \textbf{Engineering}: Analyzing the reliability of systems under uncertain conditions, such as failure rates of components.
            \item \textbf{Computer Graphics}: Rendering scenes using path tracing, where the light transport is modeled using thousands of paths calculated via random sampling.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Monte Carlo methods are versatile and applicable across diverse areas including statistics, physics, quantitative finance, and machine learning.
            \item The quality of the results generally improves with the number of samples; however, computational costs can increase, emphasizing the trade-off between accuracy and performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{What are Monte Carlo Methods? - Mathematical Foundation}
    Consider estimating the integral of a function \( f(x) \) over the interval \([a, b]\):
    
    The integral can be estimated using the Monte Carlo method by:
    \begin{equation}
        I \approx \frac{b-a}{N} \sum_{i=1}^{N} f(x_i)
    \end{equation}
    where \( x_i \) are uniformly distributed random samples between \( a \) and \( b \), and \( N \) is the total number of samples.
\end{frame}

\begin{frame}[fragile]{What are Monte Carlo Methods? - Example in Python}
    \begin{lstlisting}[language=Python]
import numpy as np

def monte_carlo_integration(func, a, b, n_samples):
    # Generate random samples in the range [a, b]
    samples = np.random.uniform(a, b, n_samples)
    # Compute the average value of the function at these samples
    mean_value = np.mean(func(samples))
    # Estimate the integral
    integral_estimate = (b - a) * mean_value
    return integral_estimate

# Example function: f(x) = x^2
integral_estimate = monte_carlo_integration(lambda x: x**2, 0, 1, 10000)
print(f"Estimated Integral: {integral_estimate}")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{What are Monte Carlo Methods? - Conclusion}
    \begin{block}{Conclusion}
        Monte Carlo methods provide a robust framework for modeling complex systems with uncertainties. By understanding their principles, applications, and mathematical foundations, students can capitalize on these powerful techniques for predictive modeling and control tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics of Monte Carlo Methods}
    Monte Carlo methods are computational algorithms that utilize repeated random sampling to obtain numerical results. They are particularly useful for simulating complex systems and processes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics - Randomness and Sampling}
    \begin{enumerate}
        \item \textbf{Randomness}:
        \begin{itemize}
            \item Monte Carlo methods rely on random variables to simulate processes and explore numerous possible outcomes.
            \item \textbf{Example}: Simulating the roll of a die allows examination of the probability distribution (1 through 6) over many trials.
        \end{itemize}

        \item \textbf{Sampling}:
        \begin{itemize}
            \item This technique involves selecting random samples from a defined space to estimate characteristics of the entire population.
            \item The quality of the estimate improves with a larger number of samples.
            \item \textbf{Example}: Randomly sampling historical return data in financial modeling helps estimate risk and return profiles.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics - Trial-and-Error and Applications}
    \begin{enumerate}[resume]
        \item \textbf{Trial-and-Error Approach}:
        \begin{itemize}
            \item Monte Carlo methods use a trial-and-error process to converge towards a solution through multiple simulations.
            \item \textbf{Example}: Estimating the value of $\pi$ by randomly placing points in a enclosing square. The ratio of points inside the circle to total points approximates $\pi$.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Applications and Importance}
        \begin{itemize}
            \item Applicable in finance (portfolio optimization), engineering (risk analysis), and environmental modeling (climate projection).
            \item Valuable for high uncertainty situations where analytical solutions are challenging.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Accuracy Improves with More Samples}: The law of large numbers suggests estimates converge towards actual values with more samples.
            \item \textbf{Flexibility}: Monte Carlo methods can model virtually any stochastic process.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Insight}
    The general formula for Monte Carlo estimation can be expressed as:

    \begin{equation}
        \hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} f(x_i)
    \end{equation}

    Where:
    \begin{itemize}
        \item \( \hat{\mu} \) = estimated mean of the function \( f \)
        \item \( N \) = number of random samples taken
        \item \( x_i \) = random sample drawn from the domain of \( f \)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Monte Carlo methods leverage randomness and sampling to solve complex problems through trial-and-error, providing insights in various fields despite uncertainties.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Prediction - Overview}
    \begin{block}{Understanding Monte Carlo Prediction}
        Monte Carlo Prediction is a statistical technique in reinforcement learning for estimating expected returns of states through random sampling.
    \end{block}
    This technique is useful for providing insights into complex systems where analytic solutions are challenging to derive.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Monte Carlo Prediction}
    \begin{enumerate}
        \item \textbf{Random Sampling}:
        \begin{itemize}
            \item Generate random samples (trajectories) of the decision process.
            \item Each sample includes states, actions, and rewards until reaching a terminal state.
        \end{itemize}

        \item \textbf{Return Calculation}:
        \begin{equation}
        G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
        \end{equation}

        \item \textbf{State Value Estimation}:
        \begin{equation}
        V(s) \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_t^i
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario: Board Game}
    \begin{block}{Game Setup}
        \begin{itemize}
            \item States: Positions on board (e.g., A, B, C, D)
            \item Rewards: Points for landing on each position (e.g., A=2, B=3, C=5, D=0)
        \end{itemize}
    \end{block}
    
    \begin{block}{Monte Carlo Process}
        \begin{enumerate}
            \item Simulate games starting from a position (e.g., A).
            \item Record trajectories of states and rewards.
            \item Calculate returns for visited states.
            \item Update state values based on collected returns.
        \end{enumerate}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Requires complete episodes for meaningful estimates.
            \item Accuracy improves with more episodes.
            \item Discount factor $\gamma$ balances immediate vs. future rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Monte Carlo Prediction}
    Monte Carlo Prediction is applicable in various domains:
    \begin{itemize}
        \item \textbf{Finance}: Stock price prediction
        \item \textbf{Game Theory}: Strategy optimization
        \item \textbf{Robotics}: Path planning
    \end{itemize}
    
    By learning these techniques, students understand foundational concepts of reinforcement learning and its applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Control - Overview}
    \begin{itemize}
        \item Monte Carlo methods facilitate learning optimal policies in reinforcement learning via sample-based approaches.
        \item Applicable for both \textbf{on-policy} and \textbf{off-policy} control.
        \item Key goal: Learn policies that maximize expected returns from states.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Control - On-Policy and Off-Policy}
    \begin{block}{On-Policy Control}
        \begin{itemize}
            \item Policy used to generate behavior data is same as the policy being improved.
            \item Example: Use an \textbf{epsilon-greedy} strategy to balance exploration and exploitation.
            \item \textbf{Algorithm Steps}:
                \begin{enumerate}
                    \item Initialize \(Q(s, a)\) and policy \(\pi(a|s)\).
                    \item Generate episodes based on \(\pi\).
                    \item Update \(Q(s, a)\) based on returns:
                        \[
                        Q(s, a) \leftarrow Q(s, a) + \alpha (G_t - Q(s, a))
                        \]
                    \item Update policy to be greedy with respect to \(Q\).
                \end{enumerate}
        \end{itemize}
    \end{block}

    \begin{block}{Off-Policy Control}
        \begin{itemize}
            \item Learn about one policy while acting under another (behavior policy).
            \item Example: Behavior policy can be random, while learning an optimal policy.
            \item \textbf{Algorithm Steps}:
                \begin{enumerate}
                    \item Initialize target policy \(\pi\) and behavior policy \(b\).
                    \item Generate episodes using \(b\).
                    \item Compute importance sampling ratios and update:
                        \[
                        Q(s, a) \leftarrow Q(s, a) + \alpha \rho (G_t - Q(s, a))
                        \]
                    \item Improve target policy based on weighted returns.
                \end{enumerate}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Control - Examples and Key Points}
    \begin{block}{Examples}
        \textbf{On-Policy Control Example:}
        \begin{itemize}
            \item Agent refines its moves in a grid world environment based on observed rewards.
        \end{itemize}

        \textbf{Off-Policy Control Example:}
        \begin{itemize}
            \item Agent uses a random exploration policy while learning the optimal path to minimize time to a goal.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Distinction between on-policy and off-policy is crucial.
            \item Importance of exploration vs. exploitation highlighted.
            \item Monte Carlo control methods applicable in real-world problems like robotics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Monte Carlo Algorithm - Overview}
    \begin{block}{Definition}
        The Monte Carlo algorithm is a fundamental technique in reinforcement learning that uses random sampling to estimate the value of policies. 
    \end{block}
    \begin{itemize}
        \item Allows agents to learn optimal strategies.
        \item Estimates returns from actions taken during episodes in an environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Monte Carlo Algorithm - Steps}
    \begin{enumerate}
        \item \textbf{Initialization}:
        \begin{itemize}
            \item Initialize policy $\pi$ (randomly or based on prior knowledge).
            \item Initialize value function $V(s)$ for each state (to zero or arbitrary values).
            \item Initialize visit count $N(s, a)$ for each state-action pair.
        \end{itemize}
        
        \item \textbf{Generate Episodes}:
        \begin{itemize}
            \item For each episode:
            \begin{itemize}
                \item Initialize the environment and observe initial state $s_0$.
                \item Select actions based on policy $\pi$ (exploration or exploitation).
                \item Observe rewards and transitions to store for evaluation.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Monte Carlo Algorithm - Returns and Updates}
    \begin{enumerate}
        \setcounter{enumi}{2} % continue numbering
        \item \textbf{Calculate Returns}:
        \begin{equation}
            G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \ldots
        \end{equation}
        Where $\gamma$ is the discount factor (0 ≤ $\gamma$ < 1).

        \item \textbf{Update Value Function}:
        \begin{itemize}
            \item For each state-action pair encountered:
            \begin{itemize}
                \item Update visit count: $N(s, a) += 1$.
                \item Update value function: 
                \[
                V(s) = V(s) + \frac{(G - V(s))}{N(s, a)}
                \]
            \end{itemize}
        \end{itemize}

        \item \textbf{Policy Improvement}:
        \begin{itemize}
            \item Determine best action $a$ for each state $s$:
            \[
            \pi(s) = \text{argmax}_a Q(s, a)
            \end{itemize}
        \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Monte Carlo Algorithm - Pseudocode}
    \begin{block}{Pseudocode}
    \begin{lstlisting}
    Initialize policy π and value function V(s) for all states s
    Initialize N(s, a) for all state-action pairs

    For each episode:
        Generate an episode using policy π
        Calculate returns G for each state-action pair

    For each state-action pair (s, a) in the episode:
        N(s, a) += 1
        V(s) += (G - V(s)) / N(s, a)

    Update policy π based on V(s) if applicable
    \end{lstlisting}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Monte Carlo Sampling: Relies on random sampling.
            \item Explore vs. Exploit: Balance in selection.
            \item Incremental Updates: Efficient updates based on returns.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration in Monte Carlo Methods - Overview}
    \begin{block}{Understanding Exploration vs. Exploitation}
        In Monte Carlo methods, particularly within reinforcement learning (RL), the balance between exploration and exploitation is crucial.
    \end{block}
    \begin{itemize}
        \item \textbf{Exploration:} Trying new actions to discover their rewards; gaining knowledge for better decision-making.
        \item \textbf{Exploitation:} Capitalizing on known actions that yield high rewards based on current knowledge; maximizing immediate returns.
    \end{itemize}
    This trade-off is a pivotal decision in RL: risk exploring the unknown for better long-term outcomes, or optimize based on existing knowledge.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance in Monte Carlo Methods}
    \begin{block}{Why Balance is Vital}
        In Monte Carlo methods, which rely on random sampling, finding the right balance between exploration and exploitation is essential for effective learning:
    \end{block}
    \begin{itemize}
        \item Over-exploration can waste resources on suboptimal actions.
        \item Early exploitation may prevent discovering superior strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies for Balancing Exploration and Exploitation}
    \begin{enumerate}
        \item \textbf{Epsilon-Greedy Strategy:}
            \begin{itemize}
                \item Choose a random action with probability $\epsilon$; otherwise, choose the action with the highest current value.
                \item \textbf{Formula:}
                \begin{equation}
                    a_t = 
                    \begin{cases}
                    \text{random action} & \text{with probability } \epsilon \\ 
                    \text{argmax } Q(a)   & \text{with probability } 1 - \epsilon 
                    \end{cases}
                \end{equation}
            \end{itemize}

        \item \textbf{Softmax Action Selection:}
            \begin{itemize}
                \item Actions are chosen based on a probability distribution that favors higher rewards.
                \item \textbf{Formula:}
                \begin{equation}
                    P(a_i) = \frac{e^{Q(a_i)/\tau}}{\sum_{j} e^{Q(a_j)/\tau}}
                \end{equation}
                where $\tau$ (temperature) controls exploration level.
            \end{itemize}

        \item \textbf{Upper Confidence Bound (UCB):}
            \begin{itemize}
                \item Select actions based on a combination of average reward and uncertainty.
                \item \textbf{Formula:}
                \begin{equation}
                    a_t = \text{argmax} \left( \overline{Q}(a) + c \sqrt{\frac{\ln t}{N(a)}} \right)
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Effective RL strategies must navigate the exploration-exploitation dilemma.
        \item The choice of exploration strategy can greatly impact learning efficiency and performance.
        \item Tune parameters carefully (e.g., $\epsilon$ in epsilon-greedy or $\tau$ in softmax) to achieve the right balance.
    \end{itemize}
    \begin{block}{Conclusion}
        Balancing exploration and exploitation influences agents' learning trajectories. Robust strategies ensure agents maximize immediate rewards while acquiring knowledge for long-term success.
    \end{block}  
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Monte Carlo Methods - Overview}
    \begin{itemize}
        \item Monte Carlo methods (MCMs) are powerful statistical techniques for numerical approximation.
        \item Despite their widespread applicability, they come with several limitations and challenges.
        \item Understanding these limitations is crucial for effective usage in practice.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Monte Carlo Methods - Key Limitations}
    \begin{enumerate}
        \item \textbf{High Variance in Estimates}
            \begin{itemize}
                \item Random sampling can lead to high variance, especially with low sample sizes.
                \item \textit{Example}: Estimating the area under a curve with few points.
                \item Mathematical Representation:
                \begin{equation}
                    \text{Var}(X) = \frac{\sigma^2}{N}
                \end{equation}
            \end{itemize}
        \item \textbf{Computational Intensity}
            \begin{itemize}
                \item More samples are needed for accuracy, leading to increased computational load.
                \item \textit{Example}: Financial modeling for option pricing may require millions of simulations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Monte Carlo Methods - Additional Challenges}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Convergence Issues}
            \begin{itemize}
                \item Convergence can be slow and depends on sampling strategy and problem structure.
                \item Poor sampling could lead to under-sampled regions and inaccurate estimates.
            \end{itemize}
        \item \textbf{Dependence on Random Number Generators}
            \begin{itemize}
                \item Results heavily depend on the quality of the random number generator (RNG).
                \item Low-quality RNG could introduce bias.
                \item Key Point: Always use well-tested RNGs.
            \end{itemize}
        \item \textbf{Difficulty in High Dimensions (Curse of Dimensionality)}
            \begin{itemize}
                \item As dimensions increase, the volume of the space increases exponentially, complicating effective sampling.
                \item \textit{Illustration}: In 3D, samples cover significant area; in 10D, they cover almost nothing.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Monte Carlo Methods - Practical Implications}
    \begin{itemize}
        \item \textbf{Strategy Development}: Consider variance reduction techniques (e.g., control variates, antithetic variates).
        \item \textbf{Careful Planning}: Understanding computational costs is crucial for effective deployment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Monte Carlo Methods - Conclusion and Further Reading}
    \begin{itemize}
        \item Awareness of limitations is essential for effective application in fields like reinforcement learning and risk analysis.
    \end{itemize}
    \textbf{Further Reading:}
    \begin{itemize}
        \item Variance Reduction Techniques: \url{http://example_reference.com}  
        \item Practical Applications in Reinforcement Learning: \url{http://example_reference_rl.com}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Cases in Reinforcement Learning}
    \begin{block}{Understanding Monte Carlo Methods}
        Monte Carlo methods are statistical techniques that rely on random sampling for numerical estimations. 
        In reinforcement learning (RL), they approximate the value of actions or states by averaging returns from multiple episodes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Use Cases of Monte Carlo Methods}
    \begin{enumerate}
        \item \textbf{Game Playing (e.g., Chess, Go)}
            \begin{itemize}
                \item Monte Carlo Tree Search (MCTS) uses sampling to decide the best moves.
                \item \textit{Example:} AlphaGo utilized MCTS to select promising paths by simulating future moves.
            \end{itemize}
        \item \textbf{Robotics}
            \begin{itemize}
                \item Monte Carlo localization estimates robot position and orientation using sensor data.
                \item \textit{Example:} A clean-up robot navigates while avoiding obstacles using particle filters.
            \end{itemize}
        \item \textbf{Traffic Management}
            \begin{itemize}
                \item RL frameworks manage traffic signals using Monte Carlo for optimal flow.
                \item \textit{Example:} A Monte Carlo approach adapts signal timings based on real-time conditions.
            \end{itemize}
        \item \textbf{Healthcare Optimization}
            \begin{itemize}
                \item ML systems simulate treatment paths to optimize patient strategies.
                \item \textit{Example:} Simulations evaluate effective treatment courses for different patient scenarios.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages & Challenges}
    \begin{block}{Advantages}
        \begin{itemize}
            \item \textbf{Non-Parametric Nature:} No assumptions about environment models, providing flexibility.
            \item \textbf{Suitability for Stochastic Domains:} Efficiently handles randomness in environments.
        \end{itemize}
    \end{block}
    
    \begin{block}{Challenges}
        \begin{itemize}
            \item \textbf{High Variance:} Returns can be noisy; strategies like control variates mitigate this.
            \item \textbf{Sample Size:} Larger samples may be needed for accurate results, leading to longer convergence.
        \end{itemize}
    \end{block}
    
    \begin{block}{Concluding Thought}
        Monte Carlo methods enhance decision-making processes in various fields, playing a crucial role in reinforcement learning applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Expected Return Calculation}
    To calculate the expected return $G_t$ from a state $s_t$:
    \begin{equation}
        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
    \end{equation}
    Where:
    \begin{itemize}
        \item $R_t$ is the reward at time $t$
        \item $\gamma$ is the discount factor (0 < $\gamma$ < 1)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Overview of Monte Carlo Methods}
    \begin{itemize}
        \item Monte Carlo methods utilize randomness to solve problems in a statistical manner.
        \item In reinforcement learning (RL), they help estimate value functions and policies from sampled episodes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points on Monte Carlo Methods}
    \begin{enumerate}
        \item \textbf{Monte Carlo Estimation}:
            \begin{itemize}
                \item Estimates value functions by averaging returns from multiple episodes.
                \item Suitable for uncertain environments.
            \end{itemize}
            \begin{block}{Formula}
            \begin{equation}
                V(s) \approx \frac{1}{N} \sum_{i=1}^{N} G_i
            \end{equation}
            \end{block}
        
        \item \textbf{Exploration vs. Exploitation}:
            \begin{itemize}
                \item Balances exploration using action-selection strategies (e.g., epsilon-greedy).
            \end{itemize}
        
        \item \textbf{Full vs. Incremental Updates}:
            \begin{itemize}
                \item Full updates use complete episodes, while incremental updates refine estimates step-by-step.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Impact of Monte Carlo Methods}
    \begin{itemize}
        \item \textbf{Applications in Reinforcement Learning}:
            \begin{itemize}
                \item Fields: game playing (e.g., AlphaGo), robotics, finance.
                \item Enables agents to learn optimal strategies in complex environments.
                \item \textbf{Example}: In Blackjack, simulate hands to learn winning probabilities.
            \end{itemize}
        
        \item \textbf{Comparative Insights}:
            \begin{itemize}
                \item No prior environmental knowledge needed, making them versatile.
                \item Often simpler compared to Temporal Difference (TD) methods.
            \end{itemize}

        \item \textbf{Final Thoughts}:
            \begin{itemize}
                \item Monte Carlo methods significantly influence modern RL algorithms.
                \item Their robust statistical foundation is effective for learning under uncertainty.
            \end{itemize}
    \end{itemize}
\end{frame}


\end{document}