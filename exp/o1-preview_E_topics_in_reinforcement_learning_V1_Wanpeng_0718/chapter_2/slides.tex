\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 2: Mathematical Foundations}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes (MDPs)}
    \begin{block}{Overview of MDPs}
        A Markov Decision Process (MDP) is a mathematical framework used to describe an environment in reinforcement learning (RL). It helps in making decisions to achieve goals where outcomes depend on actions and randomness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs}
    MDPs are defined by the following key components:
    \begin{enumerate}
        \item \textbf{States (S)}: Possible situations or configurations the agent can be in.
        \item \textbf{Actions (A)}: Choices available to the agent in each state.
        \item \textbf{Transition Probabilities (P)}:
        \[
        P(s' | s, a) = P(\text{next state is } s' | \text{current state is } s \text{ and action is } a)
        \]
        \item \textbf{Rewards (R)}: Numerical values received after transitioning between states.
        \item \textbf{Discount Factor ($\gamma$)}: Value between 0 and 1 determining the present value of future rewards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of MDPs in Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Framework for Optimal Decision Making}: MDPs serve as a foundation for developing optimal strategies.
        \item \textbf{Policy Representation}: An agent learns a policy ($\pi$) that specifies the actions in each state to maximize cumulative rewards.
        \item \textbf{Value Function}:
        \[
        V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' | s, a)V(s') \right)
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Grid World}
    Consider a simple grid world where:
    \begin{itemize}
        \item \textbf{States}: Each grid cell represents a state.
        \item \textbf{Actions}: Up, Down, Left, Right.
        \item \textbf{Transition Probabilities}: Probability of slipping while moving.
        \item \textbf{Rewards}: Positive for reaching specific cells, negative for falling into traps.
    \end{itemize}
    The agent explores the grid while learning the best actions to maximize overall reward.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item MDPs provide a robust framework for modeling decision-making problems in RL.
        \item Understanding MDP components aids in formulating and solving complex problems in various fields.
        \item Mastery of MDPs is crucial for developing effective RL algorithms and policies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are MDPs?}
    \begin{block}{Definition of Markov Decision Processes (MDPs)}
        A Markov Decision Process (MDP) is a mathematical framework used for modeling decision-making situations where outcomes are partly random and partly under the control of a decision-maker.
    \end{block}
    \begin{block}{Significance}
        MDPs are foundational in areas such as Reinforcement Learning, robotics, and operations research.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs}
    MDPs consist of four key components:

    \begin{enumerate}
        \item \textbf{States (S)}: Represents the current situation of the decision maker in an environment.
        \item \textbf{Actions (A)}: The set of all possible moves or decisions the decision-maker can make.
        \item \textbf{Transition Probabilities (P)}: Defines the likelihood of moving from one state to another, given a specific action.
        \item \textbf{Rewards (R)}: Values received after transitioning between states, representing the immediate benefit of an action.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Detailed Components of MDPs}
    
    \textbf{1. States (S)}: 
    \begin{itemize}
        \item Example: In a grid world, each cell can be a state, e.g., (2,3).
    \end{itemize}

    \textbf{2. Actions (A)}: 
    \begin{itemize}
        \item Example: In the grid world, actions may include moving up, down, left, or right.
    \end{itemize}

    \textbf{3. Transition Probabilities (P)}: 
    \begin{itemize}
        \item Example: P((2,4)|(2,3), right) = 0.7 represents probability of moving from (2,3) to (2,4).
    \end{itemize}

    \textbf{4. Rewards (R)}: 
    \begin{itemize}
        \item Example: A reward of +10 for reaching a target position, -5 for hitting an obstacle.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Mathematical Representation}
    
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item \textbf{Markov Property}: Future state depends only on the current state and action, not previous history.
        \item \textbf{Goal}: To find a policy that maximizes the expected sum of rewards over time.
        \item \textbf{Real-World Applications}: Used in robotics, finance, and game theory.
    \end{itemize}

    \textbf{Mathematical Representation:}
    \begin{equation}
        MDP = (S, A, P, R)
    \end{equation}
    where:
    \begin{itemize}
        \item \( S \) = set of states,
        \item \( A \) = set of actions,
        \item \( P: S \times A \times S \rightarrow [0, 1] \) = transition probabilities,
        \item \( R: S \times A \rightarrow \mathbb{R} \) = reward function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrap-Up}
    Understanding MDPs will provide the groundwork for exploring more complex models and algorithms in Reinforcement Learning. 
    \begin{block}{Next Steps}
        The next slide will delve into the mathematical representation and formulation of MDPs, enhancing your understanding of their operational mechanics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation of MDPs - Introduction}
    A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making situations where outcomes are partly random and partly under the control of a decision-maker. It is characterized by a tuple \( (S, A, P, R, \gamma) \):
    \begin{itemize}
        \item \textbf{S}: A set of states.
        \item \textbf{A}: A set of actions available to the agent.
        \item \textbf{P}: Transition probabilities, representing the probability of reaching a new state given the current state and action.
        \item \textbf{R}: Reward function detailing the reward received after transitioning from one state to another following an action.
        \item \textbf{$\gamma$}: Discount factor, determining the importance of future rewards compared to immediate rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation of MDPs - Formulation}
    \begin{enumerate}
        \item \textbf{States (S)}: Each possible situation in which the agent can find itself. For example:  
        \[
        S = \{s_1, s_2, \ldots, s_n\}
        \]

        \item \textbf{Actions (A)}: The set of all actions the agent can take in any state. For example:  
        \[
        A(s) = \{a_1, a_2, \ldots, a_m\} \text{ for state } s.
        \]

        \item \textbf{Transition Function (P)}: Defines how the agent transitions between states, given its current state and action: 
        \[
        P(s' | s, a) = \text{Pr}(S_{t+1} = s' | S_t = s, A_t = a)
        \]

        \item \textbf{Reward Function (R)}: The immediate reward received after transitioning from state \( s \) to state \( s' \) due to action \( a \):
        \[
        R(s, a) = \text{Expected reward after taking action } a \text{ in state } s.
        \]
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation of MDPs - Key Concepts}
    \begin{itemize}
        \item \textbf{Discount Factor ($\gamma$)}: A value between 0 and 1 weighing future rewards relative to present rewards. If \( \gamma = 0.9 \), future rewards are worth 90\% of their value today. 
        \[
        V(s) = R(s) + \gamma \sum_{s' \in S} P(s' | s, a) V(s')
        \]
        where \( V(s) \) represents the value function, indicating the maximum expected future rewards obtainable from state \( s \).
    \end{itemize}
    

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item MDPs encourage a structured approach to decision-making.
            \item Transition probabilities provide a probabilistic approach to uncertainty.
            \item The discount factor balances immediate versus future rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming Basics}
    \begin{block}{Introduction to Dynamic Programming}
        Dynamic Programming (DP) is a method for solving complex problems by breaking them down into simpler subproblems. It is particularly effective for optimization problems and is widely used in the context of Markov Decision Processes (MDPs).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming - Key Concepts}
    \begin{enumerate}
        \item \textbf{Optimal Substructure}: The optimal solution to a problem can be constructed from optimal solutions to its subproblems.
        \item \textbf{Overlapping Subproblems}: DP solves subproblems that are often repeated multiple times. Storing results (memoization) reduces computational time.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Dynamic Programming in MDPs}
    \begin{block}{Markov Decision Processes (MDPs)}
        MDPs consist of states, actions, transition probabilities, rewards, and policies. Dynamic programming computes the optimal policy through the 
        \textbf{Bellman Equation}:
        \begin{equation}
        V(s) = \max_{a \in A} \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a)V(s') \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $V(s)$ = Value of state $s$
            \item $A$ = Set of all actions
            \item $R(s, a)$ = Immediate reward for action $a$ in state $s$
            \item $\gamma$ = Discount factor (0 < $\gamma$ < 1)
            \item $P(s'|s,a)$ = Transition probability to the next state $s'$ given state $s$ and action $a$
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Fibonacci Sequence}
    \begin{block}{Dynamic Programming Example}
        A classic example of DP is computing Fibonacci numbers:
        \begin{itemize}
            \item \textbf{Naive approach}: $F(n) = F(n-1) + F(n-2)$ → Exponential time.
            \item \textbf{DP approach}: Store computed values to avoid recomputation.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python]
def fibonacci(n):
    fib = [0, 1] + [0] * (n - 1)
    for i in range(2, n + 1):
        fib[i] = fib[i - 1] + fib[i - 2]
    return fib[n]
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Dynamic programming is a powerful tool for solving MDPs. By understanding its principles—optimal substructure and overlapping subproblems—students can efficiently tackle a variety of decision-making problems in uncertain environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Algorithm - Introduction}
    \begin{itemize}
        \item Value Iteration is an algorithm for computing optimal policies in Markov Decision Processes (MDPs).
        \item Systematically updates value estimates for each state until convergence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Markov Decision Process (MDP)}: Framework for decision-making with random outcomes.
        \item \textbf{State (S)}: Current situation of the process.
        \item \textbf{Action (A)}: Choices available to the decision-maker.
        \item \textbf{Value Function (V)}: Maximum expected utility from each state.
        \item \textbf{Discount Factor ($\gamma$)}: Value between 0 and 1 that discounts future rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Algorithm: Steps}
    \begin{enumerate}
        \item \textbf{Initialization:}
            \begin{itemize}
                \item Start with arbitrary state values (often zero).
                \item Set convergence threshold $\epsilon$.
            \end{itemize}
        \item \textbf{Update Values:}
            \begin{itemize}
                \item For each state $s \in S$: 
                \begin{equation}
                Q(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V(s') \right]
                \end{equation}
                \item Update using:
                \begin{equation}
                V_{\text{new}}(s) = \max_{a} Q(s, a)
                \end{equation}
            \end{itemize}
        \item \textbf{Check for Convergence:}
            \begin{itemize}
                \item Calculate maximum change:
                \begin{equation}
                \Delta = \max_{s \in S} |V_{\text{new}}(s) - V(s)|
                \end{equation}
                \item If $\Delta < \epsilon$, stop. Otherwise, update and repeat.
            \end{itemize}
        \item \textbf{Extract Optimal Policy:}
            \begin{equation}
            \pi^*(s) = \arg\max_{a} Q(s, a)
            \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    \begin{itemize}
        \item Consider an MDP with 2 states $s_1$ and $s_2$.
        \item Transition probabilities and rewards defined.
        \item Initialize values: $V(s_1) = 0$, $V(s_2) = 0$.
        \item Update values based on actions and compute until convergence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Value Iteration guarantees finding the optimal value function for a small enough threshold $\epsilon$.
        \item Convergence typically occurs in a few iterations due to the nature of MDPs.
        \item It is computationally efficient as it does not require holding all policy data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Understanding Value Iteration sets the foundation for exploring policies.
        \item It leads to effective decision-making strategies under uncertainty, paving the way for advanced topics such as Policy Iteration.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Algorithm - Overview}
    \begin{block}{Overview}
        The Policy Iteration algorithm is a fundamental method used in Reinforcement Learning (RL) and Decision Making to find the optimal policy in Markov Decision Processes (MDPs). An optimal policy defines the best action to take in each state to maximize cumulative rewards.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Policy}: A strategy specifying actions for each state.
        \item \textbf{Value function}: Estimation of expected return from each state under a policy.
        \item \textbf{Optimal Policy}: Achieves the highest expected reward over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Algorithm - Workflow}
    \begin{enumerate}
        \item \textbf{Initialization}: 
            Start with an arbitrary policy \( \pi \) and value function \( V(s) \).
        
        \item \textbf{Policy Evaluation}:
            Calculate \( V(s) \) for policy \( \pi \):
            \begin{equation}
                V(s) = R(s) + \gamma \sum_{s'} P(s' | s, a)V(s')
            \end{equation}
            
        \item \textbf{Policy Improvement}:
            Update to a new policy \( \pi' \):
            \begin{equation}
                \pi'(s) = \arg\max_a \left( R(s) + \gamma \sum_{s'} P(s' | s, a)V(s') \right)
            \end{equation}
        
        \item \textbf{Convergence Check}:
            If \( \pi' = \pi \), converge; otherwise, update \( \pi \) and repeat.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Algorithm - Example and Key Points}
    \begin{block}{Example}
        Consider a simple grid world MDP:
        \begin{itemize}
            \item \textbf{States}: Each cell represents a state.
            \item \textbf{Actions}: Movements (Up, Down, Left, Right).
            \item \textbf{Rewards}: Costs for each step; rewards for reaching the goal.
        \end{itemize}
        \begin{enumerate}
            \item Initialize a random policy (e.g., always go 'Right').
            \item Evaluate the policy.
            \item Improve based on calculated values.
            \item Repeat until stabilization.
        \end{enumerate}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Guarantees convergence to optimal policy.
            \item Consists of two main steps: Evaluation and Improvement.
            \item May require more resources than other methods (e.g., Value Iteration) but often converges faster.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding MDPs}
    Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. 
    \begin{itemize}
        \item \textbf{States (S)}: The various situations in which an agent can find itself.
        \item \textbf{Actions (A)}: The set of all possible actions that the agent can take.
        \item \textbf{Transition Function (P)}: A probability distribution that describes the likelihood of moving from one state to another given an action.
        \item \textbf{Reward Function (R)}: A function that measures the immediate reward received after transitioning from one state to another via a specific action.
        \item \textbf{Discount Factor ($\gamma$)}: A factor between 0 and 1 that balances immediate and future rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of MDPs}
    \begin{block}{1. Gaming}
        In strategy games like Chess or Go, an AI uses MDPs to assess the best moves by evaluating the current state of the board and predicting future states.
        \begin{itemize}
            \item Players can think of the current board configuration as a state.
            \item Actions lead to new board configurations with associated rewards (winning the game).
        \end{itemize}
    \end{block}

    \begin{block}{2. Robotics}
        Autonomous robots, such as drones or self-driving cars, utilize MDPs for navigation and decision-making.
        \begin{itemize}
            \item The robot’s current location is a state.
            \item Actions include turning or accelerating. 
            \item Rewards can be based on reaching a destination quickly while avoiding collisions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continued Applications of MDPs}
    \begin{block}{3. Finance}
        MDPs model problems in the finance sector, e.g., portfolio management.
        \begin{itemize}
            \item States represent the current portfolio configuration.
            \item Actions involve buying or selling stocks.
            \item Rewards are the profits generated from these actions.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Decision-Making Framework}: MDPs provide structured methods to evaluate choices.
            \item \textbf{Handling Uncertainty}: MDPs are robust in environments with randomness, making them suitable for real-world applications.
            \item \textbf{Adaptability}: The framework can be adapted to a variety of domains.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    MDPs are a powerful tool in reinforcement learning, enabling the development of intelligent systems across various fields. They model complex decision-making scenarios crucial for technologies like gaming AI, robotics, and financial forecasting.

    \textbf{Next Steps}: In the following slide, we will discuss the challenges and limitations encountered when applying MDPs, including state space complexity and computational demands.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MDPs - Overview}
    \begin{itemize}
        \item MDPs model decision-making in uncertain environments.
        \item Several challenges exist when implementing MDPs effectively:
        \begin{enumerate}
            \item State Space Complexity
            \item Computational Complexity
            \item Partial Observability
            \item Scalability
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MDPs - State Space Complexity}
    \begin{block}{Definition}
        The state space is the set of all possible situations an agent can encounter, crucial for MDP design.
    \end{block}
    \begin{itemize}
        \item **Challenge**: The curse of dimensionality leads to enormous state spaces, complicating storage and computation.
        \item **Example**: In robotic navigation, each position and orientation in a grid results in a rapidly increasing state space size.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MDPs - Computational Complexity}
    \begin{block}{Definition}
        Refers to the time and space resources needed to solve MDPs.
    \end{block}
    \begin{itemize}
        \item **Challenge**: Algorithms like Value Iteration and Policy Iteration are computationally intensive.
        \item **Important Note**: Time to compute an optimal policy can increase exponentially with state space size, limiting real-time application.
    \end{itemize}
    \begin{equation}
        V_{k+1}(s) = \max_a \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V_k(s') \right]
    \end{equation}
    Where \( P(s'|s,a) \) is the state transition probability, \( R \) is the reward function, \( \gamma \) is the discount factor, and \( V \) is the value function.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MDPs - Other Considerations}
    \begin{itemize}
        \item **Partial Observability**: Agents may lack complete information about current states, leading to Partially Observable MDPs (POMDPs) which are harder to solve.
        \item **Scalability**: As MDPs grow, effectively scaling them to real-world applications can require reducing state space via approximations or abstractions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of State Space Complexity}
    \begin{itemize}
        \item **Robotics**: A robotic arm's state includes its position, object locations, and gripper state. Complex environments lead to unmanageable state spaces.
        \item **Gaming**: Video games generate immense states from character actions; considering all combinations over time results in significant computational demands.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Challenges of state space and computational complexity highlight the need for approximations or alternative methods in MDPs.
        \item Understanding these challenges is essential for designing effective algorithms for practical MDP applications.
    \end{itemize}
    \begin{block}{Conclusion}
        MDPs are powerful decision-making frameworks, but the challenges associated with state space complexity and computational demands require innovative approaches.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics and Implications of MDPs}
    \begin{block}{Introduction}
        Markov Decision Processes (MDPs) are foundational in Reinforcement Learning (RL) for decision-making in uncertain environments. Ethical considerations become crucial as MDPs are applied across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations}
    \begin{enumerate}
        \item \textbf{Autonomy}
            \begin{itemize}
                \item Definition: The right of individuals to make their own choices.
                \item Implication: Must respect human autonomy in automated decisions.
            \end{itemize}

        \item \textbf{Fairness}
            \begin{itemize}
                \item Definition: Treating all individuals or groups equally and without bias.
                \item Implication: MDPs can reinforce biases present in training data.
            \end{itemize}

        \item \textbf{Accountability}
            \begin{itemize}
                \item Definition: Obligation to explain decisions.
                \item Implication: Complex responsibility in adverse outcomes from MDPs.
            \end{itemize}

        \item \textbf{Privacy}
            \begin{itemize}
                \item Definition: Control over personal information.
                \item Implication: Responsible handling of data collected by MDPs is crucial.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Potential Impacts of MDPs}
    \begin{enumerate}
        \item \textbf{Societal Transformation}
            \begin{itemize}
                \item MDPs can enhance efficiency in various sectors.
            \end{itemize}

        \item \textbf{Decision-making Quality}
            \begin{itemize}
                \item High-quality decision-making possible but poor implementation can lead to disasters.
            \end{itemize}

        \item \textbf{Job Displacement}
            \begin{itemize}
                \item Automating tasks may lead to job losses, requiring workforce redefinition.
            \end{itemize}

        \item \textbf{Environmental Effects}
            \begin{itemize}
                \item Potential environmental benefits; mismanagement could worsen issues.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    \begin{block}{Example Scenario}
        A hospital using an MDP-based system to allocate emergency resources:
        \begin{itemize}
            \item Prioritizing patients solely on data-driven metrics, without individual consideration, can lead to unfair treatment.
            \item Ethical decision-making requires a balance of algorithms with human oversight.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        The application of MDPs holds immense potential but requires careful consideration of ethical implications.
    \end{block}
    \begin{itemize}
        \item MDPs must be applied with respect to ethics.
        \item Awareness of autonomy, fairness, accountability, and privacy is paramount.
        \item Societal impacts of MDP applications must be managed responsibly.
    \end{itemize}
    
    \begin{block}{Important Quote}
        “With great power comes great responsibility.”
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Overview}
    \begin{itemize}
        \item Recap of MDPs as a framework for decision-making.
        \item Dynamic programming techniques for reinforcement learning.
        \item Essential insights and real-world applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recap of Markov Decision Processes (MDPs)}
    \begin{enumerate}
        \item **Definition of MDPs**:
        \begin{itemize}
            \item **States (S)**: All possible states the agent can occupy.
            \item **Actions (A)**: All possible actions the agent can take.
            \item **Transition Function (T)**: \( T(s, a, s') = P(s' | s, a) \).
            \item **Reward Function (R)**: Rewards received: \( R(s, a) \).
            \item **Discount Factor ($\gamma$)**: Between 0 and 1, for future reward importance.
        \end{itemize}
        \item **Key Properties**:
        \begin{itemize}
            \item **Markov Property**: Next state depends only on the current state and action.
            \item **Policy ($\pi$)**: Strategy for action selection in each state.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming in Reinforcement Learning}
    \begin{enumerate}
        \item **Overview**:
        \begin{itemize}
            \item Algorithms that solve MDPs by simplifying problems.
            \item Essential for finding optimal policies in reinforcement learning.
        \end{itemize}
        \item **Key Methods**:
        \begin{itemize}
            \item **Value Iteration**:
            \begin{equation}
            V_{k+1}(s) = R(s, \pi(s)) + \gamma \sum_{s'} T(s, \pi(s), s') V_k(s')
            \end{equation}
            
            \item **Policy Iteration**: Policy evaluation and improvement.
            \item **Example**: Robot navigating a grid world with states, actions, and rewards.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Insights and Applications}
    \begin{itemize}
        \item **Applications**: Used in operations research, robotics, and economics.
        \item **Challenges**: Large state/action spaces lead to computational difficulties.
        \item **Approaches**: Development of approximations and simulation-based methods in reinforcement learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item MDPs offer a structured approach to dynamic decision problems.
        \item Dynamic programming is crucial for optimizing decision policies.
        \item Understanding MDPs helps analyze ethical implications in their applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Recap}
    \begin{equation}
    V^*(s) = \max_a \left(R(s, a) + \gamma \sum_{s'} T(s, a, s') V^*(s')\right)
    \end{equation}
\end{frame}


\end{document}