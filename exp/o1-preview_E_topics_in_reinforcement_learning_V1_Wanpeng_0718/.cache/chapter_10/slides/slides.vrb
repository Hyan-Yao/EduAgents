\frametitle{Example Output: PPO Algorithm Pseudocode}
    \begin{lstlisting}[language=Python]
for each iteration:
    collect trajectories using current policy
    calculate advantages using Generalized Advantage Estimation (GAE)
    update policy using the PPO objective:
    L = E[ min( r_t(θ) * A_t, clip(r_t(θ), 1 - ε, 1 + ε) * A_t ) ]
    \end{lstlisting}
    This pseudocode emphasizes the importance of managing the ratio of probabilities to enhance stability throughout the learning process.
