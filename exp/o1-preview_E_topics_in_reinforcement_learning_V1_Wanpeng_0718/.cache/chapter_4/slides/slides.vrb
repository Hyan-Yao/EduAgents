\frametitle{Key Components of DQN - Neural Networks & Experience Replay}
    \begin{itemize}
        \item \textbf{Neural Networks:}
        \begin{itemize}
            \item Used to approximate the Q-value function: \( Q(s, a; \theta) \)
            \item Neural network architecture:
            \begin{itemize}
                \item \textbf{Input Layer:} Represents state.
                \item \textbf{Hidden Layers:} Captures patterns between states and Q-values.
                \item \textbf{Output Layer:} Produces Q-values for all actions.
            \end{itemize}
        \end{itemize}

        \item \textbf{Experience Replay:}
        \begin{itemize}
            \item Technique for storing past experiences in a buffer to enhance learning.
            \item \textbf{Buffer:} Stores transitions \( (s, a, r, s', d) \).
            \item \textbf{Sampling:} Random batches sampled during training to reduce correlation between experiences.
        \end{itemize}
        \begin{block}{Example}
            In a video game:
            \begin{itemize}
                \item Moves and results are stored in replay buffer.
                \item Agent reviews past moves to learn effective strategies.
            \end{itemize}
        \end{block}
    \end{itemize}
