\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 3: Model-Free Reinforcement Learning}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model-Free Reinforcement Learning}
    \begin{block}{Overview of Model-Free Methods}
        Model-Free Reinforcement Learning (RL) approaches enable an agent to learn decision-making without requiring a model of the environment's dynamics. It focuses on learning policies or value functions directly from experience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Agent}: The decision-maker interacting with the environment.
        \item \textbf{Environment}: The system within which the agent operates.
        \item \textbf{State ($s$)}: Current situation representation of the agent.
        \item \textbf{Action ($a$)}: Choices available to the agent for changing the state.
        \item \textbf{Reward ($r$)}: Feedback from the environment post-action.
        \item \textbf{Policy ($\pi$)}: Strategy for choosing the next action based on the current state.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Focus on Q-Learning and SARSA}
    \begin{block}{Q-Learning}
        \begin{itemize}
            \item \textbf{Definition}: An off-policy learning algorithm that learns action values without a model.
            \item \textbf{Update Rule}:
            \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
            \end{equation}
            Where:
            \begin{itemize}
                \item $Q(s, a)$ = current estimate of action-value
                \item $\alpha$ = learning rate
                \item $r$ = received reward
                \item $\gamma$ = discount factor
                \item $s'$ = next state
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Focus on Q-Learning and SARSA (Continued)}
    \begin{block}{SARSA (State-Action-Reward-State-Action)}
        \begin{itemize}
            \item \textbf{Definition}: An on-policy algorithm where value updates are based on actions taken by the agent.
            \item \textbf{Update Rule}:
            \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
            \end{equation}
            Where:
            \begin{itemize}
                \item $a'$ = action chosen in state $s'$ according to the current policy.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Q-Learning and SARSA}
    \begin{block}{Q-Learning Example}
        In a grid world, if the agent moves from state A to state B and receives a reward of +10, it updates the Q-value for that action based on the maximum expected future rewards.
    \end{block}
    
    \begin{block}{SARSA Example}
        In the same grid world, if the agent moves from state A to state B and chooses action C in state B, it updates the Q-value based on the action taken (C) rather than the best possible action. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Model-free methods are beneficial in environments where dynamics are unknown or too complex to model.
        \item Q-Learning typically converges faster to optimal policies compared to SARSA, exploring maximum future rewards.
        \item SARSA can offer stability in certain environments by evaluating the action actually taken.
    \end{itemize}
    \begin{block}{Conclusion}
        Mastering model-free methods like Q-Learning and SARSA is crucial for developing effective learning agents that can interact with complex environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Fundamentals}
    \begin{block}{Key Concepts in Reinforcement Learning}
        Reinforcement Learning (RL) is a computational approach used to understand how agents can take actions within an environment to maximize cumulative reward. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - 1}
    \begin{enumerate}
        \item \textbf{Agent}
        \begin{itemize}
            \item \textbf{Definition}: The learner or decision-maker in RL.
            \item \textbf{Role}: Interacts with the environment to perform actions yielding rewards.
            \item \textbf{Example}: In chess, the player is the agent making moves based on the game state.
        \end{itemize}
        
        \item \textbf{Environment}
        \begin{itemize}
            \item \textbf{Definition}: Everything the agent interacts with, characterized by its states.
            \item \textbf{Role}: Changes state based on agent's actions and provides rewards.
            \item \textbf{Example}: In self-driving cars, the environment includes the road, traffic signals, etc.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{State (s)}
        \begin{itemize}
            \item \textbf{Definition}: A specific situation within the environment.
            \item \textbf{Role}: Provides context for the agent's actions.
            \item \textbf{Example}: Each position in a maze represents a different state (e.g., coordinates (x, y)).
        \end{itemize}
        
        \item \textbf{Action (a)}
        \begin{itemize}
            \item \textbf{Definition}: Decision made by the agent to interact with the environment.
            \item \textbf{Role}: Affects the state of the environment.
            \item \textbf{Example}: In a maze, possible actions include moving up, down, left, or right.
        \end{itemize}

        \item \textbf{Reward (r)}
        \begin{itemize}
            \item \textbf{Definition}: Scalar feedback signal received after an action in a state.
            \item \textbf{Role}: Evaluates actions, guiding the agent to maximize outcomes.
            \item \textbf{Example}: Reaching a goal may yield +10 points; hitting an obstacle incurs -5 points.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - 3}
    \begin{enumerate}
        \setcounter{enumi}{5} % Continue numbering
        \item \textbf{Policy (\(\pi\))}
        \begin{itemize}
            \item \textbf{Definition}: Mapping from states to actions; defines the agent's behavior.
            \item \textbf{Role}: Can be deterministic or stochastic.
            \item \textbf{Example}: Policy might dictate that in state (3, 2), move right with probability 0.8 and left with 0.2.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item RL is about learning from interaction.
            \item The agent's goal is to maximize total rewards.
            \item Understanding the relationship between states, actions, rewards, and policies is crucial for RL systems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{Summary}
        The fundamental components of reinforcement learning (agents, environments, states, actions, rewards, and policies) provide a foundation for understanding more complex algorithms like Q-learning and SARSA.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Q-learning - What is Q-learning?}
    Q-learning is a model-free reinforcement learning algorithm designed to learn the value of actions in a given state in order to maximize the total reward an agent can achieve. 
    It enables agents to make optimal decisions without requiring a model of the environment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Q-learning - Purpose and Functionality}
    \textbf{Purpose of Q-learning:}
    \begin{itemize}
        \item Learning Optimal Policy: The primary goal is to find the best action in a given state, known as the optimal policy.
        \item Handling Uncertainty: Q-learning allows agents to explore and learn from experiences in uncertain environments.
    \end{itemize}

    \textbf{How Q-learning Functions:}
    \begin{enumerate}
        \item \textbf{Q-Values:}
        \begin{itemize}
            \item Maintains a Q-table where each entry $Q(s, a)$ represents the expected utility of taking action $a$ in state $s$.
            \item Higher Q-values indicate greater expected rewards.
        \end{itemize}
        \item \textbf{Exploration vs. Exploitation:}
        \begin{itemize}
            \item Employs an exploration strategy (e.g., epsilon-greedy) to balance exploring new actions and exploiting known actions with higher Q-values.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Q-learning - Update Rule and Example}
    \textbf{Update Rule:}
    The Q-learning algorithm updates the Q-value using the formula:
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    \end{equation}
    Where:
    \begin{itemize}
        \item $Q(s, a)$: Current Q-value of action $a$ in state $s$
        \item $\alpha$: Learning rate
        \item $r$: Reward received after taking action $a$
        \item $\gamma$: Discount factor
        \item $s'$: New state after action $a$
        \item $\max_{a'} Q(s', a')$: Maximum expected future reward for the new state
    \end{itemize}

    \textbf{Example Scenario:}
    Imagine an agent navigating a grid-like maze:
    \begin{itemize}
        \item States ($s$): Each cell in the grid
        \item Actions ($a$): Moving up, down, left, or right
        \item Rewards ($r$): +10 for reaching the goal, -1 for hitting a wall, and 0 for other movements
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning Algorithm Steps - Overview}
    \begin{block}{Overview of Q-learning}
        Q-learning is a model-free reinforcement learning algorithm that learns the value of actions in given states. Its goal is to find an optimal policy that maximizes the cumulative reward over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning Algorithm Steps - Initialization}
    \begin{enumerate}
        \item \textbf{Initialization}: 
        \begin{itemize}
            \item Initialize Q-table \( Q(s, a) \) arbitrarily for all state-action pairs.
            \item Set the learning rate \( \alpha \) (0 < \( \alpha \) ≤ 1).
            \item Set the discount factor \( \gamma \) (0 ≤ \( \gamma \) < 1).
            \item Decide on an exploration strategy (e.g., epsilon-greedy).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning Algorithm Steps - Update Procedure}
    \begin{enumerate}
        \setcounter{enumi}{1}  % Start enumeration at 2
        \item \textbf{For each episode}:
        \begin{itemize}
            \item Initialize state \( s \).
        \end{itemize}
        
        \item \textbf{Repeat for each step until terminal state}:
        \begin{itemize}
            \item \textbf{Choose Action}: 
            \begin{itemize}
                \item With probability \( \epsilon \), select a random action \( a \) (exploration).
                \item With probability \( 1 - \epsilon \), select \( a = \arg\max_a Q(s, a) \) (exploitation).
            \end{itemize}
            \item \textbf{Take Action \& Observe Reward}:
            \begin{itemize}
                \item Execute action \( a \), observe reward \( r \) and new state \( s' \).
            \end{itemize}
            \item \textbf{Update Q-value}: 
            \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a)\right]
            \end{equation}
            \item \textbf{Transition to New State}: Set \( s = s' \).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q-learning Algorithm Steps - Convergence}
    \begin{enumerate}
        \setcounter{enumi}{3}  % Continue enumeration
        \item \textbf{Convergence Check}: 
        \begin{itemize}
            \item After many episodes, if the change in Q-values is minimal, the algorithm converges to the optimal Q-value function.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Exploration vs. Exploitation 
            \item Learning Rate \( \alpha \) 
            \item Discount Factor \( \gamma \)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Pseudocode}
    \begin{block}{Example}
        Assume an agent navigates a grid-world. If the agent receives a reward of +10, it updates its Q-table to improve future actions based on current experiences.
    \end{block}
    
    \begin{lstlisting}[language=Python]
for episode in range(num_episodes):
    s = initialize_state()
    for t in range(max_steps):
        a = choose_action(s)
        r, s' = perform_action(a)
        Q[s, a] = Q[s, a] + alpha * (r + gamma * max(Q[s']) - Q[s, a])
        s = s'
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs Exploitation}
    \begin{block}{Introduction to the Trade-off}
        In reinforcement learning (RL), the \textbf{exploration vs exploitation} trade-off is a fundamental dilemma. It refers to the balance between:
        \begin{itemize}
            \item \textbf{Exploration}: Trying out new actions to discover their rewards.
            \item \textbf{Exploitation}: Selecting the best-known actions based on current knowledge to maximize rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of the Trade-off}
    \begin{block}{Why is this Trade-off Important?}
        \begin{enumerate}
            \item \textbf{Learning Efficiency}: 
                Without enough exploration, an agent may never discover better actions (suboptimal policies). Conversely, excessive exploration can lead to poor performance as the agent may not leverage the knowledge it has gathered.
            \item \textbf{Convergence to Optimal Policy}:
                Proper balancing between exploration and exploitation is essential for the convergence of algorithms like Q-learning.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration Strategies}
    \begin{block}{Exploration Strategies}
        \begin{enumerate}
            \item \textbf{Epsilon-Greedy}:
                With probability \( \epsilon \), choose a random action (exploration), and with probability \( 1 - \epsilon \), choose the action with the highest Q-value (exploitation).
                \begin{itemize}
                    \item Example: If \( \epsilon = 0.1 \), the agent will explore 10\% of the time.
                \end{itemize}
            \item \textbf{Softmax Action Selection}:
                Actions are chosen based on a probability distribution derived from Q-values (higher Q-values lead to higher probabilities).
            \item \textbf{Upper Confidence Bound (UCB)}:
                Select actions based on both their Q-values and the uncertainty associated with them.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding SARSA - Overview}
    
    \begin{block}{What is SARSA?}
        SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm. It is used to estimate the Q-value function, enabling agents to make sequential decisions.
    \end{block}
    
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{On-Policy Learning:} Updates the Q-value based on actions taken from the current policy.
            \item \textbf{Temporal-Difference Learning:} Learns from incomplete episodes with updates based on current estimates.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding SARSA - Q-Value Update}
    
    \begin{block}{How SARSA Works}
        \begin{itemize}
            \item \textbf{Action Selection:} The agent selects actions based on the current policy.
            \item \textbf{Experience Tuple:} After taking action \( a \) in state \( s \), receiving reward \( r \), and moving to state \( s' \), the next action \( a' \) is chosen.
        \end{itemize}
    \end{block}
    
    \begin{block}{Q-Value Update Formula}
        The Q-value is updated using the formula:
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( Q(s, a) \) = Current Q-value for action \( a \) in state \( s \)
            \item \( \alpha \) = Learning rate (0 < \( \alpha \) ≤ 1)
            \item \( r \) = Reward received after taking action \( a \)
            \item \( \gamma \) = Discount factor (0 ≤ \( \gamma \) < 1)
            \item \( Q(s', a') \) = Estimated Q-value for the next state \( s' \) and action \( a' \)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding SARSA - Differences from Q-Learning}
    
    \begin{block}{Differences from Q-Learning}
        \begin{itemize}
            \item \textbf{Policy Type:} SARSA is on-policy; Q-learning is off-policy.
            \item \textbf{Exploration Impact:} SARSA's value updates are directly influenced by its current policy, leading to greater stability.
        \end{itemize}
    \end{block}

    \begin{block}{Example Scenario}
        Consider a grid world where an agent moves left or right. If it chooses right (action \( a \)) from state \( s \), receives a reward, transitions to state \( s' \), and selects action \( a' \), SARSA updates its Q-value based on the action taken, reflecting real experiences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding SARSA - Key Takeaways}
    
    \begin{itemize}
        \item SARSA is \textbf{on-policy} and adapts its learning based on the current policy.
        \item It is advantageous in environments where following a current strategy is preferred.
        \item A solid understanding of SARSA is essential for grasping more complex reinforcement learning algorithms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Algorithm Steps - Introduction}
    \begin{block}{What is SARSA?}
        SARSA (State-Action-Reward-State-Action) is a model-free reinforcement learning algorithm that updates the action-value function based on the current state and action taken, along with the new state and action being taken next. 
        It directly learns the Q-values, which estimate the expected future rewards for taking an action in a given state.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Algorithm Steps - Overview}
    \textbf{Steps of the SARSA Algorithm}
    \begin{enumerate}
        \item \textbf{Initialize Q-values}:
            \begin{itemize}
                \item Define state space \( S \) and action space \( A \).
                \item Initialize \( Q(s,a) \) arbitrarily (commonly to zero).
                \[
                Q(s,a) \leftarrow 0 \quad \forall s \in S, \forall a \in A
                \]
            \end{itemize}
        \item \textbf{Choose an Action}:
            \begin{itemize}
                \item Select action \( A \) using an ε-greedy policy.
                \[
                A = \begin{cases} 
                \text{random action} & \text{with probability } \epsilon \\ 
                \arg \max_a Q(S, a) & \text{with probability } 1 - \epsilon 
                \end{cases}
                \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Algorithm Steps - Continuation}
    \begin{enumerate}[resume]
        \item \textbf{Interact with the Environment}:
            \begin{itemize}
                \item Take action \( A \), observe reward \( R \) and next state \( S' \).
            \end{itemize}
        \item \textbf{Choose Next Action}:
            \begin{itemize}
                \item Select action \( A' \) in state \( S' \) using the ε-greedy policy.
            \end{itemize}
        \item \textbf{Update the Q-value}:
            \begin{itemize}
                \item Update based on the received reward and Q-value of the new state-action pair:
                \[
                Q(S, A) \leftarrow Q(S, A) + \alpha \cdot \left[ R + \gamma Q(S', A') - Q(S, A) \right]
                \]
            \end{itemize}
        \item \textbf{Transition to the Next State}:
            \begin{itemize}
                \item Set \( S \leftarrow S' \) and \( A \leftarrow A' \).
            \end{itemize}
        \item \textbf{Repeat}:
            \begin{itemize}
                \item Continue until episode ends or a predetermined number of time steps is reached.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Implementation of Q-learning and SARSA}
    \begin{block}{Introduction}
    \begin{itemize}
        \item \textbf{Q-learning} and \textbf{SARSA (State-Action-Reward-State-Action)} are foundational algorithms in reinforcement learning.
        \item Both algorithms aim to learn the action-value function \( Q(s, a) \) that estimates the expected reward of performing action \( a \) in state \( s \).
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Differences}
    \begin{itemize}
        \item \textbf{Q-learning}: Off-policy; learns about the optimal policy independently of the agent's actions.
        \item \textbf{SARSA}: On-policy; updates the action-value function based on the actions actually taken by the agent.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Q-learning Implementation Steps}
    \begin{enumerate}
        \item \textbf{Initialize}:
            \begin{itemize}
                \item Create a Q-table \( Q(s, a) \) initialized to zeros for all state-action pairs.
                \item Define parameters: learning rate \( \alpha \), discount factor \( \gamma \), and exploration factor \( \epsilon \).
            \end{itemize}
        \item \textbf{Choose Action}:
            \begin{itemize}
                \item Use an exploration strategy (like ε-greedy).
                \item Action \( a \) is chosen based on \( \epsilon \): 
                \[
                a = \begin{cases}
                \text{random action} & \text{with probability } \epsilon \\
                \text{argmax } Q(s, a) & \text{with probability } 1 - \epsilon
                \end{cases}
                \]
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Q-learning Implementation Steps (contd.)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Observe Reward and Next State}:
            \begin{itemize}
                \item Execute action \( a \), observe reward \( r \), and the next state \( s' \).
            \end{itemize}
        \item \textbf{Update Q-value}:
            \begin{itemize}
                \item Update the Q-value using the Bellman equation:
                \[
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
                \end{equation}
            \end{itemize}
        \item \textbf{Repeat}:
            \begin{itemize}
                \item Set the next state \( s' \) as the current state \( s \) and continue until convergence.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{SARSA Implementation Steps}
    \begin{enumerate}
        \item \textbf{Initialize}:
            \begin{itemize}
                \item Create a Q-table \( Q(s, a) \) initialized to zeros.
                \item Define parameters: \( \alpha \), \( \gamma \), and \( \epsilon \).
            \end{itemize}
        \item \textbf{Choose Action}:
            \begin{itemize}
                \item Use the ε-greedy approach to select the initial action \( a_0 \) from \( s_0 \).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{SARSA Implementation Steps (contd.)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Observe Reward and Next State}:
            \begin{itemize}
                \item Execute action \( a_0 \), observe reward \( r_0 \) and next state \( s_1 \).
            \end{itemize}
        \item \textbf{Choose Next Action}:
            \begin{itemize}
                \item Select the next action \( a_1 \) from the new state \( s_1 \) using ε-greedy strategy.
            \end{itemize}
        \item \textbf{Update Q-value}:
            \begin{itemize}
                \item Update the Q-value using the SARSA update rule:
                \[
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
                \]
            \end{itemize}
        \item \textbf{Repeat}:
            \begin{itemize}
                \item Continue this process in an episode until convergence.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily]
import numpy as np
import random

# Initialize parameters
num_states = 5
num_actions = 2
Q = np.zeros((num_states, num_actions))  # Q-table
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.1  # Exploration rate

# Q-learning algorithm
for episode in range(1000):
    state = np.random.randint(num_states)
    while not done:
        if random.uniform(0, 1) < epsilon:
            action = random.randint(0, num_actions - 1)  # Explore
        else:
            action = np.argmax(Q[state])  # Exploit
        
        # Take action, observe new state and reward
        new_state, reward = take_action(state, action) 
        
        # Update Q-table
        Q[state, action] += alpha * (reward + gamma * np.max(Q[new_state]) - Q[state, action])
        
        state = new_state  # Update state
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Q-learning evaluates the optimal policy while learning how to act.
        \item SARSA learns the policy followed by the agent, providing more stability in certain scenarios.
        \item Both algorithms improve through trial-and-error, iteratively refining action-value estimates.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{block}{}
    Understanding the implementation of Q-learning and SARSA equips you with the fundamental tools to tackle various reinforcement learning tasks, paving the way for experimenting with more complex models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Comparison}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Q-learning} and \textbf{SARSA} are model-free reinforcement learning algorithms.
            \item Crucial to understand their performance differences for appropriate algorithm selection.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Comparison - Metrics}
    \begin{block}{Performance Metrics}
        \begin{enumerate}
            \item \textbf{Convergence Rate}: Speed of approaching the optimal policy.
            \item \textbf{Stability}: Consistency in learning across multiple runs.
            \item \textbf{Optimal Action Selection}: Capability to choose best actions based on learned values.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Comparison - Scenarios}
    \begin{block}{Comparison Scenarios}
        \begin{itemize}
            \item \textbf{Scenario 1: Simple Grid World}
                \begin{itemize}
                    \item \textbf{Q-learning}: Faster goal achievement due to aggressive exploration.
                    \item \textbf{SARSA}: More stable learning but may take longer to converge.
                \end{itemize}

            \item \textbf{Scenario 2: Mountain Car}
                \begin{itemize}
                    \item \textbf{Q-learning}: Quickly learns the strategy but risks overshooting.
                    \item \textbf{SARSA}: More conservative learning; slower but reliable convergence.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Examples}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Exploration vs. Exploitation}:
                \begin{itemize}
                    \item Q-learning favors exploration to discover better strategies faster.
                    \item SARSA exploits known strategies sooner but may yield safer, suboptimal policies.
                \end{itemize}
            \item \textbf{Use Cases}:
                \begin{itemize}
                    \item Q-learning is effective when exploration is rewarded.
                    \item SARSA is suitable for dynamic environments requiring adaptation.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=python]
# Q-learning update rule
Q[state, action] += alpha * (reward + gamma * max(Q[next_state, a]) - Q[state, action])

# SARSA update rule
Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Both Q-learning and SARSA are effective for learning policies.
        Understanding their performance characteristics aids in selecting the right algorithm,
        depending on the needs for exploration and stability in specific applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Reinforcement Learning}
    Exploration of ethical implications associated with implementing Q-learning and SARSA in real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Ethical Considerations}
    \begin{itemize}
        \item RL has applications in healthcare, finance, robotics, and autonomous driving.
        \item Implementation of Q-learning and SARSA requires careful analysis of ethical implications.
        \item Responsible practices must guide the usage of these algorithms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Issues in RL}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item RL may perpetuate biases in domains like criminal justice.
                \item Example: An RL agent optimizing hiring processes may favor certain demographics.
            \end{itemize}

        \item \textbf{Transparency and Explainability}
            \begin{itemize}
                \item Many RL systems are 'black boxes'.
                \item Example: An RL system in healthcare may lack transparency about treatment recommendations.
            \end{itemize}
            
        \item \textbf{Safety and Security}
            \begin{itemize}
                \item Critical for applications like autonomous vehicles to ensure safe outcomes.
                \item Example: Proper training is needed for diverse driving scenarios to avoid accidents.
            \end{itemize}
            
        \item \textbf{Accountability}
            \begin{itemize}
                \item Essential to determine responsibility for RL system outcomes.
                \item Example: Clarity is required when an RL financial advisor causes losses.
            \end{itemize}

        \item \textbf{Privacy Concerns}
            \begin{itemize}
                \item Large datasets may contain sensitive personal data.
                \item Example: Healthcare RL models must protect patient data from exposure.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model-Specific Ethical Considerations}
    \begin{itemize}
        \item \textbf{Q-learning:}
            \begin{itemize}
                \item Continuous exploration may lead to exposure to risky environments.
            \end{itemize}
        
        \item \textbf{SARSA:}
            \begin{itemize}
                \item On-policy learning can result in misguided policies and suboptimal outcomes.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{itemize}
        \item Balance is needed between innovation and ethical responsibility in RL.
        \item Key points to emphasize:
        \begin{itemize}
            \item Evaluate biases in training data and reward structures.
            \item Strive for transparency in decision-making.
            \item Implement safety protocols in critical applications.
            \item Establish accountability frameworks for outcomes.
            \item Safeguard privacy and comply with data protection regulations.
        \end{itemize}
    \end{itemize}
\end{frame}


\end{document}