\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}

% Title Page Information
\title[Model Evaluation Techniques]{Week 8: Model Evaluation Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Evaluation in Data Science}
    
    \begin{block}{Definition}
        Model evaluation is a critical step in data science and machine learning. It assesses how well a model performs on a dataset, ensuring its effectiveness in predictions.
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Model Evaluation}
    
    \begin{itemize}
        \item \textbf{Model Performance Assessment}:
            \begin{itemize}
                \item Evaluates generalization to unseen data.
                \item Metrics: Accuracy, precision, recall, F1 score, and ROC-AUC.
            \end{itemize}
        
        \item \textbf{Informed Decision Making}:
            \begin{itemize}
                \item Enables data-driven decisions.
                \item Reduces risks of implementing ML solutions.
            \end{itemize}
        
        \item \textbf{Identifying Overfitting and Underfitting}:
            \begin{itemize}
                \item Distinguishes between complex (overfitting) and simple (underfitting) models.
                \item Balancing bias and variance is key.
            \end{itemize}
        
        \item \textbf{Improvement of Model}:
            \begin{itemize}
                \item Provides insights into feature importance.
                \item Facilitates comparison to select the best model.
            \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}

    \begin{itemize}
        \item \textbf{ChatGPT}:
            \begin{itemize}
                \item Utilizes evaluation techniques for enhanced conversational abilities.
                \item Ensures relevant, coherent, and user-friendly responses.
            \end{itemize}
        
        \item \textbf{Healthcare}:
            \begin{itemize}
                \item Models predict disease outbreaks or patient outcomes.
                \item Evaluation ensures safety and efficacy in predictions.
            \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    
    \begin{itemize}
        \item \textbf{Metrics Matter}: Choosing the right metrics is crucial depending on the task (classification, regression, etc.).
        \item \textbf{Continuous Process}: Model evaluation should be integrated into the model development cycle.
        \item \textbf{Involvement of Domain Expertise}: Collaboration with experts enhances evaluation by providing context to metrics.
    \end{itemize}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    \begin{block}{Summary}
        Model evaluation is indispensable for ensuring the reliability and effectiveness of machine learning models. Proper understanding and implementation of evaluation techniques lead to improved decision-making and insights.
    \end{block}

    \begin{block}{Next Steps}
        \begin{enumerate}
            \item Explore motivations behind model evaluation.
            \item Discuss specific evaluation techniques for different tasks.
            \item Review case studies on the impact of evaluation in businesses and technologies, including AI.
        \end{enumerate}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Model Evaluation is Crucial}
    \begin{block}{Understanding the Importance of Model Evaluation}
        Model evaluation is a fundamental step in the data science workflow that ensures our predictive models are effective and reliable. It serves several key motivations, particularly in decision-making and real-world applications such as AI systems, including ChatGPT.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Motivations for Model Evaluation}
    \begin{enumerate}
        \item \textbf{Model Performance Validation}
        \begin{itemize}
            \item \textit{Purpose:} Assess performance on unseen data.
            \item \textit{Benefit:} Prevents overfitting.
            \item \textit{Example:} Model A achieves 95\% accuracy on training but only 70\% on test data.
        \end{itemize}
        
        \item \textbf{Informed Decision-Making}
        \begin{itemize}
            \item \textit{Purpose:} Deliver data-driven insights.
            \item \textit{Benefit:} Encourages better strategies.
            \item \textit{Example:} Accurate financial predictions can prevent substantial losses.
        \end{itemize}

        \item \textbf{Comparative Analysis}
        \begin{itemize}
            \item \textit{Purpose:} Evaluate multiple models.
            \item \textit{Benefit:} Identifies the best-performing model.
            \item \textit{Example:} Precision and recall comparisons between Model B and C.
        \end{itemize}

        \item \textbf{Understanding Model Limitations}
        \begin{itemize}
            \item \textit{Purpose:} Recognize potential failure areas.
            \item \textit{Benefit:} Guides improvements and risk mitigation.
            \item \textit{Example:} ChatGPT's reliance on training data calls for careful application in sensitive fields.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Application: ChatGPT and AI Systems}
    \begin{itemize}
        \item ChatGPT utilizes extensive datasets to generate human-like text based on user prompts.
        \item \textbf{Evaluation Impact:}
        \begin{itemize}
            \item Regular evaluations fine-tune language capabilities and context understanding.
            \item User feedback loops and A/B testing contribute to improvements.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Model evaluation is crucial for validating performance and aiding decision-making.
            \item Real-world applications like ChatGPT exemplify the benefits of robust evaluations.
            \item Effective evaluations can mitigate risks and enhance user experiences.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    \begin{block}{Engagement Ahead}
        Delve into various performance metrics that quantify model effectiveness, including accuracy, precision, recall, and more in the upcoming slide.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Model Performance Metrics - Introduction}
    \begin{itemize}
        \item Evaluating model performance is crucial for real-world applications of predictive models.
        \item Helps developers make informed decisions on model deployment and improvement.
        \item Examples include AI-enhanced chatbots like ChatGPT.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics for Model Evaluation - Overview}
    \begin{itemize}
        \item \textbf{Accuracy}
        \item \textbf{Precision}
        \item \textbf{Recall (Sensitivity)}
        \item \textbf{F1-Score}
        \item \textbf{ROC-AUC (Receiver Operating Characteristic - Area Under Curve)}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics for Model Evaluation - Detailed Descriptions}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item Definition: Ratio of correctly predicted instances to total instances.
                \item Formula: 
                \[
                \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}} 
                \]
                \item Use Case: Suitable for balanced class distributions.
            \end{itemize}
        \item \textbf{Precision}
            \begin{itemize}
                \item Definition: Ratio of true positives to the sum of true positives and false positives.
                \item Formula:
                \[
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}} 
                \]
                \item Use Case: Critical in scenarios like medical diagnoses.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics for Model Evaluation - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2} % Resume enumeration from 3
        \item \textbf{Recall (Sensitivity)}
            \begin{itemize}
                \item Definition: Ratio of true positives to the sum of true positives and false negatives.
                \item Formula:
                \[
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}} 
                \]
                \item Use Case: Important in fraud detection.
            \end{itemize}
        \item \textbf{F1-Score}
            \begin{itemize}
                \item Definition: Harmonic mean of precision and recall.
                \item Formula:
                \[
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} 
                \]
                \item Use Case: Useful for imbalanced classes.
            \end{itemize}
        \item \textbf{ROC-AUC}
            \begin{itemize}
                \item Definition: A graph of true positive rates versus false positive rates at various thresholds.
                \item Use Case: Useful in binary classification, indicates model's ability to distinguish classes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Importance of context in choosing appropriate metrics.
        \item Accuracy can be misleading in imbalanced datasets; prefer metrics such as Precision, Recall, and F1-score.
        \item Comprehensive evaluation using multiple metrics gives a better understanding of model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy - Definition}
    \begin{block}{Definition of Accuracy}
        \textbf{Accuracy} is a performance metric that measures the proportion of correct predictions made by a model out of all predictions. It is expressed mathematically as:
        \begin{equation}
        \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        Where:
        \begin{itemize}
            \item \(TP\) = True Positives (correctly predicted positives)
            \item \(TN\) = True Negatives (correctly predicted negatives)
            \item \(FP\) = False Positives (incorrectly predicted positives)
            \item \(FN\) = False Negatives (incorrectly predicted negatives)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy - Significance and Appropriate Usage}
    \begin{block}{Significance of Accuracy}
        \begin{itemize}
            \item \textbf{Simplicity}: Accuracy is straightforward and easy to interpret, making it commonly used in initial evaluations of models.
            \item \textbf{Global Performance Indicator}: Provides a high-level view of model performance across all classes, particularly when classes are balanced.
        \end{itemize}
    \end{block}
    
    \begin{block}{When is Accuracy Appropriate?}
        \begin{itemize}
            \item \textbf{Balanced Classes}: Use accuracy when the classes are roughly equal in distribution, such as a binary classification problem with 50\% positive and 50\% negative examples.
            \item \textbf{Initial Baseline Model}: Use accuracy to gauge baseline performance before exploring more complex metrics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy - Examples and Key Points}
    \begin{block}{Examples}
        \begin{enumerate}
            \item \textbf{Email Spam Detection}: 
            \[
            \text{Accuracy} = \frac{90}{100} = 0.90 \text{ or 90\%}
            \]
            \item \textbf{Image Classification}: 
            \[
            \text{Accuracy} = \frac{180}{200} = 0.90 \text{ or 90\%}
            \]
        \end{enumerate}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Accuracy does not differentiate between the types of errors.
            \item High accuracy may not indicate a good model, especially in imbalanced class situations (e.g., fraud detection).
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        While accuracy is useful for model performance evaluation, it should be paired with metrics like precision, recall, and F1-score for a holistic assessment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Introduction}
    \begin{itemize}
        \item \textbf{What is Precision?}
        \begin{itemize}
            \item Definition: Precision measures the accuracy of positive predictions made by a model.
            \item It refers to the proportion of true positive results in all instances classified as positive.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Calculation}
    \begin{itemize}
        \item \textbf{Formula for Precision}:
        \begin{equation} 
            \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
        \end{equation}
        \item \textbf{Where}:
        \begin{itemize}
            \item True Positives (TP): The number of correct positive predictions.
            \item False Positives (FP): The number of incorrect positive predictions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Importance and Examples}
    \begin{itemize}
        \item \textbf{Why is Precision Important?}
        \begin{itemize}
            \item \textbf{Context-Sensitive}: In scenarios where the cost of false positives is high, precision becomes more important than accuracy.
            \item \textbf{Examples}:
            \begin{itemize}
                \item \textit{Email Spam Detection}: High precision reduces errors of misclassifying legitimate emails.
                \item \textit{Medical Testing}: A false positive may lead to unnecessary stress and invasive testing.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Example Scenario}
        \item Imagine a model predicting whether an email is spam:
        \begin{itemize}
            \item TP = 30 spam emails
            \item TN = 60 legitimate emails
            \item FN = 10 spam emails wrongly classified as legitimate
            \item FP = 10 legitimate emails wrongly classified as spam
        \end{itemize}
    \end{block}
    
    \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP} = \frac{30}{30 + 10} = \frac{30}{40} = 0.75
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Conclusion and Key Points}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item Precision vs. Accuracy: Precision focuses on positive predictions, while accuracy assesses overall model correctness, including true negatives.
            \item Trade-offs: Increasing precision may lower recall (and vice-versa). Choosing the right metric depends on the use case and the consequences of false positives and negatives.
        \end{itemize}
        
        \item \textbf{Conclusion}:
        \begin{itemize}
            \item Precision is vital in situations where false positives pose greater risks or costs than false negatives.
            \item Understanding this metric aids in developing more reliable predictive models, especially in sensitive areas like healthcare and finance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - Definition}
    \begin{block}{Definition of Recall}
        Recall, also known as sensitivity or true positive rate, is a performance metric for classification models that measures the ability of a model to identify all relevant instances in the dataset. It is defined mathematically as:
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}
    \end{block}
    
    \begin{itemize}
        \item \textbf{True Positives (TP)}: instances correctly predicted as positive.
        \item \textbf{False Negatives (FN)}: instances that are positive but were incorrectly predicted as negative.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - Importance in Medical Testing}
    \begin{block}{Importance of Recall}
        Recall is critical in scenarios where missing a positive instance can have severe consequences, such as:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Medical Testing:}
            In disease screening tests (e.g., cancer), failing to detect a condition (a false negative) can delay treatment and worsen health. High recall is therefore prioritized.
            \begin{itemize}
                \item \textbf{Example:} A test with 90\% recall identifies 90 out of 100 patients who have the illness correctly, while 10 are missed.
            \end{itemize}
        \item \textbf{Fraud Detection:}
            In financial services, a model aimed at detecting fraudulent transactions requires high recall to minimize financial losses from missed fraudulent activities.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - When to Prioritize}
    \begin{block}{When to Prioritize Recall}
        Recall should be prioritized in situations where:
    \end{block}
    
    \begin{itemize}
        \item The cost of false negatives is high (e.g., in life-threatening medical situations).
        \item Detecting all possible positives is more critical than including false positives.
    \end{itemize}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Recall is crucial in high-stakes environments where overlooking a positive instance is detrimental.
            \item It’s essential to balance recall and precision in scenarios requiring both metrics.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        \begin{itemize}
            \item Recall assesses the completeness of a model's positive predictions.
            \item In high-risk settings, prioritizing recall minimizes the consequences of missing critical positive instances.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Introduction}
    \begin{block}{Definition}
        The F1 score is a crucial metric in evaluating the performance of classification models, particularly when classes are imbalanced.
    \end{block}
    \begin{itemize}
        \item Balances precision and recall
        \item Provides a single score representing model performance
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Importance}
    \begin{itemize}
        \item \textbf{Balanced Evaluation:} Useful when one class dominates, like in fraud or disease detection.
        \item \textbf{Real-World Applications:} Vital in fields where false negatives are costly (e.g., medical diagnoses).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Definitions Recap}
    \begin{itemize}
        \item \textbf{Precision:} 
        \[
        \text{Precision} = \frac{TP}{TP + FP}
        \]
        \item \textbf{Recall:} 
        \[
        \text{Recall} = \frac{TP}{TP + FN}
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Formula}
    \begin{block}{F1-Score Formula}
        \[
        F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \]
        \begin{itemize}
            \item Harmonic mean ensures precision and recall are both acknowledged.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Example Calculation}
    \textbf{Example:} Spam email classification
    \begin{itemize}
        \item True Positives (TP): 70
        \item False Positives (FP): 30
        \item False Negatives (FN): 10
    \end{itemize}
    \begin{align*}
        \text{Precision} &= \frac{70}{70 + 30} = 0.7 = 70\% \\
        \text{Recall} &= \frac{70}{70 + 10} = 0.875 = 87.5\%
    \end{align*}
    \begin{block}{Calculating F1-Score}
        \[
        F1 \approx 0.776 = 77.6\%
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Key Points and Conclusion}
    \begin{itemize}
        \item Useful when precision or recall cannot be neglected
        \item Higher F1 score reflects better model effectiveness
    \end{itemize}
    \begin{block}{Conclusion}
        The F1-score is an invaluable metric, especially in scenarios where true positives and false alarms must be balanced.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC-AUC - Understanding ROC}
    \begin{itemize}
        \item The ROC curve evaluates binary classification model performance.
        \item It shows the trade-off between \textbf{True Positive Rate (TPR)} and \textbf{False Positive Rate (FPR)} across thresholds.
        \begin{equation}
            TPR = \frac{True Positives}{True Positives + False Negatives}
        \end{equation}
        \begin{equation}
            FPR = \frac{False Positives}{False Positives + True Negatives}
        \end{equation}
        \item Helps visualize model prediction changes with threshold variations.
        \item Ideal models have steep curves, indicating high TPR and low FPR.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC-AUC - AUC Explanation}
    \begin{itemize}
        \item \textbf{AUC (Area Under the Curve)} measures a model’s ability to differentiate classes.
        \item AUC ranges from 0 to 1:
            \begin{itemize}
                \item AUC = 0.5: No distinction (random guessing).
                \item AUC = 1.0: Perfect distinction.
            \end{itemize}
        \item Example: A binary classifier predicting spam:
            \begin{itemize}
                \item Vary thresholds (0.1, 0.5, 0.9) to get points on the ROC curve.
                \item AUC indicates model performance across thresholds.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC-AUC - Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Usefulness across thresholds:} ROC and AUC assess model performance irrespective of threshold choice.  
        \item \textbf{Comparative Evaluation:} Higher AUC generally indicates better performance among models.
        \item \textbf{Threshold Independence:} Effective for imbalanced datasets.
        \item \textbf{Conclusion:} Understanding ROC and AUC is vital for evaluating binary classifiers, especially in fields like medical diagnosis and spam detection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Introduction}
    \begin{block}{Understanding Model Evaluation Metrics}
        When evaluating machine learning models, the choice of metric can greatly influence perceived performance. 
        Metrics are not universally applicable; they depend on the specific objectives of the project and the nature of the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics - Part 1}
    \begin{enumerate}
        \item \textbf{Accuracy}:
        \begin{itemize}
            \item \textbf{Definition}: Proportion of true results (TP + TN) among all cases.
            \item \textbf{Trade-offs}: Can be misleading with imbalanced datasets.
            \item \textbf{Usage}: Best for balanced datasets.
        \end{itemize}
        
        \item \textbf{Precision and Recall}:
        \begin{itemize}
            \item \textbf{Precision}: \(\text{Precision} = \frac{TP}{TP + FP}\) 
            \item \textbf{Recall}: \(\text{Recall} = \frac{TP}{TP + FN}\) 
            \item \textbf{Trade-offs}: Increasing precision often reduces recall and vice versa. Use F1-score for balance.
            \item \textbf{F1-score}: \(\text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\)  
            \item \textbf{Usage}: Useful in contexts with different costs for false positives and false negatives.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue enumeration from the previous frame
        \item \textbf{ROC-AUC}:
        \begin{itemize}
            \item \textbf{ROC Curve}: Sensitivity vs. (1-specificity).
            \item \textbf{AUC}: Area under the ROC curve.
            \item \textbf{Trade-offs}: Captures model performance across thresholds, ideal for binary classification.
        \end{itemize}
        
        \item \textbf{F-beta Score}:
        \begin{itemize}
            \item \textbf{Definition}: Generalization of F1-score to tune balance between precision and recall.
            \item \( F_\beta = (1 + \beta^2) \times \frac{\text{Precision} \times \text{Recall}}{(\beta^2 \times \text{Precision}) + \text{Recall}} \)
            \item \textbf{Usage}: Adjust \( \beta \) to weigh precision or recall based on context.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric}
    \begin{itemize}
        \item \textbf{Nature of the Problem}:
        \begin{itemize}
            \item Classification vs. Regression: Metrics differ based on task type.
            \item Imbalanced Datasets: Favor precision, recall, or AUC over accuracy.
        \end{itemize}
        
        \item \textbf{Business Context}:
        \begin{itemize}
            \item Cost of Errors: Understand false positives vs. false negatives. Example: In medical diagnostics, high recall may be priority over precision.
        \end{itemize}

        \item \textbf{Insights from Data}:
        \begin{itemize}
            \item Use metrics that align with business goals rather than just statistical accuracy.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item No one-size-fits-all metric; choice depends on specific context and model goals.
        \item Understand implications of each metric; a single good metric may hide significant issues.
        \item Adapt metrics to align with the most valuable trade-offs in your application.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation in Practice}
    \begin{block}{Overview}
        Model evaluation is a critical step in the data science workflow. It helps to ascertain how well a model performs on unseen data, guiding model selection and refinement. This presentation showcases case studies that highlight various model evaluation techniques used in real-world projects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Part 1}
    \begin{itemize}
        \item \textbf{Purpose of Model Evaluation:}
        \begin{itemize}
            \item Measure and improve model performance.
            \item Prevent issues like overfitting.
            \item Enable selection of the most suitable model based on specific criteria.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Customer Churn Prediction}
    \begin{block}{Scenario}
        A telecom company aims to reduce churn by identifying customers likely to leave.
    \end{block}
    \begin{itemize}
        \item \textbf{Model Used:} Logistic Regression
        \item \textbf{Evaluation Metrics:}
        \begin{itemize}
            \item Accuracy: Overall correctness of the model.
            \item Precision: Correctly predicted churners out of all predicted churners.
            \item Recall: Correctly predicted churners out of actual churners.
        \end{itemize}
        \item \textbf{Findings:} 
        \begin{itemize}
            \item High precision with acceptable recall indicated effectiveness in identifying loyal customers.
        \end{itemize}
        \item \textbf{Example Metrics:}
        \begin{itemize}
            \item Accuracy = 85\%
            \item Precision = 90\%
            \item Recall = 70\%
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Sentiment Analysis}
    \begin{block}{Scenario}
        A retail brand gauges public sentiment towards its new product line.
    \end{block}
    \begin{itemize}
        \item \textbf{Model Used:} Support Vector Machines (SVM)
        \item \textbf{Evaluation Metrics:}
        \begin{itemize}
            \item F1 Score: Harmonic mean of precision and recall.
            \item ROC Curve: Trade-off between true positive rate and false positive rate.
        \end{itemize}
        \item \textbf{Findings:}
        \begin{itemize}
            \item F1 Score of 0.82 indicated balanced model performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 3: Image Classification in Healthcare}
    \begin{block}{Scenario}
        A healthcare startup develops a model to classify radiology images.
    \end{block}
    \begin{itemize}
        \item \textbf{Model Used:} Convolutional Neural Networks (CNNs)
        \item \textbf{Evaluation Techniques:}
        \begin{itemize}
            \item Confusion Matrix: Visual representation of true vs. predicted labels.
            \item Cross-Validation: Ensures model generalizes well.
        \end{itemize}
        \item \textbf{Findings:}
        \begin{itemize}
            \item Confusion matrix helped identify misclassifications, leading to better training data curation.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Model Evaluation is Iterative:} Continuous assessment improves predictions over time.
        \item \textbf{Context Matters:} Choose metrics based on the problem domain (e.g., precision vs. recall).
        \item \textbf{Visualization Aids Understanding:} Tools like confusion matrices and ROC curves simplify evaluation results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    Understanding model evaluation techniques through practical case studies bridges the gap between theory and application, preparing learners to tackle real-world data science problems effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Model Evaluation - Introduction}
    \begin{itemize}
        \item Model evaluation is essential in the machine learning lifecycle.
        \item Several pitfalls can jeopardize evaluation effectiveness:
        \begin{itemize}
            \item Misleading conclusions
            \item Poor model deployment decisions
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Model Evaluation - Overfitting and Underfitting}
    \begin{block}{Definition}
        \begin{itemize}
            \item \textbf{Overfitting}: Model captures noise instead of the pattern; poor performance on unseen data.
            \item \textbf{Underfitting}: Model is too simple, resulting in inadequate performance overall.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        \begin{itemize}
            \item High-degree polynomial model (overfitting)
            \item Linear model (underfitting)
        \end{itemize}
    \end{block}
    
    \begin{block}{Prevention Strategies}
        \begin{itemize}
            \item Cross-validation
            \item Regularization (L1/L2 penalties)
            \item Simpler models
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Model Evaluation - Data Leakage}
    \begin{block}{Explanation}
        Data leakage leads to overly optimistic performance metrics due to outside information influencing model training.
    \end{block}
    
    \begin{block}{Example}
        Preprocessing steps before splitting the data can leak test set information into the training phase.
    \end{block}
    
    \begin{block}{Prevention}
        \begin{itemize}
            \item Always split data before preprocessing.
            \item Fit transformations on training data only, then apply to test data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Model Evaluation - Metric Selection}
    \begin{block}{Explanation}
        Wrong evaluation metrics can misrepresent model performance, especially with imbalanced datasets.
    \end{block}
    
    \begin{block}{Example}
        A model achieving 95\% accuracy by predicting only the majority class in an imbalanced dataset.
    \end{block}
    
    \begin{block}{Prevention}
        \begin{itemize}
            \item Use metrics aligned with business objectives: precision, recall, F1-score, AUC-ROC.
            \item Consider multiple metrics for a comprehensive evaluation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Model Evaluation - Context of Data}
    \begin{block}{Explanation}
        Understanding business context is crucial for effective model evaluation and considering all variable impacts.
    \end{block}
    
    \begin{block}{Example}
        A loan default model should account for economic factors affecting borrower behavior.
    \end{block}
    
    \begin{block}{Prevention}
        Collaborate with domain experts to ensure the model reflects actual scenarios.
        Validate results with real-world outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Be aware of overfitting/underfitting: Use validation techniques and visualizations.
        \item Protect against data leakage: Always separate training and testing data.
        \item Choose metrics wisely: Align with project goals and data characteristics.
        \item Context is crucial: Integrate domain expertise into the evaluation process.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Remarks}
    Enhancing vigilance regarding these challenges improves the trustworthiness of machine learning models, ensuring meaningful insights and predictions. 
    Stay tuned for the next slide on Best Practices for Model Evaluation to refine your evaluation strategy further!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Model Evaluation - Introduction}
    \begin{itemize}
        \item Evaluating model performance is essential in data science.
        \item Informs how well our model predicts outcomes based on input data.
        \item Helps in refining models, avoiding overfitting, and ensuring generalization.
        \item Follow these best practices for robust model evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Model Evaluation - Metrics}
    \begin{block}{1. Use Appropriate Evaluation Metrics}
        \begin{itemize}
            \item \textbf{Classification Tasks:}
                \begin{itemize}
                    \item \textbf{Accuracy:} Proportion of true results among total cases.
                    \item \textbf{Precision and Recall:}
                        \begin{itemize}
                            \item Precision = True Positives / (True Positives + False Positives).
                            \item Recall = True Positives / (True Positives + False Negatives).
                        \end{itemize}
                    \item \textbf{F1 Score:} Harmonic mean of precision and recall.
                \end{itemize}
            \item \textbf{Regression Tasks:}
                \begin{itemize}
                    \item \textbf{Mean Absolute Error (MAE):} Average absolute difference between predicted and actual values.
                    \item \textbf{Root Mean Squared Error (RMSE):} Measures the standard deviation of residuals.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Model Evaluation - Techniques}
    \begin{block}{2. Cross-Validation}
        \begin{itemize}
            \item \textbf{K-Fold Cross-Validation:} Split data into K subsets (folds).
            \begin{itemize}
                \item Train on K-1 folds and validate on the remaining fold.
                \item \textbf{Example:} For K=5, train on 4 folds and test on the 5th fold.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{3. Train-Test Split}
        \begin{itemize}
            \item \textbf{Holdout Method:} Divide dataset into training and testing sets (e.g., 80\% training, 20\% testing).
            \item \textbf{Stratified Sampling:} Ensure both sets have the same distribution of classes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Model Evaluation - Further Considerations}
    \begin{block}{4. Avoid Overfitting}
        \begin{itemize}
            \item Use a validation dataset or regularization techniques.
            \item \textbf{Visualization:} Plot learning curves for training vs. validation performance.
        \end{itemize}
    \end{block}

    \begin{block}{5. Consider Real-World Implications}
        \begin{itemize}
            \item Evaluate outcomes based on real use cases.
            \begin{itemize}
                \item \textbf{Example:} High precision is critical in medical diagnoses.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{6. Document the Process and Results}
        \begin{itemize}
            \item Maintain thorough documentation of methods and metrics for reproducibility.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Closing Remarks}
    \begin{itemize}
        \item Choose suited metrics and employ cross-validation.
        \item Enhance reproducibility through detailed documentation.
        \item Incorporating these best practices is vital for effective model evaluation.
    \end{itemize}
    
    \textbf{Closing Remarks:} Ensuring models perform well in real-world applications is crucial; recent advancements, like those in AI models such as ChatGPT, highlight the importance of robust model evaluation.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points - Overview}
    \begin{block}{Overview of Model Evaluation Techniques}
        Model evaluation is critical in assessing the effectiveness of predictive models. 
        Various metrics help quantify how well a model performs and guide us in selecting the best model for our data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points - Key Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: Proportion of correct predictions out of total predictions made.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Predictions}}
            \end{equation}
            \item \textbf{Significance}: Suitable for balanced datasets but can be misleading if classes are imbalanced.
        \end{itemize}
        
        \item \textbf{Precision}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of true positive predictions to total predicted positives.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Significance}: High precision is vital when false positives are costly, e.g., spam detection.
        \end{itemize}

        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: Ratio of true positive predictions to total actual positives.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Significance}: Important when missing positive cases is critical, e.g., disease detection.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points - F1 Score & AUC-ROC}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: Harmonic mean of precision and recall.
            \item \textbf{Formula}: 
            \begin{equation}
            F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Significance}: Useful in situations with class imbalance.
        \end{itemize}

        \item \textbf{AUC-ROC Curve}
        \begin{itemize}
            \item \textbf{Definition}: Area Under Receiver Operating Characteristic curve.
            \item \textbf{Significance}: Indicates model's ability to distinguish between classes; a value closer to 1 indicates better performance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points - Conclusion}
    \begin{block}{Conclusion}
        Understanding these metrics allows data scientists to critically evaluate model performance, ensuring that selected models align with business objectives. Proper interpretation of results can significantly impact decision-making processes.
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Each metric serves a unique purpose and is context-dependent.
            \item Emphasizing the right metrics is crucial for successful model evaluation.
            \item A nuanced understanding of these metrics can improve model selection and validation processes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Model Evaluation - Introduction}
    As machine learning (ML) becomes integral to various industries, the need for effective model evaluation techniques is more critical than ever. Evaluating models is not just about accuracy; it involves understanding model robustness, interpretability, and deployment readiness. Here are key trends shaping the future of model evaluation:
    \begin{itemize}
        \item Automated Machine Learning (AutoML)
        \item Advanced Evaluation Metrics
        \item Continuous Model Evaluation
        \item Integration of Human Feedback
        \item Future Prospects in AI Applications
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Model Evaluation - Automated Machine Learning (AutoML)}
    \begin{block}{What is AutoML?}
        AutoML involves automating the process of applying machine learning to real-world problems. This means automating tasks such as data preprocessing, feature selection, model selection, and hyperparameter tuning.
    \end{block}

    \begin{block}{Importance of AutoML in Evaluation}
        \begin{itemize}
            \item \textbf{Efficiency}: Reduces time spent on manual evaluation.
            \item \textbf{Enhanced Models}: Generates optimized models that outperform traditional approaches.
        \end{itemize}
    \end{block}

    \begin{example}
        Google’s AutoML allows users to build custom models with minimal coding skills. Users can input their dataset, and AutoML suggests the best performing models and evaluation metrics.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Model Evaluation - Advanced Evaluation Metrics}
    \begin{block}{Beyond Accuracy}
        Accuracy alone may not reflect model performance. Combining metrics like precision, recall, F1 score, AUC-ROC, and confusion matrices provides a holistic view.
    \end{block}

    \begin{block}{Model Interpretability}
        Emerging trends focus on making models interpretable. Tools like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) help explain model predictions.
    \end{block}

    \begin{example}
        In a medical diagnosis model, understanding which features (e.g., age, symptoms) influenced a prediction aids clinicians in decision-making.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Model Evaluation - Continuous Model Evaluation}
    \begin{block}{Importance}
        Traditionally, models were evaluated once; however, continuous evaluation helps detect performance degradation over time due to changing data distributions.
    \end{block}

    \begin{block}{Implementation}
        Techniques such as A/B testing and monitoring downstream metrics post-deployment help maintain model efficacy.
    \end{block}

    \begin{example}
        Imagine a predictive maintenance model for machinery. Continuous evaluation ensures it adapts as new types of machinery or failure patterns emerge.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Model Evaluation - Integration of Human Feedback}
    \begin{block}{Why Human Feedback?}
        Incorporating human expertise can refine model evaluation. Crowdsourcing feedback helps identify biases, edge cases, and domain-specific nuances.
    \end{block}

    \begin{example}
        In natural language processing (NLP), user feedback on ChatGPT's responses improves its understanding of context and relevance over time.
    \end{example}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Model Evaluation - Future Prospects}
    \begin{block}{AI and Data Mining Applications}
        Leveraging recent applications like ChatGPT illustrates how model evaluation influences user experience and satisfaction. Continuous learning and evaluation prevent biases and inaccuracies in conversational AI.
    \end{block}
    
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item AutoML revolutionizes model evaluation by automating complex tasks.
        \item Advanced Metrics provide deeper insights beyond simple accuracy.
        \item Continuous Evaluation is vital for long-term model performance.
        \item Human Feedback is crucial in fine-tuning and refining models.
    \end{itemize}
    
    \textbf{Closing:} Embracing these trends will improve the effectiveness of our model evaluations, ensuring they remain adaptable and relevant in a fast-paced technological world.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session}
    % Open the floor for questions to clarify concepts and engage with the audience.
    \begin{itemize}
        \item Clarify key concepts of model evaluation techniques.
        \item Encourage audience engagement and active participation.
        \item Address any lingering questions or misinterpretations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Model Evaluation}
    \begin{enumerate}
        \item \textbf{Model Evaluation Basics}
            \begin{itemize}
                \item Importance of assessing model performance.
                \item Key metrics: Accuracy, Precision, Recall, F1 Score, ROC-AUC.
                
                \begin{block}{Example}
                    In spam detection, accuracy measures the percentage of correct predictions, while precision focuses on how many predicted spam were actually spam.
                \end{block}
            \end{itemize}
        
        \item \textbf{Cross-Validation}
            \begin{itemize}
                \item Technique to guard against overfitting by splitting data into training and validation sets multiple times.
                
                \begin{block}{Illustration: 5-Fold Cross-Validation}
                    Split dataset into 5 equal parts. Train on 4 parts, test on 1 part, rotating this process for each fold.
                \end{block}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bias-Variance Tradeoff and Automated ML}
    \begin{enumerate}    
        \item \textbf{Bias-Variance Tradeoff}
            \begin{itemize}
                \item Balance between bias (error due to oversimplification) and variance (error due to complexity).
                
                \begin{block}{Key Point}
                    High bias leads to oversimplified predictions, while high variance captures noise.
                \end{block}
            \end{itemize}
        
        \item \textbf{Automated ML Solutions}
            \begin{itemize}
                \item Role of automated tools in model evaluation, reducing human error and improving efficiency.
                
                \begin{block}{Example}
                    Automated tools can assess models and hyperparameters, selecting the best-performing configuration based on criteria.
                \end{block}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Audience Engagement}
    \begin{itemize}
        \item \textbf{Prompt for Questions:} Encourage sharing thoughts on model evaluation challenges.
        
        \item \textbf{Questions to Guide Discussion:}
            \begin{enumerate}
                \item What evaluation metrics have you found most useful in practice?
                \item How do you approach model evaluation in real-world projects?
            \end{enumerate}
        
        \item \textbf{Real-World Applications:}
            Discuss the critical role of effective model evaluation in AI applications like ChatGPT, where precise model tuning leads to more reliable responses.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Remark}
    Model evaluation is not merely a technical necessity but an art that balances complexity and simplicity to build effective, reliable models. Your questions are valuable; let's dive deeper into any areas of interest!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Introduction}
    \begin{block}{Introduction to Model Evaluation Techniques}
        Model evaluation techniques are essential for understanding the performance of predictive models. They ensure that models not only fit the training data but also generalize well to unseen data. Below is a curated list of resources for deeper understanding and practical application.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Resources - Books}
    \begin{enumerate}
        \item \textbf{Books:}
        \begin{itemize}
            \item \textit{"Pattern Recognition and Machine Learning" by Christopher M. Bishop}
            \begin{itemize}
                \item Foundational text covering topics including probability models and evaluation methods.
                \item \textit{Why Read?} It provides a detailed theoretical background necessary for understanding model selection and evaluation.
            \end{itemize}
            
            \item \textit{"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by Aurélien Géron}
            \begin{itemize}
                \item Practical guide integrating theory with hands-on projects, focusing on model performance evaluation.
                \item \textit{Why Read?} Includes examples of confusion matrices, precision-recall curves, and ROC curves.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Resources - Online Courses and Papers}
    \begin{enumerate}
        \item \textbf{Online Courses:}
        \begin{itemize}
            \item \textit{Coursera: "Machine Learning" by Andrew Ng}
            \begin{itemize}
                \item Covers various aspects including error analysis and evaluation metrics.
                \item \textit{Course Highlights:} Focus on practical application of metrics through programming assignments.
            \end{itemize}
            
            \item \textit{edX: "Data Science MicroMasters"}
            \begin{itemize}
                \item Series of courses including model evaluation techniques in data science curriculum.
                \item \textit{Course Highlights:} Interactive projects allow practical application of evaluation concepts.
            \end{itemize}
        \end{itemize}

        \item \textbf{Research Papers:}
        \begin{itemize}
            \item \textit{"A Survey of Evaluation Metrics for Data Mining"}
            \begin{itemize}
                \item Overview of metrics and methodologies for assessing data mining models.
                \item \textit{Key Points:} Detail on precision, recall, F1 score, AUC-ROC comparisons.
            \end{itemize}
            
            \item \textit{"Evaluation of Machine Learning Models" by M. Sokolova and G. Lapalme}
            \begin{itemize}
                \item Focuses on evaluating models in the context of machine learning competitions.
                \item \textit{Insight:} Practical tips on overfitting and validation techniques.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Resources - Online Platforms and Takeaways}
    \begin{enumerate}
        \item \textbf{Online Platforms:}
        \begin{itemize}
            \item \textit{Kaggle: Data Science Competitions}
            \begin{itemize}
                \item Participate in competitions for practical applications of evaluation metrics.
                \item \textit{Benefit:} Exposure to discussions and notebooks from top data scientists.
            \end{itemize}
            
            \item \textit{Towards Data Science on Medium}
            \begin{itemize}
                \item Articles by practitioners on model evaluation and tools.
                \item \textit{Examples:} Case studies highlighting various model evaluation techniques.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Key Takeaways:}
        \begin{itemize}
            \item Understanding evaluation techniques is crucial for reliable model performance on new data.
            \item Resources include books, courses, and papers, providing both theoretical and practical insights.
            \item Engaging with current literature and platforms enhances real-world understanding.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula \& Code Snippet}
    \begin{block}{Confusion Matrix}
    \begin{equation}
    \begin{bmatrix}
    \text{True Positives (TP)} & \text{False Positives (FP)} \\
    \text{False Negatives (FN)} & \text{True Negatives (TN)}
    \end{bmatrix}
    \end{equation}
    \end{block}
    
    \begin{block}{Code Example in Python}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import confusion_matrix, classification_report

# Assume y_test are true labels and y_pred are predictions
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)
print(classification_report(y_test, y_pred))
    \end{lstlisting}
    \end{block}
\end{frame}


\end{document}