\frametitle{Pruning Techniques - Example & Conclusion}

    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset and split
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)

# Initialize and fit a Decision Tree
tree = DecisionTreeClassifier(ccp_alpha=0.01)  # ccp_alpha is the complexity parameter
tree.fit(X_train, y_train)

# Predictions and Evaluation
y_pred = tree.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
        \end{lstlisting}
    \end{block}

    \begin{block}{Conclusion}
        Pruning enhances decision trees by ensuring they generalize well to unseen data. Choosing the right method, whether pre-pruning or post-pruning, is crucial for optimal performance.
    \end{block}

