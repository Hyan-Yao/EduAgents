\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Academic Template]{Week 7: Clustering Techniques}
\subtitle{}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Clustering Techniques}
    \begin{block}{What is Clustering?}
        Clustering is an unsupervised machine learning technique used to group a set of objects such that:
        \begin{itemize}
            \item Objects in the same group (or cluster) share more similarities with each other than with those in other groups.
            \item It helps discover inherent patterns or structures in data without predefined labels.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Clustering in Data Mining}
    \begin{itemize}
        \item \textbf{Data Segmentation:} Identifies segments in large datasets for targeted marketing.
        \item \textbf{Pattern Recognition:} Recognizes patterns in disease outbreaks and financial fraud.
        \item \textbf{Dimensionality Reduction:} Simplifies datasets by categorizing similar data points for further analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering}
    \begin{enumerate}
        \item \textbf{Market Segmentation:} Clustering consumers based on purchasing behaviors.
            \begin{itemize}
                \item Example: Supermarkets identify different customer segments (e.g., health-conscious vs. bargain hunters).
            \end{itemize}
        \item \textbf{Image Processing:} Used for image segmentation.
            \begin{itemize}
                \item Example: Photo editing software enhances image quality through noise reduction via clustering algorithms.
            \end{itemize}
        \item \textbf{Social Network Analysis:} Helps understand communities within networks.
            \begin{itemize}
                \item Example: Facebook suggests friends based on similar interests and user activity data.
            \end{itemize}
        \item \textbf{Anomaly Detection:} Identifies outliers that may indicate fraud.
            \begin{itemize}
                \item Example: Credit card companies analyze transactions for irregularities signaling fraud.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Clustering Algorithms}
    \begin{itemize}
        \item \textbf{K-Means Clustering:} 
            \begin{itemize}
                \item A partition-based algorithm that minimizes the distance between data points and cluster centroids.
            \end{itemize}
        \item \textbf{Hierarchical Clustering:} 
            \begin{itemize}
                \item Builds a tree of clusters visualizable as a dendrogram.
            \end{itemize}
        \item \textbf{DBSCAN:} 
            \begin{itemize}
                \item Groups together closely packed points while marking low-density points as outliers.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Clustering is central to data mining, providing insights without prior knowledge of data labels.
        \item It has diverse applications across industries like marketing, healthcare, and fraud detection.
        \item Understanding different algorithms allows for tailored implementations for specific needs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Distance Measurement in K-Means}
    \begin{equation}
        D(x_i, c_j) = \sqrt{\sum_{k = 1}^{n}(x_{ik} - c_{jk})^2}
    \end{equation}
    Where $D$ is the distance between data point $x_i$ and cluster centroid $c_j$, and $n$ is the number of dimensions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    In this slide, we will outline the key learning objectives related to clustering techniques that will guide our exploration into this important area of data analysis. By the end of this week, students should be able to:
    \begin{enumerate}
        \item Understand the concept of clustering
        \item Identify key clustering algorithms
        \item Explore applications of clustering techniques
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Clustering Concepts}
    \begin{block}{Understand the Concept of Clustering}
        \begin{itemize}
            \item \textbf{Definition}: Clustering is the process of grouping a set of objects so that objects in the same group (or cluster) are more similar to each other than to those in other groups.
            \item \textbf{Purpose}: It helps in finding patterns or structures within data.
        \end{itemize}
    \end{block}
    
    \begin{block}{Identify Key Clustering Algorithms}
        Familiarize with popular clustering algorithms:
        \begin{itemize}
            \item \textbf{K-Means}: Partitions data into K distinct clusters. Effective for spherical clusters.
            \item \textbf{Hierarchical Clustering}: Creates a tree structure; can be agglomerative or divisive.
            \item \textbf{DBSCAN}: Groups closely packed points, marking low-density points as outliers.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Applications and Key Points}
    \begin{block}{Explore Applications of Clustering Techniques}
        Real-world applications include:
        \begin{itemize}
            \item \textbf{Marketing}: Segmenting customers for tailored campaigns.
            \item \textbf{Biology}: Classifying species based on genetic information.
            \item \textbf{Social Networks}: Identifying communities on social media platforms.
            \item \textbf{Image Processing}: Grouping similar pixels for image segmentation.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Relevance of Clustering}: Enables data-driven decision-making and insight discovery.
            \item \textbf{Practical Implementation}: Knowledge of algorithms facilitates application in programming environments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet - K-Means in Python}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

# Sample data
data = np.array([[1, 2], [1, 4], [1, 0],
                 [4, 2], [4, 4], [4, 0]])

# Create K-Means model
kmeans = KMeans(n_clusters=2)
kmeans.fit(data)

# Predict cluster labels
labels = kmeans.predict(data)
print(labels)  # Output: Cluster labels for each point
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Conclusion}
    By the completion of this chapter, students will grasp the theoretical aspects of clustering techniques and gain practical skills to implement these methods. This provides a solid foundation for further exploration in data mining and machine learning domains.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Definition}
    \begin{block}{Definition of Clustering}
        Clustering is a data mining technique used to group a set of objects, such that objects in the same group (or cluster) are more similar to each other than to those in other groups. This method is particularly beneficial for uncovering patterns and insights in large datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Purpose and Significance}
    \begin{block}{Purpose in Data Mining}
        \begin{itemize}
            \item Discover natural groupings in the data.
            \item Identify relationships or patterns that may not be immediately apparent.
            \item Applications include segmentation, anomaly detection, and pattern recognition.
        \end{itemize}
    \end{block}

    \begin{block}{Significance in Data Analysis}
        \begin{itemize}
            \item \textbf{Pattern Recognition:} Identifies structures within data for useful insights.
            \item \textbf{Segmentation:} Allows businesses to tailor marketing strategies to specific customer groups.
            \item \textbf{Dimensionality Reduction:} Reduces dataset complexity for easier visualization and analysis.
            \item \textbf{Anomaly Detection:} Detects outliers that may indicate fraud or significant deviations from normal behavior.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering? - Examples and Key Points}
    \begin{block}{Examples of Clustering}
        \begin{enumerate}
            \item \textbf{Market Segmentation:} Retail companies cluster customers based on buying behavior for tailored promotions.
            \item \textbf{Image Compression:} Reduces colors in an image by grouping similar colors, maintaining integrity.
            \item \textbf{Social Network Analysis:} Clusters users to identify communities and influencers.
        \end{enumerate}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Clustering is an \textbf{unsupervised learning} technique.
            \item Various algorithms exist, such as K-means, Hierarchical clustering, and DBSCAN.
            \item Understanding data characteristics is crucial for selecting the appropriate clustering technique.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Clustering is an essential tool in data analysis that enables the discovery of meaningful patterns and insights within complex datasets. Effectively utilizing clustering techniques can assist businesses, researchers, and data analysts in making informed decisions based on empirical data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Introduction}
    \begin{block}{Introduction to Clustering Techniques}
    Clustering is an unsupervised machine learning technique that groups data points based on their similarities. Understanding the various clustering techniques available is crucial for selecting the right method for specific problems in data analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Partitioning Methods}
    \begin{block}{1. Partitioning Methods}
    Partitioning methods aim to divide the dataset into distinct, non-overlapping groups. The primary goal is to minimize the distance between points within the same cluster while maximizing the distance between clusters.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Characteristics:}
        \begin{itemize}
            \item Centroid-Based: Each cluster is represented by a center (centroid).
            \item Examples: 
            \begin{itemize}
                \item K-means Clustering
                \item K-medoids (PAM)
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - K-means & K-medoids}
    \begin{block}{K-means Clustering}
        \begin{enumerate}
            \item Select 'k' initial centroids.
            \item Assign each data point to the nearest centroid.
            \item Recalculate centroids based on assignments.
            \item Repeat steps 2 and 3 until convergence.
        \end{enumerate}
        \textbf{Advantages:} Simple, efficient with large datasets, and easy to implement.\\
        \textbf{Limitations:} Sensitive to outliers and requires predefined 'k'.
    \end{block}

    \begin{block}{K-medoids (PAM)}
        Similar concept but uses actual data points as centroids (medoids), making it less sensitive to outliers.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Hierarchical Methods}
    \begin{block}{2. Hierarchical Methods}
    Hierarchical clustering builds a tree of clusters, creating a hierarchy. It has two main types:
    \end{block}
    
    \begin{itemize}
        \item \textbf{Agglomerative (Bottom-Up):} Starts with individual data points and merges them into clusters.
        \item \textbf{Divisive (Top-Down):} Starts with the entire dataset and splits it into smaller clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Key Points}
    \begin{block}{Key Characteristics of Hierarchical Methods}
    \begin{itemize}
        \item \textbf{Tree Structure:} Produces a dendrogram (tree diagram) to visualize the clustering process.
    \end{itemize}
    \end{block}

    \begin{block}{Examples}
        \begin{itemize}
            \item Agglomerative Clustering Procedure:
            \begin{enumerate}
                \item Start with each data point as its own cluster.
                \item Find the two closest clusters and merge them.
                \item Repeat until only one cluster remains.
            \end{enumerate}
            \item Divisive Clustering: Involves repeatedly splitting clusters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering Techniques - Summary and Visuals}
    \begin{block}{Advantages and Limitations}
    \begin{itemize}
        \item \textbf{Advantages:} 
        \begin{itemize}
            \item No pre-defined number of clusters.
            \item Visual representation (dendrogram) provides insights.
        \end{itemize}
        \item \textbf{Limitations:} More computationally intensive than partitioning methods.
    \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
    \begin{itemize}
        \item Choose methods based on data characteristics and goals.
        \item Experiment with different methods for suitability.
        \item Understand trade-offs in computational efficiency.
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Overview}
    \begin{block}{Overview}
        K-means clustering is a popular partitioning method used to cluster data into distinct groups, or clusters. It aims to partition data points into **K** clusters where each point belongs to the cluster with the nearest mean.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Algorithm Procedure}
    \begin{enumerate}
        \item \textbf{Initialization:}
        \begin{itemize}
            \item Select K initial centroids (random or based on domain knowledge).
        \end{itemize}
        \item \textbf{Assignment Step:}
        \begin{itemize}
            \item Assign each data point to the closest centroid.
        \end{itemize}
        \item \textbf{Update Step:}
        \begin{itemize}
            \item Recalculate centroids as the mean of assigned points.
        \end{itemize}
        \item \textbf{Iteration:}
        \begin{itemize}
            \item Repeat Assignment and Update steps until centroids stabilize or a maximum number of iterations is reached.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Limitations of K-means}
    \begin{block}{Advantages}
        \begin{itemize}
            \item \textbf{Simplicity:} Easy to implement and understand.
            \item \textbf{Efficiency:} Works well on large datasets with time complexity of \(O(n \cdot K \cdot i)\).
            \item \textbf{Scalability:} Suitable for large datasets that grow over time.
        \end{itemize}
    \end{block}
    
    \begin{block}{Limitations}
        \begin{itemize}
            \item \textbf{K Parameter Selection:} Requires prior knowledge of the number of clusters.
            \item \textbf{Sensitive to Initialization:} Results can vary based on centroid placement.
            \item \textbf{Assumption of Spherical Clusters:} Assumes all clusters are spherical and equal in size.
            \item \textbf{Outliers Impact:} Sensitive to outliers which can skew centroid positions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of K-means Clustering}
    \begin{block}{Scenario}
        Imagine a retail store wants to segment its customers based on purchasing behavior. Using K-means clustering:
    \end{block}
    \begin{itemize}
        \item \textbf{Data:} Gather customer data with features like age, income, and purchase frequency.
        \item \textbf{Clusters:} Identify segments such as "Budget Shoppers," "Luxury Buyers," and "Frequent Shoppers."
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation}
    \begin{block}{Objective Function}
        The goal of K-means is to minimize the squared distance between data points and their respective cluster centroids:
        \begin{equation}
            J = \sum_{i=1}^{K} \sum_{x \in C_i} \|x - \mu_i\|^2
        \end{equation}
        Where:
        \begin{itemize}
            \item \(J\) = total variance (objective function to minimize),
            \item \(C_i\) = cluster \(i\),
            \item \(x\) = data point,
            \item \(\mu_i\) = centroid of cluster \(i\).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        K-means clustering is a foundational algorithm in machine learning. Understanding its advantages and limitations is crucial for effectively applying it to real-world data scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Algorithm Steps - Overview}
    \begin{itemize}
        \item K-means clustering partitions a dataset into K distinct clusters based on feature similarity.
        \item Aims to minimize variance within clusters and maximize variance between clusters.
        \item Consists of three main steps: Initialization, Assignment, Update.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Algorithm Steps - Step 1: Initialization}
    \begin{block}{Objective}
        Choose initial centroids (cluster centers).
    \end{block}
    \begin{itemize}
        \item \textbf{Methods:}
        \begin{itemize}
            \item Randomly Select K Points: Choose K data points from the dataset.
            \item K-means++: Selects initial centroids to ensure they are spread out for faster convergence.
        \end{itemize}
        \item \textbf{Example:} For a dataset of 100 points aiming for 3 clusters, randomly pick 3 points, e.g., (x1, y1), (x2, y2), (x3, y3).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Algorithm Steps - Step 2: Assignment Step}
    \begin{block}{Objective}
        Assign each data point to the nearest centroid.
    \end{block}
    \begin{itemize}
        \item \textbf{Process:}
        \begin{enumerate}
            \item Calculate the distance from each data point to each centroid (using Euclidean distance).
            \item Assign each data point to the cluster of the nearest centroid.
        \end{enumerate}
        \item \textbf{Formula:}
        \begin{equation}
            d(p, c_k) = \sqrt{(x_p - x_{c_k})^2 + (y_p - y_{c_k})^2}
        \end{equation}
        where \( p \) is a data point, and \( c_k \) is the centroid of cluster \( k \).
        \item \textbf{Example:} Point A joins cluster 1 if it is closest to centroid C1.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Algorithm Steps - Step 3: Update Step}
    \begin{block}{Objective}
        Update the centroids of each cluster.
    \end{block}
    \begin{itemize}
        \item \textbf{Process:}
        \begin{itemize}
            \item Compute the new centroid by averaging all data points in that cluster.
        \end{itemize}
        \item \textbf{Formula:}
        \begin{equation}
            c_k = \frac{1}{n_k} \sum_{p \in C_k} p
        \end{equation}
        where \( n_k \) is the number of points in cluster \( k \) and \( C_k \) is the set of points in cluster \( k \).
        \item \textbf{Example:} New centroid for cluster 1 with points (A, B, C) is the average of their coordinates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Algorithm Steps - Iteration and Conclusion}
    \begin{itemize}
        \item Steps 2 and 3 are repeated until centroids change minimally, indicating convergence.
        \item \textbf{Key Point:} The algorithm may converge to local minima; multiple runs with different initializations may help.
        \item \textbf{Conclusion:} The K-means algorithm is effective and consists of clear iterative steps crucial for data analysis tasks.
        \begin{itemize}
            \item Emphasize the importance of initialization, assignment, and update steps in achieving quality clustering.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics for K-means}
    \begin{block}{Overview}
        Evaluating the performance of K-means clustering is crucial to assess how well the algorithm has grouped data points. Two commonly used metrics are:
        \begin{itemize}
            \item Inertia
            \item Silhouette Score
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Inertia}
    \begin{block}{Definition}
        \begin{itemize}
            \item \textbf{Inertia} measures how tightly clustered data points are, calculating the sum of squared distances between each point and its assigned centroid.
        \end{itemize}
    \end{block}

    \begin{block}{Formula}
        In mathematical terms, Inertia \(J\) is defined as:
        \begin{equation}
        J = \sum_{i=1}^{k} \sum_{x \in C_i} \| x - \mu_i \|^2
        \end{equation}
        \begin{itemize}
            \item \(k\) = number of clusters
            \item \(C_i\) = cluster \(i\)
            \item \(x\) = data points belonging to cluster \(i\)
            \item \(\mu_i\) = centroid of cluster \(i\)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Inertia (cont.)}
    \begin{block}{Interpretation}
        \begin{itemize}
            \item \textbf{Lower Inertia}: Better clustering as points are closer to their centroids.
            \item Inertia aids in determining different values of \(k\) using the \textbf{Elbow Method}.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Consider clusters with centroids:
        \begin{itemize}
            \item \(C_1 (2, 3)\)
            \item \(C_2 (8, 7)\)
            \item \(C_3 (5, 9)\)
        \end{itemize}
        Calculate distances of points to their centroids to derive inertia. A lower value confirms tight packing around centroids.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Silhouette Score}
    \begin{block}{Definition}
        The \textbf{Silhouette Score} indicates how similar an object is to its own cluster compared to other clusters.
    \end{block}

    \begin{block}{Formula}
        The silhouette score \(s\) for a single point is defined as:
        \begin{equation}
        s = \frac{b - a}{\max(a, b)}
        \end{equation}
        \begin{itemize}
            \item \(a\) = average distance between the point and all other points in its cluster
            \item \(b\) = average distance between the point and points in the nearest cluster
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Silhouette Score (cont.)}
    \begin{block}{Interpretation}
        \begin{itemize}
            \item \textbf{Scores range from -1 to +1}:
                \begin{itemize}
                    \item Score close to +1: Points are well clustered.
                    \item Score around 0: Points are near the boundary of two clusters.
                    \item Negative score: Points may be wrongly assigned.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        If a data point in cluster A has:
        \begin{itemize}
            \item Distance of \(1.5\) to the same cluster
            \item Distance of \(3.0\) to the nearest cluster
        \end{itemize}
        Then the silhouette score is:
        \begin{equation}
        s = \frac{3.0 - 1.5}{\max(1.5, 3.0)} = \frac{1.5}{3.0} = 0.5
        \end{equation}
        This suggests fair clustering, indicating some ambiguity.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Both Inertia and Silhouette Score help determine the optimal number of clusters (\(k\)) and evaluate clustering quality.
        \item Combined metrics provide a comprehensive evaluation; low Inertia with high Silhouette Score indicates good clustering.
        \item These metrics guide adjustments and improvements in clustering methods.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Understanding these metrics enables practitioners to evaluate and refine K-means clustering outputs, ensuring meaningful insights from data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Introduction}
    \begin{block}{Introduction}
        Hierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters. Unlike K-means clustering, which requires the number of clusters as a parameter, hierarchical clustering creates a nested series of clusters that can be visualized as a dendrogram.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Types}
    \begin{block}{Types of Hierarchical Clustering}
        Two primary types:
        \begin{itemize}
            \item \textbf{Agglomerative Clustering (Bottom-Up Approach)}
            \begin{itemize}
                \item Starts with each data point as its own cluster.
                \item Pairs of clusters are merged iteratively.
                \item Continues until all points are clustered into a single cluster or a stopping criterion is met.
                \item \textit{Example:} Customers are clustered based on purchasing behavior.
            \end{itemize}
            
            \item \textbf{Divisive Clustering (Top-Down Approach)}
            \begin{itemize}
                \item Begins with all data points in one cluster.
                \item The most dissimilar points are split iteratively into smaller clusters.
                \item \textit{Example:} Plant species are separated based on characteristics like height.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Applications}
    \begin{block}{Applications}
        Hierarchical clustering is widely used in:
        \begin{itemize}
            \item \textbf{Biology} - Classifying species or genes based on similarities.
            \item \textbf{Market Research} - Segmenting customers for targeted marketing.
            \item \textbf{Social Science} - Exploring group behaviors or social networks.
            \item \textbf{Document Clustering} - Organizing documents based on content similarity.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Generates a dendrogram, a visual representation of the cluster hierarchy.
            \item Flexibility in data exploration due to no predetermined cluster number.
            \item Sensitive to noise and outliers, impacting cluster formation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrograms}
    \begin{block}{Introduction to Dendrograms}
        Dendrograms are tree-like diagrams that play a crucial role in visualizing the results of hierarchical clustering. They allow us to understand the structure of the data and the relationships between different clusters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Dendrogram?}
    \begin{itemize}
        \item A dendrogram represents the arrangement of clusters based on their similarity.
        \item Each leaf of the tree represents an individual data point, while the branches represent the distances or dissimilarities between points.
        \item The height at which two points (or clusters) merge visually indicates their similarity—higher merges suggest more dissimilar clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Dendrograms}
    \begin{itemize}
        \item \textbf{Visual Hierarchies}: Dendrograms provide a clear hierarchical view of how clusters are formed.
        \item \textbf{Cutting the Tree}: By cutting the dendrogram at a specific height, we can determine the number of clusters that can be formed.
        \item \textbf{Similarity Metrics}: Different linkage criteria (single, complete, average, etc.) can yield different dendrogram shapes, illustrating how choices impact clustering results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Hierarchical Clustering}
    Suppose we have the following data points representing five different fruits based on their characteristics:

    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Fruit} & \textbf{Weight (g)} & \textbf{Sugar Content (\%) } \\ \hline
            Apple     & 150        & 10                 \\ \hline
            Banana    & 120        & 12                 \\ \hline
            Cherry    & 10         & 8                  \\ \hline
            Grape     & 5          & 7                  \\ \hline
            Watermelon| 1000       & 6                  \\ \hline
        \end{tabular}
    \end{table}

    \begin{enumerate}
        \item Perform Hierarchical Clustering: Use metrics like Euclidean distance to calculate how closely related each fruit is based on weight and sugar content.
        \item Generate Dendrogram: Plot these relationships and observe that similar fruits merge first.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Code for Dendrogram}
    \begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Sample data
data = np.array([[150, 10], [120, 12], [10, 8], [5, 7], [1000, 6]])

# Hierarchical clustering
Z = linkage(data, method='ward')

# Plot dendrogram
plt.figure(figsize=(10, 6))
dendrogram(Z, labels=['Apple', 'Banana', 'Cherry', 'Grape', 'Watermelon'])
plt.title('Dendrogram of Fruit Clustering')
plt.xlabel('Fruits')
plt.ylabel('Distance')
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item A dendrogram is an essential visualization tool in hierarchical clustering.
        \item The height of merges signifies the level of dissimilarity: taller merges suggest more distinct clusters.
        \item Different clustering methods can lead to variations in the dendrogram structure.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Activity}
    \begin{block}{Conclusion}
        Understanding dendrograms is pivotal for interpreting hierarchical clustering results, enabling analysts to make informed decisions regarding the optimal number of clusters and to examine relationships within data in a clear visual format.
    \end{block}
    
    \begin{block}{Interactive Classroom Activity}
        Encourage students to create their own dendrograms using various datasets, exploring how different clustering methods affect the visualization and insight gleaned from their data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing K-means and Hierarchical Clustering - Overview}
    \begin{block}{Overview}
        Clustering is a powerful technique utilized to group similar data points, providing insights into data structure and patterns. 
        K-means and Hierarchical Clustering are two of the most widely used techniques in this domain.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing K-means and Hierarchical Clustering - Key Differences}
    \begin{enumerate}
        \item \textbf{Algorithm Structure}:
        \begin{itemize}
            \item \textbf{K-means}: 
            \begin{enumerate}
                \item Choose K initial centroids.
                \item Assign each data point to the nearest centroid.
                \item Recalculate centroids based on current cluster membership.
                \item Repeat until convergence.
            \end{enumerate}
            \item \textbf{Hierarchical Clustering}: Constructs a tree of clusters (dendrogram) based on distance metrics. Two types:
            \begin{itemize}
                \item \textbf{Agglomerative}: Merges clusters based on proximity.
                \item \textbf{Divisive}: Splits one cluster into smaller clusters.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Number of Clusters}:
        \begin{itemize}
            \item \textbf{K-means}: Requires specifying the number of clusters, K, beforehand.
            \item \textbf{Hierarchical Clustering}: Does not have a preset number of clusters; uses a linkage criterion for formation.
        \end{itemize}
        
        \item \textbf{Computational Complexity}:
        \begin{itemize}
            \item \textbf{K-means}: Time complexity of O(n * K * i).
            \item \textbf{Hierarchical Clustering}: Time complexity of O(n²) or worse.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing K-means and Hierarchical Clustering - Scalability and Use Cases}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Scalability}:
        \begin{itemize}
            \item \textbf{K-means}: Scales better to large datasets; preferred for efficiency.
            \item \textbf{Hierarchical Clustering}: Less scalable; performs poorly on large datasets.
        \end{itemize}

        \item \textbf{Cluster Shape}:
        \begin{itemize}
            \item \textbf{K-means}: Assumes spherical clusters and similar size.
            \item \textbf{Hierarchical Clustering}: Can form clusters of arbitrary shapes.
        \end{itemize}
        
        \item \textbf{When to Use Each Method}:
        \begin{itemize}
            \item \textbf{K-means} is suitable for large datasets needing speed and predefined cluster numbers.
            \item \textbf{Hierarchical Clustering} is ideal for smaller datasets focusing on understanding relationships.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering - Introduction}
    \begin{block}{Introduction to Clustering}
        Clustering is a type of unsupervised learning where the goal is to group a set of objects. 
        Objects in the same group (or cluster) are more similar to each other than to those in other groups. 
        This technique is widely employed across various fields to derive insights from data without prior labels, 
        making it a powerful analytical tool.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering - Overview}
    \begin{block}{Key Applications of Clustering}
        \begin{enumerate}
            \item \textbf{Marketing and Customer Segmentation}
                \begin{itemize}
                    \item Businesses segment customer bases for targeted marketing.
                    \item Example: Identifying "budget shoppers" vs. "luxury buyers."
                    \item Benefits: Enhanced satisfaction and increased sales.
                \end{itemize}

            \item \textbf{Biology and Genetics}
                \begin{itemize}
                    \item Analyze biological data for similar instances.
                    \item Example: Clustering genes with similar activity patterns.
                    \item Benefits: Discovery of new biomarkers and therapeutic targets.
                \end{itemize}

            \item \textbf{Image Processing}
                \begin{itemize}
                    \item Clustering algorithms like K-means are used for image segmentation.
                    \item Example: Separating healthy tissues from tumors in medical imaging.
                    \item Benefits: Improved diagnostic accuracy.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering - More Applications}
    \begin{block}{Key Applications of Clustering (Continued)}
        \begin{enumerate}
            \setcounter{enumi}{3}
            \item \textbf{Anomaly Detection}
                \begin{itemize}
                    \item Identifies outliers indicating fraud or defects.
                    \item Example: Highlighting unusual credit card transaction behavior.
                    \item Benefits: Enhanced security and risk management.
                \end{itemize}
                
            \item \textbf{Social Network Analysis}
                \begin{itemize}
                    \item Identifies communities within social networks.
                    \item Example: Clustering based on user interaction patterns.
                    \item Benefits: Insights for marketing strategies and user experiences.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering - Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Clustering is a versatile tool applicable in diverse domains.
            \item Algorithm choice (e.g., K-means, Hierarchical) affects results.
            \item Context and data type are critical for effective application.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Clustering techniques enable businesses and researchers to make sense of complex data by uncovering natural groupings. 
        By leveraging these insights, organizations can improve decision-making, enhance customer experiences, 
        and drive innovation in solving real-world problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Considerations - Introduction}
    Clustering techniques are powerful tools used to discover patterns and relationships within datasets. However, effectively implementing these techniques requires careful consideration of several practical aspects:
    \begin{itemize}
        \item \textbf{Data Preparation}
        \item \textbf{Parameter Selection}
        \item \textbf{Interpretation of Results}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Considerations - Data Preparation}
    \begin{block}{1. Data Preparation}
        \begin{itemize}
            \item \textbf{Data Cleaning}: Remove noise and eliminate outliers. Example: No negative ages in customer data.
            \item \textbf{Feature Selection}: Choose relevant features. Example: For clustering flowers, use petal length and width.
            \item \textbf{Normalization}: Apply techniques like Min-Max Scaling to achieve a uniform scale:
            \begin{equation}
            X' = \frac{X - \min(X)}{\max(X) - \min(X)}
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Considerations - Parameter Selection}
    \begin{block}{2. Parameter Selection}
        \begin{itemize}
            \item \textbf{Choosing the Number of Clusters (K)}: Use the Elbow Method to determine \( K \) by plotting the explained variance.
            \item \textbf{Distance Metric}: Selecting an appropriate metric (e.g., Euclidean, Manhattan) affects cluster formation.
            \item \textbf{Algorithm Parameters}: Define unique parameters for algorithms like DBSCAN, such as \textit{epsilon} and \textit{minPts}.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Considerations - Interpretation of Results}
    \begin{block}{3. Interpretation of Results}
        \begin{itemize}
            \item \textbf{Visualizing Clusters}: Techniques like t-SNE and PCA help in visualizing clustering results.
            \item \textbf{Evaluating Clustering Quality}: Use metrics such as Silhouette Score:
            \begin{equation}
            \text{Silhouette Score} = \frac{b - a}{\max(a, b)}
            \end{equation}
            where \( a \) is the average distance in the same cluster and \( b \) is the distance to the nearest cluster.
            \item \textbf{Domain Knowledge}: Utilize domain knowledge to provide context to the clusters formed.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Considerations - Key Takeaways}
    \begin{itemize}
        \item Successful clustering relies on meticulous \textbf{data preparation}, judicious \textbf{parameter selection}, and thoughtful \textbf{interpretation} of results.
        \item Proper data preparation and parameter selection significantly influence the effectiveness of clustering.
        \item Visual and quantitative evaluations are essential for interpreting and validating clustering outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Clustering - Overview}
    \begin{itemize}
        \item Clustering techniques are vital for data analysis across various sectors (e.g., marketing, healthcare).
        \item Ethical implications must be considered—focusing on:
        \begin{itemize}
            \item Biases in data
            \item Privacy concerns
        \end{itemize}
        \item Understanding these dimensions is crucial for fair results and respect for individual rights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Clustering - Bias in Data}
    \begin{block}{Definition}
        Bias refers to systematic errors in data that can skew results. Biased clustering algorithms may perpetuate social inequalities.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Examples:}
        \begin{enumerate}
            \item \textbf{Gender and Racial Bias:} Clustering in hiring could favor certain demographics if based on biased historical data.
            \item \textbf{Customer Segmentation:} Socio-economic biases in data can lead to unfair exclusions in targeted marketing.
        \end{enumerate}
    \end{itemize}
    
    \textbf{Key Point:} Always evaluate data sources for biases and utilize fairness-aware clustering techniques.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Clustering - Privacy Concerns}
    \begin{block}{Definition}
        The aggregation of personal data for clustering raises privacy concerns, especially regarding re-identification risks.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Implications:}
        \begin{enumerate}
            \item \textbf{Data Anonymization:} Anonymized data can still reveal sensitive information, especially in small datasets.
            \item \textbf{Consent and Transparency:} Data subjects should be informed about data usage and allowed to opt out.
        \end{enumerate}
    \end{itemize}
    
    \textbf{Key Point:} Implement data protection strategies, such as differential privacy, to safeguard personal information.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Clustering}
    \begin{itemize}
        \item \textbf{Healthcare:} Clustering patient data must ensure no unequal treatment based on race, gender, or socio-economic status.
        \item Consider potential biases and privacy risks to support ethical healthcare practices.
    \end{itemize}
    
    \begin{block}{Example Scenario}
        Imagine an algorithm clustering users for personalized recommendations. If the data is dominated by a specific age group or cultural background, recommendations may alienate other demographics. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Best Practices}
    \begin{itemize}
        \item Regularly audit datasets for biases and inconsistencies.
        \item Use techniques to anonymize data, prioritizing privacy.
        \item Engage diverse teams in the design of clustering algorithms to identify biases.
    \end{itemize}
    
    \textbf{Key Takeaway:} Addressing ethical implications enhances reliability and fosters trust with stakeholders and users.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Recap of Key Concepts}
    
    \begin{enumerate}
        \item \textbf{Definition of Clustering}: 
        Clustering groups similar data points, identifying patterns without pre-labeled outputs.
        
        \item \textbf{Common Techniques}:
        \begin{itemize}
            \item K-Means
            \item Hierarchical Clustering
            \item DBSCAN
        \end{itemize}
        
        \item \textbf{Applications}:
        \begin{itemize}
            \item Marketing
            \item Image Processing
            \item Genomics
        \end{itemize}
        
        \item \textbf{Evaluation}:
        \begin{itemize}
            \item Silhouette Score
            \item Elbow Method
        \end{itemize}
        
        \item \textbf{Ethical Considerations}:
        Ensure proper handling of data biases and privacy.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Encouragement to Apply Clustering Techniques}
    
    \begin{itemize}
        \item \textbf{Real-World Projects}:
        Think about using clustering for insights in your projects.
        
        \item \textbf{Hands-On Experience}:
        Experiment with datasets from repositories (e.g., Kaggle).
        
        \item \textbf{Collaboration and Discussion}:
        Share experiences with peers for collective learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts and Example Code Snippet}
    
    \begin{block}{Final Thoughts}
        Clustering is a versatile tool in data analysis. Grasping its techniques and ethical implications will enhance your skills for real-world challenges.
    \end{block}
    
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Sample data
data = [[1, 2], [1, 4], [1, 0],
        [4, 2], [4, 4], [4, 0]]

# Creating a K-Means model
kmeans = KMeans(n_clusters=2)
kmeans.fit(data)

# Getting the cluster centers and labels
centroids = kmeans.cluster_centers_
labels = kmeans.labels_

# Plotting the clusters
plt.scatter([x[0] for x in data], [x[1] for x in data], c=labels)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')
plt.title('K-Means Clustering Example')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussion - Overview of Clustering Techniques}
    \begin{block}{Definition}
        Clustering is an unsupervised machine learning technique that groups similar observations together based on certain features. It helps in identifying natural groupings within data, crucial for analysis and pattern recognition.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Types of Clustering Techniques:}
        \begin{itemize}
            \item Hierarchical Clustering
            \item K-means Clustering
            \item DBSCAN
        \end{itemize}
        \item \textbf{Applications of Clustering:}
        \begin{itemize}
            \item Market Segmentation
            \item Image Recognition
            \item Anomaly Detection
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussion - Engaging Students}
    \begin{block}{Discussion Prompts}
        \begin{enumerate}
            \item Have you encountered any real-world problems where clustering played a significant role? 
            \item What challenges did you face when implementing clustering techniques in projects or data analysis?
            \item Can you think of any innovative applications of clustering beyond typical examples?
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example Case Study}
        \textbf{Scenario:} A retail company wants to explore customer purchasing patterns. By applying K-means clustering, they group customers into segments (e.g., frequent buyers, occasional buyers) based on past transaction data. This allows for targeted promotions leading to higher engagement.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussion - Wrap-up}
    \begin{block}{Encouragement}
        Feel free to ask any additional questions, share insights, or discuss the complexities of choosing the right clustering algorithm for specific tasks.
    \end{block}
    
    \begin{block}{Interactive Element}
        Engage in a group discussion or breakout sessions to identify clustering scenarios from your experience, highlighting the importance of context in selecting clustering methods.
    \end{block}
\end{frame}


\end{document}