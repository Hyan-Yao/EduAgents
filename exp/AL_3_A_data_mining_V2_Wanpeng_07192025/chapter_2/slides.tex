\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 2: Data Preprocessing and Preparation]{Week 2: Data Preprocessing and Preparation}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{Importance of Data Preprocessing in Data Mining}
        Data preprocessing is crucial in data mining as it prepares raw data into a clean, organized format suitable for analysis. 
        Raw data can be inconsistent, incomplete, or noisy, which can lead to inaccurate insights. 
        Effective preprocessing enhances data quality and supports better decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Goals of Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Improving Data Quality:}
        \begin{itemize}
            \item \textbf{Accuracy:} Represents the real-world scenario accurately.
            \item \textbf{Consistency:} Resolves discrepancies across multiple data sources.
            \item \textbf{Completeness:} Fills in gaps for missing or underrepresented data.

            \begin{block}{Example}
                Consider a dataset of customer orders with missing phone numbers or incorrect addresses. 
                Data preprocessing can correct these issues by filling in missing values or rectifying inaccuracies.
            \end{block}
        \end{itemize}

        \item \textbf{Enhancing Data Usability:}
        \begin{itemize}
            \item \textbf{Format Adaptation:} Converts data into a usable format tailored to specific methods.
            \item \textbf{Reduction of Dimensionality:} Simplifies the dataset by removing redundant features.
            
            \begin{block}{Illustration}
                A database containing customer feedback in various languages can be standardized into a single language, making sentiment analysis easier.
            \end{block}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{First Impressions Matter:} High-quality data is essential for sound analysis.
        \item \textbf{Preventing GIGO:} Poorly preprocessed data can lead to faulty conclusions.
        \item \textbf{Iterative Process:} Data preprocessing often requires multiple passes for optimal results.
    \end{itemize}
    
    \begin{block}{Conclusion}
        The aim of data preprocessing is to clean and prepare data to ensure quality and utility, leading to meaningful analysis. 
        Improved data quality and usability facilitate accurate insights and informed decisions.
    \end{block}

    \begin{block}{Next Steps}
        In the next slide, we will explore specific data cleaning techniques, such as handling missing values and detecting outliers.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Introduction}
    Data cleaning is a fundamental step in the data preprocessing phase, 
    aiming to improve the quality of datasets by addressing issues such as:
    \begin{itemize}
        \item Missing values
        \item Outliers
        \item Inaccuracies
    \end{itemize}
    We will explore key data cleaning techniques critical for effective data analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Handling Missing Values}
    \textbf{Definition}: Missing values occur when data is absent for some records and can lead to biased results if not handled properly.

    \begin{block}{Techniques}
        \begin{itemize}
            \item \textbf{Deletion}: Remove rows with missing values.
            \item \textbf{Mean/Median Imputation}: Replace missing values with the mean or median of the non-missing values.
            \item \textbf{Predictive Imputation}: Use machine learning to predict missing values.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Outlier Detection}
    \textbf{Definition}: Outliers are data points that deviate significantly from other observations.

    \begin{block}{Techniques}
        \begin{itemize}
            \item \textbf{Z-Score Method}: 
            \begin{equation}
            Z = \frac{(X - \mu)}{\sigma}
            \end{equation}
            \item \textbf{IQR Method}: Calculate the interquartile range (IQR = Q3 - Q1) to determine outliers.
        \end{itemize}
    \end{block}
    \textbf{Illustrative Example}: In exam scores, a score of 150 might be an outlier if most scores are between 50 and 100.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Data Corrections}
    \textbf{Definition}: Correction techniques ensure that typos, inconsistencies, and inaccuracies in the data are fixed.

    \begin{block}{Techniques}
        \begin{itemize}
            \item \textbf{Standardization}: Ensuring uniform formats for data entries.
            \item \textbf{Validation}: Cross-checking data against predefined rules to correct errors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Tools for Data Cleaning}
    \begin{block}{Popular Tools}
        \begin{itemize}
            \item \textbf{Pandas} (Python): Robust functions for handling missing values and outlier detection.
            \item \textbf{R}: Packages such as \texttt{dplyr} and \texttt{tidyr} for data manipulation.
            \item \textbf{Excel}: Useful for smaller datasets with functions like \texttt{IFERROR}.
        \end{itemize}
    \end{block}

    \textbf{Key Points to Emphasize}:
    \begin{itemize}
        \item Proper data cleaning is crucial for analysis accuracy.
        \item Tailored approaches for different types of missing data and outliers are necessary.
        \item Familiarity with tools enhances the efficiency of data cleaning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Conclusion}
    Data cleaning is critical in data preprocessing, significantly impacting analysis quality. 
    By effectively:
    \begin{itemize}
        \item Handling missing values
        \item Detecting outliers
        \item Correcting inaccuracies
    \end{itemize}
    we ensure our datasets are accurate and reliable for further analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization and Standardization - Definition}
    \begin{block}{Definition}
        \begin{itemize}
            \item \textbf{Normalization} and \textbf{Standardization} are essential preprocessing techniques used to scale numerical data, ensuring that features contribute equally to model training and performance.
            \begin{itemize}
                \item \textbf{Normalization}: Rescales data to a specific range, typically \([0, 1]\).
                \item \textbf{Standardization}: Adjusts data to have a mean of 0 and a standard deviation of 1, following a standard normal distribution.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization and Standardization - Significance}
    \begin{block}{Significance}
        These techniques help in:
        \begin{itemize}
            \item Enhancing the convergence of optimization algorithms.
            \item Improving the performance of machine learning algorithms (e.g., k-nearest neighbors, gradient descent-based methods).
            \item Reducing biases towards features with larger ranges or different units.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods - Min-Max Scaling (Normalization)}
    \begin{block}{1. Min-Max Scaling (Normalization)}
        \textbf{Formula:} 
        \begin{equation}
            X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}
        \end{equation}

        \textbf{Example:}
        \begin{itemize}
            \item Heights: [150, 160, 170, 180, 190]
            \item Minimum (\(X_{min}\)) = 150, Maximum (\(X_{max}\)) = 190.
            \item Normalizing the value 170:
            \begin{equation}
                X_{norm} = \frac{170 - 150}{190 - 150} = \frac{20}{40} = 0.5
            \end{equation}
        \end{itemize}
        \textbf{Key Point:} Ideal for bounding data between 0 and 1 with distinct ranges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods - Z-Score Normalization (Standardization)}
    \begin{block}{2. Z-Score Normalization (Standardization)}
        \textbf{Formula:} 
        \begin{equation}
            Z = \frac{X - \mu}{\sigma}
        \end{equation}
        Where \( \mu \) is the mean and \( \sigma \) is the standard deviation.

        \textbf{Example:}
        \begin{itemize}
            \item For the height dataset: Mean (\(\mu\)) = 170, Standard Deviation (\(\sigma\)) \(\approx 15.81\).
            \item Standardizing the value 170:
            \begin{equation}
                Z = \frac{170 - 170}{15.81} = 0
            \end{equation}
            \item For height 150:
            \begin{equation}
                Z = \frac{150 - 170}{15.81} \approx -1.265
            \end{equation}
        \end{itemize}
        \textbf{Key Point:} Effective for normally distributed data and necessary for certain algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Practical Tips}
    \begin{block}{Summary}
        \begin{itemize}
            \item \textbf{Normalization}: Essential for bounded data to ensure comparability.
            \item \textbf{Standardization}: Key for normally distributed data, improving performance for sensitive algorithms.
        \end{itemize}
    \end{block}
    
    \begin{block}{Practical Tip}
        Always visualize your data distributions before and after applying normalization or standardization to better understand their impact on your models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet}
    \begin{block}{Python Code Example}
        \begin{lstlisting}[language=Python]
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler

data = np.array([[150], [160], [170], [180], [190]])

# Min-Max Normalization
scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(data)

# Z-Score Standardization
zscaler = StandardScaler()
standardized_data = zscaler.fit_transform(data)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Thoughts}
    Incorporate normalization and standardization techniques into your data preprocessing workflow to achieve better and more robust machine learning models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Introduction}
    \begin{block}{Overview}
        Data transformation is a crucial step in data preprocessing aimed at modifying attributes to improve analysis. Transformations can:
        \begin{itemize}
            \item Improve model performance
            \item Help meet assumptions of statistical techniques
            \item Stabilize variance
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Log Transformation}
    \begin{block}{Explanation}
        Log transformation replaces values with their logarithm, useful for:
        \begin{itemize}
            \item Reducing right skewness in data
            \item Minimizing influence of large values on mean and variance
        \end{itemize}
    \end{block}

    \begin{block}{When to Use}
        \begin{itemize}
            \item Data exhibits exponential growth or right skewness
            \item Presence of outliers affecting analysis
        \end{itemize}
    \end{block}

    \begin{block}{Formula}
        The log-transformed value \( y \) is calculated as:  
        \[
        y = \log(x + c) 
        \]
        where \( c \) is a constant for handling zero values.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Example of Log Transformation}
    \begin{block}{Example}
        In income data analysis, where high incomes can create a positive skew, applying log transformation can normalize the data, making it more suitable for linear regression models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Box-Cox Transformation}
    \begin{block}{Explanation}
        The Box-Cox transformation stabilizes variance and normalizes data. It is defined for positive data and utilizes a parameter (\(\lambda\)) for adaptation:
        \begin{itemize}
            \item Flexible approach tailored to the shape of the data
        \end{itemize}
    \end{block}

    \begin{block}{When to Use}
        \begin{itemize}
            \item Data is strictly positive and non-normally distributed
            \item Requires a flexible transformation method
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula}
        The Box-Cox transformation is defined as:
        \[
        y = 
        \begin{cases} 
            \frac{x^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0 \\
            \log(x) & \text{if } \lambda = 0 
        \end{cases} 
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Example of Box-Cox Transformation}
    \begin{block}{Example}
        In a healthcare dataset measuring hospital stay durations, the Box-Cox transformation can help achieve a normal distribution, enhancing the validity of statistical tests.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Key Points}
    \begin{itemize}
        \item **Data Normalization**: Critical for improving data normality, a prerequisite for many analyses.
        \item **Impact on Model Performance**: Proper transformations lead to better model fitting by meeting algorithm assumptions.
        \item **Choosing the Right Technique**: Depends on data distribution and analysis objectives; log is straightforward, Box-Cox is flexible.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Conclusion}
    \begin{block}{Summary}
        Transformation techniques play an essential role in addressing skewness and variance in data preprocessing. Knowing when and how to apply these techniques is vital for effective data analysis and modeling.
    \end{block}
    \begin{block}{In-Class Practice}
        Consider implementing these transformations using Python libraries like NumPy and SciPy to enhance practical understanding.
    \end{block}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection and Engineering - Overview}
    \begin{block}{Overview}
        Feature selection and feature engineering are critical processes in the machine learning workflow that influence model performance and interpretability.
    \end{block}
    \begin{itemize}
        \item Feature selection: Selecting a subset of relevant features for model building.
        \item Feature engineering: Creating new features from existing data to enhance model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Definition and Importance}
    \begin{block}{Feature Selection}
        \begin{itemize}
            \item \textbf{Definition}: Identifying and selecting relevant features that contribute to model prediction while eliminating redundant or irrelevant data.
            \item \textbf{Importance}:
                \begin{itemize}
                    \item Reduces overfitting.
                    \item Improves accuracy.
                    \item Decreases training time.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection - Techniques and Example}
    \begin{block}{Common Techniques}
        \begin{itemize}
            \item \textbf{Filter Methods}
            \item \textbf{Wrapper Methods}
            \item \textbf{Embedded Methods}
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        In predicting house prices, features like "House ID" or "Street Name" may be removed due to irrelevance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering - Definition and Importance}
    \begin{block}{Feature Engineering}
        \begin{itemize}
            \item \textbf{Definition}: Creating new features from existing data to improve model performance.
            \item \textbf{Importance}:
                \begin{itemize}
                    \item Captures underlying patterns.
                    \item Enhances model robustness.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering - Techniques and Example}
    \begin{block}{Common Techniques}
        \begin{itemize}
            \item \textbf{Binning}
            \item \textbf{Polynomial Features}
            \item \textbf{Date/Time Features}
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        From a timestamp, we can extract features such as "Is Weekend?" or "Hour of Day".
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Iterative process of feature selection and engineering.
        \item Importance of domain knowledge.
        \item Leveraging libraries like Scikit-learn for facilitation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Example Code}
    \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest, f_classif

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Select the top 2 features
X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)
print(X_new)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Data Preprocessing Tools}
    \begin{block}{Overview}
        Data preprocessing is vital for transforming raw data into a suitable format for analysis or modeling.
        We will explore popular Python libraries **Pandas** and **Scikit-learn**, focusing on their key functionalities and effective usage.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Data Preprocessing Steps}
    \begin{enumerate}
        \item \textbf{Handling Missing Values}
        \begin{itemize}
            \item Pandas efficiently manages missing data.
            \item \textit{Example}: Fill with mean.
            \begin{lstlisting}[language=Python]
import pandas as pd

# Load dataset
df = pd.read_csv('data.csv')
# Fill missing values with mean
df['column_name'].fillna(df['column_name'].mean(), inplace=True)
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Data Encoding}
        \begin{itemize}
            \item Crucial for converting categorical variables to numeric.
            \item \textit{Example}: One-hot encoding.
            \begin{lstlisting}[language=Python]
df = pd.get_dummies(df, columns=['categorical_column'])
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Data Preprocessing Steps (cont.)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Feature Scaling}
        \begin{itemize}
            \item Normalizing or standardizing features.
            \item \textit{Example}: Standardize data using Scikit-learn.
            \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[['feature1', 'feature2']])
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Train-Test Split}
        \begin{itemize}
            \item Essential for model evaluation.
            \item \textit{Example}:
            \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Complete Preprocessing Workflow}
    \begin{block}{Example: Step-by-Step}
    \begin{itemize}
        \item \textbf{Step 1: Load Data}
        \begin{lstlisting}[language=Python]
df = pd.read_csv('data.csv')
        \end{lstlisting}
        
        \item \textbf{Step 2: Handling Missing Values}
        \begin{lstlisting}[language=Python]
df.fillna(method='ffill', inplace=True)  # Forward fill missing values
        \end{lstlisting}

        \item \textbf{Step 3: Encoding Categorical Variables}
        \begin{lstlisting}[language=Python]
df = pd.get_dummies(df, columns=['category_col'])
        \end{lstlisting}

        \item \textbf{Step 4: Feature Scaling}
        \begin{lstlisting}[language=Python]
scaler = StandardScaler()
df[['scaled_feature']] = scaler.fit_transform(df[['feature']])
        \end{lstlisting}

        \item \textbf{Step 5: Train-Test Split}
        \begin{lstlisting}[language=Python]
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        \end{lstlisting}
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Data Preprocessing in Action}
    \begin{block}{Introduction to Data Preprocessing}
        Data preprocessing involves cleaning, transforming, and organizing raw data for better performance in data mining and machine learning tasks. It is crucial as it significantly impacts results and efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Predicting Customer Churn - Overview}
    \begin{itemize}
        \item \textbf{Overview:} A telecommunications company aimed to predict customer churn to identify at-risk customers for retention strategies.
        \item \textbf{Initial Data:}
        \begin{itemize}
            \item Data Sources: Customer demographics, call records, billing info, customer service interactions.
            \item Raw Data Size: 100,000 records with over 30 features.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges Faced Before Preprocessing}
    \begin{enumerate}
        \item Missing Values: Many records had missing fields (e.g., "monthly bill").
        \item Inconsistent Formatting: Different date formats and categorical entries (e.g., “Yes” vs. “yes”).
        \item Outliers: Unusual spikes in billing skewed analysis results.
        \item High Dimensionality: Complex datasets introduced noise into the models.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Steps Taken}
    \begin{enumerate}
        \item \textbf{Handling Missing Values:} 
        \begin{itemize}
            \item Method: Imputation (mean/median for numerical, mode for categorical).
            \item Example: Filling "monthly bill" with the average.
        \end{itemize}
        
        \item \textbf{Normalization and Encoding:}
        \begin{itemize}
            \item Normalize contract length to a [0, 1] range.
            \item One-hot encoding for categorical variables.
        \end{itemize}
        
        \item \textbf{Outlier Detection:} 
        \begin{itemize}
            \item Z-score and IQR method to identify outliers.
            \item Example: Flagging monthly bills above three standard deviations.
        \end{itemize}

        \item \textbf{Feature Selection:} 
        \begin{itemize}
            \item Techniques: Correlation matrix and Recursive Feature Elimination (RFE).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Effective Preprocessing}
    \begin{block}{Results}
        \begin{itemize}
            \item Improved Model Accuracy: Churn prediction model achieved \textbf{90\% accuracy} (up from 75\%).
            \item Reduced Training Time: Fewer features allowed for faster model training.
            \item Enhanced Interpretability: Simplification made patterns clearer for strategic decisions.
        \end{itemize}
    \end{block}
    \begin{block}{Key Takeaway}
        Effective data preprocessing improves analytics performance and reveals insights for strategic decision-making, proving its value in real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Data preprocessing is foundational for the success of data mining projects. The customer churn case exemplifies how meticulous preprocessing significantly enhances results and guides businesses toward data-driven decisions.
    \end{block}
\    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Addresses issues like missing values and outliers.
            \item Vital techniques: normalization, encoding, feature selection.
            \item Data quality directly influences predictive model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Introduction}
    \begin{block}{Introduction to Data Preprocessing Challenges}
        Data preprocessing is a crucial step in any data analysis or machine learning project. However, it presents a range of challenges that can significantly affect the quality of the resulting models and insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Common Challenges}
    \begin{enumerate}
        \item \textbf{Data Inconsistencies}
            \begin{itemize}
                \item \textbf{Definition:} Discrepancies or contradictions within data.
                \item \textbf{Examples:}
                    \begin{itemize}
                        \item Duplicate Records
                        \item Format Variability
                    \end{itemize}
            \end{itemize}
        \item \textbf{Missing Values}
            \begin{itemize}
                \item \textbf{Impact:} Can skew analysis, leading to inaccurate models.
            \end{itemize}
        \item \textbf{Outliers}
            \begin{itemize}
                \item \textbf{Definition:} Extreme values that deviate significantly.
            \end{itemize}
        \item \textbf{Scalability Issues}
            \begin{itemize}
                \item \textbf{Definition:} Growing datasets make preprocessing tasks time-consuming and resource-intensive.
            \end{itemize}
        \item \textbf{Data Integration}
            \begin{itemize}
                \item \textbf{Definition:} Complexity in combining data from different sources.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Solutions}
    \begin{enumerate}
        \item \textbf{Standardization}
            \begin{itemize}
                \item Use a uniform format for data entries.
            \end{itemize}
        \item \textbf{Handling Missing Values}
            \begin{itemize}
                \item \textbf{Methods:} Imputation, Deletion.
            \end{itemize}
        \item \textbf{Outlier Detection}
            \begin{itemize}
                \item \textbf{Techniques:} Z-score, IQR.
                \begin{lstlisting}[language=Python]
import pandas as pd
from scipy import stats

# Assuming df is your DataFrame
df_cleaned = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]
                \end{lstlisting}
            \end{itemize}
        \item \textbf{Efficient Tools for Scalability}
            \begin{itemize}
                \item Use libraries like Dask for large datasets.
            \end{itemize}
        \item \textbf{Data Validation Techniques}
            \begin{itemize}
                \item Perform routine data validation checks.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Preprocessing - Key Points}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Initiate proactive data quality assessments at the collection stage.
            \item Document preprocessing steps to ensure reproducibility.
            \item Adapt preprocessing approaches based on scale and nature of datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assessment and Reflection - Overview}
    \begin{block}{Objective}
        Assess students' comprehension and application of data preprocessing techniques through various assessment methods that foster critical thinking and real-world application.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assessment and Reflection - Assessment Methods}
    \begin{enumerate}
        \item \textbf{Quizzes and Assignments:}
        \begin{itemize}
            \item Short quizzes on key concepts (e.g. normalization, handling missing data, feature selection).
            \item Practical assignments involving preprocessing of a provided dataset.
        \end{itemize}
        
        \item \textbf{Group Projects:}
        \begin{itemize}
            \item Collaborative projects on real-world datasets (e.g. from Kaggle).
            \item Tasks include data cleaning and transformation.
            \item Team presentations on preprocessing findings.
        \end{itemize}
        
        \item \textbf{Reflective Journals:}
        \begin{itemize}
            \item Weekly journals documenting preprocessing challenges and methods.
            \item Focus on implications of data quality in real-world applications.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Preprocessing}
    \begin{block}{Foundation for Analysis}
        Data preprocessing is crucial as it influences the effectiveness of data analysis and the reliability of predictive models. Clean data leads to clearer insights and better decision-making.
    \end{block}
    
    \begin{block}{Real-World Application}
        Preprocessing can significantly impact industries such as:
        \begin{itemize}
            \item \textbf{Healthcare:} Improved patient treatment plans and outcomes.
            \item \textbf{E-commerce:} Personalized user experiences driving sales.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Overview of Data Preprocessing}
    Data preprocessing is a crucial phase in data mining and analysis. It involves transforming raw data into a clean and usable format, allowing analysts to extract meaningful insights and build accurate predictive models. Poor preprocessing can lead to misleading results.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Key Points}
    \begin{enumerate}
        \item \textbf{Importance of Data Preprocessing}
        \begin{itemize}
            \item Quality of Insights: Clean data leads to more accurate analysis and reliable outputs.
            \item Efficiency in Modeling: Preprocessed data shortens the time taken to train models and improves their performance.
        \end{itemize}
        
        \item \textbf{Common Preprocessing Techniques}
        \begin{itemize}
            \item Data Cleaning (e.g., handling missing values)
            \item Data Transformation (e.g., normalization and standardization)
            \item Feature Selection/Extraction (e.g., PCA, Recursive Feature Elimination)
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Categorical Data & Outlier Treatment}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Handling Categorical Data}
        \begin{itemize}
            \item One-Hot Encoding: Converts categorical variables into binary vectors.
            \item Label Encoding: Assigns integers to categories.
        \end{itemize}

        \item \textbf{Outlier Detection and Treatment}
        \begin{itemize}
            \item Importance: Outliers can skew data and impact model performance.
            \item Techniques: Z-score method, IQR method.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Conclusion and Call to Action}
    In summary, data preprocessing is foundational in ensuring the validity and reliability of data-driven insights. Proper techniques can enhance decision-making, leading to successful outcomes in data mining projects.
    
    \textbf{Call to Action:} Reflect on your own experiences with data. How might the techniques discussed enhance your future data analysis projects?
\end{frame}


\end{document}