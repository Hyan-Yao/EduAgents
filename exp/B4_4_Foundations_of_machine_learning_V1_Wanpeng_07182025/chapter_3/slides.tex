\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning}
    \begin{itemize}
        \item Key machine learning paradigm
        \item Trained using labeled datasets
        \item Aims to predict outcomes for unseen data
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Supervised Learning?}
    \begin{block}{Definition}
        Supervised Learning is a key paradigm in machine learning where a model is trained using a labeled dataset. Each training example consists of an input object (features) and its corresponding output value (label).
    \end{block}
    
    \begin{itemize}
        \item \textbf{Labeled Data:} Input features with corresponding outputs (e.g., image of a cat labeled as "cat").
        \item \textbf{Training Phase:} The algorithm learns from labeled data.
        \item \textbf{Prediction Phase:} The model predicts outputs for new, unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Supervised Learning}
    \begin{itemize}
        \item \textbf{Guided Learning:} Trained on labeled data leading to effective pattern recognition.
        \item \textbf{Versatile Applications:}
            \begin{itemize}
                \item \textbf{Finance:} Credit scoring
                \item \textbf{Healthcare:} Disease diagnosis
                \item \textbf{Marketing:} Customer segmentation
                \item \textbf{Image Recognition:} Facial recognition systems
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Supervised Learning}
    \begin{enumerate}
        \item \textbf{Classification Task:}
            \begin{itemize}
                \item \textit{Example:} Predict whether an email is 'spam' or 'not spam'.
                \item \textbf{Features:} Email content, sender information.
                \item \textbf{Output:} Class labels ('spam', 'not spam').
            \end{itemize}
        \item \textbf{Regression Task:}
            \begin{itemize}
                \item \textit{Example:} Predicting the price of a house.
                \item \textbf{Features:} Number of bedrooms, location size, square footage.
                \item \textbf{Output:} Continuous value (e.g., price in USD).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Challenges}
    \begin{itemize}
        \item Supervised learning improves with exposure to more data.
        \item High-quality labeled data is essential for effective training.
        \item \textbf{Challenges:} Overfitting and underfitting.
            \begin{itemize}
                \item Techniques like cross-validation can mitigate these issues.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Metrics}
    Common metrics for evaluating supervised learning models:
    
    \begin{itemize}
        \item \textbf{Accuracy:} 
        \begin{equation}
            \text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total predictions}}
        \end{equation}
        
        \item \textbf{Mean Squared Error (MSE) for Regression:}
        \begin{equation}
            MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \end{equation}
        where \(y_i\) is the true value and \(\hat{y}_i\) is the predicted value.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Supervised learning is a powerful approach in machine learning.
        \item It provides a solid foundation for exploring specific supervised tasks in future lessons.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning - Introduction}
    Supervised learning is a powerful machine learning paradigm that involves training models on labeled data.  
    We will focus on two primary types of supervised learning tasks: 
    \begin{itemize}
        \item \textbf{Regression}
        \item \textbf{Classification}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning - Regression}
    \textbf{Definition}: 
    Regression predicts continuous numerical outcomes. 
    The goal is to model the relationship between input features and a continuous target variable.
    
    \textbf{Key Characteristics}:
    \begin{itemize}
        \item Output is a real number.
        \item Used to predict quantities (e.g., price, temperature).
    \end{itemize}
    
    \textbf{Example}: Predicting house prices based on features like size and location.

    \textbf{Common Algorithms}:
    \begin{itemize}
        \item Linear Regression
        \item Polynomial Regression
        \item Support Vector Regression (SVR)
    \end{itemize}

    \textbf{Formula}:
    \begin{equation}
        Y = MX + B 
    \end{equation}
    Where $Y$ = predicted outcome, $M$ = slope, $X$ = input feature, $B$ = y-intercept.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning - Classification}
    \textbf{Definition}:
    Classification categorizes input data into predefined classes.

    \textbf{Key Characteristics}:
    \begin{itemize}
        \item Output is a discrete label.
        \item Applied to problems with categorical outcomes (e.g., yes/no).
    \end{itemize}
    
    \textbf{Example}: Classifying emails as "spam" or "not spam".

    \textbf{Common Algorithms}:
    \begin{itemize}
        \item Logistic Regression
        \item Decision Trees
        \item Support Vector Machines (SVM)
    \end{itemize}

    \textbf{Evaluation Metrics}:
    \begin{itemize}
        \item Accuracy
        \item Precision \& Recall
    \end{itemize}

    \textbf{Key Differences}:
    \begin{tabular}{|c|c|c|}
        \hline
        Feature & Regression & Classification \\
        \hline
        Output & Continuous numeric value & Discrete class label \\
        Goal & Predict a quantity & Assign a category \\
        Evaluation & MSE & Accuracy, F1 Score \\
        Example Tasks & Stock price prediction & Disease diagnosis \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the distinctions between regression and classification is crucial. 
    These differences guide the selection of appropriate algorithms and evaluation metrics in supervised learning tasks.
    Next, we will explore \textbf{Linear Regression}, a foundational technique in predictive modeling.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression}
    \begin{block}{Understanding Linear Regression}
        Linear regression is a fundamental statistical method used in supervised learning that models the relationship between a dependent variable (target) and one or more independent variables (predictors).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Key Concepts}
    \begin{enumerate}
        \item \textbf{Assumptions of Linear Regression:}
        \begin{itemize}
            \item Linearity: The relationship is linear.
            \item Independence: Observations are independent.
            \item Homoscedasticity: Constant variance of errors across all levels.
            \item Normality: Residuals should be normally distributed.
        \end{itemize}

        \item \textbf{The Linear Regression Equation:}
        \begin{equation}
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
        \end{equation}
        Where:
        \begin{itemize}
            \item $Y$: Dependent variable
            \item $X$: Independent variables
            \item $\beta_0$: Intercept
            \item $\beta_1, \beta_2, \ldots, \beta_n$: Coefficients
            \item $\epsilon$: Error term
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Applications and Example}
    \begin{block}{Application in Predictive Modeling}
        Linear regression is widely used in:
        \begin{itemize}
            \item Economics (predicting spending)
            \item Real Estate (estimating property prices)
            \item Healthcare (forecasting patient outcomes)
        \end{itemize}
        It's effective for making predictions and estimating trends.
    \end{block}

    \begin{block}{Example Use Case}
        Consider a dataset containing house prices. Apply linear regression to predict prices based on:
        \begin{itemize}
            \item Square footage
            \item Number of bedrooms
            \item Location
        \end{itemize}
        The model identifies the influence of these factors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Code Snippet}
    \begin{block}{Code Snippet (Using Python's Scikit-Learn)}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load dataset
data = pd.read_csv('housing_data.csv')

# Prepare data
X = data[['square_footage', 'bedrooms']]  # Independent variables
y = data['price']  # Dependent variable

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit model
model = LinearRegression().fit(X_train, y_train)

# Predict
predictions = model.predict(X_test)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Linear Regression}
    \begin{block}{Introduction to Performance Metrics}
        When evaluating the effectiveness of a linear regression model, two key performance metrics are commonly used: \textbf{R-squared (R²)} and \textbf{Mean Squared Error (MSE)}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Linear Regression - R-squared (R²)}
    \begin{itemize}
        \item \textbf{Definition:} R-squared is the proportion of the variance in the dependent variable that can be explained by the independent variable(s) in the model.
        \item \textbf{Interpretation:}
        \begin{itemize}
            \item R² values range from 0 to 1.
            \item An R² of 0 means no explanation of variability.
            \item An R² of 1 indicates complete explanation of variability.
        \end{itemize}
        \item \textbf{Formula:}
        \begin{equation}
            R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}
        \end{equation}
        where:
        \begin{itemize}
            \item \( SS_{\text{res}} \) = sum of squares of residuals (errors)
            \item \( SS_{\text{tot}} \) = total sum of squares (variance in observed data)
        \end{itemize}
        \item \textbf{Example:} If R² = 0.85, this indicates that 85\% of the variance can be explained by the model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Linear Regression - Mean Squared Error (MSE)}
    \begin{itemize}
        \item \textbf{Definition:} MSE measures the average of the squares of the errors; that is, the average squared difference between estimated values and actual values.
        \item \textbf{Interpretation:}
        \begin{itemize}
            \item Lower MSE values indicate a better fit.
            \item Sensitive to outliers, emphasizing larger discrepancies.
        \end{itemize}
        \item \textbf{Formula:}
        \begin{equation}
            MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \end{equation}
        where:
        \begin{itemize}
            \item \( n \) = number of observations
            \item \( y_i \) = actual values
            \item \( \hat{y}_i \) = predicted values
        \end{itemize}
        \item \textbf{Example:} If MSE = 4, this implies an average squared error of 4.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item R² indicates model explanatory power; MSE evaluates prediction accuracy. Use both for comprehensive evaluation.
            \item A high R² with a low MSE indicates a well-performing model.
            \item Limitations: R² does not account for model complexity; consider adjusted R² for multiple regression. MSE does not reveal error direction.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        These performance metrics are crucial for assessing and improving linear regression models. Understanding R-squared and Mean Squared Error will enhance your model evaluation and predictive accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Introduction}
    \begin{itemize}
        \item Logistic Regression is a statistical method for binary classification.
        \item It predicts outcomes with two possible categories (e.g., success/failure).
        \item Unlike linear regression, it estimates the probability of a class label.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Key Concepts}
    \begin{enumerate}
        \item \textbf{Logistic Function (Sigmoid Function)}:
        \begin{block}{Formula}
            \[
            \sigma(z) = \frac{1}{1 + e^{-z}}
            \]
        \end{block}
        Where \( z = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n \).
        
        \item \textbf{Interpretation of the Output}:
        \begin{itemize}
            \item Output \( P(Y=1 | X) \) is the probability of class '1' given features \( X \).
        \end{itemize}
        
        \item \textbf{Decision Boundary}:
        \begin{itemize}
            \item If \( P(Y=1 | X) \geq 0.5 \), classify as 1; otherwise, classify as 0.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Example and Applications}
    \begin{itemize}
        \item \textbf{Example}:
        \begin{itemize}
            \item Predicting student success based on study hours and attendance:
            \[
            z = -4 + 0.5X_1 + 1.2X_2
            \]
        \end{itemize}
        
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Designed for binary outcomes; modifications needed for multi-class.
            \item Assumes a linear relationship between log-odds and independent variables.
            \item Widely used in healthcare, finance, and social sciences.
        \end{itemize}
        
        \item \textbf{Conclusion}:
        \begin{itemize}
            \item A powerful and interpretable tool for binary classification problems.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Logistic Regression - Introduction}
    \begin{block}{Introduction}
        Evaluating the performance of a logistic regression model is crucial to understanding its predictive power and reliability. Various performance metrics help us assess the model's effectiveness in classifying instances into binary categories.
    \end{block}
    \begin{itemize}
        \item Accuracy
        \item Precision
        \item Recall
        \item F1-Score
        \item ROC-AUC
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Accuracy and Precision}
    \begin{block}{1. Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted observations to the total observations.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
            \end{equation}
            \item \textbf{Example}: If your model predicts 80 out of 100 instances correctly, accuracy is \( \frac{80}{100} = 0.80 \) or 80\%.
        \end{itemize}
    \end{block}
    
    \begin{block}{2. Precision}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted positive observations to the total predicted positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Precision} = \frac{TP}{TP + FP}
            \end{equation}
            \item \textbf{Example}: If there are 70 true positives and 30 false positives, precision is \( \frac{70}{70 + 30} = 0.70 \).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Recall, F1-Score, and ROC-AUC}
    \begin{block}{3. Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted positive observations to all the actual positives.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Recall} = \frac{TP}{TP + FN}
            \end{equation}
            \item \textbf{Example}: With 70 true positives and 10 false negatives, recall is \( \frac{70}{70 + 10} = 0.88 \).
        \end{itemize}
    \end{block}

    \begin{block}{4. F1-Score}
        \begin{itemize}
            \item \textbf{Definition}: The weighted average of Precision and Recall.
            \item \textbf{Formula}:
            \begin{equation}
                \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Example}: If Precision = 0.70 and Recall = 0.88, then F1-Score is approximately 0.78.
        \end{itemize}
    \end{block}

    \begin{block}{5. ROC-AUC}
        \begin{itemize}
            \item \textbf{Definition}: Graphical plot illustrating the diagnostic ability of a binary classifier as its discrimination threshold varies.
            \item \textbf{Interpretation}: AUC ranges from 0 to 1; 1 indicates a perfect model, while 0.5 indicates no discrimination ability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Each metric serves a different purpose in model evaluation.
            \item Consider multiple metrics, especially with imbalanced datasets.
            \item F1-Score is useful when costs of false positives and negatives differ.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Evaluating logistic regression models through these metrics allows for informed decisions based on model performance. Understanding these nuances will enhance the effectiveness of classification tasks in practical applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Overview}
    \begin{block}{Understanding Decision Trees}
        A Decision Tree is a flowchart-like structure used for classification and regression tasks. It consists of nodes that represent decisions based on feature values, leading to outcomes at the leaf nodes.
    \end{block}
    
    \begin{block}{Basic Structure}
        \begin{itemize}
            \item \textbf{Root Node}: Represents the whole dataset, the starting point of the tree.
            \item \textbf{Internal Nodes}: Decision points that split the dataset based on feature values.
            \item \textbf{Branches}: Connections between nodes, showing the flow from question to answer.
            \item \textbf{Leaf Nodes}: Final outcomes or predictions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Example}
    \begin{block}{Example of a Simple Decision Tree}
        Imagine we want to classify whether an animal is a "Cat" or "Dog" based on two features: size and fur length.
        
        \begin{center}
            \begin{verbatim}
                      [Is it large?]
                          /  \
                         Yes  No
                        /      \
                [Is fur long?]   [Cat]
                      /  \
                     Yes  No
                     /      \
                   Dog      Cat
            \end{verbatim}
        \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Construction and Pruning}
    \begin{block}{Construction of Decision Trees}
        \begin{enumerate}
            \item \textbf{Select the Best Feature}: Use metrics like Gini Impurity or Information Gain.
            \item \textbf{Split the Data}: Divide the dataset into subsets based on the chosen feature.
            \item \textbf{Repeat Recursively}: Apply the same process iteratively on each subset.
            \item \textbf{Stop Criteria}: Stop when:
            \begin{itemize}
                \item All samples belong to one class.
                \item No further improvement can be achieved.
                \item A pre-defined tree depth is reached.
            \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Tree Pruning Techniques}
        \begin{itemize}
            \item \textbf{Pre-Pruning}: Halt growth of the tree based on certain criteria.
            \item \textbf{Post-Pruning}: Remove nodes from a fully grown tree that provide little predictive power.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision Trees - Key Points and Implementation}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Decision trees handle both numerical and categorical data effectively.
            \item They are intuitive and easy to interpret.
            \item Overfitting can be a major drawback; hence the need for pruning strategies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Practical Implementation Example (Python)}
        \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier

# Sample dataset
X = [[0, 0], [1, 1]]  # Features
y = [0, 1]             # Labels

# Create Decision Tree model
model = DecisionTreeClassifier()
model.fit(X, y)
predictions = model.predict([[2, 2]])

print(predictions)  # Output the prediction
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Decision Trees}
    \begin{block}{Introduction}
        Evaluating the performance of decision trees is critical to understanding their effectiveness in classification tasks.
        This slide highlights three essential metrics: the confusion matrix, accuracy, and decision boundaries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Confusion Matrix}
    \begin{block}{What is a Confusion Matrix?}
        A confusion matrix is a table that assesses the performance of a classification model by summarizing the results of predictions against actual outcomes.
    \end{block}
    
    \begin{itemize}
        \item \textbf{True Positive (TP)}: Correctly predicted positive instances.
        \item \textbf{True Negative (TN)}: Correctly predicted negative instances.
        \item \textbf{False Positive (FP)}: Incorrectly predicted positive instances (Type I error).
        \item \textbf{False Negative (FN)}: Incorrectly predicted negative instances (Type II error).
    \end{itemize}
    
    \begin{block}{Confusion Matrix Structure}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
                  & Actual Positive & Actual Negative \\
        \hline
        Predicted Positive & TP               & FP               \\
        \hline
        Predicted Negative & FN               & TN               \\
        \hline
    \end{tabular}
    \end{block}
    
    \begin{itemize}
        \item Helps identify classification performance for positive and negative classes.
        \item Provides insights into error types made by the model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Accuracy}
    \begin{block}{Definition}
        Accuracy is defined as the proportion of true results (true positives and true negatives) in all examined cases.
    \end{block}
    
    \begin{block}{Formula}
    \begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    \end{block}
    
    \begin{itemize}
        \item High accuracy indicates good performance but can be misleading with imbalanced datasets.
    \end{itemize}
    
    \begin{block}{Example}
        For a model with:
        \begin{itemize}
            \item TP = 40
            \item TN = 30
            \item FP = 10
            \item FN = 20
        \end{itemize}
        \[
        \text{Accuracy} = \frac{40 + 30}{40 + 30 + 10 + 20} = \frac{70}{100} = 0.70 \text{ or } 70\%
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Decision Boundaries}
    \begin{block}{Definition}
        A decision boundary is a hypersurface that partitions the feature space into regions corresponding to different class labels.
    \end{block}
    
    \begin{itemize}
        \item **Visualizing Decision Boundaries**: Plotting decision boundaries shows how the model divides the feature space.
        \item Decision trees create perpendicular splits, resulting in rectangular regions.
    \end{itemize}
    
    \begin{block}{Example}
        For a simple 2D dataset with features (X1 and X2), the decision boundary illustrates classification into classes A and B.
    \end{block}
    
    \begin{block}{Summary}
        \begin{itemize}
            \item Confusion Matrix: Valuable for understanding performance and classification errors.
            \item Accuracy: Simple measure but should be used cautiously with imbalanced data.
            \item Decision Boundaries: Visual aid for interpreting class separations by models.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ensemble Methods}
    \begin{block}{Introduction}
        Ensemble methods are powerful techniques that combine predictions from multiple models to improve overall performance, enhance robustness, and reduce the risk of overfitting. 
    \end{block}
    \begin{itemize}
        \item Leverage strengths of different models
        \item Minimize weaknesses of individual models
        \item Common methods: Bagging, Boosting, Stacking
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bagging (Bootstrap Aggregating)}
    \begin{block}{Definition}
        Improves stability and accuracy by combining multiple models trained on different subsets of data.
    \end{block}
    \begin{itemize}
        \item Generate bootstrapped subsets from original dataset.
        \item Train a model on each subset independently.
        \item Final prediction: Average for regression or majority voting for classification.
    \end{itemize}
    \begin{block}{Example}
        Random Forest: uses multiple decision trees.
    \end{block}
    \begin{equation}
    \hat{y} = \frac{1}{M} \sum_{m=1}^{M} \hat{y}_m
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Boosting}
    \begin{block}{Definition}
        A sequential ensemble technique that corrects errors made by previous models by focusing on difficult instances.
    \end{block}
    \begin{itemize}
        \item Models are trained sequentially.
        \item Each model learns from the errors of its predecessor.
        \item Weighted voting or averaging for final prediction.
    \end{itemize}
    \begin{block}{Example}
        AdaBoost: increases weights of incorrectly classified instances.
    \end{block}
    \begin{equation}
    \hat{y} = \sum_{m=1}^{M} \alpha_m h_m(x)
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Random Forests}
    \begin{block}{Understanding Random Forests}
        Random Forests are an ensemble learning technique used for classification and regression tasks. 
        By constructing multiple decision trees during training and aggregating their predictions, this algorithm enhances accuracy and robustness. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Ensemble Learning:} 
        Combines multiple models for a stronger, overall model; tackles overfitting and variance issues found in single decision trees.
        
        \item \textbf{Decision Trees:} 
        Individual base learners in Random Forests; may overfit especially on small datasets.
        
        \item \textbf{Bagging Technique:} 
        Samples data with replacement to create subsets for training each tree, enhancing tree diversity for better generalization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Random Forests Work}
    \begin{enumerate}
        \item \textbf{Data Sampling:} 
        Randomly select subsets of the original dataset (with replacement).
        
        \item \textbf{Tree Building:} 
        For each subset, grow a Decision Tree by:
        \begin{itemize}
            \item Choosing a random subset of features at each split.
            \item Splitting nodes to maximize purity (using Gini impurity or entropy).
        \end{itemize}
        
        \item \textbf{Aggregating Predictions:} 
        For Classification: the class with the most votes wins; for Regression: average the predictions from all trees.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example}
    Imagine predicting whether a patient has a disease (Yes/No) using features like age, blood pressure, and cholesterol levels:
    \begin{itemize}
        \item Create multiple decision trees using random samples and features of the dataset.
        \item Each tree makes a prediction for a new patient.
        \item Combine all predictions to determine the final prediction outcome.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Limitations}
    \begin{block}{Advantages}
        \begin{itemize}
            \item \textbf{Improved accuracy:} Reduces overfitting and improves generalization.
            \item \textbf{Robust to noise:} Majority vote accounts for outliers and noise.
            \item \textbf{Feature importance:} Identifies significant predictors in the dataset.
        \end{itemize}
    \end{block}
    
    \begin{block}{Limitations}
        \begin{itemize}
            \item \textbf{Computational Load:} Requires significant resources for training.
            \item \textbf{Less Interpretability:} Harder to explain decisions compared to a single decision tree.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet}
    Here’s a simple implementation using the \texttt{RandomForestClassifier} from the \texttt{sklearn} library:
    \begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Random Forests leverage the power of multiple decision trees to enhance model performance. 
    This ensemble method is essential for increasing accuracy and reducing overfitting, vital for effective data-driven decision-making in various applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Boosting Algorithms}
    \begin{block}{Overview of Boosting}
        Boosting is a powerful ensemble learning technique that combines multiple weak learners to improve predictive accuracy. Unlike bagging, boosting trains models sequentially, focusing on correcting errors made by previous models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Boosting Techniques - Part 1}
    \begin{enumerate}
        \item \textbf{AdaBoost (Adaptive Boosting)}:
        \begin{itemize}
            \item Focuses on creating a strong classifier from multiple weak classifiers (e.g., decision stumps).
            \item Adjusts instance weights based on the errors of previous classifiers.
            \item \textbf{Final Model}: A weighted sum of all classifiers' predictions.
        \end{itemize}
        \begin{equation}
            F(x) = \sum_{m=1}^{M} \alpha_m h_m(x)
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Boosting Techniques - Part 2}
    \begin{enumerate}[resume]
        \item \textbf{Gradient Boosting}:
        \begin{itemize}
            \item Improves models by minimizing the loss function through gradient descent.
            \item Each tree predicts the residuals of the previous predictions.
            \item The learning rate (\( \nu \)) regulates each tree's contribution.
        \end{itemize}
        \begin{equation}
            F_m(x) = F_{m-1}(x) + \nu h_m(x)
        \end{equation}

        \item \textbf{XGBoost (Extreme Gradient Boosting)}:
        \begin{itemize}
            \item An optimized gradient boosting implementation with regularization.
            \item Efficiently handles large datasets, including missing values.
            \item Incorporates L1 and L2 regularization, preventing overfitting.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Ensemble Method}: Boosting sequentially combines models to enhance accuracy by focusing on errors.
        \item \textbf{Weight Adjustment}: Variability in data point importance across AdaBoost and Gradient Boosting.
        \item \textbf{Efficiency and Versatility}: XGBoost excels in large, complex datasets and competitive scenarios.
    \end{itemize}

    \begin{block}{Conclusion}
        Boosting algorithms, including AdaBoost, Gradient Boosting, and XGBoost, significantly enhance model accuracy by innovatively aggregating weak learners.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Overview}
    \begin{block}{What are Neural Networks?}
        Neural Networks (NNs) are a class of machine learning algorithms inspired by the human brain. 
        They consist of interconnected layers of nodes (neurons) that process data and can learn complex patterns through experience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Architecture}
    \begin{itemize}
        \item \textbf{Layers:}
            \begin{itemize}
                \item \textbf{Input Layer:} Receives the input features.
                \item \textbf{Hidden Layer(s):} Perform calculations and extract features; can have multiple layers in a deep neural network.
                \item \textbf{Output Layer:} Produces the final outcome (e.g., classification or regression).
            \end{itemize}
        \item \textbf{Nodes (Neurons):} Individual nodes take inputs, apply weights, sum them, add a bias, and use an activation function.
    \end{itemize}
    \begin{block}{Basic Structure}
        Input Layer $\rightarrow$ Hidden Layer(s) $\rightarrow$ Output Layer
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neurons and Activation Functions}
    \begin{itemize}
        \item \textbf{Neurons: The Building Blocks}
            \begin{block}{Input Equation}
                \begin{equation}
                    z = \sum (w_i \cdot x_i) + b
                \end{equation}
                where:
                \begin{itemize}
                    \item $w_i$: Weights associated with the inputs.
                    \item $x_i$: Input features.
                    \item $b$: Bias term.
                \end{itemize}
            \end{block}
        \item \textbf{Activation Functions:} 
            Introduce non-linearity, essential for learning complex patterns:
            \begin{itemize}
                \item \textbf{Sigmoid:} 
                    \begin{equation} 
                        \sigma(z) = \frac{1}{1 + e^{-z}} 
                    \end{equation}
                    Output: $(0, 1)$
                \item \textbf{ReLU (Rectified Linear Unit):}
                    \begin{equation} 
                        f(z) = \max(0, z) 
                    \end{equation}
                    Output: $[0, \infty)$
                \item \textbf{Softmax:}
                    \begin{equation} 
                        \text{softmax}(z_i) = \frac{e^{z_i}}{\sum e^{z_j}} 
                    \end{equation}
                    Output: Probability distribution across multiple classes.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks}
    \begin{itemize}
        \item \textbf{Image Recognition:} Classifying objects within images (e.g., identifying animals in pictures).
        \item \textbf{Natural Language Processing:} Tasks like sentiment analysis and translation.
        \item \textbf{Medical Diagnosis:} Predicting diseases based on medical images and records.
        \item \textbf{Autonomous Vehicles:} Recognizing obstacles and navigating in real-time.
    \end{itemize}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item NNs mimic brain functionality and capture complex correlations.
            \item Activation functions are crucial for learning.
            \item Versatile applications make NNs powerful tools in AI.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Overview}
    \begin{block}{Fundamentals of Training Neural Networks}
        This slide covers the essential components involved in training neural networks. We will discuss:
        \begin{itemize}
            \item Forward Propagation
            \item Backpropagation
            \item Overfitting
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Forward Propagation}
    \begin{block}{Forward Propagation}
        \textbf{Definition}: The process of passing input data through the neural network to calculate the output.
        \begin{itemize}
            \item Each neuron receives inputs, applies a weighted sum and a bias, and then passes the result through an activation function.
        \end{itemize}
        
        \textbf{Mathematical Representation}:
        \begin{equation}
            z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b
        \end{equation}
        \begin{equation}
            a = \sigma(z)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( z \) is the weighted sum
            \item \( w \) are weights
            \item \( x \) are inputs
            \item \( b \) is bias
            \item \( \sigma \) is the activation function
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Backpropagation and Overfitting}
    \begin{block}{Backpropagation}
        \textbf{Definition}: A method for training neural networks that adjusts weights based on the prediction error.
        \begin{itemize}
            \item Calculate loss using a loss function (e.g., Mean Squared Error).
            \item Compute the gradient of the loss with respect to the output.
            \item Propagate the error back, updating weights using gradient descent.
        \end{itemize}
        
        \textbf{Weight Update Formula}:
        \begin{equation}
            w_i = w_i - \eta \cdot \frac{\partial L}{\partial w_i}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( \eta \) is the learning rate
            \item \( L \) is the loss function
        \end{itemize}
    \end{block}

    \begin{block}{Overfitting}
        \textbf{Definition}: When the model learns training data too well, causing poor generalization to new data.
        \begin{itemize}
            \item Signs: High training accuracy, low validation/test accuracy.
        \end{itemize}
        
        \textbf{Prevention Techniques}:
        \begin{itemize}
            \item Regularization (e.g., L2 regularization)
            \item Dropout: Randomly zeroing inputs during training.
            \item Early Stopping: Stop training when validation performance degrades.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation of Algorithms}
    \begin{block}{Introduction to Performance Evaluation}
        Performance evaluation is crucial in supervised learning as it indicates how well an algorithm generalizes to unseen data. 
        Various metrics are used to measure and compare the efficacy of different supervised learning algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Performance Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item \textbf{Definition}: The ratio of correctly predicted instances to total instances.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
                \end{equation}
                \item \textbf{Example}: If your model correctly predicts 80 out of 100 instances, the accuracy is 80\%.
            \end{itemize}
        
        \item \textbf{Precision}
            \begin{itemize}
                \item \textbf{Definition}: The ratio of true positive predictions to the total positive predictions made by the model.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
                \end{equation}
                \item \textbf{Example}: If a spam filter identifies 10 emails as spam, of which 7 are actual spam, the precision is \( \frac{7}{10} = 0.7 \).
            \end{itemize}
        
        \item \textbf{Recall (Sensitivity)}
            \begin{itemize}
                \item \textbf{Definition}: The ratio of true positive predictions to the actual total positives in the dataset.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
                \end{equation}
                \item \textbf{Example}: If there were 15 spam emails and the filter identified 10 correctly, the recall is \( \frac{10}{15} = 0.67 \).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Performance Metrics}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{F1 Score}
            \begin{itemize}
                \item \textbf{Definition}: The harmonic mean of precision and recall, useful for balancing the two.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                \end{equation}
                \item \textbf{Example}: If precision is 0.7 and recall is 0.67, then \( F1 = 2 \times \frac{0.7 \times 0.67}{0.7 + 0.67} \approx 0.68 \).
            \end{itemize}

        \item \textbf{ROC Curve and AUC}
            \begin{itemize}
                \item \textbf{Definition}: The ROC curve shows the trade-off between sensitivity and specificity, while the AUC quantifies overall performance.
                \item \textbf{Key Point}: A model with an AUC of 1 is perfect, while an AUC of 0.5 indicates no better than random chance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Supervised Learning}
    \begin{block}{Introduction to Ethics}
        As we leverage supervised learning models to make decisions across various domains, it is crucial to recognize the ethical implications of these technologies.
        Key concerns:
        \begin{itemize}
            \item Biases
            \item Fairness
            \item Transparency
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Biases}
    \begin{block}{Definition}
        Bias in supervised learning arises when a model's predictions reflect prejudiced or unbalanced data, leading to unfair outcomes.
    \end{block}
    
    \begin{block}{Types of Bias}
        \begin{itemize}
            \item \textbf{Sample Bias}: Training data is not representative.
            \begin{itemize}
                \item \textit{Example}: A facial recognition model trained mainly on light-skinned individuals may perform poorly on darker-skinned faces.
            \end{itemize}
            \item \textbf{Label Bias}: Human judgement in labeling is biased.
            \begin{itemize}
                \item \textit{Example}: Sentiment analysis datasets may reflect cultural biases.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Fairness and 3. Transparency}
    \begin{block}{Fairness}
        \begin{itemize}
            \item **Importance**: Ensures decisions do not discriminate based on identity factors.
            \item **Measuring Fairness**:
            \begin{itemize}
                \item \textbf{Equal Opportunity}: Similar true positive rates across demographic groups.
                \item \textbf{Statistical Parity}: Equally distributed outcomes across groups.
            \end{itemize}
            \item \textit{Example}: Analyzing hiring rates across demographic groups in hiring algorithms.
        \end{itemize}
    \end{block}
    
    \begin{block}{Transparency}
        \begin{itemize}
            \item **Definition**: Making decision-making processes understandable.
            \item **Why Critical**:
            \begin{itemize}
                \item Builds trust in algorithms.
                \item Allows for accountability.
            \end{itemize}
            \item **Techniques for Transparency**:
            \begin{itemize}
                \item Model Interpretability: Techniques like SHAP and LIME.
                \item Documentation: Clear data sources and decision criteria.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Part 1}
    
    \textbf{Key Takeaways from Supervised Learning Techniques}
    
    \begin{enumerate}
        \item \textbf{Definition and Importance}:
        \begin{itemize}
            \item Supervised learning uses labeled datasets to train algorithms, enabling them to predict or classify data.
            \item Essential in applications like finance, healthcare, and marketing.
        \end{itemize}

        \item \textbf{Core Concepts}:
        \begin{itemize}
            \item \textbf{Training and Testing}: Necessary for model performance evaluation.
            \item \textbf{Algorithms}:
            \begin{itemize}
                \item Linear Regression
                \item Logistic Regression
                \item Decision Trees
                \item Support Vector Machines (SVM)
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Model Evaluation Metrics}:
        \begin{itemize}
            \item \textbf{Accuracy}: Ratio of correctly predicted instances to total instances.
            \item \textbf{Precision and Recall}: Measures performance in imbalanced datasets.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Part 2}
    
    \textbf{Key Takeaways from Supervised Learning Techniques (cont.)}

    \begin{enumerate}
        \setcounter{enumi}{3} % Adjusts the numbering
        \item \textbf{Ethical Considerations}:
        \begin{itemize}
            \item Fairness and transparency are crucial in deploying supervised learning models.
            \item Attention to bias and ethical implications can significantly influence outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Supervised Learning}

    \begin{enumerate}
        \item \textbf{Improving Model Interpretability}:
        \begin{itemize}
            \item Research into creating interpretable models and explaining complex algorithms' decisions.
        \end{itemize}
        
        \item \textbf{Handling Imbalanced Data}:
        \begin{itemize}
            \item Focus on enhanced strategies to improve model accuracy and reduce bias.
        \end{itemize}

        \item \textbf{Integration with Unsupervised Learning}:
        \begin{itemize}
            \item Semi-supervised learning combines labeled and unlabeled data for better robustness.
        \end{itemize}

        \item \textbf{Ethics and Fairness}:
        \begin{itemize}
            \item Continuous improvement of techniques to ensure fair automated decision-making.
        \end{itemize}

        \item \textbf{Expanding Applications}:
        \begin{itemize}
            \item As computational power increases, advanced supervised learning techniques are necessary for growth in fields like healthcare, finance, and autonomous systems.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formulas to Remember}

    \begin{block}{Accuracy}
        \begin{equation}
        \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Population}}
        \end{equation}
    \end{block}
    
    \begin{block}{Precision}
        \begin{equation}
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \end{equation}
    \end{block}
    
    \begin{block}{Recall}
        \begin{equation}
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}
    \end{block}
\end{frame}


\end{document}