\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Visualization \& Feature Extraction}
    \begin{block}{Overview of Week's Focus}
        In this week’s session, we will explore two crucial aspects of data analysis: \textbf{Data Visualization} and \textbf{Feature Extraction}. 
        Understanding these concepts is essential for effectively leveraging data to gain insights and make informed decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Visualization}
    \begin{block}{Definition}
        Data Visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, we can quickly understand trends, patterns, and outliers within the data.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Simplifies complexity:} Transforms complex data sets into accessible visuals.
        \item \textbf{Aids in communication:} Allows better storytelling through data.
        \item \textbf{Enhances understanding:} Makes patterns and insights easily recognizable.
    \end{itemize}

    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Bar Charts} for comparing quantities across categories.
            \item \textbf{Line Graphs} for illustrating trends over a continuous variable (e.g., sales over time).
            \item \textbf{Heat Maps} for showing the intensity of data at geographical locations.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Extraction}
    \begin{block}{Definition}
        Feature Extraction involves selecting and transforming data into a format suitable for a machine learning model. It aims to enhance the representation of the data while reducing dimensionality.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Reduces noise:} Helps in eliminating unnecessary features, improving model accuracy.
        \item \textbf{Maintains relevance:} Focuses on the most informative features that contribute effectively to predictive analytics.
        \item \textbf{Increases efficiency:} Less data reduces processing time and resource consumption.
    \end{itemize}
    
    \begin{block}{Techniques}
        \begin{itemize}
            \item \textbf{Normalization:} Adjusts the value scale of features (e.g., Min-Max Scaling, Z-Score Normalization).
            \item \textbf{Transformation:} Applies mathematical functions to create new features (e.g., logarithmic transformation).
            \item \textbf{Feature Selection:} Employs methods to select relevant features based on certain criteria (e.g., Recursive Feature Elimination, LASSO).
        \end{itemize}
    \end{block}
    
    \begin{block}{Formulas}
        \begin{equation}
            X' = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
        \end{equation}
        \begin{equation}
            X' = \frac{X - \mu}{\sigma}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques - Introduction}
    \begin{block}{Introduction to Normalization}
        \begin{itemize}
            \item \textbf{Definition:} Normalization is the process of scaling individual data points in a dataset to bring them into a common scale without distorting differences in the ranges of values.
            \item \textbf{Importance in Data Preprocessing:}
            \begin{itemize}
                \item Ensures that each feature contributes equally to distance calculations in algorithms such as K-Means clustering and K-Nearest Neighbors.
                \item Helps improve model convergence when training machine learning models.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques - Methods}
    \begin{block}{Common Normalization Techniques}
        \begin{enumerate}
            \item \textbf{Min-Max Scaling:}
                \begin{itemize}
                    \item \textbf{Formula:} 
                    \[
                    X' = \frac{X - X_{min}}{X_{max} - X_{min}}
                    \]
                    \item \textbf{Explanation:} Transforms features into a range between 0 and 1.
                    \item \textbf{Example:} If a feature's values range from 50 to 100:
                    \begin{itemize}
                        \item Value = 75,
                        \item Normalized value: 
                        \[
                        \frac{75 - 50}{100 - 50} = \frac{25}{50} = 0.5
                        \]
                    \end{itemize}
                \end{itemize}
                
            \item \textbf{Z-Score Normalization (Standardization):}
                \begin{itemize}
                    \item \textbf{Formula:}
                    \[
                    Z = \frac{X - \mu}{\sigma}
                    \]
                    where \( \mu \) is the mean and \( \sigma \) is the standard deviation.
                    \item \textbf{Explanation:} Centers the data around 0 with a standard deviation of 1.
                    \item \textbf{Example:} For a feature with a mean of 70 and standard deviation of 10:
                    \begin{itemize}
                        \item Value = 75,
                        \item Z-score: 
                        \[
                        Z = \frac{75 - 70}{10} = 0.5
                        \]
                    \end{itemize}
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques - Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Choosing the right normalization technique is crucial based on the data distribution and the algorithm being used.
            \item Min-max scaling preserves zeroes and outliers, while z-score normalization is useful when data follows a Gaussian distribution.
            \item It’s common to visualize the changes in data distribution before and after normalization to understand its impact better.
        \end{itemize}
    \end{block}

    \begin{block}{Code Snippet Example (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import numpy as np

# Sample data
data = np.array([[50], [75], [100]])

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
data_min_max = min_max_scaler.fit_transform(data)

# Z-Score Normalization
standard_scaler = StandardScaler()
data_standardized = standard_scaler.fit_transform(data)

print("Min-Max Scaled:\n", data_min_max)
print("Z-Score Normalized:\n", data_standardized)
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Conclusion}
        Normalization is a critical step in data preprocessing that can significantly affect the performance of machine learning models. Understanding and applying min-max scaling and z-score normalization will prepare you for more advanced techniques in data analysis and machine learning.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Transformation Techniques - Overview}
    \begin{itemize}
        \item Transformation methods are essential in data preprocessing.
        \item They help:
        \begin{itemize}
            \item Stabilize variance
            \item Normalize data distribution
            \end{itemize}
        \item Common techniques include:
        \begin{itemize}
            \item Log Transformation
            \item Square Root Transformation
            \item Box-Cox Transformation
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Log Transformation}
    \begin{block}{Definition}
        Log transformation utilizes the logarithm of data values.
    \end{block}
    
    \begin{block}{Formula}
    \begin{equation}
        Y' = \log(Y)
    \end{equation}
    \end{block}
    
    \begin{itemize}
        \item \textbf{Use Cases:}
        \begin{itemize}
            \item Reduces impact of large outliers
            \item Makes data more normally distributed
        \end{itemize}
        \item \textbf{Example:} For income levels [10, 20, 30, 100, 150, 300]:
        \begin{itemize}
            \item Transformed values: \( \log(10), \log(20), \log(30), \log(100), \log(150), \log(300) \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformation Techniques - Square Root \& Box-Cox}
    \begin{block}{Square Root Transformation}
        \begin{itemize}
            \item \textbf{Definition:} Taking square root of data values, suitable for moderate skewness.
            \item \textbf{Formula:}
            \begin{equation}
                Y' = \sqrt{Y}
            \end{equation}
            \item \textbf{Example:} For counts [1, 4, 9, 16, 25]:
            \begin{itemize}
                \item Transformed values: \( \sqrt{1}, \sqrt{4}, \sqrt{9}, \sqrt{16}, \sqrt{25} \)
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Box-Cox Transformation}
        \begin{itemize}
            \item \textbf{Definition:} A family of power transformations suitable for both positive and zero values, introducing parameter \( \lambda \).
            \item \textbf{Formula:}
            \begin{equation}
            Y' = 
            \begin{cases} 
            \frac{Y^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0 \\
            \log(Y) & \text{if } \lambda = 0 
            \end{cases}
            \end{equation}
            \item \textbf{Use Cases:} Transforming non-normally distributed data to approximately normal distribution.
            \item \textbf{Example:} For data [1, 2, 3, 4, 5] with \( \lambda = 0.5 \):
            \begin{itemize}
                \item Transformed values: \( \frac{1^{0.5} - 1}{0.5}, \frac{2^{0.5} - 1}{0.5}, \ldots \)
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Code Snippet}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Transformation techniques help achieve normality and stabilize variance.
            \item Choosing the method depends on the data distribution:
            \begin{itemize}
                \item Log for right-skewed data
                \item Square root for count data
                \item Box-Cox for flexibility based on data characteristics
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{lstlisting}[language=Python, caption={Example Code for Transformations}]
import numpy as np
import pandas as pd
from scipy import stats

data = pd.Series([10, 20, 30, 100, 150, 300])

# Log Transformation
log_transformed = np.log(data)

# Square Root Transformation
sqrt_transformed = np.sqrt(data)

# Box-Cox Transformation (requires that data is positive)
boxcox_transformed, lambda_value = stats.boxcox(data[data > 0])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Overview}
    
    \begin{block}{Definition of Feature Selection}
        Feature selection is the process of identifying and selecting a subset of relevant features (variables, predictors) that contribute most significantly to the predictive power of a model. This is crucial for enhancing model explainability and performance.
    \end{block}
    
    \begin{block}{Significance of Feature Selection}
        \begin{itemize}
            \item Improves model performance by reducing complexity and enhancing accuracy.
            \item Reduces overfitting by promoting generalization and reducing variance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Feature Selection}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Essential for model accuracy and interpretability.
            \item Mitigates the curse of dimensionality, improving performance.
            \item Leads to better computational efficiency and reduced training time.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Consider a dataset with 100 features to predict housing prices. Using only 10 highly correlated features can simplify the model and enhance predictive performance by reducing overfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques and Implementation of Feature Selection}

    \begin{block}{Basic Techniques}
        \begin{itemize}
            \item \textbf{Filter Methods:} Assess relevance independently from the model.
            \item \textbf{Wrapper Methods:} Evaluate performance using different subsets of features.
            \item \textbf{Embedded Methods:} Perform selection as part of model training.
        \end{itemize}
    \end{block}
    
    \begin{block}{Practical Application}
        \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest, f_classif

# Load data
data = load_iris()
X, y = data.data, data.target

# Select top 2 features
selector = SelectKBest(score_func=f_classif, k=2)
X_selected = selector.fit_transform(X, y)
print(X_selected)
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Conclusion}
        Feature selection is vital for robust machine learning models. Selecting the right features enhances accuracy and simplifies interpretability, enabling better insights from data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Overview}
    \begin{block}{Overview of Filter Methods for Feature Selection}
        Filter methods are a statistical approach for feature selection in machine learning, evaluating the relevance of features based on their statistical properties independently of algorithmic influence.
    \end{block}

    \begin{itemize}
        \item Independence from Learning Algorithm
        \item Speed and Simplicity
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Filter Methods}
    \begin{enumerate}
        \item Variance Thresholding
            \begin{itemize}
                \item Removes features with low variance.
                \item Features with little variation provide limited information.
                \item Formula: 
                \begin{equation}
                \text{Variance}(X) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2
                \end{equation}
            \end{itemize}

        \item Correlation Coefficient
            \begin{itemize}
                \item Measures linear relationship between features and target variable.
                \item Formula:
                \begin{equation}
                r = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Examples of Filter Methods}
    \begin{block}{Variance Thresholding Example}
        \begin{itemize}
            \item Features with variances: Feature A: 0.1, Feature B: 0.5, Feature C: 0.02.
            \item Using a threshold of 0.05, Feature C is removed.
        \end{itemize}
    \end{block}

    \begin{block}{Correlation Coefficient Example}
        \begin{itemize}
            \item Features with coefficients: Feature D: 0.8, Feature E: -0.1, Feature F: 0.05.
            \item Using a threshold of 0.1, Features E and F might be removed.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation in Python}
    \begin{lstlisting}[language=Python]
from sklearn.feature_selection import VarianceThreshold
import pandas as pd

# Assuming df is your DataFrame and 'target' is your target variable
# Variance Thresholding
selector = VarianceThreshold(threshold=0.05)
filtered_data = selector.fit_transform(df.drop(columns=['target']))

# Correlation Coefficient
correlations = df.corr()['target'].abs()
selected_features = correlations[correlations > 0.1].index.tolist() # Features selected
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrapper Methods}
    \begin{block}{What are Wrapper Methods?}
        Wrapper methods are a subset of feature selection techniques that evaluate the performance of a model using a specified feature subset. 
        \begin{itemize}
            \item They depend on the specific machine learning algorithm.
            \item They can be more accurate in identifying the best features for a particular model compared to filter methods.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Do Wrapper Methods Work?}
    \begin{enumerate}
        \item \textbf{Define the model:} Choose a machine learning algorithm to evaluate.
        \item \textbf{Feature subset selection:} Start with a set of features and select a subset to evaluate.
        \item \textbf{Model training and evaluation:} Train the model using the selected features and assess performance (e.g., accuracy, F1 score).
        \item \textbf{Iterative process:} Employ techniques like:
        \begin{itemize}
            \item Adding features (forward selection)
            \item Removing features (backward elimination)
            \item Recursive Feature Elimination (RFE)
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recursive Feature Elimination (RFE)}
    \begin{block}{Process of RFE}
        \begin{enumerate}
            \item Train the model on the full feature set.
            \item Rank features based on their importance.
            \item Remove the least important feature(s).
            \item Repeat until a specified number of features remain or performance stabilizes.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of RFE with Python}
    \begin{lstlisting}[language=Python]
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# Sample data
X, y = load_data()  # Load your feature matrix and target variable

# Initialize the model
model = RandomForestClassifier()

# Initialize RFE
rfe = RFE(estimator=model, n_features_to_select=5)

# Fit RFE
X_rfe = rfe.fit_transform(X, y)

selected_features = X.columns[rfe.support_]
print("Selected features:", selected_features)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Wrapper Methods}
    \begin{itemize}
        \item Model-specific feature selection customizes feature subsets to the chosen algorithm.
        \item Higher accuracy by evaluating combinations of features compared to filter methods.
    \end{itemize}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Wrapper methods are computationally expensive due to multiple model trainings.
            \item Best when the number of features is relatively small.
            \item Effective where feature dependencies are significant.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Wrapper methods, particularly Recursive Feature Elimination (RFE), offer powerful, model-specific feature selection capabilities that lead to enhanced model performance. A systematic evaluation of feature importance based on model results improves accuracy and efficiency in machine learning workflows.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Embedded Methods}
    \begin{block}{Introduction}
        Embedded methods integrate feature selection into the model training phase, balancing efficiency and model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Embedded Methods}
    \begin{itemize}
        \item \textbf{Efficiency:} More computationally efficient than wrapper methods due to reduced model training.
        \item \textbf{Model-Specific:} Tailored feature selection based on model characteristics.
        \item \textbf{Regularization:} Utilizes techniques to prevent overfitting and influence feature selection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Embedded Methods}
    \begin{enumerate}
        \item \textbf{LASSO (Least Absolute Shrinkage and Selection Operator)}
            \begin{itemize}
                \item \textbf{Concept:} Adds L1 regularization, shrinking some coefficients to zero.
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{Loss} = \text{RSS} + \lambda \sum_{j=1}^{n} |\beta_j|
                \end{equation}
                \item \textbf{Example:} Reduces irrelevant features in high-dimensional datasets.
            \end{itemize}
            
        \item \textbf{Tree-Based Methods (e.g., Decision Trees, Random Forests)}
            \begin{itemize}
                \item \textbf{Concept:} Utilizes hierarchical splits based on feature importance.
                \item \textbf{Importance Calculation:} Based on Gini impurity or Information Gain.
                \item \textbf{Example:} Identifies key features based on their contribution to prediction accuracy.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Embedded Methods?}
    \begin{itemize}
        \item \textbf{Balance:} Combines predictive performance with model interpretability.
        \item \textbf{Reduced Overfitting:} Regularization aids in generalizing models to unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code with Scikit-learn}
    \begin{lstlisting}[language=Python]
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X, y = data.data, data.target

# LASSO example
lasso = Lasso(alpha=0.1)
lasso.fit(X, y)
print("LASSO Coefficients:", lasso.coef_)

# Random Forest example
rf = RandomForestClassifier()
rf.fit(X, y)
print("Feature Importances:", rf.feature_importances_)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Embedded methods like LASSO and tree-based techniques enhance model performance and efficiency in feature selection, leading to cleaner and more interpretable models in data science practice.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Data Preprocessing Techniques}
    \begin{block}{Overview}
        Data preprocessing is a critical step in the data analysis pipeline. It involves transforming raw data into a clean dataset suitable for analysis, model building, or machine learning. This presentation focuses on:
        \begin{itemize}
            \item Handling missing values
            \item Outlier treatment
            \item Data encoding
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values}
    Missing data can lead to biased results and affect the performance of machine learning models. Here are several techniques to handle missing values:
    \begin{itemize}
        \item \textbf{Deletion}: Remove records with missing values.
        \item \textbf{Imputation}: Replace missing values with substituted values.
            \begin{itemize}
                \item Mean/Median Imputation:
                \begin{lstlisting}
import pandas as pd
df['column_name'].fillna(df['column_name'].mean(), inplace=True)
                \end{lstlisting}
                \item Mode Imputation: Useful for categorical data.
                \item Predictive Model Imputation: Algorithms can predict missing values.
            \end{itemize}
    \end{itemize}
    \begin{block}{Key Point}
        Always analyze the extent of missing data before deciding how to handle it; sometimes, the nature of the data can guide your decision.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Outlier Treatment}
    Outliers can skew your data and impact models significantly. Identify and decide how to treat them:
    \begin{itemize}
        \item \textbf{Identification}: Use Z-score or IQR to find outliers.
        \item \textbf{Treatment Options}:
            \begin{itemize}
                \item \textbf{Removal}: Remove outliers if identified as errors.
                \item \textbf{Transformation}: Logarithmic transformation to lessen impacts.
                \begin{lstlisting}
import numpy as np
df['column_name'] = np.log1p(df['column_name'])
                \end{lstlisting}
            \end{itemize}
    \end{itemize}
    \begin{block}{Key Point}
        Evaluate the impact of outliers on your dataset before making decisions; some outliers may carry important information.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Encoding}
    Machine learning algorithms require numerical input; categorical data must be converted into numerical format:
    \begin{itemize}
        \item \textbf{Label Encoding}: Convert categorical labels to integers.
        \item \textbf{One-Hot Encoding}: Binary columns for each category.
            \begin{lstlisting}
df = pd.get_dummies(df, columns=['column_name'], drop_first=True)
            \end{lstlisting}
    \end{itemize}
    \begin{block}{Key Point}
        Choose the appropriate encoding method based on the algorithm and nature of your categorical data.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Data preprocessing is vital for preparing datasets for analysis and model training. By handling missing values effectively, managing outliers, and encoding categorical variables, you enhance the quality of your data, leading to more accurate and reliable predictions.
\end{frame}

\begin{frame}
    \frametitle{Next Slide Preview}
    Implementing Data Preprocessing: Practical techniques using Python libraries like pandas and Scikit-learn.
\end{frame}

\begin{frame}
    \frametitle{Implementing Data Preprocessing - Overview}
    \begin{block}{Overview}
        Data preprocessing is a crucial step in the data analysis pipeline, involving the transformation of raw data into a clean and organized format. 
        This process ensures that the data is suitable for analysis and model training, improving the quality of results.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Data Preprocessing - Key Concepts}
    \begin{enumerate}
        \item \textbf{Handling Missing Values}
        \begin{itemize}
            \item \textit{Importance:} Addressing missing data is essential as it can skew results and impact model accuracy.
            \item \textit{Techniques:}
            \begin{itemize}
                \item Imputation: Replace missing values with statistical measures (mean, median, mode).
                \item Dropping Rows/Columns: Eliminate data points or features if missing data is significant.
            \end{itemize}
        \end{itemize}

        \item \textbf{Outlier Treatment}
        \begin{itemize}
            \item \textit{Importance:} Outliers can distort statistical analyses and model performance.
            \item \textit{Techniques:}
            \begin{itemize}
                \item Z-Score Method
                \item IQR Method
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Data Preprocessing - Code Examples}
    \begin{block}{Handling Missing Values Example}
    \begin{lstlisting}[language=Python]
    import pandas as pd
    
    # Load dataset
    df = pd.read_csv('data.csv')

    # Impute missing values with mean
    df.fillna(df.mean(), inplace=True)
    \end{lstlisting}
    \end{block}

    \begin{block}{Outlier Treatment Example}
    \begin{lstlisting}[language=Python]
    # Removing outliers using IQR
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Data Preprocessing - Additional Concepts}
    \begin{enumerate}
        \item \textbf{Data Encoding}
        \begin{itemize}
            \item \textit{Importance:} Convert categorical variables into numerical format for analysis.
            \item \textit{Techniques:}
            \begin{itemize}
                \item Label Encoding
                \item One-Hot Encoding
            \end{itemize}
        \end{itemize}

        \item \textbf{Feature Scaling}
        \begin{itemize}
            \item \textit{Importance:} Ensures that all features contribute equally to distance calculations.
            \item \textit{Techniques:}
            \begin{itemize}
                \item Standardization
                \item Normalization
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Data Preprocessing - Additional Code Examples}
    \begin{block}{Data Encoding Example}
    \begin{lstlisting}[language=Python]
    from sklearn.preprocessing import OneHotEncoder

    # One-hot encoding
    encoder = OneHotEncoder(sparse=False)
    encoded_data = encoder.fit_transform(df[['categorical_column']])
    \end{lstlisting}
    \end{block}

    \begin{block}{Feature Scaling Example}
    \begin{lstlisting}[language=Python]
    from sklearn.preprocessing import StandardScaler

    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(df[['feature1', 'feature2']])
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Data Preprocessing - Key Points}
    \begin{itemize}
        \item \textbf{Data Integrity is Critical:} Proper preprocessing is essential for meaningful analyses and model building.
        \item \textbf{Choose Techniques Wisely:} Depending on the dataset’s characteristics and analysis goals, determine the best preprocessing methods.
        \item \textbf{Automation and Reproducibility:} Automated preprocessing pipelines help maintain consistency across datasets and analyses.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Extraction Techniques - Introduction}
    \begin{block}{Introduction to Feature Extraction}
        Feature extraction is a crucial process in machine learning and data analysis. It transforms raw data into a format that is easier to interpret and analyze. The goal is to reduce the dataset size while preserving essential information for modeling.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Extraction Techniques - Key Techniques}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA)}
            \begin{itemize}
                \item \textbf{Overview}: A statistical technique for dimensionality reduction, transforming datasets into a new coordinate system that captures the most variance.
                \item \textbf{Mathematical Basis}:
                    \begin{equation}
                        C = \frac{1}{n-1} (X - \mu)^T (X - \mu)
                    \end{equation}
                    where \(X\) is the data matrix and \(\mu\) is the mean of the data.
                \item \textbf{Use Case}: Reducing dimensionality of image data while preserving essential features.
            \end{itemize}
        
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
            \begin{itemize}
                \item \textbf{Overview}: A non-linear dimensionality reduction technique for visualizing high-dimensional data, preserving local structures. 
                \item \textbf{Use Case}: Effective for visualizing clusters such as in word embeddings or image recognition datasets.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Extraction Techniques - Examples and Code}
    \begin{block}{Example - PCA}
        Given a dataset with features \((x_1, x_2)\), PCA transforms it into \((PC_1, PC_2)\) where \(PC_1\) may capture 90\% of the variance, allowing us to retain only \(PC_1\) for analysis.
    \end{block}

    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(data)
    \end{lstlisting}

    \begin{block}{Example - t-SNE}
        t-SNE visualizes how similar images group together in a 2D space, helping reveal inherent clusters.
    \end{block}

    \begin{lstlisting}[language=Python]
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2)
reduced_data = tsne.fit_transform(data)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction with PCA - Overview}
    
    \begin{block}{What is PCA?}
        Principal Component Analysis (PCA) is a statistical technique for reducing the dimensionality of datasets. It aims to transform high-dimensional data into a lower-dimensional form while preserving variance.
    \end{block}
    
    \begin{itemize}
        \item Simplifies models
        \item Reduces storage costs
        \item Improves visualization
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA - Mathematical Basis}
    
    \begin{enumerate}
        \item \textbf{Data Centering:} Subtract the mean of each variable.
        \begin{equation}
            X_{centered} = X - \text{mean}(X)
        \end{equation}
        
        \item \textbf{Covariance Matrix:} Determine how variables co-vary.
        \begin{equation}
            Cov(X) = \frac{1}{n-1} X_{centered}^T X_{centered}
        \end{equation}
        
        \item \textbf{Eigenvalues and Eigenvectors:} Compute to find principal component directions.
        \begin{equation}
            Cov(X) v = \lambda v 
        \end{equation}
        
        \item \textbf{Selecting Principal Components:} Choose the top \(k\) eigenvectors.
        
        \item \textbf{Transforming the Data:} Project onto new basis.
        \begin{equation}
            X_{reduced} = X_{centered} \cdot V_k 
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Key Points}
    
    \begin{block}{Example - PCA Application}
        - Data Points: (2,3), (3,3), (6,8), (8,8)\\
        - After PCA, project onto principal component capturing 95\% variance.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Dimensionality Reduction:} Helps mitigate curse of dimensionality.
        \item \textbf{Variance Preservation:} Retains as much variance as possible.
        \item \textbf{Feature Extraction:} New features for analysis and modeling.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet - PCA in Python}
    
    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
import numpy as np

# Sample data
X = np.array([[2, 3], [3, 3], [6, 8], [8, 8]])

# Initialize PCA
pca = PCA(n_components=1)

# Fit and transform data
X_reduced = pca.fit_transform(X)

print("Reduced Data:\n", X_reduced)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    PCA is a powerful tool that reduces complexity while retaining essential information. Its mathematical foundation and applications enable effective use in high-dimensional data contexts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{t-SNE for Visualization and Feature Extraction}
    \begin{block}{Overview of t-SNE}
        t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique particularly well-suited for visualizing high-dimensional data in a low-dimensional space (usually 2D or 3D).
    \end{block}
    \begin{itemize}
        \item \textbf{Why t-SNE?} 
        In high-dimensional spaces, data points are often complex and hard to interpret. t-SNE helps in revealing the natural clusters and relationships in the data, allowing for intuitive understanding and exploration.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of t-SNE}
    \begin{itemize}
        \item \textbf{Neighborhood Preservation:} 
        t-SNE converts pairwise Euclidean distances between high-dimensional points into conditional probabilities, keeping similar points close together in the lower-dimensional embedding.
        
        \item \textbf{Cost Function:} 
        t-SNE minimizes the divergence between two probability distributions using the Kullback-Leibler divergence:
    \end{itemize}
    \begin{equation}
        C = KL(P \| Q) = \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right)
    \end{equation}
    Where:
    \begin{itemize}
        \item \(P\) is the conditional probability distribution of the high-dimensional data.
        \item \(Q\) is the conditional probability distribution of the low-dimensional projection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Key Points}
    \begin{itemize}
        \item \textbf{Applications:}
        \begin{itemize}
            \item \textbf{Data Visualization:} 
            Used for visualizing images, text, and genomic data. Example: Visualizing MNIST digit images shows similar digits clustering together.
            \item \textbf{Feature Extraction:} 
            Helps in extracting important features for classification tasks. Example: In recommendation systems, t-SNE extracts latent user preferences.
        \end{itemize}

        \item \textbf{Key Points to Emphasize:}
        \begin{itemize}
            \item \textbf{Nonlinear Embedding:} Captures complex, non-linear structures unlike PCA.
            \item \textbf{Scalability:} Can struggle with very large datasets; optimizations like Barnes-Hut t-SNE improve efficiency.
            \item \textbf{Parameter Sensitivity:} Performance can be sensitive to hyperparameters (e.g., perplexity).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies: Data Visualization \& Feature Extraction}
    \begin{block}{Introduction}
        Data visualization and feature extraction are critical processes that enable industries to make sense of complex data and derive actionable insights. This slide examines real-world applications where these techniques have proven effective.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Healthcare: Disease Prediction Models}
    \begin{itemize}
        \item \textbf{Context:} Healthcare relies heavily on data to make informed decisions for patient care.
        \item \textbf{Example:}
        \begin{itemize}
            \item \textbf{Feature Extraction:} Patient data (age, BMI, blood pressure) is used for predicting diseases like diabetes.
            \item \textbf{Visualization:} t-SNE visualizes patterns in patient data, showing clustering based on risk factors.
        \end{itemize}
        \item \textbf{Key Point:} Effective visualization can directly improve diagnostic processes and tailor patient treatment plans.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Finance: Fraud Detection}
    \begin{itemize}
        \item \textbf{Context:} The banking sector faces challenges from fraudulent activities.
        \item \textbf{Example:}
        \begin{itemize}
            \item \textbf{Feature Extraction:} Extract features such as transaction amounts, locations, and timestamps from historical data.
            \item \textbf{Visualization:} Heatmaps and graph visualizations show transaction anomalies, aiding in real-time fraud detection.
        \end{itemize}
        \item \textbf{Key Point:} Visualizations help quickly identify trends and anomalies, enhancing security measures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Retail: Customer Behavior Analysis}
    \begin{itemize}
        \item \textbf{Context:} Retailers utilize data to tailor marketing strategies based on customer preferences.
        \item \textbf{Example:}
        \begin{itemize}
            \item \textbf{Feature Extraction:} Retail datasets include features like purchase history, demographics, and web behaviors.
            \item \textbf{Visualization:} Clustering techniques (e.g., k-means) visualized with scatter plots segment customers for tailored advertisements.
        \end{itemize}
        \item \textbf{Key Point:} Data visualization of customer segments drives targeted marketing and improves customer engagement.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Transportation: Route Optimization}
    \begin{itemize}
        \item \textbf{Context:} Transportation companies require efficient route management to reduce costs.
        \item \textbf{Example:}
        \begin{itemize}
            \item \textbf{Feature Extraction:} Traffic data including time of day, weather conditions, and historical patterns is used.
            \item \textbf{Visualization:} Geographic Information Systems (GIS) display traffic conditions, optimizing delivery routes.
        \end{itemize}
        \item \textbf{Key Point:} Visualization aids in decision-making for logistics, resulting in cost and time savings.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion \& Next Steps}
    \begin{block}{Conclusion}
        Utilizing effective data visualization and feature extraction is pivotal across various industries. By translating complex data into understandable insights, organizations can enhance decision-making processes and drive success.
    \end{block}
    \begin{block}{Reminder for Practical Exercise}
        Now that we've explored real-world applications, we will move on to a practical exercise in data preprocessing and feature extraction using a sample dataset.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Exercise}
    \begin{block}{Description}
        In this interactive exercise, you'll implement data preprocessing and feature extraction on a sample dataset. This hands-on experience will solidify your understanding of crucial data manipulation techniques that enhance model performance in real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}

    \textbf{1. Data Preprocessing}
    \begin{itemize}
        \item \textbf{Handling Missing Values}: Identify and manage missing data through techniques like imputation or removal.
        \item \textbf{Normalization/Standardization}: Adjusting values in the dataset to a common scale.
    \end{itemize}

    \textbf{2. Feature Extraction}
    \begin{itemize}
        \item \textbf{Dimensionality Reduction}: Techniques like PCA to reduce the number of input variables.
        \item \textbf{Creating New Features}: Combining existing features to improve model learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing and Feature Extraction Steps}
    
    \textbf{Hands-On Exercise Steps}
    \begin{enumerate}
        \item Load Dataset: Use a sample dataset (e.g., Titanic dataset).
        \item Handle Missing Values: Apply imputation methods.
        \item Standardize Features: Normalize or standardize features.
        \item Create New Features: Generate meaningful features.
        \item Visualize Your Process: Create plots to showcase changes.
    \end{enumerate}
    
    \begin{block}{Conclusion}
        Through this practical exercise, you will learn how to effectively preprocess your data and extract important features, paving the way for successful data visualization and machine learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code for Data Preprocessing}
    
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv('titanic.csv')

# Handle missing values
df['Age'].fillna(df['Age'].mean(), inplace=True)

# Standardizing 'Fare' feature
scaler = StandardScaler()
df['Fare_scaled'] = scaler.fit_transform(df[['Fare']])

# Create a new feature
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    \end{lstlisting}
    
    Engage actively with this exercise, and ask questions as needed!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Handling - Introduction}
    \begin{itemize}
        \item Importance of addressing ethical considerations in data preprocessing and feature extraction.
        \item Data ethics focuses on responsible use and handling of data:
            \begin{itemize}
                \item Fairness
                \item Transparency
                \item Accountability
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Concepts}
    \begin{enumerate}
        \item \textbf{Informed Consent}:
            \begin{itemize}
                \item Collect data with individuals' understanding and agreement.
                \item \textit{Example}: Participants in health studies should know if data may be shared.
            \end{itemize}
            
        \item \textbf{Data Privacy}:
            \begin{itemize}
                \item Implement measures to protect personal information.
                \item \textit{Example}: Anonymizing data before analysis.
            \end{itemize}
        
        \item \textbf{Fairness}:
            \begin{itemize}
                \item Ensure no group is unfairly disadvantaged or advantaged.
                \item \textit{Example}: Biased historical data can reinforce stereotypes in models.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Bias Mitigation & Practical Steps}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Bias Mitigation}:
            \begin{itemize}
                \item Identify and minimize bias in datasets.
                \item \textit{Example}: Facial recognition trained on biased ethnic data may underperform on others.
            \end{itemize}
            
        \item \textbf{Transparency and Accountability}:
            \begin{itemize}
                \item Maintain clarity on data handling processes.
                \item \textit{Example}: Document data sources and preprocessing steps.
            \end{itemize}
    \end{enumerate}

    \textbf{Practical Steps for Ethical Data Handling}:
    \begin{itemize}
        \item Conduct a data ethics review for each project.
        \item Employ diversity in datasets for balanced representation.
        \item Use fairness-aware machine learning algorithms to manage bias.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Review - Key Topics}
    \begin{block}{Key Topics Covered in Chapter 3}
        \begin{enumerate}
            \item Data Visualization
            \item Feature Extraction
            \item Ethical Considerations in Data Handling
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Visualization}
    \begin{itemize}
        \item \textbf{Definition}: The process of representing data in graphical formats to facilitate understanding and insight extraction.
        \item \textbf{Importance}:
            \begin{itemize}
                \item Makes complex data sets more accessible.
                \item Aids in identifying trends, patterns, and outliers.
            \end{itemize}
        \item \textbf{Examples}:
            \begin{itemize}
                \item Histograms: Useful for visualizing the distribution of a single variable.
                \item Scatter Plots: Effective in showing relationships between two variables.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Extraction and Ethical Considerations}
    \begin{itemize}
        \item \textbf{Feature Extraction}:
            \begin{itemize}
                \item \textbf{Definition}: The process of transforming data into features that can be effectively used in modeling.
                \item \textbf{Importance}:
                    \begin{itemize}
                        \item Enables better performance in machine learning algorithms.
                        \item Reduces dimensionality, helping mitigate the "curse of dimensionality."
                    \end{itemize}
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item Text Data: Converting raw text into numerical features using techniques like TF-IDF.
                        \item Image Data: Using edge detection techniques to create features that represent image content.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
    
    \begin{itemize}
        \item \textbf{Ethical Considerations}:
            \begin{itemize}
                \item Acknowledging the need for fairness in data processing and feature selection.
                \item Understanding biases that can arise during feature extraction and ensuring that visualizations represent data truthfully.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Closing Thoughts}
    \begin{itemize}
        \item \textbf{Effective Communication}: Data visualization acts as a bridge between data analysis and understanding, helping convey findings effectively.
        \item \textbf{Focus on Relevance}: Selecting the right features is crucial for building accurate and interpretable predictive models.
    \end{itemize}
    
    \begin{block}{Closing Thoughts}
        Mastering data visualization techniques and feature extraction strategies is fundamental to successful data mining projects. Together, they empower data scientists to uncover insights and drive informed decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Feature Extraction}
    \begin{lstlisting}[language=Python]
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample text data
documents = ["I love data science.", "Data science is amazing.", "I enjoy exploring data."]

# Create the TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Transform the documents into TF-IDF features
tfidf_matrix = vectorizer.fit_transform(documents)

# Display the features
print(vectorizer.get_feature_names_out())
print(tfidf_matrix.toarray())
    \end{lstlisting}
\end{frame}


\end{document}