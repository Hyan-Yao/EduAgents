\frametitle{3. Deep Q-Networks (DQN)}

    \begin{block}{Concept}
        DQN combines Q-learning with deep learning, allowing the agent to handle high-dimensional action spaces through neural networks.
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Utilizes a neural network to approximate the Q-function.
            \item Introduces techniques like experience replay and target networks to stabilize training.
        \end{itemize}
    \end{block}

    \begin{block}{Architecture}
        \begin{itemize}
            \item Input: Current state (e.g., a game frame).
            \item Output: Q-values for all possible actions.
        \end{itemize}
    \end{block}

    \begin{block}{Example (Pseudocode)}
        \begin{lstlisting}[language=Python]
initialize replay_memory
initialize Q_network
for episode in range(num_episodes):
    state = reset_environment()
    while not done:
        action = select_action(state)  # Îµ-greedy policy
        next_state, reward, done = take_action(action)
        store_experience(state, action, reward, next_state, done)
        if replay_memory is large_enough:
            sample = replay_memory.sample()
            train_Q_network(sample)
        state = next_state
        \end{lstlisting}
    \end{block}
