\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 4: Supervised Learning]{Week 4: Supervised Learning - Logistic Regression}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning}
    \begin{block}{What is Supervised Learning?}
        Supervised learning is a type of machine learning where an algorithm learns from labeled training data to make predictions on unseen data. Each training example consists of an input-output pair, where the input is the feature set and the output is the label.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics}
    \begin{itemize}
        \item \textbf{Labeled Data:} Each training sample includes known outputs (labels).
        \item \textbf{Predictive Modeling:} The goal is to create a model that can predict the output for new, unseen data based on the learned relationships.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Supervised Learning in Data Mining}
    \begin{enumerate}
        \item \textbf{Decision Making:} Aids in informed decision-making by predicting outcomes based on historical data.
        \begin{itemize}
            \item Example: Predicting whether a customer will buy a product based on age, gender, and previous purchasing behavior.
        \end{itemize}
        \item \textbf{Classification and Regression:} Solves both classification problems (e.g., spam detection) and regression problems (e.g., sales forecasting).
        \item \textbf{Versatility:} Wide range of applications in finance (credit scoring), healthcare (disease diagnosis), and marketing (customer segmentation).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types and Algorithms of Supervised Learning}
    \begin{itemize}
        \item \textbf{Types:}
        \begin{itemize}
            \item \textbf{Classification:} Predicting discrete labels (e.g., spam or not spam).
            \item \textbf{Regression:} Predicting continuous values (e.g., house prices).
        \end{itemize}
        
        \item \textbf{Common Algorithms:}
        \begin{itemize}
            \item Logistic Regression
            \item Decision Trees
            \item Support Vector Machines (SVM)
            \item Random Forests
            \item Neural Networks
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    \begin{block}{Scenario}
        Consider a bank that wants to forecast whether a loan applicant will default on a loan. The past data includes various applicants' features (income, credit history, etc.) and whether they defaulted or not (the label). A supervised learning algorithm can learn from this dataset to predict the likelihood of default for future applicants.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding supervised learning is fundamental to harnessing the potential of data mining for predictive analytics. It equips data scientists with methodologies to turn historical data into actionable insights.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Simple Example with Python}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Sample Data
X = [[1, 2], [2, 3], [3, 4], [4, 5]]  # Features
y = [0, 0, 1, 1]  # Labels

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Model Training
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
predictions = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, predictions))
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Logistic Regression? - Definition}
    \begin{block}{Definition}
        \textbf{Logistic Regression} is a statistical method used for binary classification tasks, where the output or target variable can take one of two possible values or classes (e.g., 0 and 1, Yes and No, True and False).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Logistic Regression? - Purpose and Key Concepts}
    \begin{block}{Purpose}
        The primary purpose of logistic regression is to model the probability that a given input point belongs to a particular category.
    \end{block}

    \begin{itemize}
        \item \textbf{Binary Classification}:
        \begin{itemize}
            \item Logistic regression is suitable for scenarios where the outcome is binary. Example: Predicting whether an email is ``spam'' (1) or ``not spam'' (0).
        \end{itemize}
        
        \item \textbf{Odds and Probability}:
        \begin{itemize}
            \item Logistic regression predicts outcomes using the concept of odds. The odds represent the ratio of the probability of an event occurring to the probability of it not occurring.
            \item Probability (P) of an event occurring is transformed with the logistic function to ensure outputs are between 0 and 1.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Logistic Regression? - Logistic Function and Example}
    \begin{block}{Logistic Function}
        The logistic function (also known as the sigmoid function) is used to model the relationship between one or more independent variables and the binary response variable:

        \begin{equation}
        P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n)}}
        \end{equation}

        Here, $P(Y=1|X)$ is the predicted probability of the outcome, $X$ represents the independent variables, and $\beta_0, \beta_1, \ldots, \beta_n$ are coefficients that the model tries to learn.
    \end{block}

    \begin{block}{Example}
        Consider a healthcare scenario to predict if a patient has a disease (1) or not (0):
        \begin{itemize}
            \item \textbf{Input Variables}: Age, Blood Pressure
            \item \textbf{Outcome Variable}: Disease Status (1 = Disease, 0 = No Disease)
        \end{itemize}
        Using logistic regression, we develop a model that outputs the probability that a patient with specific age and blood pressure readings has the disease.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation of Logistic Regression - Part 1}
    \textbf{Understanding the Logistic Function}
    
    The logistic function transforms a linear combination of input features into a probability value between 0 and 1, defined as:
    
    \begin{equation}
        f(z) = \frac{1}{1 + e^{-z}}
    \end{equation}
    
    Where:
    \begin{itemize}
        \item \( z \) is the linear combination: \( z = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n \)
        \item \( e \) is the base of the natural logarithm (approximately 2.71828).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation of Logistic Regression - Part 2}
    
    \textbf{Key Properties of the Logistic Function:}
    \begin{enumerate}
        \item \textbf{S-shaped Curve}: As \( z \to -\infty \), \( f(z) \to 0 \) and as \( z \to +\infty \), \( f(z) \to 1 \).
        \item \textbf{Threshold}: A common threshold at \( f(z) = 0.5 \) classifies observations into class 0 or class 1.
    \end{enumerate}

    \textbf{Example:}
    \begin{itemize}
        \item For \( X = 0 \): \( f(0) = \frac{1}{1 + 1} = 0.5 \).
        \item As \( z \) increases (e.g., \( z = 2 \)): \( f(2) \approx 0.88 \), indicating higher probability of class 1.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation of Logistic Regression - Part 3}
    
    \textbf{Odds Ratio}
    
    The odds ratio indicates the odds of success versus failure:
    \begin{equation}
        \text{Odds} = \frac{P(Y=1)}{P(Y=0)} = \frac{f(z)}{1 - f(z)}
    \end{equation}
    
    \textbf{Key Points:}
    \begin{itemize}
        \item Odds ratio \( > 1 \): Positive association (greater odds of outcome).
        \item Odds ratio \( < 1 \): Negative association.
    \end{itemize}

    \textbf{Example:}
    \begin{itemize}
        \item Odds ratio of 3 means the event is 3 times more likely to occur than not.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation of Logistic Regression - Summary}
    
    \textbf{Summary of Key Concepts:}
    \begin{itemize}
        \item \textbf{Logistic Regression}: Utilizes the logistic function for binary outcomes.
        \item \textbf{Logistic Function}: Maps real values into the (0, 1) interval.
        \item \textbf{Odds Ratio}: Measures effect size of predictors and assists in interpretation.
    \end{itemize}

    \textbf{Next Steps:}
    In the upcoming slide, we will discuss the \textbf{Assumptions of Logistic Regression} to ensure the validity of the model's outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Logistic Regression}
    \begin{block}{Overview}
        Logistic Regression is a powerful statistical tool used for binary classification problems. To ensure the validity and reliability of the model's outputs, it is essential to adhere to certain key assumptions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Assumptions - 1}
    \begin{enumerate}
        \item \textbf{Binary Outcome Variable}
        \begin{itemize}
            \item \textbf{Explanation:} The dependent variable must be binary (0/1, True/False, Yes/No).
            \item \textbf{Example:} Predicting whether a student passes (1) or fails (0) an exam based on study hours.
        \end{itemize}

        \item \textbf{Independent Observations}
        \begin{itemize}
            \item \textbf{Explanation:} Each observation in the dataset must be independent of others.
            \item \textbf{Example:} In a study to determine if diet impacts weight loss, the weight loss of one participant should not influence the weight loss of another.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Assumptions - 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Linearity of the Logit}
        \begin{itemize}
            \item \textbf{Explanation:} There should be a linear relationship between the independent variables and the log-odds of the dependent variable.
            \item \textbf{Illustration:} 
            \begin{equation}
            \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n
            \end{equation}
        \end{itemize}

        \item \textbf{No Multicollinearity}
        \begin{itemize}
            \item \textbf{Explanation:} The independent variables should not be too highly correlated with each other.
            \item \textbf{Example:} Using both ‘height’ and ‘weight’ can be problematic if they are strongly correlated, making it difficult to determine the individual impact of each on the outcome.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Assumptions - 3}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Adequate Sample Size}
        \begin{itemize}
            \item \textbf{Explanation:} Logistic regression requires a sufficient sample size to provide reliable estimates.
            \item \textbf{Key Point:} A rule of thumb is to have at least 10 events for each predictor variable to ensure the model is stable.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Always check the assumptions before interpreting the results.
            \item Visualize data to understand relationships (scatter plots for linearity, VIF for multicollinearity).
            \item Consider transformations or variable selections if assumptions are violated.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding and validating the assumptions of logistic regression not only enhances the model's predictive power but also ensures more robust conclusions and insights drawn from the model. 
    Prepare to apply these principles when implementing logistic regression in real-world scenarios!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Logistic Regression - Overview}
    \begin{itemize}
        \item Logistic regression is a statistical method for binary classification.
        \item It predicts the probability of a data point belonging to a certain category.
        \item Utilizes a logistic function to map predictions to probabilities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Logistic Regression - Steps}
    \begin{enumerate}
        \item \textbf{Import Libraries}
            \begin{lstlisting}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
            \end{lstlisting}
        \item \textbf{Load and Prepare Dataset}
            \begin{lstlisting}
data = pd.read_csv('data.csv')  # Replace 'data.csv' with your dataset
            \end{lstlisting}
        \item \textbf{Explore the Dataset}
            \begin{itemize}
                \item Examine the first few rows with \texttt{data.head()}.
                \item Understand features and target variables.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Logistic Regression - Continued Steps}
    \begin{enumerate}[resume]
        \item \textbf{Data Preprocessing}
            \begin{lstlisting}
data['category'] = data['category'].map({'class_0': 0, 'class_1': 1})
            \end{lstlisting}
        \item \textbf{Split the Dataset}
            \begin{lstlisting}
X = data.drop('target', axis=1)  # Replace 'target' with your dependent variable
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            \end{lstlisting}
        \item \textbf{Initialize and Train the Model}
            \begin{lstlisting}
model = LogisticRegression()
model.fit(X_train, y_train)
            \end{lstlisting}
        \item \textbf{Make Predictions}
            \begin{lstlisting}
y_pred = model.predict(X_test)
            \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Logistic Regression - Evaluation}
    \begin{itemize}
        \item \textbf{Evaluate the Model}
            \begin{lstlisting}
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", cm)
            \end{lstlisting}
        \item \textbf{Key Points}
            \begin{itemize}
                \item Logistic function: \( \sigma(z) = \frac{1}{1 + e^{-z}} \)
                \item Evaluate your model using multiple metrics.
                \item Data quality is crucial for model performance.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Logistic Regression - Conclusion}
    Implementing logistic regression with Python and Scikit-learn is straightforward. Key points of focus include:
    \begin{itemize}
        \item Importance of proper data preparation.
        \item Necessity of thorough model evaluation.
        \item Reliability of predictions hinges on input data quality.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Preparation for Logistic Regression - Introduction}
  \begin{block}{Importance of Data Preprocessing}
    Data preprocessing is a critical step in building a logistic regression model. Well-prepared data enhances prediction accuracy and allows the model to learn effectively. Key components include:
  \end{block}
  \begin{itemize}
    \item Feature Scaling
    \item Encoding Categorical Variables
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Preparation for Logistic Regression - Feature Scaling}
  \begin{block}{Feature Scaling}
    Feature scaling adjusts the range of the data. Logistic regression is sensitive to the scale of input features, thus scaling is essential.
  \end{block}
  
  \begin{itemize}
    \item \textbf{Standardization}: Transforms data to have mean $0$ and standard deviation $1$.
      \begin{equation}
      z = \frac{x - \mu}{\sigma}
      \end{equation}
    \item \textbf{Normalization}: Rescales data to a range of [0, 1].
      \begin{equation}
      x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)}
      \end{equation}
  \end{itemize}

  \begin{block}{Example}
    A dataset may have features like age (0 to 100) and income (20,000 to 120,000). Without scaling, income could dominate due to its larger range.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Preparation for Logistic Regression - Encoding Categorical Variables}
  \begin{block}{Encoding Categorical Variables}
    Logistic regression requires numerical inputs, necessitating conversion of categorical variables.
  \end{block}
  
  \begin{itemize}
    \item \textbf{One-Hot Encoding}: Creates binary columns for each category. Useful for nominal variables.
      \begin{itemize}
        \item Example: "Color" = ["Red", "Green", "Blue"] results in:
        \begin{itemize}
          \item Color\_Red: [1, 0, 0]
          \item Color\_Green: [0, 1, 0]
          \item Color\_Blue: [0, 0, 1]
        \end{itemize}
      \end{itemize}
    
    \item \textbf{Label Encoding}: Assigns integers to categories. Appropriate for ordinal variables.
      \begin{itemize}
        \item Example: "Size" = ["Small", "Medium", "Large"] maps to:
        \begin{itemize}
          \item Small: 0
          \item Medium: 1
          \item Large: 2
        \end{itemize}
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Preparation for Logistic Regression - Key Points and Code Snippet}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Careful data preparation enhances model performance.
      \item Scaling ensures features are on a similar scale, reducing bias.
      \item Correct encoding is essential for interpreting feature relationships.
    \end{itemize}
  \end{block}
  
  \begin{block}{Code Snippet Example (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Data preparation pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Age', 'Income']),
        ('cat', OneHotEncoder(), ['Color'])
    ])

# Apply the transformations
X_prepared = preprocessor.fit_transform(X)
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Splitting the Dataset - Overview}
  \begin{block}{Best Practices for Dividing Data}
    In supervised learning, especially with logistic regression, it is vital to split your dataset into:
    \begin{itemize}
      \item \textbf{Training Set}: Used to fit the model.
      \item \textbf{Testing Set}: Used to evaluate model performance.
    \end{itemize}
    Proper dataset splitting is essential for model generalization to unseen data.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Splitting the Dataset - Key Concepts}
  \begin{enumerate}
    \item \textbf{Training Set}:
      \begin{itemize}
        \item Typically comprises \textbf{70-80\%} of total data.
        \item Used for training the logistic regression model.
      \end{itemize}
      
    \item \textbf{Testing Set}:
      \begin{itemize}
        \item Contains the remaining \textbf{20-30\%}.
        \item Validates the model's performance after training.
      \end{itemize}
      
    \item \textbf{Validation Set} (optional):
      \begin{itemize}
        \item A third set created from training data for tuning model parameters.
        \item Common split: 60\% training, 20\% validation, 20\% testing.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Splitting the Dataset - Best Practices}
  \begin{itemize}
    \item \textbf{Random Sampling}:
      \begin{itemize}
        \item Splits dataset randomly to ensure representation.
      \end{itemize}
      
    \item \textbf{Stratified Sampling}:
      \begin{itemize}
        \item Ensures proportional representation in both sets.
        \item Important for binary classification tasks.
      \end{itemize}
      
    \item \textbf{Avoiding Data Leakage}:
      \begin{itemize}
        \item Prevents testing set information from corrupting the training set.
      \end{itemize}
  \end{itemize}
  
  \begin{block}{Example}
    Consider a dataset with 1000 samples:
    \begin{itemize}
      \item \textbf{Total Samples:} 1000
      \item \textbf{Training Set:} 800 samples (80\%)
      \item \textbf{Testing Set:} 200 samples (20\%)
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Splitting the Dataset - Code Example}
  Here is a sample code snippet using \texttt{scikit-learn} for data splitting:
  
  \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

# Assuming X is your feature set and y is the target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    random_state=42, 
                                                    stratify=y)
  \end{lstlisting}

  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Importance of proper splitting influences model accuracy.
      \item Choose between Random and Stratified based on dataset needs.
      \item Consistency in random seed for reproducibility.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training the Model}
    \begin{block}{Understanding Logistic Regression Training}
        Logistic Regression is a statistical method for binary classification. It predicts the probability of a binary outcome based on one or more predictor variables (features). 
        To train a logistic regression model, a dataset must be split into \textbf{training} and \textbf{testing} sets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Process to Train the Model}
    \begin{enumerate}
        \item \textbf{Data Preparation:}
            \begin{itemize}
                \item Ensure data is clean and preprocessed.
                \item Handle missing values, encode categorical variables, and normalize or scale numerical features when necessary.
            \end{itemize}
        
        \item \textbf{Splitting the Dataset:}
            \begin{itemize}
                \item Divide the dataset into \textbf{training (70-80\%)} and \textbf{testing sets (20-30\%)}, where the training set is used to train the model.
            \end{itemize}
        
        \item \textbf{Choosing the Model:} 
            The logistic regression model can be expressed as:
            \begin{equation}
                P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n)}}
            \end{equation}
        \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fitting and Evaluating the Model}
    \begin{enumerate}[resume]
        \item \textbf{Fitting the Model:}
            \begin{itemize}
                \item Use a training algorithm to find optimal parameters by minimizing the difference between observed outcomes and predicted probabilities. The common approach is \textbf{maximum likelihood estimation (MLE)}.
                \item Example in Python:
                \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Evaluating the Model:}
            \begin{itemize}
                \item Assess model performance using metrics such as \textbf{accuracy}, \textbf{precision}, \textbf{recall}, and \textbf{F1 score}.
                \item Use functions like \texttt{cross\_val\_score} or tools like \texttt{confusion\_matrix} in scikit-learn.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Making Predictions - Overview}
  In this slide, we will explore how to utilize a trained logistic regression model to make predictions on new, unseen data. 
  This is the crucial step where our model moves from theory to practice, seeking to apply what it has learned.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Making Predictions - Key Concepts}
  \begin{enumerate}
    \item \textbf{Model Prediction}:
      \begin{itemize}
        \item After training the logistic regression model, we use learned coefficients to relate independent variables (features) to a dependent variable (target).
        \item New data is input into the logistic regression equation to generate probabilities for target classes.
      \end{itemize}

    \item \textbf{Logistic Regression Equation}:
      \begin{equation}
        P(y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n)}}
      \end{equation}
      Here, \( P(y=1 | X) \) is the probability that the target variable \( y \) equals 1 given features \( X \).

    \item \textbf{Thresholding}:
      \begin{itemize}
        \item To convert predicted probabilities into class labels, apply a threshold (commonly 0.5):
        \begin{itemize}
          \item If \( P(y=1 | X) > 0.5 \), classify as 1 (positive class).
          \item If \( P(y=1 | X) \leq 0.5 \), classify as 0 (negative class).
        \end{itemize}
      \end{itemize}

  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Making Predictions - Example}
  Assume we have a trained logistic regression model for predicting whether a student passes (1) or fails (0) an exam based on hours studied and previous grades.

  \begin{itemize}
    \item \textbf{New Data Input:}
      \begin{itemize}
        \item Hours studied = 5
        \item Previous grade = 80
      \end{itemize}

    \item \textbf{Calculation:}
      \begin{equation}
        P(y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \cdot 5 + \beta_2 \cdot 80)}}
      \end{equation}
      Let's say this results in a predicted probability of 0.7.

    \item \textbf{Classification:}
      \begin{itemize}
        \item Since 0.7 > 0.5, we classify the student as passing the exam (1).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Making Predictions - Key Points}
  \begin{itemize}
    \item Predictions from a logistic regression model yield probabilities, not direct class predictions.
    \item Choosing the right threshold is crucial and can be adjusted depending on the application, such as medical diagnoses.
    \item Always back predictions with model evaluation metrics to gauge accuracy and reliability.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Making Predictions - Code Snippet}
  \begin{lstlisting}[language=Python]
import numpy as np
from sklearn.linear_model import LogisticRegression

# Assume X_train and y_train are your training data and target variable
model = LogisticRegression()
model.fit(X_train, y_train)

# New data for prediction
new_data = np.array([[5, 80]])  # New student data with hours studied and previous grade
probability = model.predict_proba(new_data)[:, 1]  # Probability of passing (class 1)

# Thresholding to make class prediction
prediction = (probability > 0.5).astype(int)
print(f'Predicted Probability: {probability}, Class Prediction: {prediction}')
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - Overview}
    \begin{block}{Key Metrics for Logistic Regression}
        Evaluating the performance of a logistic regression model is crucial in understanding its effectiveness. Below, we detail the essential metrics used for this evaluation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - 1. Accuracy}
    \begin{itemize}
        \item \textbf{Definition:} Measures the proportion of true results (both true positives and true negatives) among the total cases examined.
        \item \textbf{Formula:} 
        \begin{equation}
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        \item Where:
        \begin{itemize}
            \item \( TP \) = True Positives
            \item \( TN \) = True Negatives
            \item \( FP \) = False Positives
            \item \( FN \) = False Negatives
        \end{itemize}
        \item \textbf{Example:} If a model predicts 80 correct and 20 incorrect out of 100 instances, the accuracy is \( 80\% \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - 2. Precision}
    \begin{itemize}
        \item \textbf{Definition:} Proportion of positive identifications that were actually correct.
        \item \textbf{Formula:} 
        \begin{equation}
            \text{Precision} = \frac{TP}{TP + FP}
        \end{equation}
        \item \textbf{Example:} If out of 30 predicted positives, 20 are true positives and 10 are false positives, then precision is \( \frac{20}{30} = 66.67\% \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - 3. Recall (Sensitivity)}
    \begin{itemize}
        \item \textbf{Definition:} Proportion of actual positives that were correctly identified.
        \item \textbf{Formula:} 
        \begin{equation}
            \text{Recall} = \frac{TP}{TP + FN}
        \end{equation}
        \item \textbf{Example:} If there are 25 actual positive cases and the model identifies 20 correctly, recall is \( \frac{20}{25} = 80\% \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - 4. F1 Score}
    \begin{itemize}
        \item \textbf{Definition:} Harmonic mean of precision and recall, balancing both metrics.
        \item \textbf{Formula:} 
        \begin{equation}
            F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
        \item \textbf{Example:} With precision of 66.67\% and recall of 80\%, the F1 score is calculated as:
        \begin{equation}
            F1 \approx 72.73\%
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - 5. ROC-AUC}
    \begin{itemize}
        \item \textbf{Definition:} Plots true positive rate (Recall) against false positive rate (FPR) at various thresholds.
        \item \textbf{Interpretation:}
        \begin{itemize}
            \item AUC = 1: Perfect model
            \item AUC > 0.5: Better than random guessing
            \item AUC = 0.5: No discrimination ability
        \end{itemize}
        \item \textbf{Example:} If a model has an AUC of 0.85, it indicates strong performance in distinguishing between classes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - Key Points}
    \begin{itemize}
        \item Different metrics provide insights into different aspects of model performance.
        \item Accuracy can be misleading, especially in imbalanced datasets.
        \item F1 score is preferred over accuracy when dealing with uneven class distributions.
        \item ROC-AUC is valuable for understanding the trade-offs between true and false positive rates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - Conclusion}
    Understanding and utilizing these performance metrics allows data scientists to better gauge their logistic regression model's effectiveness and make informed decisions for improvement or deployment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Model Coefficients}
    % Overview of interpreting coefficients in logistic regression
    Understanding coefficients in logistic regression is essential for understanding the relationships between predictors and outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Coefficients in Logistic Regression}
    \begin{itemize}
        \item \textbf{Logistic Regression Overview}:
        \begin{itemize}
            \item A statistical method for predicting binary classes (0 or 1).
            \item Models the probability of a given input belonging to a specific category.
        \end{itemize}
        \item \textbf{Coefficients Explanation}:
        \begin{itemize}
            \item Indicate strength and direction of association between predictors and log-odds of the outcome.
            \item Logistic regression equation:
            \begin{equation}
                P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n)}}
            \end{equation}
            where \( \beta_0 \) is the intercept, and \( \beta_i \) are coefficients for predictors \( X_i \).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Coefficients}
    \begin{itemize}
        \item \textbf{Positive Coefficient}:
        \begin{itemize}
            \item Indicates an increase in odds of the outcome.
            \item Example: \( \beta_1 = 0.5 \) implies a log-odds increase of 0.5 for a unit increase in \( X_1 \).
        \end{itemize}
        \item \textbf{Negative Coefficient}:
        \begin{itemize}
            \item Indicates a decrease in odds of the outcome.
            \item Example: \( \beta_2 = -0.7 \) implies a log-odds decrease of 0.7 for a unit increase in \( X_2 \).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Odds Ratio and Example}
    \begin{itemize}
        \item \textbf{Odds Ratio}:
        \begin{equation}
            \text{Odds Ratio} = e^{\beta_i}
        \end{equation}
        For example, if \( \beta_1 = 0.5 \), then \( e^{0.5} \approx 1.65 \).
        \item \textbf{Example of Logistic Regression Model}:
        \begin{itemize}
            \item Predicting whether a student will pass an exam based on hours studied and attendance.
            \item Coefficients: \( \beta_1 = 0.4 \) (hours studied), \( \beta_2 = 1.2 \) (attendance).
        \end{itemize}
        \item \textbf{Interpretation}:
        \begin{itemize}
            \item Hours studied: Odds ratio \( e^{0.4} \approx 1.49 \) (49\% more likely to pass).
            \item Attendance: Odds ratio \( e^{1.2} \approx 3.32 \), indicates significantly higher likelihood of passing with increased attendance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Understanding the direction and magnitude of coefficients is crucial for interpreting the model.
        \item Coefficients and their odds ratios provide insights into predictor contributions to the outcome probability.
        \item Effective interpretation supports informed decision-making and understanding of data trends.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues and Solutions in Logistic Regression - Introduction}
    Logistic regression is a powerful tool for binary classification, but it can encounter several common issues:
    \begin{itemize}
        \item Multicollinearity
        \item Overfitting
        \item Underfitting
    \end{itemize}
    Understanding these issues and how to address them is crucial for robust model development.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues and Solutions - Multicollinearity}
    \begin{block}{Multicollinearity}
        \begin{itemize}
            \item \textbf{Definition:} Occurs when independent variables are highly correlated.
            \item \textbf{Impact:} Inflates standard errors, leading to misleading significance tests.
            \item \textbf{Detection:} Variance Inflation Factor (VIF):
            \begin{equation}
                VIF = \frac{1}{1 - R^2}
            \end{equation}
            \item \textbf{Solutions:}
            \begin{itemize}
                \item Remove one of the correlated variables.
                \item Combine correlated variables (e.g., PCA).
                \item Use Lasso (L1) regression as regularization.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues and Solutions - Overfitting and Underfitting}
    \begin{block}{Overfitting}
        \begin{itemize}
            \item \textbf{Definition:} Model learns noise in addition to the pattern.
            \item \textbf{Impact:} Good training performance, poor validation/testing performance.
            \item \textbf{Solutions:}
            \begin{itemize}
                \item Cross-Validation (e.g., k-fold).
                \item Regularization (L1 or L2).
                \item Simplification of the model.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Underfitting}
        \begin{itemize}
            \item \textbf{Definition:} Model too simple to capture trends.
            \item \textbf{Impact:} Poor performance on training and test data.
            \item \textbf{Solutions:}
            \begin{itemize}
                \item Increase model complexity (add polynomial terms).
                \item Include all relevant features.
                \item Try different algorithms.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues and Solutions - Conclusion and Example Code}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Monitor multicollinearity using VIF.
            \item Balance model complexity to avoid overfitting/underfitting.
            \item Use appropriate techniques like cross-validation and regularization.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Code Snippet (Python)}
        \begin{lstlisting}[language=Python]
import pandas as pd
import statsmodels.api as sm

# Calculate VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

# Fit logistic regression model
X = df[['feature1', 'feature2', 'feature3']]  # predictor variables
y = df['target']  # target variable
X = sm.add_constant(X)  # add intercept

model = sm.Logit(y, X).fit()
print(model.summary())
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Use Cases of Logistic Regression}
    \begin{block}{Understanding Logistic Regression}
        Logistic Regression is a statistical method used for binary classification problems, where the outcome variable is categorical and typically takes on one of two values. It predicts the probability that a given input point belongs to a particular category.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-world Applications}
    \begin{enumerate}
        \item \textbf{Healthcare}
            \begin{itemize}
                \item Disease Diagnosis: Predicts presence or absence of a disease based on patient data, e.g., age, weight, blood pressure.
                \item \textit{Example:} Predicting whether a patient has diabetes (yes/no) based on glucose levels and other features.
            \end{itemize}
        \item \textbf{Finance}
            \begin{itemize}
                \item Credit Scoring: Evaluates whether an applicant will default on a loan based on historical data.
                \item \textit{Example:} Identifying whether a credit applicant will default based solely on credit history and income.
            \end{itemize}
        \item \textbf{Marketing}
            \begin{itemize}
                \item Customer Retention: Analyzes whether customers are likely to churn based on their interaction with products.
                \item \textit{Example:} Assessing if a customer will renew their subscription based on usage patterns.
            \end{itemize}
        \item \textbf{E-commerce}
            \begin{itemize}
                \item Purchase Prediction: Forecasts whether a user will make a purchase after engaging with an advertisement.
                \item \textit{Example:} Predicting if visitors to an e-commerce site will buy a product based on their browsing behavior.
            \end{itemize}
        \item \textbf{Social Sciences}
            \begin{itemize}
                \item Survey Analysis: Analyzes survey responses to determine the likelihood of a response category.
                \item \textit{Example:} Determining if a voter will support a candidate based on demographics and issues.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Binary Outcome:} Logistic regression is suitable for problems with two outcome categories.
            \item \textbf{Interpretability:} Coefficients provide insightful information about the relationship between predictors and the outcome.
            \item \textbf{Probabilistic Framework:} Instead of predicting binary outcomes directly, it predicts the probability that an instance will fall into a category.
        \end{itemize}
    \end{block}
    
    \begin{block}{Logistic Regression Formula}
        The logistic function is defined mathematically as:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
        \end{equation}
        Where:
        \begin{itemize}
            \item $P(Y=1 | X)$ is the probability of the instance being in class 1.
            \item $\beta_0$ is the intercept.
            \item $\beta_1, \beta_2, \ldots, \beta_n$ are the coefficients of the predictor variables $X_1, X_2, \ldots, X_n$.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Logistic regression is a versatile and widely-used algorithm across diverse fields, allowing organizations to make data-driven decisions effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advanced Topics in Logistic Regression}
    \begin{block}{Introduction to Regularization}
        Regularization is a technique used in logistic regression to prevent overfitting, especially when we have a large number of features. It adds a penalty to the loss function based on the magnitude of the coefficients.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Lasso Regularization (L1)}
    \begin{itemize}
        \item \textbf{Definition:} Adds the absolute value of the coefficients as a penalty term to the loss function.
        \item \textbf{Loss Function:} 
        \begin{equation}
            J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))] + \lambda \sum_{j=1}^{n} |\theta_j|
        \end{equation}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Encourages sparsity: Reduces some coefficients to exactly zero, performing feature selection.
            \item Useful in high-dimensional datasets.
        \end{itemize}
        \item \textbf{Example:} Helps identify key features in datasets with many features by setting irrelevant coefficients to zero.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ridge Regularization (L2)}
    \begin{itemize}
        \item \textbf{Definition:} Adds the squared coefficients as a penalty term to the loss function.
        \item \textbf{Loss Function:} 
        \begin{equation}
            J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))] + \lambda \sum_{j=1}^{n} \theta_j^2
        \end{equation}
        \item \textbf{Key Features:}
        \begin{itemize}
            \item Does not perform variable selection; shrinks coefficients but keeps them non-zero.
            \item Helps in dealing with multicollinearity.
        \end{itemize}
        \item \textbf{Example:} Stabilizes estimates in datasets with correlated features, improving predictions through all predictors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Lasso and Ridge}
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Feature} & \textbf{Lasso (L1)} & \textbf{Ridge (L2)} \\ 
        \hline
        Coefficient Shrinkage & Can reduce coefficients to zero & Shrinks coefficients but not to zero  \\ 
        \hline
        Feature Selection & Yes & No \\ 
        \hline
        Best for & High-dimensional data & Correlated variables \\ 
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Regularization helps manage overfitting by adjusting model complexity.
        \item Choosing between Lasso and Ridge depends on dataset needs (feature selection vs. multicollinearity).
        \item Regularization parameters ($\lambda$) should be tuned with techniques like cross-validation.
    \end{itemize}
    \begin{block}{Conclusion}
        Integrating Lasso and Ridge in logistic regression leads to more robust and generalizable models, essential for effective data analysis in complex datasets.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Practical Exercise}
    \textbf{Objective:} \\
    In this hands-on activity, you will implement \textbf{Logistic Regression} on a sample dataset to classify binary outcomes. This exercise will help you understand how to train a model, interpret its results, and evaluate its performance.
\end{frame}

\begin{frame}
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Logistic Regression} is a statistical method for predicting binary classes.
        \item The outcome is modeled as a function of the independent variables using the logistic function.
        \item The formula for logistic regression is:
        \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n)}}
        \end{equation}
        \begin{itemize}
            \item \( P(Y=1|X) \): Probability of the positive class (class 1)
            \item \( \beta_0 \): Intercept
            \item \( \beta_1, \beta_2, ..., \beta_n \): Coefficients for each independent variable \( X_1, X_2, ..., X_n \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps for Implementation}
    \textbf{1. Load the Dataset:} \\
    Use a CSV file or a preloaded dataset available in your Python environment. For this exercise, we will use the \textbf{Iris dataset}.
    \begin{lstlisting}[language=Python]
    import pandas as pd
    from sklearn.datasets import load_iris

    data = load_iris()
    df = pd.DataFrame(data.data, columns=data.feature_names)
    df['target'] = (data.target == 0).astype(int)  # 1 for Setosa, 0 for others
    \end{lstlisting}
    
    \textbf{2. Explore the Data:} \\
    Perform a preliminary analysis to understand the features and target variable.
    \begin{lstlisting}[language=Python]
    print(df.head())
    print(df.describe())
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps for Implementation (cont.)}
    \textbf{3. Split the Data:} \\
    Divide the dataset into training and testing sets.
    \begin{lstlisting}[language=Python]
    from sklearn.model_selection import train_test_split

    X = df[data.feature_names]
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    \end{lstlisting}

    \textbf{4. Create and Train the Model:} \\
    Utilize \texttt{LogisticRegression} from \texttt{sklearn}.
    \begin{lstlisting}[language=Python]
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps for Implementation (cont.)}
    \textbf{5. Make Predictions:} \\
    Use the model to predict outcomes on the test set.
    \begin{lstlisting}[language=Python]
    y_pred = model.predict(X_test)
    \end{lstlisting}

    \textbf{6. Evaluate the Model:} \\
    Assess performance using metrics like accuracy, precision, and the confusion matrix.
    \begin{lstlisting}[language=Python]
    from sklearn.metrics import accuracy_score, confusion_matrix

    accuracy = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    print(f"Accuracy: {accuracy}")
    print(f"Confusion Matrix:\n{cm}")
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Logistic regression is suited for binary classification problems.
        \item Properly splitting your data into training and testing sets is crucial for unbiased evaluation.
        \item Understanding evaluation metrics helps assess model performance beyond mere accuracy.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion and Next Steps}
    \textbf{Conclusion:} \\
    By completing this exercise, you will acquire practical skills in applying logistic regression, understand its mechanics, and appreciate the importance of model evaluation.
    
    \textbf{Next Steps:} \\
    Prepare for our concluding slide, encapsulating your key learnings and suggesting areas for further exploration in supervised learning concepts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways}
    As we wrap up our exploration of Logistic Regression, let’s summarize the key points and encourage further exploration into supervised learning.
    
    \begin{enumerate}
        \item \textbf{Logistic Regression Definition:} 
        A statistical method for binary classification predicting the probability of a categorical dependent variable based on independent variables.
        
        \item \textbf{Sigmoid Function:} 
        The logistic function transforms any real-valued number into a value between 0 and 1, suitable for estimating probabilities.
        
        \item \textbf{Interpretation of Coefficients:} 
        Coefficients (\(\beta\)) represent the log odds of the output being 1 (positive class) relative to input features \(X\).
        
        \item \textbf{Loss Function:} 
        Using Log Loss (Cross-Entropy Loss) to evaluate performance, minimizing the difference between predicted probabilities and actual outcomes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Practical Applications}
    Logistic regression has numerous practical applications across various fields:

    \begin{itemize}
        \item \textbf{Medicine:} Predicting disease presence.
        \item \textbf{Marketing:} Customer conversion analysis.
        \item \textbf{Social Sciences:} Understanding voting behavior.
    \end{itemize}

    Moreover, there are additional learning opportunities to further develop your understanding.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Encouragement to Explore Further}
    This concludes our fundamental overview of Logistic Regression, a core technique in supervised learning. We encourage you to:
    
    \begin{itemize}
        \item Experiment with different datasets using the concepts learned today.
        \item Implement and analyze model performance using metrics such as accuracy, precision, and recall.
        \item Engage with advanced literature and online courses to expand your knowledge base and capabilities in machine learning.
    \end{itemize}
    
    Thank you for participating this week; your journey into supervised learning is just beginning!
\end{frame}


\end{document}