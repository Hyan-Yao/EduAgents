\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Data-Driven Challenges in Supervised Learning]{Chapter 4: Data-Driven Challenges in Supervised Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Supervised Learning}
    
    \begin{block}{What is Supervised Learning?}
        Supervised learning is a type of machine learning where we train a model on a labeled dataset. 
        This means that for each input, we have a corresponding output (label), which helps the model learn to make predictions.
    \end{block}
    
    \begin{itemize}
        \item Classifying emails as spam or not spam.
        \item Predicting house prices based on features like size, location, and amenities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data-Driven Challenges}
    
    \begin{block}{Importance of Data-Driven Challenges}
        Data-driven challenges refer to obstacles that arise from the data itself and can significantly impact the performance and effectiveness of supervised learning models. 
        Understanding these challenges is crucial for building accurate and reliable models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in Supervised Learning}
    
    \begin{enumerate}
        \item \textbf{Quality of Data}
        \begin{itemize}
            \item Challenge: Incomplete or noisy data can lead to incorrect predictions.
            \item Example: Predicting student performance based on scores that are missing or incorrectly recorded.
        \end{itemize}

        \item \textbf{Quantity of Data}
        \begin{itemize}
            \item Challenge: Insufficient data can hinder effective learning.
            \item Example: Developing a model for rare diseases may lack enough training examples.
        \end{itemize}

        \item \textbf{Bias in Data}
        \begin{itemize}
            \item Challenge: Non-representative training data can introduce biases.
            \item Example: A facial recognition system trained primarily on light-skinned individuals.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in Supervised Learning (cont'd)}
    
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Feature Selection}
        \begin{itemize}
            \item Challenge: Identifying relevant input features can be complex.
            \item Example: In customer churn prediction, service interactions may be more relevant than age.
        \end{itemize}

        \item \textbf{Overfitting and Underfitting}
        \begin{itemize}
            \item Concept: Overfitting is learning noise instead of signals; underfitting is being too simplistic.
            \item Key Insight: Balance between complexity and simplicity is essential.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Addressing Data-Driven Challenges}
    
    \begin{block}{Why Address These Challenges?}
        Understanding and addressing data-driven challenges enhances model performance, reliability, and fairness in real-world applications.
    \end{block}
    
    \begin{itemize}
        \item How can we ensure our dataset is representative of varied real-world scenarios?
        \item What strategies can mitigate bias in our predictive models?
    \end{itemize}
    
    \begin{block}{Summary}
        Data-driven challenges are vital for supervised learning. Addressing these challenges allows us to harness machine learning for informed decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Objectives of the Chapter - Engagement with Real Datasets}
    \begin{block}{Objective 1: Engage with Real Datasets}
        \begin{itemize}
            \item \textbf{Concept Explanation:} Throughout this chapter, we will utilize actual datasets to illustrate the challenges of supervised learning. Focus on how data quality, complexity, and variability influence predictive modeling.
            \item \textbf{Example:} Analyze a dataset on housing prices, incorporating features such as:
            \begin{itemize}
                \item Location 
                \item Square footage 
                \item Number of bedrooms
            \end{itemize}
            By working with this real-world dataset, students will understand how these factors interact and impact the outcome—the price of a house.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Objectives of the Chapter - Applying Regression Techniques}
    \begin{block}{Objective 2: Apply Regression Techniques}
        \begin{itemize}
            \item \textbf{Concept Explanation:} Regression is a technique in supervised learning used to predict continuous outcomes. We will implement various regression models, understand their assumptions, and evaluate their performance.
            \item \textbf{Example:} Using linear regression to predict house prices based on the dataset. A simple model can be represented as:
            \begin{equation}
            \text{Price} = \beta_0 + \beta_1 \times \text{Square Footage} + \beta_2 \times \text{Number of Bedrooms} + \epsilon
            \end{equation}
            Where:
            \begin{itemize}
                \item $\beta_0$: Intercept
                \item $\beta_1$, $\beta_2$: Coefficients showing the impact of each feature
                \item $\epsilon$: Error term
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Objectives of the Chapter - Identifying Data Challenges}
    \begin{block}{Objective 3: Identify and Address Common Data-Driven Challenges}
        \begin{itemize}
            \item \textbf{Concept Explanation:} Real datasets can present challenges like missing values, outliers, and multicollinearity. It's crucial to understand these issues for model accuracy.
            \item \textbf{Example:} Learning techniques to handle missing values, such as:
            \begin{itemize}
                \item Imputation (e.g., replacing missing values with the mean)
                \item Removal of incomplete records
            \end{itemize}
            Addressing these challenges enhances the robustness of predictive models.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Real-world data presents complexities not found in synthetic datasets.
            \item Foundational principles of regression are essential for advanced modeling.
            \item Hands-on experience with data challenges develops analytical skills.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Definition}
    \begin{block}{Definition of Supervised Learning}
        Supervised learning is a category of machine learning that uses labeled data to train algorithms. In this approach, the model learns to map inputs (features) to outputs (labels or targets) by learning from example data pairs.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Input Features (X):} Variables used by the model to make predictions.
        \item \textbf{Output Labels (Y):} Target values that the model aims to predict.
        \item \textbf{Training Data:} A set of input-output pairs used to teach the model.
    \end{itemize}
    
    \begin{block}{Example Scenario}
        Imagine you have a dataset of house prices. The features could include square footage, number of bedrooms, and location—while the label would be the selling price. The model learns to predict prices based on input features.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Importance}
    \begin{block}{Importance of Supervised Learning}
        Supervised learning is crucial for many real-world applications and enables predictive modeling. Here are some key reasons:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Practical Applications:} Used in image recognition, spam detection, and medical diagnosis.
            \begin{itemize}
                \item \textit{Example:} In healthcare, it can predict diseases based on symptoms and medical history.
            \end{itemize}
        \item \textbf{Improves Decision-Making:} Leverages historical data for informed decisions.
            \begin{itemize}
                \item \textit{Example:} Predicting loan defaults based on customer profiles in finance.
            \end{itemize}
        \item \textbf{Enhancements in Automation:} Automates tasks like email classification and image sorting.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Labeled Data Requirement:} Dependent on the availability of labeled datasets, which can be costly to create.
            \item \textbf{Overfitting Risk:} Important to prevent overfitting by using methods like cross-validation.
            \item \textbf{Evaluation Metrics:} Metrics include accuracy, precision, recall, and F1 score.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Example Task}
    \begin{block}{Example of a Supervised Learning Task}
        \textbf{Task:} Predicting student exam scores based on study hours and attendance.
    \end{block}

    \begin{itemize}
        \item \textbf{Input Features:}
            \begin{itemize}
                \item Study Hours (X1)
                \item Attendance Percentage (X2)
            \end{itemize}
        \item \textbf{Output Label:} Predicted Exam Score (Y)
    \end{itemize}

    \begin{block}{Data Table:}
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Study Hours (X1)} & \textbf{Attendance (X2)} & \textbf{Exam Score (Y)} \\
            \hline
            2 & 80 & 75 \\
            4 & 90 & 85 \\
            6 & 95 & 92 \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Data in AI - Introduction}
    \begin{block}{Introduction}
        Data is often referred to as the "oil of the 21st century." In supervised learning, the quality and quantity of data are paramount for training machine learning models effectively. This presentation will discuss:
    \end{block}
    \begin{itemize}
        \item Why data is pivotal in AI
        \item How it influences model performance
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Data in AI - Importance}
    \begin{block}{The Importance of Data}
        \begin{enumerate}
            \item \textbf{Foundation of Machine Learning:}
                  \begin{itemize}
                      \item Supervised learning requires labeled datasets. Without this structured data, machines cannot learn.
                      \item \textit{Example:} In spam detection, emails (input) are labeled as 'spam' or 'not spam' (output).
                  \end{itemize}
                  
            \item \textbf{Training Process:}
                  \begin{itemize}
                      \item Models recognize patterns using data. A comprehensive dataset enables better generalization.
                      \item \textit{Illustration:} Teaching a child to differentiate animals with multiple examples (e.g., various pictures of cats and dogs).
                  \end{itemize}
                  
            \item \textbf{Impact on Performance:}
                  \begin{itemize}
                      \item High-quality, diverse datasets enhance accuracy and reliability.
                      \item \textit{Example:} A facial recognition system trained on one demographic may perform poorly on others.
                  \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Data in AI - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Diversity in Data:} Improves a model's generalization ability.
            \item \textbf{Size Matters:} Larger datasets provide more examples for better learning.
            \item \textbf{Data Quality Over Quantity:} Relevant, accurate, and clean data is crucial; noisy data degrades performance.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Data is the backbone of supervised learning. Ensuring high-quality datasets leads to more effective and reliable models. Next, we will explore how to assess and improve data quality.
    \end{block}

    \begin{block}{Engaging Questions for Consideration}
        \begin{itemize}
            \item How can bias in data affect the outcomes of models?
            \item What are common sources of data for supervised learning?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Quality of Data}
    \begin{block}{Understanding High-Quality Data}
        High-quality data is paramount for the effectiveness of machine learning (ML) models, particularly in supervised learning. Key components of high-quality data include:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Quality of Data - Key Components}
    \begin{enumerate}
        \item \textbf{Accuracy}: Data should be correct and error-free. Incorrect data skew predictions.
        \item \textbf{Completeness}: Datasets must aim for minimal missing information to avoid ineffective strategies.
        \item \textbf{Consistency}: Data must be free from contradictions, ensuring clarity in interpretation.
        \item \textbf{Relevance}: Data should be applicable to the specific task to maintain model effectiveness.
        \item \textbf{Timeliness}: Data must be up-to-date to reflect current trends, especially in dynamic fields.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Data Quality on Model Effectiveness}
    \begin{itemize}
        \item \textbf{Model Performance}: Low-quality data can cause overfitting or underfitting, leading to incorrect predictions.
        \item \textbf{Generalization}: High-quality datasets enhance the model's ability to generalize and perform well on unseen data.
        \item \textbf{Example}: Erroneous medical records in a health tech company can lead to harmful treatment predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion: Why Quality Matters}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item High-quality data is foundational for reliable AI outcomes.
            \item Continuous validation, cleaning, and monitoring of data are essential.
            \item Understanding data issues leads to better preparation and model training.
        \end{itemize}
    \end{block}
    \begin{block}{Notes for Further Exploration}
        \begin{itemize}
            \item Discuss data sourcing techniques and methods for evaluating data quality.
            \item Encourage student engagement with questions regarding low-quality data impacts.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Data Handling - Introduction}
    Data handling is a critical step in supervised learning, as the effectiveness of the model relies heavily on the quality and organization of the data at hand. Several common challenges arise during this process, including:
    
    \begin{enumerate}
        \item Data Cleaning
        \item Normalization
        \item Feature Selection
    \end{enumerate}
    
    Understanding these challenges is essential for building robust, efficient models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Data Handling - Data Cleaning}
    \begin{block}{Explanation}
        Data cleaning involves identifying and correcting inaccuracies or inconsistencies in the data. This can include removing duplicates, dealing with missing values, and correcting erroneous entries.
    \end{block}

    \begin{block}{Common Issues}
        \begin{itemize}
            \item \textbf{Missing Values:} Data entries may be incomplete. 
            \begin{itemize}
                \item Example: In a dataset of patient records, some entries might lack age or medical history.
            \end{itemize}
            \item \textbf{Duplicates:} Repeated entries can skew results.
            \begin{itemize}
                \item Example: Multiple records of the same patient due to clerical errors.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Solutions}
        \begin{itemize}
            \item \textbf{Imputation:} Fill in missing values with mean, median, or mode.
            \item \textbf{Removal:} Delete duplicates based on specific criteria.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Data Handling - Normalization and Feature Selection}
    \begin{block}{Normalization}
        Normalization is the process of scaling features to a similar range or distribution. This ensures that no single feature disproportionately influences the model due to its scale.

        \begin{itemize}
            \item \textbf{Min-Max Scaling:} Rescales values to a range of [0, 1].
            \begin{equation}
            X' = \frac{X - X_{min}}{X_{max} - X_{min}}
            \end{equation}
            \item \textbf{Z-score Normalization:} Centers the data with a mean of 0 and standard deviation of 1.
            \begin{equation}
            Z = \frac{X - \mu}{\sigma}
            \end{equation}
        \end{itemize}
        
        Importance: Normalization improves the performance of algorithms that depend on distance metrics, such as k-NN and neural networks.
    \end{block}

    \begin{block}{Feature Selection}
        Feature selection involves identifying and selecting the most relevant features (variables) that contribute to the predictive power of the model.
        
        Key Benefits:
        \begin{itemize}
            \item Enhances model performance.
            \item Reduces computational complexity.
        \end{itemize}

        Techniques for Feature Selection:
        \begin{itemize}
            \item Filter Methods
            \item Wrapper Methods
            \item Embedded Methods
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Data Handling - Key Points and Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Effective data handling is crucial for quality prediction in supervised learning.
            \item Data cleaning ensures the integrity of your dataset; normalization makes it comparable; and feature selection enhances model efficiency.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Recognizing and addressing these common challenges is essential for building accurate and reliable models. By mastering data handling techniques, you pave the way for successful supervised learning outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regression Techniques Overview - What is Regression?}
    \begin{itemize}
        \item \textbf{Definition:} A statistical method in supervised learning that models the relationship between a dependent variable (outcome) and independent variables (predictors).
        \item \textbf{Purpose:} To predict continuous outcomes, such as housing prices based on factors like size, location, and condition.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regression Techniques Overview - Why Use Regression?}
    \begin{itemize}
        \item \textbf{Predictive Analytics:} Useful for forecasting based on historical data (e.g., sales forecasts).
        \item \textbf{Data Interpretation:} Helps quantify variable relationships (e.g., impact of advertising on sales).
        \item \textbf{Problem-Solving:} Addresses challenges in risk assessment, resource allocation, and market analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regression Techniques Overview - Common Applications}
    \begin{enumerate}
        \item \textbf{Economics:} Estimating market demand or economic growth based on consumer/interest parameters.
        \item \textbf{Medical Research:} Predicting disease progression from clinical indicators or treatment effects.
        \item \textbf{Environmental Science:} Assessing pollutant impacts on wildlife populations.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regression Techniques Overview - Simple Example}
    \begin{block}{Scenario}
        A real estate company wants to predict house prices.
    \end{block}
    \begin{itemize}
        \item \textbf{Variables:}
            \begin{itemize}
                \item Dependent Variable (Y): House Price
                \item Independent Variables (X): Size (sq ft), number of bedrooms, age of house
            \end{itemize}
        \item \textbf{Concept:} Model the relationship where each additional square foot increases the price by a specific amount using regression techniques.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regression Techniques Overview - Key Points}
    \begin{itemize}
        \item \textbf{Types of Relationships:} Regression helps understand linear and non-linear relationships.
        \item \textbf{Model Fit:} Evaluated with metrics like R-squared, indicating how well the model explains outcome variability.
        \item \textbf{Interpretation:} Coefficients show expected changes in the dependent variable for a one-unit change in an independent variable.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regression Techniques Overview - Next Steps}
    \begin{itemize}
        \item \textbf{Types of Regression Models:}
            \begin{itemize}
                \item Linear Regression: For simple linear relationships.
                \item Polynomial Regression: For non-linear relationships.
                \item Ridge and Lasso Regression: Regularization techniques to prevent overfitting.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression Models - Overview}
    \begin{block}{Introduction}
        Regression models are essential in supervised learning, allowing us to predict relationships between dependent and independent variables.
        Let's explore the major types of regression models and their characteristics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression Models - Linear Regression}
    \begin{itemize}
        \item \textbf{Concept}: 
        \begin{equation}
            Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n + \epsilon 
        \end{equation}
        where $b$ are coefficients and $\epsilon$ is the error term.
        
        \item \textbf{Example}: Predicting house prices based on area size (square feet).
        
        \item \textbf{Key Point}: Simple to interpret, but may not capture complex relationships.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression Models - Polynomial Regression}
    \begin{itemize}
        \item \textbf{Concept}:
        \begin{equation}
            Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n + \epsilon
        \end{equation}

        \item \textbf{Example}: Modeling the trajectory of a ball in motion with a parabolic curve.

        \item \textbf{Key Point}: More flexible than linear regression, yet can lead to overfitting with higher polynomial degrees.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression Models - Ridge and Lasso Regression}
    \begin{itemize}
        \item \textbf{Ridge Regression}:
        \begin{equation}
            Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n + \lambda \sum_{j=1}^n b_j^2
        \end{equation}
        \begin{itemize}
            \item \textbf{Example}: Useful in predicting sales with multiple correlated marketing channels.
            \item \textbf{Key Point}: Prevents overfitting by shrinking coefficients.
        \end{itemize}

        \item \textbf{Lasso Regression}:
        \begin{equation}
            Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n + \lambda \sum_{j=1}^n |b_j|
        \end{equation}
        \begin{itemize}
            \item \textbf{Example}: Effective in genomics for feature selection.
            \item \textbf{Key Point}: Reduces overfitting and may eliminate irrelevant features.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression Models - Quantile Regression}
    \begin{itemize}
        \item \textbf{Concept}: Predicts any quantile (e.g., median) of the dependent variable.
        
        \item \textbf{Example}: Useful in finance for analyzing different percentiles of investment returns.
        
        \item \textbf{Key Point}: Provides insights into the distribution of the target variable, not just the average.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression Models - Conclusion}
    Understanding various regression models helps in tackling different data-driven challenges. Each model has its unique advantages; selecting the right one is crucial based on your data and goals.

    \begin{block}{Upcoming Activity}
        Feel free to test different models during our hands-on activity where you’ll apply these concepts to real datasets!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-on Activity: Exploring Datasets}
    
    \begin{block}{Objectives}
        \begin{itemize}
            \item Apply regression techniques using real datasets.
            \item Understand the nuances of data preparation and model selection.
            \item Gain hands-on experience with tools for data analysis and visualization.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity Overview}
    
    In this activity, you will explore various datasets, apply regression techniques, and analyze the results.
    By engaging with real data, you will gain insights into the practical applications of supervised learning concepts discussed in Chapter 4.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Follow - Part 1}

    \begin{enumerate}
        \item \textbf{Select a Dataset}:
            \begin{itemize}
                \item Choose from sources like Kaggle, UCI Machine Learning Repository, Open Data Portal.
            \end{itemize}
            
        \item \textbf{Data Preparation}:
            \begin{itemize}
                \item Load the data using Python's pandas:
                \begin{lstlisting}[language=Python]
import pandas as pd
dataset = pd.read_csv('your_dataset.csv')
                \end{lstlisting}
                
                \item Explore the data using:
                \lstinline!dataset.head()!
                \item Clean the data (handle missing values and outliers).
                \item Feature selection: Identify predictors for your regression model.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Follow - Part 2}

    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Model Implementation}:
            \begin{itemize}
                \item Split your dataset into training and testing sets:
                \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
X = dataset[['feature1', 'feature2']]
y = dataset['target_variable']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                \end{lstlisting}
                \item Fit your regression model:
                \begin{lstlisting}[language=Python]
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{Evaluate Your Model}:
            \begin{itemize}
                \item Use metrics like R² score and Mean Absolute Error (MAE):
                \begin{lstlisting}[language=Python]
from sklearn.metrics import mean_absolute_error, r2_score
predictions = model.predict(X_test)
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'MAE: {mae}, R²: {r2}')
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps to Follow - Part 3}

    \begin{enumerate}
        \setcounter{enumi}{5}
        \item \textbf{Visualize the Results}:
            \begin{itemize}
                \item Create visualizations like scatter plots to understand model performance:
                \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
plt.scatter(y_test, predictions)
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('True vs Predicted Values')
plt.show()
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item Importance of data preparation for accurate regression results.
        \item Selecting appropriate regression techniques based on data characteristics.
        \item Evaluating models using relevant metrics for effective decision-making.
        \item Visualization aids in interpreting model performance and understanding results.
    \end{itemize}
    
    This hands-on activity bridges theory and real-world applications of supervised learning concepts. Happy exploring!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Regression Models}
    \begin{itemize}
        \item Metrics are essential for assessing the performance of regression models.
        \item Key metrics: 
        \begin{itemize}
            \item R² Score (Coefficient of Determination)
            \item Mean Absolute Error (MAE)
        \end{itemize}
        \item Understanding these metrics facilitates model comparison and evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics for Evaluation - R² Score}
    \begin{block}{R² Score (Coefficient of Determination)}
        \begin{itemize}
            \item **Definition**: Measures the proportion of variance in the dependent variable explained by independent variables.
            \item **Range**: 0 to 1; closer to 1 is better.
            \item **Interpretation**:
            \begin{itemize}
                \item R² = 0: No explanatory power.
                \item R² = 1: Perfect explanation of variability.
            \end{itemize}
            \item **Example**: R² = 0.85 in house price predictions means 85\% of price variation can be explained by features.
        \end{itemize}
    \end{block}

    \begin{equation} 
        R^2 = 1 - \frac{SS_{res}}{SS_{tot}} 
    \end{equation}
    Where:
    \begin{itemize}
        \item $SS_{res}$ = Sum of squares of residuals
        \item $SS_{tot}$ = Total sum of squares
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics for Evaluation - Mean Absolute Error}
    \begin{block}{Mean Absolute Error (MAE)}
        \begin{itemize}
            \item **Definition**: Average of the absolute differences between predicted and actual values.
            \item **Range**: Non-negative; lower is better.
            \item **Interpretation**: A MAE of \$4,000 indicates the average error is \$4,000 in price predictions.
        \end{itemize}
    \end{block}

    \begin{equation} 
        MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| 
    \end{equation}
    Where:
    \begin{itemize}
        \item $y_i$ = Actual values
        \item $\hat{y}_i$ = Predicted values
        \item $n$ = Number of observations
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Practical Application}
    \begin{itemize}
        \item R² indicates model fit but does not signify prediction bias or inconsistency.
        \item MAE shows average errors without considering their magnitudes.
        \item Using multiple metrics gives a comprehensive performance overview.
    \end{itemize}

    \begin{block}{Practical Example}
        \begin{itemize}
            \item Predicting retail sales:
            \begin{itemize}
                \item R² = 0.78: Explains 78\% of sales variance.
                \item MAE = \$150: Average prediction error of \$150.
            \end{itemize}
            \item Suggests a reasonably effective model, potential for improvement with more variables.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Evaluating regression models is essential for ensuring predictive accuracy and reliability through metrics like R² and MAE.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies of Regression in Action}
    \begin{block}{Introduction to Regression}
        Regression analysis is a powerful statistical method to understand relationships between variables. In supervised learning, it's used to predict a continuous outcome variable based on one or more predictor variables.
    \end{block}
    This slide presents real-world case studies showcasing how regression techniques are effectively utilized across various fields.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Predicting Housing Prices}
    \begin{itemize}
        \item \textbf{Context}: Real estate agencies use regression to predict house prices based on factors like location and size.
        \item \textbf{Implementation}:
        \begin{itemize}
            \item Linear Regression Model
            \item Input variables: Square footage, number of bedrooms, number of bathrooms, and neighborhood ratings.
        \end{itemize}
        \item \textbf{Results}: The model provides a predicted price, assisting buyers and sellers in making informed decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Case Studies and Conclusions}
    \begin{block}{Case Study 2: Sales Forecasting for Retail}
        \begin{itemize}
            \item \textbf{Context}: Retail companies use regression to forecast future sales based on historical data.
            \item \textbf{Implementation}:
            \begin{itemize}
                \item Multiple Linear Regression to account for seasonality, promotions, and economic conditions.
            \end{itemize}
            \item \textbf{Results}: Accurate sales predictions aid inventory management and staffing decisions.
        \end{itemize}
    \end{block}

    \begin{block}{Case Study 3: Predicting Healthcare Outcomes}
        \begin{itemize}
            \item \textbf{Context}: Hospitals estimate patient outcomes like recovery times or readmission rates.
            \item \textbf{Implementation}: Logistic Regression for binary outcomes using demographics, medical history, and treatment protocols.
            \item \textbf{Results}: Improved patient care through better resource allocation.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Regression analysis is a vital tool in leveraging data to predict outcomes. It showcases practical applications in solving real-world challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges Faced in Model Training}
    \begin{block}{Introduction}
        Training machine learning models with real datasets presents several challenges that significantly impact performance, accuracy, and overall effectiveness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Quality Issues}
    \begin{itemize}
        \item \textbf{Description:} Real-world datasets often contain noise, missing values, and outliers.
        \item \textbf{Example:} In house price prediction, missing prices can hinder learning.
        \item \textbf{Solution:} Employ data cleaning techniques like imputation and outlier detection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Imbalanced Datasets}
    \begin{itemize}
        \item \textbf{Description:} Unequal class distributions can lead to biased predictions.
        \item \textbf{Example:} In fraud detection, with 95\% legitimate and 5\% fraudulent transactions, the model may ignore fraud.
        \item \textbf{Solution:} Use resampling strategies or specialized algorithms like SMOTE.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting}
    \begin{itemize}
        \item \textbf{Description:} Overfitting captures noise; underfitting misses trends.
        \item \textbf{Example:} A model may perfectly fit training data yet perform poorly on new data.
        \item \textbf{Solution:} Utilize techniques like cross-validation, regularization, and selecting appropriate model complexity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection and Scalability}
    \begin{itemize}
        \item \textbf{Feature Selection:}
            \begin{itemize}
                \item \textbf{Description:} Selecting critical features affects performance.
                \item \textbf{Example:} Ignoring customer service interactions in churn prediction leads to poor outcomes.
                \item \textbf{Solution:} Use forward selection, backward elimination, or tree-based models.
            \end{itemize}
            
        \item \textbf{Scalability:}
            \begin{itemize}
                \item \textbf{Description:} Large datasets require efficient processing.
                \item \textbf{Example:} Training deep learning models on extensive data needs significant resources.
                \item \textbf{Solution:} Leveraging cloud solutions or frameworks like Apache Spark can help manage scale.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Data Preparation is Crucial:} Quality of data directly affects outcomes.
        \item \textbf{Balanced Datasets:} Addressing class imbalance is necessary for fairness.
        \item \textbf{Model Complexity Matters:} Balancing underfitting and overfitting is essential for accuracy.
        \item \textbf{Feature Relevance is Key:} Selecting the right features influences model effectiveness.
        \item \textbf{Scalability Challenges Exist:} Readiness for larger datasets is vital for real-world applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Recognizing and addressing these challenges can enhance the reliability and accuracy of models, leading to better predictions and outcomes. Effective training and deployment require understanding these challenges.
    \end{block}
    \begin{quote}
        "The goal of machine learning is to outperform human capabilities, but we must be mindful of the inherent challenges it presents."
    \end{quote}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Regression Analysis}
    \begin{block}{Understanding Regression Analysis}
        Regression analysis aims to model the relationship between a dependent variable (target) and one or more independent variables (predictors). It helps in predicting outcomes and understanding relationships in data.
    \end{block}
    \begin{itemize}
        \item \textbf{Dependent Variable (Y)}: The output you are trying to predict (e.g., house price).
        \item \textbf{Independent Variables (X)}: Factors influencing the dependent variable (e.g., size, location).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Regression Analysis - Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Data Preprocessing}
        \begin{itemize}
            \item \textbf{Missing Values}: Handle missing data through imputation or removal.
            \begin{itemize}
                \item \textit{Example}: Fill in the average size or remove rows with missing 'size' values.
            \end{itemize}
            \item \textbf{Outlier Detection}: Identify and consider removing outliers as they can skew results.
            \begin{itemize}
                \item \textit{Example}: A house priced significantly higher than others may not be representative.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Regression Analysis - Model Validation}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Feature Selection}
        \begin{itemize}
            \item Use correlation analysis or feature importance metrics to select meaningful variables.
            \item \textit{Key Point}: Avoid including too many irrelevant variables to prevent overfitting.
        \end{itemize}
        
        \item \textbf{Model Validation Techniques}
        \begin{itemize}
            \item \textbf{Train-Validation Split}: Use 80\% of data for training and 20\% for validation.
            \item \textbf{Cross-Validation}: Apply k-fold cross-validation to evaluate model robustness.
            \begin{itemize}
                \item \textit{Key Point}: Ensures model generalizes well to unseen data.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Regression Analysis - Testing and Interpretation}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Testing the Model}
        \begin{itemize}
            \item Always evaluate with a separate test dataset.
            \item \textbf{Performance Metrics}: Use metrics like R-squared, MAE, and RMSE to assess accuracy.
            \begin{itemize}
                \item \textit{Example Metrics}:
                \begin{itemize}
                    \item R-squared: Proportion of variance explained.
                    \item MAE: Average absolute differences.
                \end{itemize}
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Interpretation}
        \begin{itemize}
            \item Understand and explain regression coefficients.
            \begin{itemize}
                \item \textit{Key Point}: Coefficients indicate expected changes in the dependent variable.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Regression Analysis - Assumptions and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{5}
        \item \textbf{Assumptions Checking}
        \begin{itemize}
            \item \textbf{Linearity}: Check the linear relationship between predictors and outcome.
            \item \textbf{Homoscedasticity}: Verify constant variance in residuals.
            \item \textbf{Normality of Residuals}: Ensure residuals are approximately normally distributed.
        \end{itemize}
        
        \item \textbf{Conclusion}
        \begin{itemize}
            \item Follow best practices to enhance the reliability and interpretability of regression models, balancing complexity and simplicity for meaningful insights.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet for Linear Regression}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load dataset
data = pd.read_csv('housing_data.csv')

# Split data into features and target
X = data[['size', 'location']]
y = data['price']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaway}
    \begin{block}{Effective Regression Analysis}
        It is a blend of careful data preparation, thoughtful modeling, and thorough validation/interpretation processes which together guide analysts toward actionable insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Group Project Introduction - Overview}
    \begin{block}{Introduction}
        In this group project, you will apply the concepts learned in our course to tackle real-world data-driven challenges within the context of supervised learning. The project emphasizes collaboration and practical applications while requiring teamwork.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Group Project Introduction - Objectives}
    \begin{itemize}
        \item \textbf{Collaborative Learning:} Foster teamwork by solving problems together.
        \item \textbf{Application of Knowledge:} Utilize theoretical knowledge in a practical setting.
        \item \textbf{Critical Thinking:} Engage in problem-solving and critical analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How It Works}
    \begin{enumerate}
        \item \textbf{Team Formation:} Form small groups of 3-5 students.
        \item \textbf{Selection of Topic:} Choose a real-world challenge addressed with supervised learning techniques.
        \item \textbf{Data Collection:} Gather relevant datasets from sources like Kaggle or UCI Machine Learning Repository.
        \item \textbf{Project Phases:}
            \begin{itemize}
                \item Phase 1: Research existing solutions and methods.
                \item Phase 2: Data Preparation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation and Evaluation}
    \begin{enumerate}[resume]
        \item \textbf{Implementation:} 
            \begin{itemize}
                \item Model Building: Choose supervised learning algorithms.
                \item Example Concept: Predicting housing prices using linear regression.
            \end{itemize}
        \item \textbf{Evaluation:}
            \begin{itemize}
                \item Model Testing: Use cross-validation to evaluate performance.
                \item Outcome Measurement: Present results using metrics like RMSE or accuracy score.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Group Presentation}
    \begin{block}{Final Presentation}
        Each group will present their findings, methodologies, and insights gained during the project.
        \begin{itemize}
            \item Encourage use of visualization tools (charts, graphs) to illustrate findings.
            \item Highlight the importance of collaboration and practical applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts}
    \begin{block}{Engagement Opportunities}
        Embrace this project as a chance to engage creatively, challenge yourselves, and apply the learning from this course.
        \begin{itemize}
            \item Think critically and use individual strengths to achieve a common goal.
            \item Learning from feedback is essential for your development.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Tools and Resources for Implementation}
    \begin{block}{Overview}
        This slide provides an overview of tools and resources available for conducting regression analysis and data processing in the course.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Tools for Regression Analysis}
    In the realm of supervised learning, especially in regression analysis, choosing appropriate tools is vital. This section covers:
    \begin{itemize}
        \item Programming languages and libraries
        \item Integrated Development Environments (IDEs)
        \item Online resources and communities
        \item Data visualization tools
        \item Learning platforms
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Programming Languages and Libraries}
    \begin{itemize}
        \item \textbf{Python}: A versatile language for data analysis and machine learning.
        \begin{itemize}
            \item \textbf{Pandas}: For data manipulation and analysis.
            \begin{lstlisting}[language=Python]
import pandas as pd
data = pd.read_csv('data.csv')
summary = data.describe()
            \end{lstlisting}
            \item \textbf{NumPy}: For numerical operations on arrays and matrices.
            \item \textbf{Scikit-learn}: A robust library for machine learning tasks.
            \begin{lstlisting}[language=Python]
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
            \end{lstlisting}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Integrated Development Environments (IDEs)}
    \begin{itemize}
        \item \textbf{Jupyter Notebook}: An interactive tool for code execution and data visualization.
        \item \textbf{Google Colab}: A cloud-based environment offering easy access and computational power (e.g., GPUs).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Online Resources and Communities}
    \begin{itemize}
        \item \textbf{Kaggle}: A platform for competitions and dataset sharing.
        \item \textbf{Stack Overflow}: A Q\&A community for coding challenges and solutions.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Data Visualization Tools}
    \begin{itemize}
        \item \textbf{Matplotlib}: For creating a wide range of visualizations.
        \item \textbf{Seaborn}: Built on Matplotlib, it simplifies the creation of attractive statistical graphics.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Learning Platforms}
    \begin{itemize}
        \item \textbf{Coursera and edX}: Offer online courses on machine learning and regression analysis.
        \item \textbf{YouTube}: Provides tutorials and lectures on practical applications and concepts.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Choosing the right tools simplifies tasks and enhances learning.
        \item Engaging with community resources offers varied perspectives and techniques.
        \item These tools foster collaboration and support in supervised learning projects.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Summary of Key Concepts}
    
    \begin{enumerate}
        \item \textbf{Supervised Learning Basics}  
            Supervised learning involves training models on labeled data to make predictions and improve accuracy.
        
        \item \textbf{Data Quality and Preprocessing}  
            Success in supervised learning depends on:
            \begin{itemize}
                \item Data Cleaning: Removing inaccuracies
                \item Feature Selection: Identifying relevant features
                \item Data Transformation: Normalization and standardization
            \end{itemize}
        
        \item \textbf{Model Evaluation Metrics}  
            Key performance metrics include:
            \begin{itemize}
                \item Accuracy
                \item Precision and Recall
                \item F1 Score
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Key Takeaways}
    
    \begin{enumerate}
        \item \textbf{Real-World Application}  
            Supervised learning can be applied significantly in various fields. For instance:
            \begin{itemize}
                \item Finance: Credit scoring
                \item Healthcare: Disease prediction
                \item Marketing: Customer segmentation
            \end{itemize}
            \textit{Example: A model predicting loan defaults learns from historical loan data.}

        \item \textbf{Challenges in Implementation}  
            Common issues include:
            \begin{itemize}
                \item Overfitting: Learning noise instead of signals
                \item Underfitting: Failing to capture trends
            \end{itemize}
            \textit{Illustration: Overfitting is like memorizing answers; underfitting is not learning at all.}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Continuous Improvement and Reflection}
    
    \begin{enumerate}
        \item \textbf{Continuous Improvement}  
            Supervised learning is iterative:
            \begin{itemize}
                \item Monitor model performance
                \item Incorporate new data
            \end{itemize}

        \item \textbf{Ethical Considerations}  
            Recognize bias in data and its impact on decisions:
            \begin{itemize}
                \item Ensure fairness
                \item Maintain transparency
            \end{itemize}

        \item \textbf{Engaging Questions for Reflection}
            \begin{itemize}
                \item How might bias in data influence model outcomes?
                \item Identify real-world scenarios where models might perpetuate inequalities.
            \end{itemize}
    \end{enumerate}
\end{frame}


\end{document}