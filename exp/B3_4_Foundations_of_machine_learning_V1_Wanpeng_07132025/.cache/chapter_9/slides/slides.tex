\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}

% Title Page Information
\title[Unsupervised Learning Techniques]{Chapter 9: Unsupervised Learning Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning - Overview}
    
    \begin{block}{What is Unsupervised Learning?}
        Unsupervised Learning is a type of machine learning where models are trained on unlabeled data. Unlike supervised learning, the algorithm finds patterns and structures within data without explicit guidance.
    \end{block}
    
    \begin{block}{Why is Unsupervised Learning Important?}
        \begin{itemize}
            \item \textbf{Data Insights:} Discovers hidden patterns and structures in data.
            \item \textbf{Preprocessing:} Enhances data quality for supervised learning by identifying features or reducing dimensionality.
            \item \textbf{Exploration:} Aids exploratory data analysis through segmentation or clustering of data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning - Applications}
    
    \begin{block}{Real-World Applications}
        \begin{enumerate}
            \item \textbf{Customer Segmentation:} Clustering techniques like K-Means to group customers based on behavior.
            \item \textbf{Anomaly Detection:} Identifying unusual transactions for fraud detection in finance.
            \item \textbf{Genomic Data Analysis:} Discovering patterns in genetic sequences related to diseases.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Techniques in Unsupervised Learning}
        \begin{itemize}
            \item \textbf{Clustering:} Groups data points based on similarity (e.g., K-Means, Hierarchical).
            \item \textbf{Dimensionality Reduction:} Techniques like Principal Component Analysis (PCA) preserve information while reducing features.
            \item \textbf{Association Rules:} Identifies relationships in datasets (e.g., market basket analysis).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning - Key Points}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item \textbf{Unlabeled Data:} Operates independently of predefined labels, allowing for autonomous structure discovery.
            \item \textbf{Versatile Applications:} Valuable in marketing, finance, biology, and more.
            \item \textbf{Foundational:} Acts as a precursor to advanced methods like deep learning architectures employing unsupervised strategies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Discussion Prompt}
        How can you envision applying unsupervised learning techniques to the data you work with?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Unsupervised Learning vs. Supervised Learning - Overview}
    \begin{itemize}
        \item Understanding the key concepts of supervised and unsupervised learning.
        \item Differentiating between the techniques based on data requirements and learning approaches.
        \item Exploring applications and implications of using these learning types.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning}
    \begin{block}{Supervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: Training on labeled data (input + output).
            \item \textbf{Goal}: To predict outcomes for new, unseen data.
            \item \textbf{Example}: Predicting house prices based on features such as size, location, and number of rooms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Unsupervised Learning}
    \begin{block}{Unsupervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: Training on unlabeled data (only inputs provided).
            \item \textbf{Goal}: To discover patterns or structures in data.
            \item \textbf{Example}: Grouping customers by purchasing behavior without predefined categories.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences between Learning Types}
    \begin{table}[htbp]
        \centering
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Aspect} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} \\
            \hline
            Data Requirement & Requires labeled data (input + output) & Works with unlabeled data (input only) \\
            \hline
            Learning Approach & Learns function from inputs to outputs & Discovers hidden patterns or groupings \\
            \hline
            Output & Predictive outcomes (classification, regression) & Insights (clusters, associations) \\
            \hline
            Techniques & Decision Trees, Neural Networks, SVM & Clustering, Dimensionality Reduction \\
            \hline
            Use Cases & Spam detection, Credit scoring & Market segmentation, Anomaly detection \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Predict vs. Discover}: Supervised predicts outcomes, unsupervised discovers patterns.
        \item \textbf{Method of Training}: Supervised uses labeled examples, unsupervised operates independently.
        \item \textbf{Applications}: Distinct applications in fields illustrate the importance of choosing the right method.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inspiring Questions}
    \begin{itemize}
        \item How might customer behavior change in the absence of explicit labels?
        \item What could we uncover about our data when we let algorithms find the patterns themselves?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By understanding fundamental differences between supervised and unsupervised learning, you are better equipped to choose the appropriate method for diverse data scenarios, paving the way to explore deeper applications in unsupervised learning.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Unsupervised Learning - Overview}
  \begin{block}{Overview}
    Unsupervised learning is a machine learning branch focused on finding patterns in unlabeled data. It helps organizations gain valuable insights without prior outcomes.
  \end{block}
  \begin{itemize}
    \item Customer Segmentation
    \item Anomaly Detection
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Unsupervised Learning - Customer Segmentation}
  \begin{block}{Definition}
    Customer segmentation divides customers into groups based on shared characteristics, enhancing targeted marketing strategies.
  \end{block}
  
  \begin{itemize}
    \item Algorithms Used:
    \begin{itemize}
        \item K-means clustering
        \item Hierarchical clustering
    \end{itemize}
    \item \textbf{Example:}
    \begin{itemize}
        \item An e-commerce company segments customers into budget-conscious buyers, luxury customers, and impulsive shoppers.
    \end{itemize}
    \item \textbf{Outcome:} Tailored promotions can be sent to each group, improving customer satisfaction.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Unsupervised Learning - Anomaly Detection}
  \begin{block}{Definition}
    Anomaly detection identifies rare items or events that differ significantly from the majority of data, signaling potential issues.
  \end{block}
  
  \begin{itemize}
    \item Algorithms Used:
    \begin{itemize}
        \item Isolation Forest
        \item DBSCAN
    \end{itemize}
    \item \textbf{Example:}
    \begin{itemize}
        \item In financial fraud detection, unsupervised algorithms can identify unusual spending patterns, such as large transactions in different locations.
    \end{itemize}
    \item \textbf{Outcome:} Alerts can trigger investigations, potentially preventing financial losses.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Clustering Algorithms}
    \begin{block}{What is Clustering?}
        Clustering is an unsupervised learning technique used to group similar data points into clusters, allowing for the discovery of inherent structures in unlabeled data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Clustering}
    \begin{enumerate}
        \item \textbf{Organizing Data}:
        Clustering simplifies data interpretation by revealing patterns and relationships.
        
        \item \textbf{Data Summarization}:
        Reduces complexity by summarizing large datasets into fewer, representative groups.

        \item \textbf{Enhancing Decision-Making}:
        Helps businesses tailor strategies based on identified customer segments.
        
        \item \textbf{Foundation for Other Analysis}:
        Acts as a preliminary step for other analyses like classification.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Example and Key Points}
    \begin{block}{Example: Customer Segmentation}
        In retail, clustering can be applied to categorize customers based on their purchasing behavior, allowing targeted promotions for groups such as "frequent buyers" or "bargain hunters".
    \end{block}

    \begin{itemize}
        \item Clustering is essential for uncovering complex data relationships.
        \item Applicable in various domains such as marketing and healthcare.
        \item Prepares ground for actionable insights from raw data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding clustering is vital for effective data analysis. 
    In the next slides, we will explore specific clustering algorithms and their transformative potential on raw data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering}
    \begin{block}{What is K-means Clustering?}
        K-means clustering is an unsupervised learning algorithm that partitions a set of data points into K distinct clusters based on their features. The primary goal is to group similar data points together while maximizing the variance between different clusters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithm Steps - Part 1}
    \begin{enumerate}
        \item \textbf{Initialization:}
        \begin{itemize}
            \item Choose the number of clusters (K).
            \item Randomly select K data points as initial centroids.
            \item \textit{Example}: For 10 data points with K=3, randomly select 3.
        \end{itemize}
        
        \item \textbf{Assignment:}
        \begin{itemize}
            \item Assign each data point to the nearest centroid based on a distance metric.
            \item \textit{Illustration}: If Point A is closer to Centroid 1, assign it to Cluster 1.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithm Steps - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Update:}
        \begin{itemize}
            \item Recalculate the positions of K centroids by taking the mean of all data points in each cluster.
            \item \textit{Formula}:
            \begin{equation}
                \mu_k = \frac{1}{|C|} \sum_{x \in C} x
            \end{equation}
        \end{itemize}

        \item \textbf{Convergence:}
        \begin{itemize}
            \item Repeat assignment and update until centroids stabilize.
            \item \textit{Key Point}: Further iterations will not significantly change outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Practical Applications}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Flexible and Scalable}: Works efficiently on large datasets.
            \item \textbf{Distance Metric Matters}: Choice affects cluster formation.
            \item \textbf{Sensitive to Initial Centroids}: Multiple runs may be needed for optimal results.
            \item \textbf{Determine K}: Use heuristic methods like the elbow method.
        \end{itemize}
    \end{block}

    \begin{block}{Practical Applications}
        \begin{itemize}
            \item Customer segmentation in marketing.
            \item Image compression in computer vision.
            \item Anomaly detection in fraud detection systems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion & Code Snippet}
    \begin{block}{Conclusion}
        K-means clustering is fundamental in unsupervised machine learning, aiding in data analysis and revealing natural groupings.
    \end{block}

    \begin{lstlisting}[language=Python, basicstyle=\ttfamily]
from sklearn.cluster import KMeans
import numpy as np

# Sample data points
data_points = np.array([[1, 2], [1, 4], [1, 0],
                        [4, 2], [4, 4], [4, 0]])

# Running K-means
kmeans = KMeans(n_clusters=2, random_state=0).fit(data_points)

# Printing resulting labels and centroids
print("Labels:", kmeans.labels_)
print("Centroids:", kmeans.cluster_centers_)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering: A Simple Example}
    \begin{block}{What is K-means Clustering?}
        K-means is an unsupervised learning algorithm used to partition a dataset into \textbf{K distinct groups (clusters)} based on feature similarity. The goal is to minimize the variance within each cluster while maximizing the variance between different clusters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Does K-means Work?}
    \begin{enumerate}
        \item \textbf{Initialization:} Select K random data points as cluster centroids.
        \item \textbf{Assignment Step:} Assign each data point to the nearest centroid, forming K clusters.
        \item \textbf{Update Step:} Recalculate the centroid of each cluster based on the assigned points.
        \item \textbf{Convergence:} Repeat the assignment and update steps until the centroids no longer change significantly.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Example with 2D Data Points}
    \begin{block}{Initial Setup}
        \begin{itemize}
            \item \textbf{Data Points:}
                \begin{itemize}
                    \item Point A: (1, 2)
                    \item Point B: (1, 4)
                    \item Point C: (1, 0)
                    \item Point D: (10, 2)
                    \item Point E: (10, 4)
                    \item Point F: (10, 0)
                \end{itemize}
            \item \textbf{Choose K:} For this example, we use K=2.
        \end{itemize}
    \end{block}
    
    \begin{block}{Step 2: Initialization}
        Randomly select 2 points as initial centroids:
        \begin{itemize}
            \item Centroid 1 (C1): (1, 2)
            \item Centroid 2 (C2): (10, 2)
        \end{itemize}
    \end{block}

    \begin{block}{Step 3: Assignment Step}
        Assign each point to the nearest centroid, resulting in:
        \begin{itemize}
            \item Cluster 1: {A, B, C}
            \item Cluster 2: {D, E, F}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Example Continued}
    \begin{block}{Step 4: Update Step}
        Recalculate centroids for each cluster:
        \begin{itemize}
            \item New C1 = Mean of Points A, B, C = \(\left(\frac{1+1+1}{3}, \frac{2+4+0}{3}\right) = (1, 2)\)
            \item New C2 = Mean of Points D, E, F = \(\left(\frac{10+10+10}{3}, \frac{2+4+0}{3}\right) = (10, 2)\)
        \end{itemize}
    \end{block}
    
    \begin{block}{Step 5: Convergence}
        Since the centroids have not changed, we conclude the process. The final clusters are stable.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item K-means is sensitive to initial centroid placement; different initializations can lead to different clustering results.
            \item The choice of K significantly impacts the results; methods like the elbow method can help determine the best value for K.
            \item K-means works best with spherical clusters and when clusters are of similar sizes.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        K-means is a fundamental algorithm in machine learning, providing a straightforward approach to clustering. Understanding its steps and behavior can empower you to analyze complex datasets effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing K in K-means}
    \begin{itemize}
        \item K represents the number of clusters formed in K-means clustering.
        \item The choice of K is crucial as it influences clustering outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Choosing K}
    \begin{itemize}
        \item If K is too low:
        \begin{itemize}
            \item Model oversimplifies the data.
            \item Leads to poor cluster representation.
        \end{itemize}
        \item If K is too high:
        \begin{itemize}
            \item Model fits noise instead of data structure.
            \item Results in overfitting.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods for Choosing K}
    \begin{enumerate}
        \item \textbf{Elbow Method}
        \begin{itemize}
            \item Graphing inertia vs. K to find optimal clusters.
            \item Elbow point suggests ideal K. 
            \item Example: Inertia flattens after K=4.
        \end{itemize}
        \item \textbf{Silhouette Score}
        \begin{itemize}
            \item Measures similarity within clusters versus other clusters.
            \item Score range: -1 (misclassified) to +1 (well-clustered).
            \item Select K with highest average silhouette score.
        \end{itemize}
        \item \textbf{Cross-Validation}
        \begin{itemize}
            \item Dividing data into subsets and assessing K across them.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item The choice of K is vital for effective clustering.
        \item Combine methods like elbow and silhouette for stronger decisions.
        \item No single method is foolproof; often, multiple techniques are used.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Selecting K in K-means can be challenging but important.
        \item Using methods like the elbow method and silhouette score provides better insights.
        \item Understanding these methods enhances your ability to analyze complex datasets effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-means}
    K-means clustering is a widely-used unsupervised learning algorithm, but it has several notable limitations that need to be considered:
    \begin{itemize}
        \item Sensitivity to Initialization
        \item Scalability Issues
        \item Assumption of Spherical Clusters
        \item Sensitivity to Outliers
    \end{itemize}
    Understanding these limitations is crucial for effective data analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Sensitivity to Initialization}
    \begin{block}{Explanation}
        The K-means algorithm starts with randomly initialized centroids, affecting clustering outcomes.
    \end{block}
    \begin{itemize}
        \item Different initializations lead to different cluster assignments.
        \item This can result in unreliable results.
    \end{itemize}
    \begin{block}{Example}
        Clustering animals based on attributes may misclassify animals (e.g., grouping elephants with cats) if initial centroids are poorly chosen.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Scalability Issues}
    \begin{block}{Explanation}
        K-means necessitates multiple iterations to converge, which may be computationally expensive for large datasets.
    \end{block}
    \begin{itemize}
        \item Increased data points lead to longer distances computation to centroids.
        \item This results in longer processing times.
    \end{itemize}
    \begin{block}{Example}
        Running K-means on millions of user transactions in e-commerce can significantly delay insights due to processing time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Assumption of Spherical Clusters}
    \begin{block}{Explanation}
        K-means assumes clusters to be spherical and similar in size, limiting its effectiveness for non-spherical clusters.
    \end{block}
    \begin{itemize}
        \item Real-world data often contains clusters that are irregularly shaped or differently sized.
    \end{itemize}
    \begin{block}{Example}
        In image segmentation, K-means may struggle to differentiate varying regions (e.g., sky, trees) with differing shapes and densities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Sensitivity to Outliers}
    \begin{block}{Explanation}
        Outliers can significantly skew centroid positions, leading to misleading clustering results.
    \end{block}
    \begin{itemize}
        \item A single outlier can distort the overall clustering result.
    \end{itemize}
    \begin{block}{Example}
        In social media data, an outlier user with distinct posting behavior may misrepresent typical cluster patterns.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    While K-means is a powerful clustering tool, awareness of its limitations is essential for effective application:
    \begin{itemize}
        \item Initialization matters: Results vary with centroid starting points.
        \item Watch for scalability: Performance decreases with larger datasets.
        \item Cluster shapes: Best used with well-separated, spherical clusters.
        \item Outliers are problematic: They can distort centroids and results.
    \end{itemize}
    Understanding these limitations facilitates informed decision-making about clustering strategies!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    In the upcoming slide, we will explore \textbf{Hierarchical Clustering} as an alternative method that can address some of the limitations of K-means!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Introduction}
    \begin{block}{What is Hierarchical Clustering?}
        Hierarchical clustering is an unsupervised learning technique that groups similar objects into clusters based on their characteristics.
        It creates a tree-like structure, known as a dendrogram, illustrating the arrangement of clusters at different levels of granularity.
    \end{block}
    
    \begin{itemize}
        \item \textbf{No pre-defined number of clusters:} Unlike K-means, hierarchical clustering reveals the natural grouping of data.
        \item \textbf{Agglomerative vs. Divisive Methods:} Two approaches exist - agglomerative (bottom-up) and divisive (top-down).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering vs. K-means}
    \begin{enumerate}
        \item \textbf{Initialization:}
            \begin{itemize}
                \item Hierarchical Clustering: Automatically builds clusters without initial assignments.
                \item K-means: Requires initial centroids, sensitive to initial conditions.
            \end{itemize}
        \item \textbf{Flexibility:}
            \begin{itemize}
                \item Hierarchical Clustering: Produces a hierarchy, allowing exploration of varying levels of granularity.
                \item K-means: Outputs a fixed number of clusters as defined by K.
            \end{itemize}
        \item \textbf{Data Structure:}
            \begin{itemize}
                \item Hierarchical Clustering: Uses dendrograms to visualize relationships between clusters.
                \item K-means: Represents clusters with centroids, lacks detailed relationships.
            \end{itemize}
        \item \textbf{Scalability:}
            \begin{itemize}
                \item Hierarchical Clustering: Less scalable for large datasets compared to K-means.
                \item K-means: More efficient and faster due to its iterative approach.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Conclusion}
    \begin{block}{Illustrative Example}
        Imagine organizing a library of books:
        \begin{itemize}
            \item \textbf{K-means:} Categorize books into a set number of genres like Fiction, Non-fiction, Science.
            \item \textbf{Hierarchical Clustering:} Start broadly and group them into more specific categories down to titles (e.g., Fiction -> Historical Fiction, Mystery).
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Provides a comprehensive view of data relationships.
            \item Useful for exploratory data analysis when the number of clusters is unknown.
            \item Dendrograms visualize the clustering process for informed decisions.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Hierarchical clustering reveals structured relationships in data and provides flexibility, making it a powerful exploratory tool.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering Methods - Overview}
    \begin{itemize}
        \item Hierarchical clustering is a powerful unsupervised learning technique that creates a hierarchy of clusters.
        \item Useful for understanding relationships within a dataset.
        \item It can be divided into two main methods:
        \begin{itemize}
            \item Agglomerative Hierarchical Clustering (Bottom-Up Approach)
            \item Divisive Hierarchical Clustering (Top-Down Approach)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Hierarchical Clustering}
    \begin{itemize}
        \item **Concept**: Start with each data point as an individual cluster; gradually merge clusters.
        \item **Process**:
        \begin{enumerate}
            \item Each data point starts as its own cluster.
            \item Calculate distances between clusters.
            \item Merge closest clusters.
            \item Repeat until one cluster remains.
        \end{enumerate}
        \item **Distance Metrics Used**:
        \begin{itemize}
            \item Euclidean Distance
            \item Manhattan Distance
        \end{itemize}
        \item **Example**: Merging customers based on purchasing habits into clusters like "Frequent Shoppers" and "Occasional Buyers."
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Divisive Hierarchical Clustering}
    \begin{itemize}
        \item **Concept**: Begin with the entire dataset as one cluster and split it into smaller clusters.
        \item **Process**:
        \begin{enumerate}
            \item Start with one cluster containing all data points.
            \item Identify the least similar points or subgroups.
            \item Split the cluster into smaller clusters.
            \item Repeat for each newly formed cluster.
        \end{enumerate}
        \item **Example**: Splitting a customer dataset into distinct groups like "Luxury Buyers" and "Budget Shoppers."
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Use Cases}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item **Versatility**: No need to preset the number of clusters.
            \item **Representation**: Clusters visualized using dendrograms.
            \item **Interpretability**: Easy to understand hierarchical relationships.
        \end{itemize}
    \end{block}
    
    \begin{block}{Use Cases}
        \begin{itemize}
            \item Market Segmentation: Identifying customer segments.
            \item Gene Analysis: Grouping gene expression patterns.
            \item Social Network Analysis: Analyzing user interactions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrogram Representation}
    \begin{block}{Key Concept}
        Visualization of Hierarchical Clusters
    \end{block}
    A dendrogram is a tree-like diagram that visually represents the arrangement of clusters formed through hierarchical clustering methods. Its primary purpose is to illustrate how individual data points or clusters are nested within one another, demonstrating the hierarchical structure that emerges from the data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Dendrograms}
    \begin{itemize}
        \item \textbf{Structure}: Each horizontal line represents a merge between clusters, while the vertical axis indicates the distance or dissimilarity.
        \item \textbf{Leaf Nodes}: Represent individual data points (objects, persons, units of analysis).
        \item \textbf{Branches}: Signify connections between clusters, showcasing the order of merges based on similarity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Dendrograms}
    \begin{itemize}
        \item \textbf{Height of Merges}: The height indicates distance; closer merges suggest higher similarity.
        \item \textbf{Deciding on Clusters}: "Cutting" the dendrogram at a specific height determines the number of clusters.
    \end{itemize}
    
    \begin{block}{Example Usage}
    Consider a dataset of animals based on features like size, habitat, and diet. After clustering, "Lions" and "Tigers" may be merged closer than "Lions" and "Elephants", indicating distinct clusters.
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Dendrograms visualize complex relationships, aiding in exploratory data analysis.
            \item They enhance understanding of the data's hierarchical structure.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Hierarchical Clustering}
    \begin{enumerate}
        \item \textbf{Dendrogram Representation}
        \begin{itemize}
            \item Visual representation of cluster structure.
            \item \textbf{Example}: Shows similarity among flower species based on attributes like petal width and length.
        \end{itemize}

        \item \textbf{No Need for Predefined Number of Clusters}
        \begin{itemize}
            \item Flexibility to explore cluster granularity through the dendrogram.
            \end{itemize}

        \item \textbf{Sensitive to Data Distribution}
        \begin{itemize}
            \item Effective for capturing non-spherical cluster shapes.
            \item \textbf{Example}: Clusters on behavioral data in social sciences.
        \end{itemize}

        \item \textbf{Interpretability}
        \begin{itemize}
            \item Clear insights into relationships between data points simplify decision making.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Disadvantages of Hierarchical Clustering}
    \begin{enumerate}
        \item \textbf{Computational Complexity}
        \begin{itemize}
            \item Time complexity can be significant, often O(n^3) for naïve implementations.
            \item \textbf{Key Point}: Consider alternatives for large datasets.
        \end{itemize}

        \item \textbf{Sensitivity to Noise and Outliers}
        \begin{itemize}
            \item Outliers can distort clustering results.
            \item \textbf{Illustration}: Significant outliers may shift the overall clustering structure.
        \end{itemize}

        \item \textbf{Difficulty in Handling High-Dimensional Data}
        \begin{itemize}
            \item Performance may degrade due to the "curse of dimensionality."
            \item \textbf{Example}: Clusters in text data with thousands of features (words).
        \end{itemize}

        \item \textbf{Lack of Reproducibility}
        \begin{itemize}
            \item Different linkage methods yield varied clustering outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Questions}
    \begin{block}{Conclusion}
        Hierarchical clustering offers intuitive visualizations and flexible cluster determination but poses challenges in computational demand and sensitivity to outliers. Careful consideration is necessary for its application in large or noisy datasets.
    \end{block}

    \begin{block}{Engaging Questions}
        \begin{itemize}
            \item How might the interpretability of clusters in hierarchical clustering influence decision-making in real-world applications?
            \item In what scenarios do you think the advantages of hierarchical clustering outweigh its disadvantages?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Summary of Clustering Algorithms: K-means vs. Hierarchical Clustering}
    
    \begin{block}{Introduction to Clustering}
        Clustering is an unsupervised learning technique used to group similar data points together. 
        Two popular clustering methods are:
        \begin{itemize}
            \item K-means
            \item Hierarchical Clustering
        \end{itemize}
        While both aim to identify structure within unlabeled data, they do so through different approaches.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering}

    \begin{block}{Key Features}
        \begin{itemize}
            \item \textbf{Algorithm Basis:} Assigns data points to the nearest cluster centroid.
            \item \textbf{Number of Clusters:} Requires specifying the number (K) beforehand.
            \item \textbf{Distance Metric:} Typically uses Euclidean distance.
        \end{itemize}
    \end{block}
    
    \begin{block}{Advantages}
        \begin{enumerate}
            \item Speed and efficiency in handling large datasets.
            \item Simplicity and ease of implementation.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Disadvantages}
        \begin{enumerate}
            \item Requires prior knowledge of K.
            \item Sensitive to initial centroid placements.
            \item Struggles with complex cluster shapes.
        \end{enumerate}
    \end{block}

    \begin{exampleblock}{Example}
        Clustering customers based on purchasing patterns into K segments for tailored marketing.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering}

    \begin{block}{Key Features}
        \begin{itemize}
            \item \textbf{Algorithm Basis:} Builds a dendrogram using either agglomerative or divisive methods.
            \item \textbf{Cluster Count:} Determined by the hierarchy formed; no predefined number required.
            \item \textbf{Distance Metrics:} Can utilize various metrics and linkage methods.
        \end{itemize}
    \end{block}
    
    \begin{block}{Advantages}
        \begin{enumerate}
            \item Flexibility; suitable for exploratory analysis.
            \item Produces a dendrogram for better interpretability.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Disadvantages}
        \begin{enumerate}
            \item More computationally intensive than K-means.
            \item Sensitive to noise and outliers.
        \end{enumerate}
    \end{block}

    \begin{exampleblock}{Example}
        Visualizing relationships among animal species based on morphological measurements.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Side-by-Side Comparison of K-means and Hierarchical Clustering}

    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Feature} & \textbf{K-means} & \textbf{Hierarchical Clustering} \\
        \hline
        Number of Clusters & Must be specified (K) & Determined hierarchically \\
        Time Complexity & Faster, linear & Slower, quadratic \\
        Cluster Shape & Assumes spherical shapes & Can handle various shapes \\
        Interpretability & Less intuitive & Highly interpretable with dendrogram \\
        Scalability & Scales well with large data & Less scalable for large data \\
        \hline
        \end{tabular}
    \end{table}
    
    \begin{block}{Conclusion}
        Both K-means and hierarchical clustering have unique strengths and weaknesses. Choosing between them depends on dataset characteristics and specific goals of the analysis.
    \end{block}

    \begin{block}{Questions to Consider}
        \begin{itemize}
            \item When would you prefer K-means over hierarchical clustering?
            \item How does data nature influence your clustering method choice?
            \item Could combining both methods provide complementary insights?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Case Study}
    \begin{block}{Title}
        Clustering Techniques in Marketing: Segmenting Customers for Targeted Campaigns
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Clustering in Marketing}
    \begin{itemize}
        \item Clustering is an unsupervised learning method for grouping similar data points.
        \item Enables businesses to understand their customer base.
        \item Allows for tailored marketing strategies that effectively target audiences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Retail Store Customer Segmentation}
    \textbf{Context:} A retail store aims to enhance customer engagement and increase sales through personalized marketing efforts.
    
    \textbf{Objective:} Utilize clustering techniques to identify distinct customer segments based on purchasing behavior.

    \textbf{Data Collected:}
    \begin{itemize}
        \item age, gender, average spend per visit, purchase frequency, product categories purchased
        \item 10,000 customer records
    \end{itemize}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps in Clustering}
    \begin{enumerate}
        \item Data Preprocessing
            \begin{itemize}
                \item Normalize numerical features (e.g., spend and frequency).
                \item Encode categorical variables (e.g., gender).
            \end{itemize}
            
        \item Choosing a Clustering Algorithm
            \begin{itemize}
                \item K-Means Clustering selected for efficiency, K = 4 (Elbow Method).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applying the K-Means Algorithm}
    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Load the data
data = pd.read_csv('customer_data.csv')

# Preprocess the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[['Age', 'AverageSpend', 'PurchaseFrequency']])

# Apply K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
data['Cluster'] = kmeans.fit_predict(scaled_data)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Analysis of Clusters}
    \begin{itemize}
        \item \textbf{Cluster 1:} Young, low spenders (18-25 years old).
        \item \textbf{Cluster 2:} Middle-aged, high-frequency shoppers.
        \item \textbf{Cluster 3:} Senior customers, brand-focused, moderate spend.
        \item \textbf{Cluster 4:} Families, high spenders, diverse product preference.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Insights Gained}
    \begin{itemize}
        \item Tailored marketing messages for each group (e.g., discounts for Cluster 1).
        \item Optimize inventory based on cluster preferences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Actionable Insights:} Clustering transforms raw data into strategic insights.
        \item \textbf{Flexibility of Clustering:} Techniques can adapt to dataset and objectives.
        \item \textbf{Impact on Business:} Defined customer segments lead to satisfaction and increased sales.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    This case study illustrates how unsupervised learning, especially clustering, helps comprehending customer behavior and guiding marketing initiatives. Understanding distinct customer segments significantly enhances engagement and ensures relevant marketing efforts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Unsupervised Learning}
    \begin{block}{Introduction}
        Unsupervised learning techniques uncover hidden patterns in data without labeled outputs. Upcoming trends are shaping this field, focusing on advancements and their implications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Advanced Clustering Techniques}
    \begin{itemize}
        \item \textbf{New Algorithms:} 
        Traditional methods like K-Means are being enhanced with DBSCAN and hierarchical clustering, better handling of complex datasets.
        
        \item \textbf{Example:} 
        In marketing, advanced clustering identifies customer segments based on behavior rather than just demographics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Integration with Deep Learning}
    \begin{itemize}
        \item \textbf{Neural Networks:} 
        Integration of neural networks with unsupervised techniques is growing. Models like autoencoders and GANs enhance traditional methods.
        
        \item \textbf{Example:} 
        In image processing, autoencoders compress and reconstruct images to discover underlying features without labeled data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Incremental or Online Learning}
    \begin{itemize}
        \item \textbf{Real-Time Data Handling:} 
        Models that learn incrementally are in demand as data is continuously generated.
        
        \item \textbf{Example:} 
        E-commerce platforms use online learning to adjust product recommendations based on real-time browsing or purchasing behaviors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Explainable AI (XAI)}
    \begin{itemize}
        \item \textbf{Interpreting Models:} 
        Need for interpretable models enhances trust in AI as techniques become complex.
        
        \item \textbf{Example:} 
        In healthcare, understanding how features contribute to clustering patient data is crucial for treatment pathways.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Federated Learning for Privacy}
    \begin{itemize}
        \item \textbf{Decentralized Learning:} 
        Federated learning allows models to learn from decentralized data, preserving user privacy.
        
        \item \textbf{Example:} 
        In finance, transaction data used for behavior analysis is kept secure on user devices.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Reflection}
    The future of unsupervised learning is promising, driven by advancements and focus on privacy and interpretability.
    \begin{itemize}
        \item Enhanced clustering techniques improve segmentation.
        \item Deep learning integration expands processing capabilities.
        \item Incremental learning tackles continuous data challenges.
        \item Explainable AI fosters trust in models.
        \item Federated learning prioritizes privacy.
    \end{itemize}
    \begin{block}{Call to Action}
        Consider how these trends can be applied in your research or future careers!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Summary}
    In this chapter, we explored the fascinating world of unsupervised learning, which uncovers hidden patterns in unlabeled datasets.
    \begin{itemize}
        \item **Definition and Importance:** Essential for exploratory data analysis and clustering.
        \item **Common Techniques:**
        \begin{itemize}
            \item Clustering (e.g., K-Means, Hierarchical)
            \item Dimensionality Reduction (e.g., PCA, t-SNE)
        \end{itemize}
        \item **Applications:** Market Basket Analysis, Anomaly Detection.
        \item **Future Directions:** Deep learning techniques and advancements in architectures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Encouragement for Q\&A}
    Let’s engage in an interactive discussion to deepen our understanding of unsupervised learning.
    \begin{block}{Reflection Questions}
        \begin{itemize}
            \item How do you see unsupervised learning applying to real-world scenarios?
            \item Which techniques do you find most compelling, and why?
            \item Are there specific examples you’d like to explore further?
        \end{itemize}
    \end{block}
\end{frame}


\end{document}