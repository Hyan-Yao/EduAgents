\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

\title[Chapter 3: Feature Engineering]{Chapter 3: Feature Engineering}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Feature Engineering}
    \begin{block}{What is Feature Engineering?}
        Feature Engineering is the process of using domain knowledge to select, modify, or create input variables (features) for machine learning models. 
    \end{block}
    \begin{block}{Significance}
        The quality and relevance of the features used can significantly impact the performance of the model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Feature Engineering Significant?}
    \begin{enumerate}
        \item \textbf{Improves Model Performance:}
            \begin{itemize}
                \item High-quality features can lead to better predictive performance.
                \item Example: Transforming raw dates into "days since last purchase."
            \end{itemize}
        \item \textbf{Reduces Overfitting:}
            \begin{itemize}
                \item Simplifying models by selecting relevant features enhances generalization.
            \end{itemize}
        \item \textbf{Enhances Interpretability:}
            \begin{itemize}
                \item Thoughtfully engineered features improve stakeholder understanding.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Everyday Examples and Key Points}
    \begin{block}{Everyday Examples of Feature Engineering}
        \begin{itemize}
            \item \textbf{Text Data:} Converting raw text into features such as word counts or sentiment scores.
            \item \textbf{Image Data:} Utilizing features like color histograms or edge detection for image classification.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Domain knowledge is crucial for effective feature engineering.
            \item Feature engineering is an iterative process; experimentation is key.
            \item Tools like Pandas and Scikit-learn can simplify the process.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Features}
    \begin{block}{What are Features?}
        In the context of machine learning, \textbf{features} are individual measurable properties or characteristics of the data. They are the input variables used by machines to learn patterns and make predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Features in Models}
    \begin{enumerate}
        \item \textbf{Input for Learning}: Features provide the information from which machine learning algorithms learn. 
        \begin{itemize}
            \item Example: In predicting house prices, features might include square footage, number of bedrooms, location, and age of the house.
        \end{itemize}
        
        \item \textbf{Influence on Performance}: The quality and relevance of features directly affect the model's performance.
        \begin{itemize}
            \item \textbf{Good Features}: Strong correlations with the target variable lead to better predictions.
            \item \textbf{Bad Features}: Noisy or irrelevant features can confuse the model, causing poor performance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Features}
    \begin{itemize}
        \item \textbf{Numerical Features}: Continuous values such as age, income, or temperature.
        \begin{itemize}
            \item Example: "Annual income" in a dataset of customer information.
        \end{itemize}
        
        \item \textbf{Categorical Features}: Qualitative variables with a limited number of possible values.
        \begin{itemize}
            \item Example: "Car color" with values like red, blue, and green.
        \end{itemize}
        
        \item \textbf{Date/Time Features}: Represent timestamps, capturing temporal information.
        \begin{itemize}
            \item Example: "Purchase date" can help identify trends over time.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Scaling Techniques - Overview}
    \begin{block}{Introduction to Feature Scaling}
        Feature scaling is an essential preprocessing step in machine learning. It ensures that different features contribute equally to the model's performance. Techniques like Min-Max Scaling and Standardization are key for algorithms sensitive to feature scales.
    \end{block}
    
    \begin{block}{Why is Feature Scaling Important?}
        \begin{itemize}
            \item \textbf{Equal Contribution:} Prevents features with larger scales from dominating.
            \item \textbf{Improved Convergence:} Leads to faster convergence in gradient-based algorithms.
            \item \textbf{Enhanced Accuracy:} Leads to better model performance and predictive accuracy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Scaling Techniques - Min-Max Scaling}
    \begin{block}{Min-Max Scaling}
        \begin{itemize}
            \item \textbf{Description:} Transforms features to a common scale (typically between 0 and 1).
            \item \textbf{Formula:}
            \begin{equation}
            X' = \frac{X - X_{min}}{X_{max} - X_{min}}
            \end{equation}
            \item \textbf{Example:} 
            \begin{itemize}
                \item Feature: House Prices = [200, 300, 500]
                \begin{itemize}
                    \item Scaled values: 
                    \begin{itemize}
                        \item 200 becomes 0
                        \item 300 becomes 0.33
                        \item 500 becomes 1
                    \end{itemize}
                \end{itemize}
            \end{itemize}
            \item \textbf{When to Use:} Ideal for algorithms based on distance metrics while maintaining value relationships.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Scaling Techniques - Standardization}
    \begin{block}{Standardization (Z-score Normalization)}
        \begin{itemize}
            \item \textbf{Description:} Rescales features to have a mean of 0 and a standard deviation of 1.
            \item \textbf{Formula:}
            \begin{equation}
            X' = \frac{X - \mu}{\sigma}
            \end{equation}
            where $\mu$ is the mean and $\sigma$ is the standard deviation.
            \item \textbf{Example:} 
            \begin{itemize}
                \item Feature: Test Scores = [60, 70, 90]
                \begin{itemize}
                    \item Mean: $\mu = 73.33$ 
                    \item Standard Deviation: $\sigma$ (calculate)
                    \item Scaled scores: 
                    \begin{itemize}
                        \item For 60: $\frac{60 - 73.33}{\sigma}$
                        \item For 70: $\frac{70 - 73.33}{\sigma}$
                        \item For 90: $\frac{90 - 73.33}{\sigma}$
                    \end{itemize}
                \end{itemize}
            \end{itemize}
            \item \textbf{When to Use:} Useful when features have different units or unknown distributions.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Feature scaling significantly impacts model performance.
            \item Choice between techniques depends on data distribution and algorithm type.
            \item Scaled features enhance learning efficiency and convergence rates.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Min-Max Scaling - Overview}
    \begin{block}{What is Min-Max Scaling?}
        Min-Max Scaling is a feature scaling technique used to transform features to a common scale without distorting differences in the ranges of values. It transforms the data into a specified range (usually between 0 and 1).
    \end{block}
    
    \begin{block}{Why Use Min-Max Scaling?}
        \begin{itemize}
            \item \textbf{Uniformity:} Addresses the discrepancy in scales among different features.
            \item \textbf{Preservation of Relationships:} Maintains relationships between the original values.
            \item \textbf{Improved Model Performance:} Helps sensitive algorithms converge faster and yield better accuracy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Min-Max Scaling - Formula}
    \begin{block}{Formula for Min-Max Scaling}
        The formula for Min-Max Scaling is given by:
        \begin{equation}
            X' = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
        \end{equation}
        Where:
        \begin{itemize}
            \item \( X' \) = transformed value
            \item \( X \) = original value
            \item \( X_{\text{min}} \) = minimum value in the feature's range
            \item \( X_{\text{max}} \) = maximum value in the feature's range
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Min-Max Scaling - Example}
    \begin{block}{Example of Min-Max Scaling}
        Consider feature "Age" with values: [22, 30, 25, 40].
        
        \begin{enumerate}
            \item \textbf{Identify Minimum and Maximum:}
                \begin{itemize}
                    \item \( X_{\text{min}} = 22 \)
                    \item \( X_{\text{max}} = 40 \)
                \end{itemize}
            \item \textbf{Apply Min-Max Scaling:}
                \begin{itemize}
                    \item For \( X = 22: \quad X' = \frac{22 - 22}{40 - 22} = 0 \)
                    \item For \( X = 30: \quad X' = \frac{30 - 22}{40 - 22} = \frac{8}{18} \approx 0.44 \)
                    \item For \( X = 25: \quad X' = \frac{25 - 22}{40 - 22} = \frac{3}{18} \approx 0.17 \)
                    \item For \( X = 40: \quad X' = 1 \)
                \end{itemize}
        \end{enumerate}
        
        \textbf{Scaled Age values:} [0, 0.44, 0.17, 1]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Standardization - Overview}
    \begin{block}{What is Standardization?}
        Standardization is a feature scaling technique that transforms features to have a mean of zero and a standard deviation of one. 
        This is crucial for machine learning algorithms that are sensitive to the scale of input features.
    \end{block}

    \begin{block}{Importance}
        Standardization helps:
        \begin{itemize}
            \item Enhance algorithm performance,
            \item Accelerate convergence in optimization algorithms,
            \item Improve interpretability of model outputs.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Standardization - Formula}
    \begin{block}{Mathematical Representation}
        The standardized value \(X'\) is calculated as:
        \[
        X' = \frac{X - \mu}{\sigma}
        \]
        Where:
        \begin{itemize}
            \item \(X\) is the original value,
            \item \(\mu\) is the mean of the feature,
            \item \(\sigma\) is the standard deviation of the feature,
            \item \(X'\) is the standardized value.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Standardization - Example}
    \begin{block}{Illustrative Example}
        Consider a dataset with features: height (cm) and weight (kg):
        \begin{itemize}
            \item Height: [150, 160, 170, 180, 190]
            \item Weight: [50, 60, 70, 80, 90]
        \end{itemize}
        Before standardization, height may dominate due to its larger range.
    \end{block}

    \begin{block}{Step-by-Step Standardization}
        \begin{enumerate}
            \item Calculate Mean and Standard Deviation:
            \begin{itemize}
                \item Mean Height: \(\mu_h = 170\)
                \item Standard Deviation Height: \(\sigma_h = 14.1\)
                \item Mean Weight: \(\mu_w = 70\)
                \item Standard Deviation Weight: \(\sigma_w = 14.1\)
            \end{itemize}
            \item Apply Standardization:
            \begin{itemize}
                \item For Height: 
                \[
                X'_h = \frac{X_h - 170}{14.1}
                \]
                \item For Weight: 
                \[
                X'_w = \frac{X_w - 70}{14.1}
                \]
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Always standardize training data before testing data to avoid data leakage.
        \item Standardization is crucial for algorithms sensitive to feature scale, such as:
        \begin{itemize}
            \item K-Means Clustering
            \item Neural Networks
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Summary}
        Standardization normalizes feature ranges, optimizing the learning process and enhancing model interpretability.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Encoding Categorical Variables}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item Categorical data vs. numerical data
            \item Importance of encoding for model performance
            \item Types of encoding: Label Encoding and One-Hot Encoding
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{What is Categorical Data?}
    \begin{itemize}
        \item Categorical data represents characteristics divided into groups:
        \begin{itemize}
            \item \textbf{Nominal}: No natural order (e.g., colors like red, blue)
            \item \textbf{Ordinal}: Has a specific order (e.g., education levels)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Importance of Encoding}
    \begin{itemize}
        \item Machine learning models need numerical inputs; raw categorical data is not interpretable.
        \item Benefits of encoding:
        \begin{enumerate}
            \item \textbf{Model Compatibility}: Prevents errors in algorithms assuming numerical input.
            \item \textbf{Improved Performance}: Helps models recognize patterns effectively.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Impact on Model Performance}
    \begin{itemize}
        \item \textbf{Example Scenario: Car Color} 
        \begin{itemize}
            \item Without encoding: Model misinterprets Red, Blue, Green as arbitrary numbers.
            \item With One-Hot Encoding: 
            \begin{itemize}
                \item Red: 1, Blue: 0, Green: 0
                \item Treats colors independently without implying hierarchy.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of Encoding}
    \begin{itemize}
        \item \textbf{Label Encoding}: Assigns unique integers to each category.
        \item \textbf{One-Hot Encoding}: Creates binary columns for each category.
    \end{itemize}
    \begin{block}{Choosing Encoding Techniques}
        Depends on:
        \begin{itemize}
            \item Nature of variable (nominal vs. ordinal)
            \item Algorithm used
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for One-Hot Encoding}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Sample data
df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue']})

# One-Hot Encoding
encoded_df = pd.get_dummies(df, columns=['Color'], drop_first=True)
print(encoded_df)
    \end{lstlisting}
    \begin{block}{Purpose of Code}
        This snippet demonstrates how to convert categorical colors into binary columns for model training.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item Encoding techniques enhance model capabilities to analyze and predict effectively.
        \item Experimentation with different encoding can lead to improved outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{One-Hot Encoding - Introduction}
    \begin{block}{What is One-Hot Encoding?}
        One-hot encoding is a method of converting categorical variables into a format 
        that can be provided to machine learning algorithms to improve predictions. 
        It transforms each categorical value into a new categorical column and assigns 
        a 1 or 0 (True/False) value to those columns.
    \end{block}
    
    \begin{block}{Importance of One-Hot Encoding}
        \begin{itemize}
            \item \textbf{Model Compatibility:} Many ML models require numerical input; categorical data must be encoded.
            \item \textbf{Preserving Information:} It captures the relationship between categories without implying ordinal relationships.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{One-Hot Encoding - How It Works}
    \begin{block}{Example of One-Hot Encoding}
        Consider a categorical feature called \texttt{Color} with three categories: 
        \texttt{Red}, \texttt{Blue}, and \texttt{Green}. The one-hot encoding creates 
        the following binary columns:
        \begin{itemize}
            \item Color\_Red
            \item Color\_Blue
            \item Color\_Green
        \end{itemize}
        
        Examples for each color:
        \begin{itemize}
            \item If \texttt{Color} is \texttt{Red}: (1, 0, 0)
            \item If \texttt{Color} is \texttt{Blue}: (0, 1, 0)
            \item If \texttt{Color} is \texttt{Green}: (0, 0, 1)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{One-Hot Encoding - Example in Python}
    \begin{block}{Python Implementation}
        Here’s a simple example demonstrating one-hot encoding using Python and the 
        \texttt{pandas} library:
        \begin{lstlisting}[language=Python]
import pandas as pd

# Sample data
data = {
    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']
}

# Create DataFrame
df = pd.DataFrame(data)

# Apply One-Hot Encoding
one_hot_encoded_df = pd.get_dummies(df, columns=['Color'])

print(one_hot_encoded_df)
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Use Cases}
        \begin{itemize}
            \item \textbf{Machine Learning Models:} Useful in regression and classification problems.
            \item \textbf{Natural Language Processing (NLP):} Converts text data into numerical form.
            \item \textbf{Recommendation Systems:} Encodes properties like genre or category based on user preferences.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Label Encoding - Overview}
    \begin{block}{What is Label Encoding?}
        Label encoding is a technique to convert categorical variables into numerical format, essential for many machine learning algorithms that require numerical input.
    \end{block}
    \begin{itemize}
        \item Each unique category is assigned an integer.
        \item Example: For the feature "Color":
        \begin{itemize}
            \item Red $\to$ 0
            \item Green $\to$ 1
            \item Blue $\to$ 2
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Label Encoding - When to Use}
    \begin{block}{When to Use Label Encoding}
        \begin{itemize}
            \item \textbf{Ordinal Data:} Use when categories have a meaningful order.
            \begin{itemize}
                \item Example: {"Low", "Medium", "High"} can be encoded as:
                \begin{itemize}
                    \item Low $\to$ 0
                    \item Medium $\to$ 1
                    \item High $\to$ 2
                \end{itemize}
            \end{itemize}
            \item \textbf{Tree-Based Algorithms:} Useful for preprocessing data for decision trees and random forests.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Label Encoding - Limitations}
    \begin{block}{Limitations of Label Encoding}
        \begin{enumerate}
            \item \textbf{Assumption of Ordinality:} Suggests a natural order in categories, misleading for nominal features.
            \item \textbf{Impact on Distance Metrics:} Distances can be distorted leading to misleading results in algorithms like K-Nearest Neighbors.
            \item \textbf{Not Suitable for Nominal Data:} Can introduce bias for purely categorical variables without inherent order.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Label Encoding in Python}
    \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Create a sample DataFrame
data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']}
df = pd.DataFrame(data)

# Initialize the LabelEncoder
labelencoder = LabelEncoder()

# Apply Label Encoding
df['Color_Label'] = labelencoder.fit_transform(df['Color'])
print(df)
        \end{lstlisting}
    \end{block}
    \begin{block}{Output}
        \begin{verbatim}
   Color  Color_Label
0    Red            2
1  Green            1
2   Blue            0
3  Green            1
4    Red            2
        \end{verbatim}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Label encoding is beneficial for ordinal data but misrepresents nominal data.
        \item Evaluate the nature of categorical data before applying label encoding.
        \item Consider combining label encoding with other techniques (like one-hot encoding) for better results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Encoding Method}
    \begin{block}{Introduction}
        Understanding the appropriate encoding methods for categorical data is essential for machine learning performance. This slide outlines guidelines to help in the selection of encoding techniques based on the data type and model characteristics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Data}
    \begin{itemize}
        \item \textbf{Categorical Data}: Discrete values without intrinsic order (e.g., fruit types: apple, banana, orange).
        \item \textbf{Ordinal Data}: Categories with a clear order or rank (e.g., education: high school, bachelor’s, master’s).
        \item \textbf{Numerical Data}: Continuous or discrete numerical values (e.g., age, income).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Encoding Methods \& When to Use Them}
    \begin{enumerate}
        \item \textbf{Label Encoding}
            \begin{itemize}
                \item Converts categories to unique integer values.
                \item Use for ordinal data. 
                \item Example: 'cold' = 0, 'warm' = 1, 'hot' = 2.
            \end{itemize}
        
        \item \textbf{One-Hot Encoding}
            \begin{itemize}
                \item Creates binary columns for each category.
                \item Use for nominal data.
                \item Example: 'apple' = [1, 0, 0], 'banana' = [0, 1, 0], 'orange' = [0, 0, 1].
            \end{itemize}

        \item \textbf{Binary Encoding}
            \begin{itemize}
                \item Combines label and one-hot encoding strengths.
                \item Use for large categories.
                \item Example: 'red' = 00, 'blue' = 01, 'green' = 10.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Encoding Methods \& Considerations}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Target Encoding}
            \begin{itemize}
                \item Uses the mean of the target variable for encoding.
                \item Effective for high cardinality categorical features.
                \item Risk of overfitting requires careful cross-validation.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Consider}
        \begin{itemize}
            \item \textbf{Model Type}: Different models have varying sensitivities to encoding (e.g., tree-based models like Random Forests).
            \item \textbf{High Cardinality}: Choose target or binary encoding to avoid dimensionality issues.
            \item \textbf{Interpretability}: Consider how predictions may be influenced by the encoding method.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Choosing the right encoding method is essential for transforming categorical data to improve model performance. Evaluate your data's characteristics and model requirements to optimize encoding techniques, always validating your choices with proper cross-validation.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Overview}
    Feature selection is a crucial step in data preprocessing. It improves:
    \begin{itemize}
        \item Model accuracy
        \item Reduces overfitting
        \item Enhances model interpretability
    \end{itemize}
    The three primary techniques for feature selection are:
    \begin{enumerate}
        \item Filter Methods
        \item Wrapper Methods
        \item Embedded Methods
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Filter Methods}
    \begin{block}{Definition}
        Filter methods assess relevance based on intrinsic properties, independent of machine learning algorithms.
    \end{block}
    \textbf{Techniques:}
    \begin{itemize}
        \item \textbf{Correlation Coefficients}:
            \begin{itemize}
                \item Measures linear relationship strength between features and target variables.
                \item \textit{Example}: Predicting house prices with a strong correlation between size and price.
            \end{itemize}
        \item \textbf{Chi-Squared Test}:
            \begin{itemize}
                \item Checks if distributions of categorical variables differ significantly.
                \item \textit{Example}: Relationship between customer age group and purchasing behavior.
            \end{itemize}
    \end{itemize}
    \textbf{Key Point}: Filter methods are efficient in eliminating less relevant features quickly.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Wrapper and Embedded Methods}
    \textbf{Wrapper Methods:}
    \begin{block}{Definition}
        Evaluate feature subsets based on the performance of a specific model.
    \end{block}
    \textbf{Technique:}
    \begin{itemize}
        \item \textbf{Recursive Feature Elimination (RFE)}:
            \begin{itemize}
                \item Iteratively removes the least significant feature.
                \item \textit{Example}: Start with all features, remove the least important until desired number is reached.
            \end{itemize}
    \end{itemize}
    \textbf{Key Point}: Wrapper methods can provide superior accuracy but are computationally intensive.

    \bigskip

    \textbf{Embedded Methods:}
    \begin{block}{Definition}
        Perform feature selection as part of the model training process.
    \end{block}
    \textbf{Technique:}
    \begin{itemize}
        \item \textbf{Lasso Regression}:
            \begin{itemize}
                \item Uses L1 regularization to penalize and shrink some coefficients to zero.
                \item \textit{Example}: Reduces irrelevant feature weights while retaining important ones.
            \end{itemize}
    \end{itemize}
    \textbf{Key Point}: Strikes a balance between filter and wrapper methods, providing efficiency and accuracy.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection Techniques - Conclusion and Reflection}
    \textbf{Considerations:}
    \begin{itemize}
        \item Nature of your data: Categorical or numerical features?
        \item Model choice: Specific techniques may benefit certain models.
        \item Computational resources: Filter methods are less resource-intensive compared to wrapper methods.
    \end{itemize}
    \textbf{Inspiration Question}: 
    \begin{quote}
        How will the features you choose influence the decisions made by your model?
    \end{quote}
    Understanding features leads to better models and more interpretable outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Overview}
    \begin{block}{Overview of Filter Methods}
        Filter methods are feature selection techniques that assess the relevance of features independently from any machine learning model. They evaluate each feature's contribution to the target variable, allowing practitioners to filter out irrelevant or redundant features before model building.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Key Concepts}
    \begin{itemize}
        \item \textbf{Independence from Models:} Filter methods rely solely on data properties, making them computationally efficient.
        \item \textbf{Importance of Selection:} Effective feature selection enhances model performance, reduces overfitting, and improves interpretability by focusing on impactful data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Common Techniques}
    \begin{enumerate}
        \item \textbf{Chi-Squared Test (χ² Test)}
            \begin{itemize}
                \item \textbf{Concept:} Compares observed vs. expected frequencies to assess independence.
                \item \textbf{Use Case:} Applied in categorical data to determine relationships between features and targets.
                \item \textbf{Formula:}
                \begin{equation}
                \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
                \end{equation}
                \item \textbf{Example:} Evaluating if a color feature affects purchasing behavior.
            \end{itemize}
        
        \item \textbf{Correlation Coefficients}
            \begin{itemize}
                \item \textbf{Concept:} Quantifies the linear relationship between two variables.
                \item \textbf{Use Case:} Identifies redundant features by measuring correlation with the target variable.
                \item \textbf{Formula:}
                \begin{equation}
                r = \frac{n(\sum xy) - (\sum x)(\sum y)}{\sqrt{[n \sum x^2 - (\sum x)^2][n \sum y^2 - (\sum y)^2]}}
                \end{equation}
                \item \textbf{Example:} High correlation between income and expenditure suggests redundancy.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filter Methods - Practical Application}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Efficiency:} Fast and effective for large datasets.
            \item \textbf{Independence from Model Performance:} Insights gained without specific machine learning context.
            \item \textbf{Feature Performance:} High Chi-Squared or correlation scores indicate strong features.
        \end{itemize}
    \end{block}
    
    \begin{block}{Applying Filter Methods}
        Start with filter methods for feature selection. Subsequently use more complex methods for selected features, ensuring an effective layering approach in feature engineering.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrapper Methods - Overview}
    \begin{itemize}
        \item Wrapper methods are feature selection techniques that evaluate subsets of variables through model training.
        \item Unlike filter methods, they consider model performance with specific feature subsets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Feature Subset:} A selection chosen from the full set for model training.
        \item \textbf{Model Training:} Repeatedly train the model with different subsets of features.
        \item \textbf{Performance Metric:} Accuracy, precision, recall, etc., are used to evaluate performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps in Wrapper Methods}
    \begin{enumerate}
        \item \textbf{Feature Selection:} Start with the full set of features.
        \item \textbf{Model Training:} Train the model with a subset of features.
        \item \textbf{Evaluation:} Assess performance using a chosen metric.
        \item \textbf{Iteration:} Repeat with different subsets to find the optimal set.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recursive Feature Elimination (RFE)}
    \begin{block}{Overview}
        RFE is a wrapper method that removes features systematically to build a model.
    \end{block}
    \begin{itemize}
        \item \textbf{Step 1:} Rank features based on model performance.
        \item \textbf{Step 2:} Eliminate the least important feature(s).
        \item \textbf{Step 3:} Repeat until the desired number of features is reached.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of RFE}
    \begin{itemize}
        \item **Dataset with 10 features.**
        \item Step 1: Train a model on all features.
        \item Step 2: Calculate feature importance scores.
        \item Step 3: Remove the least important feature (e.g., Feature 8).
        \item Step 4: Train the model again with remaining features.
        \item Step 5: Repeat until optimal performance is achieved.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Pros and Cons of Wrapper Methods}
    \begin{itemize}
        \item \textbf{Pros:}
        \begin{itemize}
            \item Tailors feature selection to specific predictive models.
            \item Enhances model performance by trimming irrelevant features.
        \end{itemize}
        \item \textbf{Cons:}
        \begin{itemize}
            \item Computationally expensive due to repeated training.
            \item Risk of overfitting with complex models.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Wrapper methods, particularly Recursive Feature Elimination, offer an efficient model-focused approach to feature selection, enhancing accuracy while reducing complexity.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Embedded Methods - Overview}
    \begin{itemize}
        \item Embedded methods integrate feature selection within model training.
        \item Offers a holistic approach compared to wrapper and filter methods.
        \item More efficient as feature selection is part of the training routine.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Embedded Methods}
    \begin{itemize}
        \item \textbf{Model Training Integration:} 
            \begin{itemize}
                \item Combines feature selection and model training.
                \item Models identify useful features during learning.
            \end{itemize}
        \item \textbf{Regularization:} 
            \begin{itemize}
                \item Penalizes certain coefficients to enhance feature selection.
                \item Reduces overfitting, improving model performance.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Lasso Regression}
    \begin{block}{Example: Lasso Regression}
        \begin{itemize}
            \item \textbf{Lasso (Least Absolute Shrinkage and Selection Operator):} 
            \begin{itemize}
                \item Employs L1 regularization.
                \item Shrinks coefficients of less important features to zero.
            \end{itemize}
            \item \textbf{Cost Function:}
            \begin{equation}
              \text{Minimize } \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
            \end{equation}
            where \( \lambda \) is the regularization parameter.
            \item \textbf{Scenario:} Features like "number of bedrooms" and "year built" may be selected while others are excluded.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Questions}
    \begin{itemize}
        \item Embedded methods like Lasso simplify feature selection and model training.
        \item Regularization aids in enhancing the model and reducing overfitting.
        \item The choice of the regularization parameter (\( \lambda \)) is crucial and can be optimized using cross-validation.
    \end{itemize}
    \begin{block}{Engaging Questions}
        \begin{itemize}
            \item How would you determine the value of \( \lambda \) when using Lasso?
            \item In what scenarios is Lasso particularly beneficial for your dataset?
            \item Can you think of features at risk of being eliminated by Lasso but are important for predictions?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating New Features - Introduction}
    \begin{block}{Introduction to Feature Creation}
        Feature creation is a crucial step in the data preprocessing phase of a machine learning pipeline. It involves transforming existing data into new features that can improve model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating New Features - Techniques}
    \begin{itemize}
        \item \textbf{Why Create New Features?}
        \begin{itemize}
            \item Enhance Model Performance
            \item Utilize Domain Knowledge
            \item Reduce Overfitting
        \end{itemize}
        
        \item \textbf{Techniques for Creating New Features}
        \begin{enumerate}
            \item \textbf{Mathematical Transformations}
            \begin{itemize}
                \item Polynomial Features: E.g., \(x^2\)
                \item Logarithmic Transformations: Reduces skewness
                \item Interactions: E.g., body mass index from height and weight
            \end{itemize}
            \item \textbf{Domain Knowledge-Based Features}
            \begin{itemize}
                \item Custom Features: E.g., price per square foot
                \item Categorical Feature Encoding: One-hot encoding
            \end{itemize}
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating New Features - Example Code}
    \begin{block}{Example Code Snippet (Python)}
        \begin{lstlisting}[language=Python]
import pandas as pd
import numpy as np

# Sample DataFrame
data = pd.DataFrame({'age': [22, 25, 30], 'income': [30000, 50000, 70000]})

# Creating new features
data['age_squared'] = data['age'] ** 2
data['log_income'] = np.log(data['income'])
data['bmi'] = data['weight'] / (data['height'] ** 2)  # hypothetical height and weight columns

print(data)
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Conclusion}
        Creating new features is both an art and a science. By effectively using mathematical transformations and leveraging domain knowledge, you can significantly enhance your model's predictive power.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Feature Engineering Best Practices}
    \begin{block}{Importance of Feature Engineering}
        Feature Engineering is crucial in machine learning, greatly affecting model performance. High-quality features lead to improved accuracy, while poor features can yield misleading results.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Feature Engineering - Part 1}
    \begin{enumerate}
        \item \textbf{Domain Knowledge Utilization}
        \begin{itemize}
            \item Use insights from the problem domain to create meaningful features. For example, neighborhood qualities can greatly influence house price predictions.
        \end{itemize}
        
        \item \textbf{Feature Creation through Transformation}
        \begin{itemize}
            \item Employ mathematical transformations to derive new features.
            \item \textbf{Log Transformation:}
            \begin{lstlisting}[language=Python]
import numpy as np
df['log_feature'] = np.log(df['original_feature'] + 1)
            \end{lstlisting}
            \item \textbf{Polynomial Features:}
            \begin{lstlisting}[language=Python]
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for Feature Engineering - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue enumerating from the previous frame
        
        \item \textbf{Feature Selection}
        \begin{itemize}
            \item Identify the most informative features through:
            \begin{itemize}
                \item \textbf{Tree-Based Feature Importance:} Techniques like Random Forests can indicate feature significance.
                \item \textbf{Recursive Feature Elimination (RFE):} Removes less important features iteratively.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Iterative Process}
        \begin{itemize}
            \item Continue refining and testing features. Engage with the data regularly to uncover new insights and better features.
        \end{itemize}
        
        \item \textbf{Testing and Validation}
        \begin{itemize}
            \item Rigorously evaluate feature effectiveness using metrics such as precision, recall, and F1-score.
            \item \textbf{Example:}
            \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2)
model = RandomForestClassifier()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, predictions))
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Takeaways and Conclusion}
    \begin{itemize}
        \item \textbf{Iterate and Adapt:} Regularly revisit the feature set as both the model and data evolve.
        \item \textbf{Collaboration with Domain Experts:} Engage experts to gain innovative feature ideas.
        \item \textbf{Performance Monitoring:} Continuously test and validate features for ongoing improvement.
    \end{itemize}

    \begin{block}{Conclusion}
        Effective feature engineering balances creativity with technical skills. Adopting best practices and an iterative approach can significantly enhance model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Recap of Feature Engineering}
    \begin{block}{Importance of Feature Engineering}
        Feature engineering is a crucial step in the machine learning pipeline that directly impacts the accuracy and performance of models.
    \end{block}
    \begin{itemize}
        \item Involves creating, modifying, and selecting features relevant to predictive analysis.
        \item Well-engineered features can significantly improve model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Points to Remember}
    \begin{enumerate}
        \item \textbf{Definition and Importance}
        \begin{itemize}
            \item Feature engineering: Using domain knowledge to extract effective features from raw data.
            \item Impact: Better features often enhance performance more than model adjustments.
        \end{itemize}

        \item \textbf{Best Practices}
        \begin{itemize}
            \item Iterate and test: Refine features as models evolve.
            \item Leverage domain knowledge for relevant features.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Examples and Takeaway}
    \begin{enumerate}
        \item \textbf{Successful Feature Engineering Examples}
        \begin{itemize}
            \item Date features, e.g., year, month, and "is\_weekend" for seasonality.
            \item Text data transformation via TF-IDF or Word Embeddings.
            \item Categorical variables: Dummy variables or target encoding.
        \end{itemize}

        \item \textbf{Evaluation Metrics}
        \begin{itemize}
            \item Assess impacts with accuracy, precision, recall, F1-score.
        \end{itemize}
    \end{enumerate}
    \begin{block}{Inspirational Closing}
        Reflect on how few adjustments to features can transform models. Embrace experimentation and learn from data!
    \end{block}
\end{frame}


\end{document}