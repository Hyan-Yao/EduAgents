\frametitle{Rewards in MDPs - Example and Cumulative Reward}
    \textbf{Illustrative Example:}
    Imagine an agent in a simple grid world, navigating from the starting point to a goal point.
    \begin{itemize}
        \item \textbf{States (S):} Each position on the grid represents a state.
        \item \textbf{Actions (A):} The agent can move up, down, left, or right.
        \item Rewards:
        \begin{itemize}
            \item +10 reward for reaching the goal state,
            \item -1 reward for each step taken.
        \end{itemize}
    \end{itemize}

    \textbf{Cumulative Reward Calculation:}
    \begin{equation}
    G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
    \end{equation}
    Where:
    \begin{itemize}
        \item \( t \) is the current time step,
        \item \( R_t \) is the immediate reward at time \( t \),
        \item \( \gamma \) (0 â‰¤ \( \gamma \) < 1) is the discount factor.
    \end{itemize}
