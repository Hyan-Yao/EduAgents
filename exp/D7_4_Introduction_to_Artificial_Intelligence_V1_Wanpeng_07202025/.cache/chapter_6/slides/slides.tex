\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Decision Making - Overview}
    \begin{itemize}
        \item Decision making is the process of selecting a course of action from multiple alternatives.
        \item Integral to both human cognition and artificial intelligence (AI).
        \item Effective AI decision-making systems evaluate factors and outcomes in dynamic environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Decision Making - Importance in AI}
    \begin{itemize}
        \item AI applications like robotics, game playing, and autonomous vehicles rely on decision-making.
        \item Key benefits of structured decision-making in AI:
        \begin{itemize}
            \item \textbf{Efficiency}: Automates problem-solving processes.
            \item \textbf{Performance}: Optimizes actions for better outcomes.
            \item \textbf{Adaptability}: Facilitates learning from experiences to improve future decisions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Decision Making}
    \begin{enumerate}
        \item \textbf{Decision-Making Under Uncertainty}: Real-world environments contain uncertainty (e.g., unpredictable events, incomplete information).
        \item \textbf{Policy}: A strategy mapping from states to actions.
        \item \textbf{Value Function}: Measure of expected future rewards from states or state-action pairs.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes (MDPs)}
    \begin{itemize}
        \item \textbf{Definition}: An MDP is a mathematical model for decision-making sequentially in stochastic environments. It includes:
        \begin{itemize}
            \item \textbf{States (S)}: All possible configurations of the environment.
            \item \textbf{Actions (A)}: Choices available to the agent.
            \item \textbf{Transition Probabilities (P)}: Probability of reaching a new state given the current state and action.
            \item \textbf{Rewards (R)}: Feedback received after taking action from a state, guiding the agent's learning process.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning (RL)}
    \begin{itemize}
        \item RL focuses on how agents should take actions in an environment to maximize cumulative rewards.
        \item It combines MDP concepts with learning: agents explore and exploit actions to learn optimal policies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Decision Making}
    \begin{itemize}
        \item \textbf{Autonomous Driving}: 
        \begin{itemize}
            \item A self-driving car (agent) decides whether to stop or continue based on its position on the road (state), traffic conditions (transitions), and safety ratings (rewards).
        \end{itemize}
        \item \textbf{Game Playing}:
        \begin{itemize}
            \item In games like Chess, each position is a state, potential moves are actions, and outcomes (win/loss) influence learning strategies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Structured decision-making is crucial for navigating complexities in AI.
        \item Understanding MDPs forms the foundation for advanced topics like reinforcement learning.
        \item Engaging with real-world examples enhances comprehension of abstract concepts.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Diagrams}
    \begin{block}{MDP Formulation}
        An MDP is formally defined by the tuple (S, A, P, R).
    \end{block}
    \begin{block}{Optimal Policy}
        A policy $\pi^*$ maximizes the expected reward:
        \begin{equation}
            V^*(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]
        \end{equation}
        where $\gamma$ is the discount factor for future rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Markov Decision Processes (MDPs) - Definition}
    \begin{block}{Definition}
        A \textbf{Markov Decision Process (MDP)} is a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the control of a decision-maker.
        \begin{itemize}
            \item Serves as a foundation in reinforcement learning.
            \item Guides agents on how to take actions in a given state to maximize expected rewards over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Markov Decision Processes (MDPs) - Components}
    MDPs consist of four key components:
    \begin{enumerate}
        \item \textbf{States (S)}
            \begin{itemize}
                \item \textbf{Definition}: A specific situation in which an agent can find itself.
                \item \textbf{Example}: In chess, each arrangement of pieces is a different state.
            \end{itemize}
        \item \textbf{Actions (A)}
            \begin{itemize}
                \item \textbf{Definition}: Choices available to an agent for transitioning between states.
                \item \textbf{Example}: Moving a pawn or capturing a piece in chess.
            \end{itemize}
        \item \textbf{Rewards (R)}
            \begin{itemize}
                \item \textbf{Definition}: Feedback signal indicating the value of an action; can be positive (reward) or negative (penalty).
                \item \textbf{Example}: Capturing a piece could yield a reward of +1; losing a piece might incur -1.
            \end{itemize}
        \item \textbf{Transition Probabilities (P)}
            \begin{itemize}
                \item \textbf{Definition}: Likelihood of moving from one state to another given a specific action.
                \item \textbf{Mathematical Representation}:
                \[
                P(s' | s, a) = \text{Probability of reaching state } s' \text{ from state } s \text{ after taking action } a
                \]
                \item \textbf{Example}: In a dice game, rolling a die defines the probability distribution for possible outcomes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Markov Decision Processes (MDPs) - Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item MDPs are suited for environments where decisions affect future states and rewards.
            \item They underpin various practical applications, such as:
            \begin{itemize}
                \item Robotics
                \item Automated control
                \item Gaming
            \end{itemize}
            \item The Markov property indicates that the future state depends only on the current state and action.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        MDPs create a structured approach to decision-making, enabling the analysis and implementation of strategies in reinforcement learning. Understanding these components is crucial for developing algorithms for optimal choices in uncertain environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Components Explained}
    \begin{block}{Introduction to MDP Components}
        Markov Decision Processes (MDPs) model decision-making where outcomes are partly random and partly controlled. Key components include:
        \begin{itemize}
            \item States
            \item Actions
            \item Transition Dynamics
            \item Rewards
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. States (S)}
    \begin{itemize}
        \item \textbf{Definition:} Represents the current situation at a time, capturing all necessary information for decision-making.
        \item \textbf{Example:} In a grid world, each cell represents a state. For instance, (1,1) indicates the agent's current position.
    \end{itemize}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Basis of decision-making.
            \item Can be discrete or continuous.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Actions (A)}
    \begin{itemize}
        \item \textbf{Definition:} A choice made by the agent that can change the state of the environment.
        \item \textbf{Example:} In the grid world, actions include: move up, down, left, or right.
    \end{itemize}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Determined by the agent's policy.
            \item Set of actions can vary by state.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Transition Dynamics (P)}
    \begin{itemize}
        \item \textbf{Definition:} Probabilities of moving between states given an action, denoted as \( P(s' | s, a) \).
        \item \textbf{Example:} If at (1,1) and move right, there is a 90% chance to reach (1,2) and a 10% chance to stay at (1,1).
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Crucial for understanding environment unpredictability.
            \item Can model stochastic behaviors in actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Rewards (R)}
    \begin{itemize}
        \item \textbf{Definition:} Numerical value received after an action in a state, indicating the benefit, denoted as \( R(s, a) \).
        \item \textbf{Example:} If the agent moves to a goal state, it may receive +10; hitting a wall could mean -1.
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Guide the agent towards desirable outcomes.
            \item Can be shaped to encourage specific behaviors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding MDP components is essential for applying reinforcement learning techniques effectively. By defining states, actions, transition dynamics, and rewards, one can build decision-making frameworks accommodating uncertainties.

    \begin{block}{Practical Application}
        Consider programming environments for agents navigating using MDP components, such as Q-learning or policy gradients.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Properties - Learning Objectives}
    \begin{itemize}
        \item Understand the Markov property and its significance in MDPs.
        \item Define and differentiate between policies and value functions within the context of MDPs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Properties - Markov Property}
    \begin{block}{Markov Property}
        The Markov property is the foundation of Markov Decision Processes (MDP). It stipulates that the future state of a system is conditionally independent of its past states given the present state. 
    \end{block}
    \begin{itemize}
        \item \textbf{Memoryless Property:} The decision-making process relies solely on the current state (s) to predict the future.
    \end{itemize}
    \begin{block}{Example}
        \textbf{Scenario:} A player in a board game.\\
        - \textbf{Current State (s):} Position on the board.\\
        - \textbf{Action (a):} Rolling the die.\\
        - \textbf{Next State (s'):} Based only on the current position and the die roll.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Properties - Policy (π)}
    \begin{block}{Policy}
        A policy defines a strategy that dictates the behavior of an agent at each state. 
    \end{block}
    \begin{itemize}
        \item \textbf{Deterministic:} A specific action from each state (π(s) = a).
        \item \textbf{Stochastic:} A probability distribution over actions is defined (π(a|s) = P(a|s)).
    \end{itemize}
    \begin{block}{Key Point}
        Suboptimal Policies: Not all policies guarantee the best outcome; evaluation and optimization are crucial.
    \end{block}
    \begin{block}{Example}
        \textbf{Scenario:} Navigation task at a crossroad (s).\\
        The action (a) may be to go left or right based on goal or defined probabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Properties - Value Functions (V and Q)}
    \begin{block}{Value Functions}
        Value functions measure the long-term return or utility of states (V) or state-action pairs (Q).
    \end{block}
    \begin{itemize}
        \item \textbf{State Value Function (V(s)):}
        \[
        V(s) = \mathbb{E}[\text{Return} | s_t = s, \pi]
        \]
        
        \item \textbf{Action Value Function (Q(s, a)):}
        \[
        Q(s, a) = \mathbb{E}[\text{Return} | s_t = s, a_t = a, \pi]
        \]
    \end{itemize}
    \begin{block}{Key Point}
        Utility Assessment: Value functions allow agents to evaluate policies based on expected future rewards.
    \end{block}
    \begin{block}{Example}
        \textbf{Scenario:} A grid-world where being in a state influences actions leading to goals or penalties.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Properties - Summary}
    MDPs encapsulate decision problems in stochastic environments with the following properties:
    \begin{itemize}
        \item \textbf{Markov Property:} Current state contains all necessary information for future state predictions.
        \item \textbf{Policies:} Define action selection strategies at each state.
        \item \textbf{Value Functions:} Help evaluate potential returns of states and actions, guiding optimal decision-making.
    \end{itemize}
    Understanding these properties is crucial for applying MDPs effectively in reinforcement learning and various AI applications.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Solving MDPs - Introduction}
  \begin{itemize}
    \item A Markov Decision Process (MDP) models decision-making where outcomes are partially random and controllable.
    \item Goal: Find a policy that maximizes expected cumulative reward from states to actions.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Solving MDPs - Methods}
  \begin{block}{Techniques for Solving MDPs}
    Several methods to find optimal policies:
    \begin{enumerate}
      \item \textbf{Dynamic Programming (DP) Techniques}
      \item \textbf{Monte Carlo Methods}
      \item \textbf{Temporal Difference Learning}
    \end{enumerate}
  \end{block}
  
  \begin{block}{Dynamic Programming Techniques}
    \begin{itemize}
      \item Leverages the Markov property.
      \item Focuses on updating value functions to find optimal policies.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Dynamic Programming Techniques}
  \begin{block}{Value Iteration}
    \begin{equation}
      V(s) = \max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]
    \end{equation}
    Where:
    \begin{itemize}
      \item \(V(s)\): Value of state \(s\)
      \item \(P(s'|s,a)\): Probability of moving to state \(s'\)
      \item \(R(s,a,s')\): Reward function
      \item \(\gamma\): Discount factor (0 ≤ \(\gamma\) < 1)
    \end{itemize}
  \end{block}

  \begin{block}{Policy Iteration}
    \begin{itemize}
      \item \textbf{Policy Evaluation}: Compute \(V^{\pi}(s)\) for a given policy \(\pi\).
      \item \textbf{Policy Improvement}: Update policy:
      \begin{equation}
        \pi'(s) = \arg\max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]
      \end{equation}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Algorithm}
    
    \begin{block}{Understanding Value Iteration}
        Value Iteration is a dynamic programming algorithm used to compute the optimal policy and value function for a Markov Decision Process (MDP). It iteratively updates the value of each state until convergence, allowing us to find the best action to take from each state.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    
    \begin{itemize}
        \item \textbf{States (S)}: Possible situations or conditions in which the agent can find itself.
        \item \textbf{Actions (A)}: Choices available to the agent that affect its state.
        \item \textbf{Rewards (R)}: Feedback received from the environment after taking an action.
        \item \textbf{Transition Function (P)}: Describes the probability of moving from one state to another after taking an action.
        \item \textbf{Discount Factor ($\gamma$)}: A value between 0 and 1 that prioritizes immediate rewards over future rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Value Iteration Process}

    \begin{enumerate}
        \item \textbf{Initialization}:
        \[
        V(s) = 0 \quad \text{for all } s \in S
        \]

        \item \textbf{Iterative Update}:
        \[
        V_{new}(s) = R(s) + \gamma \sum_{s' \in S} P(s' | s, a) V(s')
        \]

        \item \textbf{Convergence Check}:
        \[
        \text{Stop if } \| V_{new} - V \| < \epsilon
        \]
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Calculation}

    Consider a simple MDP with states $S = \{s_1, s_2\}$, actions $A = \{a_1, a_2\}$:
    
    \begin{block}{Rewards and Transition Probabilities}
        \begin{itemize}
            \item Rewards:
                \begin{itemize}
                    \item $R(s_1) = 1$
                    \item $R(s_2) = 0$
                \end{itemize}
            \item Transition Probabilities:
                \begin{itemize}
                    \item From $s_1$ taking $a_1$: $P(s_1 | s_1, a_1) = 0.8$, $P(s_2 | s_1, a_1) = 0.2$
                    \item From $s_2$ taking $a_2$: $P(s_1 | s_2, a_2) = 1.0$
                \end{itemize}
        \end{itemize}
    \end{block}
    
    Assume a discount factor $\gamma = 0.9$.

    \textbf{Initialization:}
    \[
    V(s_1) = 0, \quad V(s_2) = 0
    \]
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Example - First Iteration}

    \begin{block}{First Iteration ($t=0$)}
        Update values:
        \[
        V_{new}(s_1) = R(s_1) + \gamma [0.8 \cdot V(s_1) + 0.2 \cdot V(s_2)] = 1 + 0.9[0.8 \cdot 0 + 0.2 \cdot 0] = 1
        \]
        
        \[
        V_{new}(s_2) = R(s_2) + \gamma [1.0 \cdot V(s_1)] = 0 + 0.9 \cdot 1 = 0.9
        \]
        
        \textbf{Update Values:}
        \[
        V(s_1) = 1, \quad V(s_2) = 0.9
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}

    \begin{itemize}
        \item \textbf{Convergence}: Value iteration guarantees convergence to the optimal value function.
        \item \textbf{Optimal Policy Extraction}: Once the value function converges, the optimal policy $\pi^*(s)$ can be derived:
        \[
        \pi^*(s) = \arg \max_a \sum_{s'} P(s' | s, a) [R(s) + \gamma V(s')]
        \]
    \end{itemize}

    Value iteration is a powerful technique in reinforcement learning for solving MDPs effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Algorithm - Key Concepts}
    \begin{itemize}
        \item \textbf{Policy} ($\pi$): A strategy that an agent follows to determine the action in a given state. It can be:
        \begin{itemize}
            \item Deterministic: Always selects the same action for a state.
            \item Stochastic: Chooses actions based on a probability distribution.
        \end{itemize}
        
        \item \textbf{Value Function} ($V(s)$): Represents the expected return starting from state $s$, and following policy $\pi$. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Overview}
    \textbf{Policy Iteration} is an algorithm that repeatedly evaluates and improves a policy until convergence. It consists of two main steps:
    
    \begin{enumerate}
        \item \textbf{Policy Evaluation}: Calculate the value function $V$ of the current policy $\pi$ using 
        \begin{equation}
            V(s) = \mathbb{E}[R + \gamma V(s')]
        \end{equation}
        where $R$ is the reward, $\gamma$ is the discount factor, and $s'$ are the successor states.
        
        \item \textbf{Policy Improvement}: Update the policy by selecting actions that maximize the expected value:
        \begin{equation}
            \pi'(s) = \arg\max_a \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V(s')]
        \end{equation}
        where $P(s'|s, a)$ is the state transition probability.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of the Policy Iteration Algorithm}
    \begin{enumerate}
        \item \textbf{Initialization}: Start with an arbitrary policy $\pi$.
        \item \textbf{Repeat Until Convergence}:
        \begin{itemize}
            \item \textbf{Policy Evaluation}: Calculate the value function for $\pi$ until it converges.
            \item \textbf{Policy Improvement}: Update the policy based on the value function. 
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Grid World Policy Iteration}
    Consider a simple 2x2 grid world where an agent can move in four directions. The process includes:
    \begin{enumerate}
        \item \textbf{Initial Policy ($\pi$)}: The agent starts by randomly selecting actions for each grid cell.
        \item \textbf{Policy Evaluation}: Calculate the expected rewards for all states based on the current policy.
        \item \textbf{Policy Improvement}: For each cell, determine the action that yields the highest expected reward using the value function.
        \item \textbf{Repeat}: Continue through policy evaluation and improvement until the policy stabilizes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Convergence}: Policy Iteration guarantees convergence to the optimal policy, typically faster than value iteration.
        \item \textbf{Exploration vs. Exploitation}: Systematic evaluation and refinement of policies.
        \item \textbf{Applications}: Used in robotics, automated decision-making, and more where MDP frameworks apply.
    \end{itemize}

    \textbf{Summary:} Policy Iteration is essential in reinforcement learning for determining optimal policies through iterative refinement based on the value function.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning}

    \begin{block}{Definition of Reinforcement Learning (RL)}
        Reinforcement Learning is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative rewards over time. It differs from supervised learning as it doesn’t rely on labeled input/output pairs but on the consequences of actions taken.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relation to Markov Decision Processes (MDPs)}

    \begin{itemize}
        \item \textbf{MDPs Overview:}
        \begin{itemize}
            \item MDPs are a mathematical framework for modeling decision-making.
            \item An MDP is defined by:
            \begin{enumerate}
                \item \textbf{States (S)}: Situations the agent can be in.
                \item \textbf{Actions (A)}: Decisions the agent can make.
                \item \textbf{Transition probabilities (P)}: Likelihood of moving from one state to another given an action.
                \item \textbf{Rewards (R)}: Feedback from the environment after taking an action.
            \end{enumerate}
        \end{itemize}

        \item \textbf{The RL Process:}
        \begin{itemize}
            \item The agent interacts with the environment in discrete time steps.
            \item Observes current state $s$, selects action $a$ based on policy $\pi$, and receives the new state $s'$ and reward $r$.
            \item The goal is to learn a policy that maximizes expected reward ($G_t$):
            \begin{equation}
                G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
            \end{equation}
            where $0 \leq \gamma < 1$ is the discount factor.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning from Interaction}

    \begin{itemize}
        \item \textbf{Trial and Error}: 
        \begin{itemize}
            \item Learning through experience.
            \item The agent refines its policy based on past experiences.
        \end{itemize}

        \item \textbf{Exploration vs. Exploitation}:
        \begin{itemize}
            \item The agent must explore new actions to discover effects or exploit known high-reward actions.
            \item Balancing these strategies is crucial for effective learning.
        \end{itemize}

        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item Adaptive learning allows RL agents to improve through feedback.
            \item Designed for dynamic environments, RL applies to robotics, gaming, and autonomous vehicles.
            \item Focuses on multi-step decision making, considering future rewards.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Reinforcement Learning - Overview}
    \begin{block}{Key Concepts}
        This slide discusses core components of reinforcement learning, focusing on:
        \begin{itemize}
            \item Agents
            \item Environments
            \item Actions
            \item Rewards
            \item Policies
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Reinforcement Learning - Key Concepts}
    \begin{enumerate}
        \item \textbf{Agent}
        \begin{itemize}
            \item \textbf{Definition:} The decision-maker that interacts with the environment.
            \item \textbf{Example:} In chess, the player selects moves.
        \end{itemize}
        
        \item \textbf{Environment}
        \begin{itemize}
            \item \textbf{Definition:} Everything the agent interacts with, responding to agent's actions.
            \item \textbf{Example:} The chessboard and pieces.
        \end{itemize}
        
        \item \textbf{Action}
        \begin{itemize}
            \item \textbf{Definition:} Choices available to the agent at any state.
            \item \textbf{Example:} Moving a pawn or casting in chess.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Reinforcement Learning - Continuing Concepts}
    \begin{enumerate}[resume]
        \item \textbf{Reward}
        \begin{itemize}
            \item \textbf{Definition:} Feedback signal indicating success in meeting objectives.
            \item \textbf{Example:} Capturing a piece for a positive reward, losing one for a negative reward.
        \end{itemize}
        
        \item \textbf{Policy}
        \begin{itemize}
            \item \textbf{Definition:} Strategy defining actions taken in given states.
            \item \textbf{Example:} A player prioritizing captures in chess.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Learning through interactions, not explicit programming.
            \item The agent-environment interplay is crucial for learning and rewards.
            \item A well-defined policy is vital for decision-making.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation}
    \begin{block}{Key Concepts}
        In reinforcement learning, agents must make choices about how to act in an environment to maximize cumulative rewards.
        This introduces the \textbf{exploration-exploitation dilemma}, a critical concept in the field.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definitions}
    \begin{itemize}
        \item \textbf{Exploration}: Trying new actions to discover their potential rewards. 
        \item \textbf{Exploitation}: Selecting the action that has the highest known reward based on current knowledge.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Dilemma}
    \begin{block}{Balancing Strategies}
        \begin{itemize}
            \item Too much exploration can lead to suboptimal performance by trying less favorable actions.
            \item Too much exploitation may prevent discovering better strategies that could yield higher rewards in the long term.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance}
    \begin{itemize}
        \item \textbf{Long-Term Learning}: A balance of both strategies is required for effective decision-making and adaptation in changing environments.
        \item \textbf{Performance Optimization}: Good exploration strategies improve overall learning efficiency and prevent getting stuck in local optima.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Consider a robot navigating a maze:
    \begin{itemize}
        \item If the robot \textbf{explores} various pathways, it might discover a shortcut (new rewards).
        \item If it \textbf{exploits} knowledge of the best-known route, it may complete the task quickly but overlook more efficient paths.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Address the Dilemma}
    \begin{enumerate}
        \item \textbf{Epsilon-Greedy Strategy}:
            \begin{itemize}
                \item With probability $\epsilon$, explore; with probability $1-\epsilon$, exploit.
                \item Example: Let $\epsilon = 0.1$ (10\% of the time explore, 90\% exploit).
            \end{itemize}
        
        \item \textbf{Softmax Selection}: 
            \begin{itemize}
                \item Actions are selected probabilistically based on estimated value.
            \end{itemize}
        
        \item \textbf{Upper Confidence Bound (UCB)}: 
            \begin{itemize}
                \item Select actions based on their upper confidence bounds considering average reward and uncertainty.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding and managing the exploration-exploitation dilemma is fundamental for effective reinforcement learning. 
    It enables agents to adapt, learn, and ultimately succeed in dynamic environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example: Epsilon-Greedy Strategy}
    \begin{lstlisting}[language=Python]
import random

def epsilon_greedy_policy(Q, epsilon):
    if random.random() < epsilon:
        # Explore: select a random action
        action = random.choice(range(len(Q)))
    else:
        # Exploit: select the best-known action
        action = np.argmax(Q)
    return action
    \end{lstlisting}
    This function implements the epsilon-greedy approach for action selection based on current Q-values. Adjust epsilon based on the desired exploration level.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Algorithms - Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand key reinforcement learning (RL) algorithms: Q-learning and SARSA.
            \item Compare and contrast the algorithms in terms of their approach to learning from the environment.
        \end{itemize}
    \end{block}

    \begin{block}{Overview}
        Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by:
        \begin{itemize}
            \item Interacting with the environment.
            \item Maximizing cumulative rewards over time.
        \end{itemize}
        Two primary algorithms are Q-learning and SARSA, differing in their strategy of updating the value of actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Algorithms - Key Algorithms}
    \begin{enumerate}
        \item \textbf{Q-Learning}
        \begin{itemize}
            \item \textbf{Type}: Off-policy learning algorithm.
            \item \textbf{Approach}: Learns the value of the optimal action without needing to follow the current policy.
            \item \textbf{Q-value Function}: Estimates future rewards of taking action \( a \) in state \( s \).

            \item \textbf{Update Formula}:
            \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
            \end{equation}
        \end{itemize}

        \item \textbf{Example}:
        \begin{itemize}
            \item Imagine a robot navigating a maze. Q-learning helps discover the best path by evaluating actions based on cumulative rewards.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Algorithms - SARSA}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{SARSA (State-Action-Reward-State-Action)}
        \begin{itemize}
            \item \textbf{Type}: On-policy learning algorithm.
            \item \textbf{Approach}: Learns the current policy's value while updating both policy and action selection.
            
            \item \textbf{Update Formula}:
            \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q(s', a') - Q(s, a) \right)
            \end{equation}
        \end{itemize}

        \item \textbf{Example}:
        \begin{itemize}
            \item Using the robot in the maze, SARSA updates actions based on the actions taken according to the current policy.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Q-Learning and SARSA}
    \begin{itemize}
        \item \textbf{Exploration vs. Exploitation}:
        \begin{itemize}
            \item Q-learning leans towards exploitation, aiming to maximize expected reward without following the current policy.
            \item SARSA maintains a balance between exploration and exploitation, updating values based on the current policy.
        \end{itemize}

        \item \textbf{Convergence to Optimal Policy}:
        \begin{itemize}
            \item Both algorithms can converge to an optimal policy with sufficient exploration but may differ in path, particularly in stochastic environments.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Reinforcement Learning - Overview}
    \begin{block}{Definition}
        Deep Reinforcement Learning (DRL) is a powerful integration of Deep Learning (DL) and Reinforcement Learning (RL). 
    \end{block}
    \begin{itemize}
        \item Enables agents to learn optimal behaviors in complex environments.
        \item Works directly with high-dimensional inputs: images, audio, and text.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Reinforcement Learning - Key Concepts}
    \begin{enumerate}
        \item \textbf{Reinforcement Learning Basics}
            \begin{itemize}
                \item Agent interacts with Environment to maximize cumulative reward.
                \item Key components: Agent, Environment, State (s), Action (a), Reward (r).
            \end{itemize}
        \item \textbf{Deep Learning}
            \begin{itemize}
                \item Utilizes neural networks to model complex patterns.
                \item Captures high-level abstractions and approximates complex functions.
            \end{itemize}
        \item \textbf{Integration}
            \begin{itemize}
                \item Enhances traditional RL methods using neural networks.
                \item Improves generalization and scalability in high-dimensional state spaces.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Deep Q-Networks (DQN)}
    \begin{block}{Algorithm}
        Combines Q-learning with Deep Learning.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Components}:
            \begin{itemize}
                \item \textbf{Experience Replay}: Stores experiences, samples them randomly during training.
                \item \textbf{Target Network}: Stabilizes learning by reducing fluctuations.
            \end{itemize}
        \item \textbf{Update Rule}:
            \begin{equation}
                Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q'(s', a') - Q(s, a) \right]
            \end{equation}
            Where:
            \begin{itemize}
                \item $\alpha$ = learning rate
                \item $\gamma$ = discount factor
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Deep Reinforcement Learning}
    \begin{itemize}
        \item \textbf{Gaming}:
            \begin{itemize}
                \item AlphaGo defeated human champions using vast gameplay data.
                \item DQNs achieved superhuman performance in various Atari 2600 games.
            \end{itemize}
        \item \textbf{Robotics}:
            \begin{itemize}
                \item Training robots for tasks via trial and error in simulations.
            \end{itemize}
        \item \textbf{Autonomous Vehicles}:
            \begin{itemize}
                \item Aiding in decision-making and vehicle dynamics control.
            \end{itemize}
        \item \textbf{Healthcare}:
            \begin{itemize}
                \item Optimizing personalized medicine treatment policies through adaptive learning.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item DRL allows for learning from raw sensory data, enhancing adaptability in complex environments.
        \item The synergy between DL and RL leads to significant breakthroughs across industries.
    \end{itemize}
    \begin{block}{Conclusion}
        Deep Reinforcement Learning is reshaping decision-making approaches, presenting both opportunities and challenges in diverse fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies in Reinforcement Learning}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the application of reinforcement learning (RL) in various domains.
            \item Analyze real-world examples of RL, focusing on games and robotics.
            \item Recognize the impact and effectiveness of RL in solving complex decision-making problems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning}
    \begin{itemize}
        \item Reinforcement Learning (RL) is a type of machine learning.
        \item An agent learns to make decisions by taking actions in an environment to maximize cumulative rewards.
        \item Crucial for domains requiring complex decision-making capabilities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Reinforcement Learning - Games}
    \begin{enumerate}
        \item \textbf{AlphaGo}
        \begin{itemize}
            \item Developed by DeepMind, defeated Lee Sedol in 2016.
            \item Key Concept: Used deep learning and RL, learning from millions of games.
            \item Technique: Monte Carlo Tree Search (MCTS) for enhanced decision-making.
        \end{itemize}

        \item \textbf{OpenAI Five}
        \begin{itemize}
            \item Played Dota 2, achieving professional-level skill.
            \item Key Concept: Large-scale distributed learning, multiple agents improving collaboratively.
            \item Outcome: Demonstrated RL’s handling of high-dimensional action spaces and team dynamics.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Reinforcement Learning - Robotics}
    \begin{enumerate}
        \item \textbf{Robot Navigation}
        \begin{itemize}
            \item Autonomous robots (e.g., Amazon Robotics) use RL for efficient navigation.
            \item Key Concept: Learn optimal paths adapting to obstacles and environments.
            \item Approach: Techniques like Q-learning or Proximal Policy Optimization (PPO).
        \end{itemize}

        \item \textbf{Robot Manipulation}
        \begin{itemize}
            \item Used in tasks such as assembly or pick-and-place operations (e.g., OpenAI, Boston Dynamics).
            \item Key Concept: Rewards for task completion lead to refined motions.
            \item Platform Example: Robotics simulation environments (OpenAI Gym, PyBullet).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Further Exploration}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Reinforcement Learning's Flexibility:} Adapts to diverse problem spaces.
            \item \textbf{Real-Time Learning:} Agents improve decision-making continuously.
            \item \textbf{Computational Power:} Combines RL with deep learning for sophisticated tasks.
        \end{itemize}
    \end{block}

    \begin{block}{Further Exploration}
        \begin{itemize}
            \item Explore mathematical foundations: Markov Decision Processes (MDPs) and value functions.
            \item Practical implementations using popular libraries like TensorFlow or PyTorch.
            \item Investigate complex use-cases in fields like finance or healthcare.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Reinforcement Learning Models}
    % Introduction to Evaluation Metrics
    Evaluating reinforcement learning (RL) models is crucial to understanding their effectiveness.
    \begin{itemize}
        \item Focus on two primary aspects: 
        \begin{itemize}
            \item \textbf{Performance}
            \item \textbf{Convergence}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Reinforcement Learning Models - Performance Metrics}
    % Performance Metrics
    Performance metrics evaluate how well an RL model performs.
    \begin{itemize}
        \item \textbf{Cumulative Reward}: Total reward collected over time.
        \begin{itemize}
            \item \textit{Example}: Rewards of 5, 10, and 15 yield a cumulative reward of \(30\).
        \end{itemize}
        \item \textbf{Average Reward}: Average reward per time step or episode.
        \begin{equation}
            \text{Average Reward} = \frac{\text{Total Reward}}{\text{Total Episodes}}
        \end{equation}
        \item \textbf{Return}: Cumulative reward from specific time step onward.
        \begin{equation}
            R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Reinforcement Learning Models - Convergence Metrics}
    % Convergence Metrics
    Convergence evaluates how quickly an RL model approaches an optimal policy.
    \begin{itemize}
        \item \textbf{Policy Convergence}: Policy does not significantly change in subsequent iterations.
        \item \textbf{Value Function Stability}: Estimates stabilize indicating minimal changes with further learning.
        \item \textbf{Episode Length}: Consistent completion in fewer steps suggests improved learning.
        \item \textbf{Training Loss}: Monitoring loss during training indicates model learning effectiveness. 
        \begin{itemize}
            \item \textit{Example Method}: Mean Squared Error (MSE) to assess prediction accuracy.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Reinforcement Learning Models - Key Points and Conclusion}
    % Key Points and Conclusion
    \begin{block}{Key Points}
        \begin{itemize}
            \item Balancing exploration and exploitation is crucial for model evaluation.
            \item Benchmarking against baselines provides context for performance assessment.
            \item Consistent evaluation across episodes helps track learning trends.
        \end{itemize}
    \end{block}
    
    \textbf{Conclusion:} Utilizing appropriate metrics is essential for effective evaluation of RL models, enhancing learning capabilities across applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Future Directions in MDPs and Reinforcement Learning}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Identify key challenges in the application of MDPs and reinforcement learning (RL).
            \item Understand concepts of scalability and sample efficiency in RL.
            \item Explore potential future directions in improving RL methods.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - MDPs}
    \begin{itemize}
        \item \textbf{Markov Decision Processes (MDPs)}:
        \begin{itemize}
            \item MDPs model decision-making with partly random outcomes.
            \item Defined by:
            \begin{itemize}
                \item A set of states \( S \)
                \item A set of actions \( A \)
                \item A transition function \( P(s'|s,a) \)
                \item A reward function \( R(s,a) \)
                \item A discount factor \( \gamma \)
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MDPs and Reinforcement Learning}
    \begin{block}{Scalability}
        \begin{itemize}
            \item As state and action spaces grow, the number of possible state-action pairs increases exponentially.
            \item Example: A chess game has a vast number of possible states, leading to impractically large MDPs.
            \item Solutions: Function approximation techniques (e.g., deep learning).
        \end{itemize}
    \end{block}
    
    \begin{block}{Sample Efficiency}
        \begin{itemize}
            \item Refers to the interactions required to learn effective policies.
            \item RL algorithms often need large data to converge, which is costly or time-consuming.
            \item Example: Training a robot in a maze can require thousands of trials.
            \item Solutions: Experience replay and transfer learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in RL}
    \begin{itemize}
        \item \textbf{Hierarchical Reinforcement Learning}:
        \begin{itemize}
            \item Develop multi-level abstraction approaches to handle complex tasks.
        \end{itemize}
        \item \textbf{Robustness and Generalization}:
        \begin{itemize}
            \item Create models that generalize across environments, not just specific tasks.
        \end{itemize}
        \item \textbf{Integration with Other Learning Paradigms}:
        \begin{itemize}
            \item Combine RL with supervised, unsupervised, or imitation learning.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Application of MDPs}
    \begin{itemize}
        \item A robot's MDP could include:
        \begin{itemize}
            \item State: Position, Velocity
            \item Actions: Move Up, Down, Left, Right
            \item Transitions: Learning through generalized actions across similar states.
        \end{itemize}
        \item Key Importance:
        \begin{itemize}
            \item Highlights scalability and sample efficiency in real-world applications.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Addressing scalability and sample efficiency enhances the effectiveness of MDPs and RL.
        \item Future research directions pave the way for innovative solutions in AI and autonomous systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Overview}
  \begin{block}{Overview of Key Concepts in MDPs and Reinforcement Learning}
    \begin{itemize}
      \item \textbf{Markov Decision Process (MDP)}
      \begin{itemize}
        \item Provides a framework for decision-making in uncertain environments.
        \item Components include states (S), actions (A), transition function (T), reward function (R), and policy (π).
      \end{itemize}
      \item \textbf{Reinforcement Learning (RL)}
      \begin{itemize}
        \item A subset of machine learning where agents learn through interaction.
        \item Key elements include the agent, environment, exploration vs. exploitation, value functions, and Q-learning.
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Importance}
  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item \textbf{Importance in AI:}
      \begin{itemize}
        \item Fundamental for developing intelligent systems that adapt and make autonomous decisions.
        \item Applications in various fields like robotics, finance, and gaming.
      \end{itemize}
      \item \textbf{Challenges:}
      \begin{itemize}
        \item \textit{Scalability:} Handling large state and action spaces effectively.
        \item \textit{Sample Efficiency:} Learning effectively with minimal interactions.
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Illustrative Example}
  \begin{block}{Example Scenario: Robot Navigation}
    \begin{itemize}
      \item \textbf{State (S):} Robot's position in a grid.
      \item \textbf{Action (A):} Move left, right, up, down.
      \item \textbf{Transition (T):} Probabilities of reaching new positions.
      \item \textbf{Reward (R):} Positive for success, negative for hitting obstacles.
      \item \textbf{Policy (π):} Move towards the most rewarding neighboring position.
    \end{itemize}
  \end{block}
  
  \begin{equation}
  V^*(s) = \max_{a \in A}\left[ R(s, a) + \gamma \sum_{s'} T(s, a, s')V^*(s') \right]
  \end{equation}
  \begin{itemize}
    \item \textit{Where $\gamma$ is the discount factor for future rewards.}
  \end{itemize}
\end{frame}


\end{document}