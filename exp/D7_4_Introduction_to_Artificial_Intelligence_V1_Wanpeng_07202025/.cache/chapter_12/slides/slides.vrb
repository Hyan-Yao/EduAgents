\frametitle{Key Concepts of TD Learning}
    \begin{itemize}
        \item \textbf{Learning from Experience:}
            TD learning updates value estimates based on differences between predicted and actual rewards over time.
        \item \textbf{Bootstrapping:}
            TD methods use existing knowledge to enhance future value estimates, leading to efficient learning.
        \item \textbf{TD Error:}
            The error in value estimation, given by:
            \begin{equation}
                \delta_t = R_t + \gamma V(S_{t+1}) - V(S_t)
            \end{equation}
            where:
            \begin{itemize}
                \item $R_t$: Immediate reward after taking action in state $S_t$
                \item $\gamma$: Discount factor for future rewards (0 â‰¤ $\gamma$ < 1)
                \item $V(S_t)$: Current estimate of state $S_t$
                \item $V(S_{t+1})$: Estimated value of next state $S_{t+1}$
            \end{itemize}
    \end{itemize}
