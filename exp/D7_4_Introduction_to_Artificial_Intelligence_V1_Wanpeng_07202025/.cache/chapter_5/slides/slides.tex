\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Probabilistic Reasoning}
    Probabilistic reasoning is a process of drawing conclusions or making decisions based on uncertain or incomplete information using probability theory.
    
    It is crucial in AI for managing uncertainty and improving prediction capabilities.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance in AI}
    \begin{itemize}
        \item \textbf{Handling Uncertainty:} 
        Many real-world situations, like weather forecasts, involve uncertainty. Probabilistic reasoning allows AI systems to quantify and manage this uncertainty.
        
        \item \textbf{Informed Decision-Making:} 
        Applications such as medical diagnosis and autonomous driving depend on probabilistic models to assess outcomes based on evidence.
        
        \item \textbf{Adaptability:} 
        AI systems can update their beliefs with new evidence, leveraging techniques like Bayesian updating.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Probabilistic Reasoning}
    \begin{enumerate}
        \item \textbf{Probabilistic Models:}
        Represent knowledge under uncertainty. Common models include:
            \begin{itemize}
                \item \textbf{Bayesian Networks:} 
                Directed acyclic graphs representing variables and their dependencies.
                \item \textbf{Markov Models:} 
                Depend only on the current state for future predictions.
            \end{itemize}

        \item \textbf{Real-world Application:} 
        E.g., spam detection assigns probabilities to emails based on features such as keywords.

        \item \textbf{Probabilistic Inference:} 
        Making predictions based on probabilities, learning patterns from data (e.g., Monte Carlo simulations, Expectation-Maximization).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Medical Diagnosis}
    Consider a scenario where:
    \begin{itemize}
        \item A doctor needs to assess the probability of a patient having a disease based on symptoms and test results.
        \item By applying Bayes' Theorem, the doctor integrates prior and conditional probabilities to refine their diagnosis:
    \end{itemize}

    \begin{equation}
    P(Disease \mid Test) = \frac{P(Test \mid Disease) \cdot P(Disease)}{P(Test)}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Probabilistic reasoning plays an essential role in artificial intelligence, facilitating intelligent handling of uncertainty. 
    Understanding these concepts prepares us for deeper explorations of complex probabilistic models and their comprehensive applications in AI.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamental Concepts in Probability}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand basic terms and definitions related to probability.
            \item Identify and describe events and sample spaces.
            \item Recognize the types of probability distributions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Probability - Part 1}
    \begin{block}{1. Probability}
        Probability quantifies uncertainty. It is a measure of how likely an event is to occur, represented mathematically as a number between 0 and 1.
    \end{block}
    \begin{equation}
    P(E) = \frac{\text{Number of favorable outcomes}}{\text{Total number of possible outcomes}}
    \end{equation}
    
    \begin{block}{2. Sample Space (S)}
        The sample space is the set of all possible outcomes of a random experiment.
        \begin{itemize}
            \item For a single coin toss: $S = \{\text{Heads, Tails}\}$
            \item For rolling a 6-sided die: $S = \{1, 2, 3, 4, 5, 6\}$
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Probability - Part 2}
    \begin{block}{3. Events}
        An event is a specific outcome or a set of outcomes from a random experiment. 
        \begin{itemize}
            \item \textbf{Simple Event}: An event with a single outcome (e.g., rolling a 3).
            \item \textbf{Compound Event}: An event consisting of two or more simple events (e.g., rolling an even number: $\{2, 4, 6\}$).
        \end{itemize}
    \end{block}
    
    \begin{block}{4. Probability Distributions}
        A probability distribution describes how probabilities are distributed over the values of a random variable.
        \begin{itemize}
            \item \textbf{Discrete Probability Distributions}: For variables that take on a countable number of values.
            \item \textbf{Continuous Probability Distributions}: For variables that can take any value within a given range.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Probability Distributions}
    \begin{block}{Binomial Distribution}
        Used for a fixed number of independent trials, each with two outcomes.
        \begin{equation}
        P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
        \end{equation}
        where:
        \begin{itemize}
            \item $n$: number of trials
            \item $k$: number of successes
            \item $p$: probability of success on a single trial
        \end{itemize}
    \end{block}

    \begin{block}{Normal Distribution}
        A continuous distribution defined by its mean ($\mu$) and standard deviation ($\sigma$).
        \begin{equation}
        f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Understanding foundational concepts is critical for grasping further topics in probabilistic reasoning.
            \item Probability has wide applications in fields like statistics, machine learning, and various branches of science and engineering.
        \end{itemize}
    \end{block}
    
    \begin{block}{Next Steps}
        By mastering these concepts, students will be well-prepared to explore more complex topics such as Bayesian reasoning, as introduced in the next chapter.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayesian Thinking}
    \begin{block}{Understanding Bayesian Probability}
        \begin{enumerate}
            \item \textbf{Definition:} 
                Bayesian probability is a method of statistical inference that updates the probability of a hypothesis as more evidence becomes available. It incorporates prior knowledge into the analysis.
            \item \textbf{Key Difference from Frequentist Methods:}
                \begin{itemize}
                    \item \textbf{Frequentist Approach:}
                        \begin{itemize}
                            \item Focuses on long-run frequency of events.
                            \item Parameters are considered fixed and unknown.
                            \item Hypothesis testing uses p-values (e.g., rejecting or not rejecting a null hypothesis).
                        \end{itemize}
                    \item \textbf{Bayesian Approach:}
                        \begin{itemize}
                            \item Probability reflects a degree of belief or certainty about an event.
                            \item Parameters are treated as random variables with distributions (prior, likelihood, posterior).
                            \item Updates beliefs in light of new evidence, leading to a posterior distribution.
                        \end{itemize}
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Bayesian Thinking}
    \begin{itemize}
        \item \textbf{Prior Probability (Prior):} 
            Initial belief about a hypothesis before observing data.
        \item \textbf{Likelihood:} 
            Probability of observing the data given a hypothesis.
        \item \textbf{Posterior Probability (Posterior):} 
            Updated belief about a hypothesis after observing the data.
    \end{itemize}
    
    \begin{block}{Bayes' Theorem:}
    \begin{equation}
        P(H | D) = \frac{P(D | H) \cdot P(H)}{P(D)}
    \end{equation}
    Where:
    \begin{itemize}
        \item $P(H | D)$ = Posterior probability (probability of hypothesis $H$ given data $D$)
        \item $P(D | H)$ = Likelihood (probability of data $D$ given hypothesis $H$)
        \item $P(H)$ = Prior probability (initial belief about $H$)
        \item $P(D)$ = Marginal likelihood (total probability of observing data $D$)
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Application}
    \begin{block}{Medical Diagnosis:}
        Suppose a patient has symptoms suggesting a particular disease (Hypothesis $H$). 
        \begin{itemize}
            \item Prior probability from historical prevalence suggests a $10\%$ chance the patient has the disease.
            \item Given a positive test result, the likelihood is recalculated based on the test's accuracy.
            \item Using Bayes' theorem, we can find the updated probability (posterior) that the patient has the disease, considering both prior and likelihood from the test result.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize:}
        \begin{itemize}
            \item Bayesian thinking allows for flexibility and updates beliefs based on new evidence.
            \item It contrasts sharply with frequentist methods by treating probability as a subjective measure of belief rather than a long-run frequency.
            \item Understanding Bayesian reasoning enables better decision-making in uncertain environments, particularly useful in AI, medicine, and finance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bayes' Theorem - Learning Objectives}
    \begin{enumerate}
        \item Understand the formulation of Bayes' Theorem.
        \item Apply Bayes' Theorem to real-world problems, particularly in AI.
        \item Recognize the implications of Bayesian reasoning in decision-making.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Bayes' Theorem?}
    \begin{block}{Definition}
        Bayes' Theorem provides a way to update our beliefs about the probability of an event based on new evidence. It relates the conditional and marginal probabilities of random events.
    \end{block}
    
    \begin{equation}
        P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
    \end{equation}
    \begin{itemize}
        \item \textbf{P(A|B)}: Probability of event A given that B is true (posterior).
        \item \textbf{P(B|A)}: Probability of event B given that A is true (likelihood).
        \item \textbf{P(A)}: Probability of event A (prior).
        \item \textbf{P(B)}: Probability of event B (evidence).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example: Medical Diagnosis}
    \begin{block}{Scenario}
        Determine the probability that a patient has a disease (D) given that they have tested positive (T).
    \end{block}
    
    \begin{itemize}
        \item \textbf{Given Data:}
            \begin{itemize}
                \item \textbf{P(D)}: Prevalence of disease = 0.01 (1\%)
                \item \textbf{P(T|D)}: Probability of testing positive given disease = 0.9 (90\%)
                \item \textbf{P(T|¬D)}: Probability of testing positive given no disease = 0.05 (5\%)
            \end{itemize}
        \item \textbf{Steps to Calculate:}
            \begin{itemize}
                \item Calculate \(P(T)\):
                    \[
                    P(T) = P(T|D) \cdot P(D) + P(T|¬D) \cdot P(¬D)
                    \]
                    Where \(P(¬D) = 1 - P(D) = 0.99\):
                    \[
                    P(T) = (0.9 \times 0.01) + (0.05 \times 0.99) = 0.0585
                    \]
                \item Apply Bayes' Theorem:
                    \[
                    P(D|T) = \frac{P(T|D) \cdot P(D)}{P(T)} \approx 0.1538 \text{ (or 15.38\%)}
                    \]
            \end{itemize}
        \item \textbf{Interpretation:} A positive test indicates a 15.38\% probability of having the disease.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Revising Beliefs:} Bayes' Theorem revises beliefs in light of new evidence.
        \item \textbf{Real-World Applications:} Used in medical diagnosis, spam detection, and machine learning algorithms.
        \item \textbf{Important Concept:} Results can be counterintuitive, e.g., positive tests do not always imply high likelihood of conditions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in AI}
    \begin{itemize}
        \item \textbf{Spam Filtering:} 
            \begin{itemize}
                \item Classifies emails based on word occurrences.
            \end{itemize}
        \item \textbf{Recommendation Systems:} 
            \begin{itemize}
                \item Updates user preferences based on interactions and new behavior data.
            \end{itemize}
        \item \textbf{Predictive Modeling:} 
            \begin{itemize}
                \item Adjusts predictions as new data arrives.
            \end{itemize}
    \end{itemize}
    \begin{block}{Conclusion}
        By leveraging Bayes' Theorem, AI systems can enhance informed predictions and improve decision-making capabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Bayesian Networks}
    \begin{block}{What are Bayesian Networks?}
        Bayesian networks (BNs) are graphical models that represent probabilistic relationships among a set of variables using directed acyclic graphs (DAGs). Nodes represent random variables, and edges signify conditional dependencies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structure of a Bayesian Network}
    \begin{itemize}
        \item \textbf{Nodes}: Represent random variables (e.g., CoinToss, Temperature).
        \item \textbf{Edges}: Directed edges indicate probabilistic dependencies (e.g., Weather influences Car Usage).
        \item \textbf{Conditional Probability Tables (CPTs)}: Each node has a CPT quantifying the effects of its parent nodes on its distribution.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visual Example of a Bayesian Network}
    \begin{block}{Example Components}
        \begin{itemize}
            \item \textbf{Nodes}: Weather (Sunny, Rainy), Car Usage (Yes, No), Traffic (Heavy, Light).
            \item \textbf{Edges}:
            \begin{itemize}
                \item Weather $\rightarrow$ Car Usage
                \item Weather $\rightarrow$ Traffic
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{CPT Example for Traffic}
        \begin{align*}
            P(Traffic | Weather = Sunny) &= 0.3 \text{ (Light)}, \ 0.7 \text{ (Heavy)} \\
            P(Traffic | Weather = Rainy) &= 0.9 \text{ (Light)}, \ 0.1 \text{ (Heavy)}
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points of Bayesian Networks}
    \begin{itemize}
        \item \textbf{Probabilistic Inference}: Update beliefs when new evidence appears (e.g., heavy traffic updates beliefs about weather).
        \item \textbf{Handling Uncertainty}: Useful in domains with uncertain information (e.g., medical diagnoses).
        \item \textbf{Computational Efficiency}: Algorithms and computing advancements have expanded their application in AI and machine learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayesian Networks}
    \begin{itemize}
        \item \textbf{Medical Diagnosis}: Assessing the likelihood of diseases based on symptoms and tests.
        \item \textbf{Risk Assessment}: Analyzing interdependent risks in finance or engineering.
        \item \textbf{Natural Language Processing}: Capturing relationships in language models to improve understanding and prediction.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Formula}
    Bayesian networks employ Bayes' Theorem, stated as:
    \begin{equation}
        P(H | E) = \frac{P(E | H) \cdot P(H)}{P(E)}
    \end{equation}
    Where:
    \begin{itemize}
        \item $P(H | E)$ = Posterior probability (updated belief after evidence E)
        \item $P(E | H)$ = Likelihood (probability of evidence E given hypothesis H)
        \item $P(H)$ = Prior probability (initial belief about hypothesis H)
        \item $P(E)$ = Marginal likelihood of evidence E
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Bayesian networks are essential for modeling probabilistic relationships, enabling informed decision-making in uncertain environments. Understanding their structure and applications is crucial for fields requiring analytical rigor.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Bayesian Networks - Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the fundamental components of Bayesian networks.
            \item Explain the roles of nodes, edges, and conditional probability tables (CPTs) in representing probabilistic relationships.
        \end{itemize}
    \end{block}

    \begin{block}{Overview}
        Bayesian networks are graphical models that represent a set of variables and their probabilistic dependencies through directed acyclic graphs (DAGs). Key components include:
        \begin{itemize}
            \item Nodes
            \item Edges
            \item Conditional Probability Tables (CPTs)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Bayesian Networks - Nodes and Edges}
    \begin{block}{Nodes}
        \begin{itemize}
            \item Each node represents a random variable (discrete or continuous).
            \item \textbf{Examples:} "Fever", "Cough", and "Flu".
            \item \textbf{Types:}
            \begin{itemize}
                \item Leaf Nodes: Variables with no children (e.g., "Flu").
                \item Parent Nodes: Nodes with directed edges to children (e.g., "Fever" as a parent of "Cough").
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Edges}
        \begin{itemize}
            \item Directed arrows connecting nodes, indicating influence.
            \item \textbf{Example:} Edge from "Flu" to "Fever" implies flu increases fever likelihood.
            \item \textbf{Properties:} The graph structure is a DAG, ensuring no cyclic relationships.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Bayesian Networks - CPTs and Summary}
    \begin{block}{Conditional Probability Tables (CPTs)}
        \begin{itemize}
            \item A CPT quantifies the relationship between a node and its parents.
            \item \textbf{Example:} For the node "Fever":
            \begin{itemize}
                \item P(Fever = True | Flu = True) = 0.9
                \item P(Fever = True | Flu = False) = 0.1
            \end{itemize}
            \item \textbf{Importance:} Allows calculation of the joint probability distribution of all variables.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Bayesian networks combine nodes and edges to model complex relationships.
            \item CPTs are crucial for defining probabilistic relationships, aiding in inference and decision-making.
        \end{itemize}
    \end{block}

    \begin{block}{Illustration}
        Consider a simple network: 
        \begin{center}
            \texttt{[Flu] ---> [Fever]}
            \newline
            \texttt{      \ \  /}
            \newline
            \texttt{       ---> [Cough]}
        \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating Bayesian Networks - Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the process of constructing a Bayesian Network.
            \item Familiarize with common tools and libraries used in Bayesian Network development.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating Bayesian Networks - Steps}
    \begin{enumerate}
        \item \textbf{Define the Problem Domain:}
            \begin{itemize}
                \item Identify the specific problem to solve.
                \item \textit{Example:} Analyzing customer satisfaction factors.
            \end{itemize}
        \item \textbf{Identify Variables:}
            \begin{itemize}
                \item Determine relevant influencing variables (nodes).
                \item \textit{Example:} Service Quality, Wait Time, Customer Feedback.
            \end{itemize}
        \item \textbf{Construct the Structure:}
            \begin{itemize}
                \item Establish directed edges to represent dependencies.
                \item \textit{Example Structure:}
                \begin{itemize}
                    \item Service Quality $\rightarrow$ Customer Feedback
                    \item Wait Time $\rightarrow$ Customer Feedback
                \end{itemize}
            \end{itemize}
        \item \textbf{Define Conditional Probability Tables (CPTs):}
            \begin{itemize}
                \item Specify probabilities conditional on parent nodes.
                \item \textit{Example:} If Service Quality is "Good" and Wait Time is "Short", P(Positive Customer Feedback) = 0.9.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating Bayesian Networks - Tools & Validation}
    \begin{enumerate}[resume]
        \item \textbf{Parameter Learning (if necessary):}
            \begin{itemize}
                \item Utilize data to learn network parameters (CPTs) via methods like Maximum Likelihood Estimation.
            \end{itemize}
        \item \textbf{Validation:}
            \begin{itemize}
                \item Test model accuracy with a validation dataset. Adjust structure/probabilities as needed.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Common Tools \& Libraries}
        \begin{itemize}
            \item \textbf{pgmpy:} A Python library for Bayesian Networks.
            \begin{lstlisting}
from pgmpy.models import BayesianModel
model = BayesianModel([('Quality', 'Feedback'), ('Time', 'Feedback')])
            \end{lstlisting}
            \item \textbf{Bayes Server:} A commercial tool for Bayesian Networks.
            \item \textbf{Netica:} User-friendly software for building Bayesian networks.
            \item \textbf{BNFinder:} Automatic learning of structure from data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating Bayesian Networks - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Probabilities Matter:} Accuracy hinges on the correctness of CPTs.
            \item \textbf{Iterative Process:} Refinement is often needed based on results/new information.
            \item \textbf{Applications:} Used in medical diagnosis, risk assessment, and decision-making.
        \end{itemize}
    \end{block}

    \begin{block}{Example Illustration}
        Consider a simple Bayesian Network for diagnosing a disease based on symptoms:
        \begin{center}
            \texttt{[Disease]}
            \begin{center}
                \texttt{/ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }
                \texttt{/ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }
                \texttt{[Symptom1] \hspace{2cm} [Symptom2]}
            \end{center}
        \end{center}
        In this example, the likelihood of symptoms is influenced by the disease.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating Bayesian Networks - Conclusion}
    \begin{block}{Conclusion}
        Constructing a Bayesian Network involves:
        \begin{itemize}
            \item Careful consideration of the problem domain.
            \item Establishing relationships between variables.
            \item Specifying conditional probabilities.
            \item Utilizing appropriate tools and methods for modeling.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inference in Bayesian Networks - Introduction}
    \begin{block}{Inference Overview}
        Inference in Bayesian networks involves updating our beliefs about uncertain events based on observed data. Bayesian networks are represented as directed acyclic graphs (DAGs), where:
    \end{block}
    \begin{itemize}
        \item Each node corresponds to a variable.
        \item Edges represent probabilistic relationships.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inference in Bayesian Networks - Types}
    Inference methods in Bayesian networks can be categorized into two main types:
    
    \begin{enumerate}
        \item \textbf{Exact Inference}
        \item \textbf{Approximate Inference}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exact Inference}
    \begin{block}{Definition}
        Exact inference computes the exact posterior probabilities given observed evidence.
    \end{block}
    \begin{itemize}
        \item \textbf{Methods:}
            \begin{itemize}
                \item Variable Elimination
                \item Belief Propagation
            \end{itemize}
        \item \textbf{Example:}
            If we observe it’s raining, compute:
            \[
            P(\text{Picnic} | \text{Rainy})
            \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Approximate Inference}
    \begin{block}{Definition}
        Approximate inference provides a way to compute probabilities when exact methods are impractical.
    \end{block}
    \begin{itemize}
        \item \textbf{Methods:}
            \begin{itemize}
                \item Monte Carlo Sampling
                \item MCMC (Markov Chain Monte Carlo)
            \end{itemize}
        \item \textbf{Example:}
            Use Monte Carlo to estimate:
            \[
            P(\text{Picnic} | \text{Rainy})
            \]
            by simulating scenarios of weather and traffic.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Choice between exact and approximate inference depends on network size and computational resources.
            \item Exact methods guarantee accuracy but may slow down for large networks.
            \item Approximate methods are faster but may compromise accuracy.
            \item Applications include medical diagnosis, risk assessment, and decision-making.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding inference in Bayesian networks is crucial for applications involving uncertainty and probabilistic reasoning. The choice of inference method significantly influences analysis outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayesian Networks - Overview}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the diverse applications of Bayesian networks in various fields.
            \item Analyze how Bayesian networks enhance decision-making processes through probabilistic reasoning.
        \end{itemize}
    \end{block}
    
    \begin{block}{Overview}
        Bayesian networks are powerful graphical models that represent a set of variables and their conditional dependencies via a directed acyclic graph (DAG). They are invaluable in various real-world scenarios due to their ability to handle uncertainty and provide probabilistic inference.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayesian Networks - Key Applications}
    \begin{enumerate}
        \item \textbf{Medical Diagnosis:}
        \begin{itemize}
            \item Integrates patient data and prior knowledge of disease prevalence.
            \item Example: Diagnosing respiratory diseases using symptoms (cough, fever) and lab results (X-rays).
        \end{itemize}
        
        \item \textbf{Financial Risk Assessment:}
        \begin{itemize}
            \item Models relationships between financial variables for risk management.
            \item Example: Evaluating loan default risk based on credit score and income.
        \end{itemize}
        
        \item \textbf{Predictive Maintenance:}
        \begin{itemize}
            \item Predicts equipment failure and optimizes maintenance schedules.
            \item Example: Sensors collect data to analyze failure probabilities.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Bayesian Networks - Additional Applications}
    \begin{enumerate}[resume]
        \item \textbf{Natural Language Processing (NLP):}
        \begin{itemize}
            \item Enhances understanding of languages in AI systems.
            \item Example: Spam detection using email features to classify messages.
        \end{itemize}
        
        \item \textbf{Recommendation Systems:}
        \begin{itemize}
            \item Models user behavior to provide tailored recommendations.
            \item Example: Netflix recommends shows based on viewing history.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Conclusion}
        Bayesian networks provide significant advantages across various domains. Their structured dependency representation and ability to manage uncertainty enhance decision-making processes profoundly.
    \end{block}
    
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item Structured Representation
            \item Probabilistic Inference
            \item Widely Applicable
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Bayesian Networks}
    \begin{block}{Learning Objectives}
        \begin{enumerate}
            \item Identify the common challenges faced in utilizing Bayesian Networks in AI.
            \item Recognize the limitations that may restrict the effectiveness of Bayesian Networks.
            \item Assess strategies to mitigate these challenges in real-world applications.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations of Bayesian Networks}
    
    Bayesian networks (BNs) are powerful tools for probabilistic reasoning and decision-making under uncertainty, but they do face several challenges:
    
    \begin{itemize}
        \item \textbf{Complexity of Structure}
        \begin{itemize}
            \item Designing a BN requires careful consideration of relationships between variables.
            \item Increased variables result in a complex structure.
            \item \emph{Example:} New symptoms in medical diagnosis complicate dependency definitions.
        \end{itemize}
        
        \item \textbf{Data Requirements}
        \begin{itemize}
            \item BNs require substantial data to estimate conditional probabilities accurately.
            \item Sparse data may lead to unreliable models.
            \item \emph{Example:} Diagnostic models for rare diseases struggle with limited patient data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Challenges of Bayesian Networks}
    
    Continuing with the challenges:
    
    \begin{itemize}
        \item \textbf{Assumption of Independence}
        \begin{itemize}
            \item BNs assume conditional independence which may not hold in practice.
            \item \emph{Example:} Friendships in a social network can create dependencies.
        \end{itemize}
        
        \item \textbf{Learning Challenges}
        \begin{itemize}
            \item Structure and parameter learning from data can be computationally intensive.
            \item \emph{Example:} Algorithms like K2 become inefficient with many nodes.
        \end{itemize}
        
        \item \textbf{Interpretability and Usability}
        \begin{itemize}
            \item Probability distributions may be unintuitive for non-experts.
            \item Domain specialists may struggle with the significance of relationships without training.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conditional Probability}
    
    \begin{block}{Summary}
        Bayesian networks offer a robust framework for reasoning under uncertainty; however, they face significant challenges, including:
        \begin{itemize}
            \item Structural complexity
            \item Data requirements
            \item Independence assumptions
            \item Learning efficiency
            \item Interpretability
        \end{itemize}
        Awareness of these challenges is essential for effective real-world application.
    \end{block}
    
    \begin{block}{Formula for Conditional Probability}
        In Bayesian networks, the joint probability of a set of variables can be expressed as:
        \begin{equation}
            P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i \mid \text{Parents}(X_i))
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other Approaches - Overview}
    \begin{itemize}
        \item Bayesian Networks (BNs) and Markov Networks (MNs) are key frameworks for modeling uncertainties.
        \item Understanding differences between these approaches is crucial for model selection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other Approaches - Key Concepts}
    
    \begin{block}{Bayesian Networks}
        \begin{itemize}
            \item \textbf{Definition:} Directed acyclic graph (DAG) representing random variables and conditional dependencies.
            \item \textbf{Functionality:} Encodes probabilistic relationships through conditional probability tables (CPTs).
            \item \textbf{Inference:} Efficient belief updates using Bayes' theorem.
        \end{itemize}
    \end{block}

    \begin{block}{Markov Networks}
        \begin{itemize}
            \item \textbf{Definition:} Undirected graph representing dependencies among variables.
            \item \textbf{Functionality:} Variables are conditionally independent given their neighbors.
            \item \textbf{Inference:} Uses belief propagation and Gibbs sampling.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other Approaches - Key Comparisons}

    \begin{enumerate}
        \item \textbf{Structure:}
            \begin{itemize}
                \item BNs: Directed graphs (causal relationships).
                \item MNs: Undirected graphs (symmetrical relationships).
            \end{itemize}
            
        \item \textbf{Conditional Independence:}
            \begin{itemize}
                \item BNs: Depend on directed paths.
                \item MNs: Depend on neighbors.
            \end{itemize}
            
        \item \textbf{Use Cases:}
            \begin{itemize}
                \item BNs: Suitable for explicit causal modeling (e.g., medical diagnosis).
                \item MNs: Better for symmetric relationships (e.g., image analysis).
            \end{itemize}
            
        \item \textbf{Inference Algorithms:}
            \begin{itemize}
                \item BNs: Variable elimination, junction tree algorithm.
                \item MNs: Belief propagation and Markov Chain Monte Carlo methods.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison with Other Approaches - Examples and Conclusion}
    
    \begin{block}{Examples}
        \textbf{Bayesian Network Example:}
        \begin{itemize}
            \item Nodes: Fever, Cough, Flu.
            \item Relationships: Flu $\to$ Fever, Flu $\to$ Cough.
        \end{itemize}
        
        \textbf{Markov Network Example:}
        \begin{itemize}
            \item Nodes: Pixel values in an image.
            \item Relationships: Each pixel influenced by adjacent pixel values.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Both BNs and MNs are powerful for probabilistic reasoning. The choice depends on desired dependency representation and specific problem requirements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Utilizing Bayesian Networks for Decision Making - Introduction}
    \begin{block}{Definition}
        A Bayesian network (or belief network) is a graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Each node represents a variable, and the directed edges represent relationships and dependencies.
    \end{block}

    \begin{block}{Purpose}
        Bayesian networks are useful for reasoning under uncertainty, allowing users to:
        \begin{itemize}
            \item Make predictions
            \item Understand how variables influence each other
            \item Make informed decisions based on available data
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision-Making Process Using Bayesian Networks}
    \begin{enumerate}
        \item \textbf{Model Construction}
            \begin{itemize}
                \item Identify relevant variables for the decision problem.
                \item Structure the graph to establish relationships among variables.
                \item Specify conditional probabilities (e.g., $P(\text{Disease} | \text{Symptom A}, \text{Risk Factor}) = 0.8$).
            \end{itemize}
        \item \textbf{Data Input}
            \begin{itemize}
                \item Gather data relevant to the variables. 
                \item Use new evidence to influence state.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision-Making Process Using Bayesian Networks (Cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Inference}
            \begin{itemize}
                \item Update probabilities using Bayesian inference, employing methods like:
                \begin{itemize}
                    \item Variable Elimination
                    \item Belief Propagation
                \end{itemize}
            \end{itemize}
        \item \textbf{Decision Making}
            \begin{itemize}
                \item Evaluate updated probabilities to guide decisions (e.g., proceed with diagnostic tests).
                \item Consider utility functions and cost-benefit analyses.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Applications}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Flexibility}: Accommodates diverse data types and updates dynamically.
            \item \textbf{Interpretability}: Visualizes relationships and influences, aiding in understanding.
            \item \textbf{Real-World Applications}: Includes medical diagnosis, risk assessment, and classification in AI.
        \end{itemize}
    \end{block}

    \begin{block}{Example Application}
        Medical Diagnosis: Model symptoms (e.g., fever), risk factors, and diseases to inform decision-making based on probabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Bayesian networks offer a powerful method for informed decision-making under uncertainty. They enable analytical skill enhancement critical across various professional fields, promoting optimized decisions and predictive capabilities.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Probabilistic Reasoning}
    \begin{block}{Introduction}
        Probabilistic reasoning leverages probability theory to model and quantify uncertainty. It aids in making informed decisions under ambiguity.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Emerging Trends in Probabilistic Reasoning - Part 1}
    \begin{enumerate}
        \item \textbf{Integration with Machine Learning}
            \begin{itemize}
                \item Combining probabilistic models with machine learning systems.
                \item \textbf{Example:} Bayesian deep learning for image classification indicating confidence levels.
            \end{itemize}
        
        \item \textbf{Graphical Models and Explainability}
            \begin{itemize}
                \item Growing demand for explainable AI (XAI).
                \item Bayesian networks enhance interpretability of complex models.
                \item \textbf{Key Point:} Visualizing dependencies aids in understanding decision-making.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Emerging Trends in Probabilistic Reasoning - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Scalability with Big Data}
            \begin{itemize}
                \item Advances in algorithms enable processing of large datasets.
                \item \textbf{Example:} Online recommendation systems using Bayesian techniques.
            \end{itemize}

        \item \textbf{Cross-Domain Applications}
            \begin{itemize}
                \item Increasing use of probabilistic reasoning in various fields.
                \item \textbf{Example:} Climate modeling for weather patterns and climate change assessment.
            \end{itemize}

        \item \textbf{Enhanced Uncertainty Modeling}
            \begin{itemize}
                \item Better algorithms for quantifying and understanding uncertainty.
                \item \textbf{Key Point:} Importance for industries like autonomous driving.
            \end{itemize}
        
        \item \textbf{Conclusion}
            \begin{itemize}
                \item Ongoing evolution of probabilistic reasoning signifies new impactful applications.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formulas and Learning Objectives}
    \begin{block}{Bayes' Theorem}
        The basis for Bayesian reasoning is given by:
        \begin{equation}
        P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
        \end{equation}
        where:
        \begin{itemize}
            \item $P(A|B)$ is the posterior probability.
            \item $P(B|A)$ is the likelihood.
            \item $P(A)$ is the prior probability.
            \item $P(B)$ is the marginal likelihood.
        \end{itemize}
    \end{block}
    
    \begin{block}{Learning Objectives}
        By the end, students should be able to:
        \begin{itemize}
            \item Identify key trends in probabilistic reasoning.
            \item Understand real-world applications of these trends.
            \item Appreciate the significance of Bayesian networks in decision-making.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    \begin{itemize}
        \item Bayesian networks and probabilistic reasoning are powerful tools in AI for decision-making under uncertainty.
        \item These technologies prompt important ethical questions that must be effectively addressed for positive societal impact.
        \item Ethical implications include concerns about fairness, transparency, privacy, and accountability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Issues}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item Probabilistic models can encode historical biases, affecting fairness.
                \item Example: Biased data in criminal justice systems can reinforce social inequities.
                \item \textit{Key Point:} Continuous assessment and mitigation of bias are essential.
            \end{itemize}
        
        \item \textbf{Transparency and Interpretability}
            \begin{itemize}
                \item Bayesian models can act as "black boxes," obscuring decision processes.
                \item Example: Lack of clear reasoning can lead to distrust in healthcare diagnoses.
                \item \textit{Key Point:} Transparency fosters trust and accountability.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Continuation}
    \begin{enumerate}[resume]
        \item \textbf{Privacy Concerns}
            \begin{itemize}
                \item Sensitive data usage can breach personal privacy if mismanaged.
                \item Example: Predictive policing can lead to unauthorized data use and surveillance.
                \item \textit{Key Point:} Data governance is critical to safeguard privacy rights.
            \end{itemize}

        \item \textbf{Accountability}
            \begin{itemize}
                \item Difficulty in determining responsibility for adverse effects of model decisions.
                \item Example: Misclassification in financial models leading to financial loss complicates liability.
                \item \textit{Key Point:} Clear accountability structures are necessary to manage AI risks.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Conclusion}
    \begin{itemize}
        \item Integrating Bayesian networks in AI demands a proactive approach to ethical considerations.
        \item The aim is to maximize technological benefits while minimizing harm to individuals and society.
        \item Engaging with these ethical issues is essential for responsible AI development.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Application of Bayesian Networks in Medical Diagnosis}
    \begin{block}{Overview of Bayesian Networks}
        \begin{itemize}
            \item \textbf{Definition}: A Bayesian network is a graphical model that represents a set of variables and their conditional dependencies using directed acyclic graphs (DAGs).
            \item \textbf{Purpose}: Used to model uncertainty in various fields such as medicine, finance, and artificial intelligence.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Diagnosing Lung Cancer}
    \begin{block}{Context}
        Lung cancer is a leading cause of death worldwide. Accurate and early diagnosis can significantly improve survival rates.
    \end{block}

    \begin{block}{Application of Bayesian Networks}
        \begin{enumerate}
            \item \textbf{Variables}:
                \begin{itemize}
                    \item Symptoms: Cough, weight loss, fatigue
                    \item Risk Factors: Smoking history, family history of cancer, exposure to asbestos
                    \item Diagnosis: Positive or Negative for lung cancer
                \end{itemize}
            \item \textbf{Model Structure}:
                \begin{itemize}
                    \item Nodes represent variables (e.g., symptoms and risk factors).
                    \item Directed edges indicate probabilistic dependencies (e.g., smoking increases the likelihood of symptoms).
                \end{itemize}
            \item \textbf{Conditional Probabilities}:
                Example probabilities might include:
                \begin{itemize}
                    \item $P(\text{Smoking} = \text{Yes} | \text{Lung Cancer} = \text{Yes}) = 0.85$
                    \item $P(\text{Cough} | \text{Lung Cancer} = \text{Yes}) = 0.70$
                    \item $P(\text{Cough} | \text{Lung Cancer} = \text{No}) = 0.20$
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inference Process and Example Calculation}
    \begin{block}{Inference Process}
        \begin{itemize}
            \item Collect prior information (e.g., a patient presents specific symptoms).
            \item Bayesian inference updates beliefs (e.g., calculating $P(\text{Lung Cancer} = \text{Yes} | \text{Symptoms})$).
            \item The model calculates posterior probabilities given the symptoms and risk factors.
        \end{itemize}
    \end{block}

    \begin{block}{Example Calculation}
        Using Bayes' Theorem:
        \begin{equation}
            P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
        \end{equation}
        Where:
        \begin{itemize}
            \item $A$ = Event of interest (e.g., Lung Cancer)
            \item $B$ = Evidence (e.g., symptoms)
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Decision Support}: Assisting healthcare professionals in making informed decisions.
            \item \textbf{Dynamic Updating}: Incorporating new data for continual learning.
            \item \textbf{Interdisciplinary Impact}: Applications beyond healthcare include finance and environmental science.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Bayesian networks provide a powerful tool for dealing with uncertainty in complex domains. The case study of lung cancer diagnosis illustrates their effectiveness in enhancing decision-making through probabilistic reasoning, leading to improved healthcare solutions.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Q\&A and Discussion - Overview}
  \begin{block}{Learning Objectives}
    \begin{enumerate}
      \item Understand the role of probabilistic reasoning in decision-making.
      \item Discuss the structure and function of Bayesian networks.
      \item Explore real-world applications through case studies.
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts}
  \begin{block}{Probabilistic Reasoning}
    \begin{itemize}
      \item A method to draw conclusions from uncertain information and make predictions based on probabilities.
      \item \textbf{Example:} Predicting if it will rain based on historical weather data and current conditions.
    \end{itemize}
  \end{block}
  
  \begin{block}{Bayesian Networks}
    \begin{itemize}
      \item Directed acyclic graphs (DAGs) representing variables and their dependencies.
      \item Nodes = random variables; Edges = probabilistic dependencies.
      \item \textbf{Example:} A network predicting disease outcomes based on symptoms and test results.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Discussion Points}
  \begin{block}{Applications of Probabilistic Reasoning}
    \begin{enumerate}
      \item How can Bayesian networks improve decision-making in fields like healthcare, finance, or AI?
      \item Discuss scenarios where Bayesian reasoning resolves uncertainty.
    \end{enumerate}
  \end{block}
  
  \begin{block}{Structure of Bayesian Networks}
    \begin{itemize}
      \item Central components: 
      \begin{itemize}
        \item \textbf{Nodes:} Represent random variables (e.g., symptoms, diseases).
        \item \textbf{Edges:} Show relationships between these variables.
      \end{itemize}
      \item \textbf{Example structure:}
        \begin{itemize}
          \item Disease A $\rightarrow$ Symptom 1
          \item Disease A $\rightarrow$ Symptom 2
          \item Disease B $\rightarrow$ Symptom 1
        \end{itemize}
    \end{itemize}
  \end{block}
  
  \begin{block}{Calculating Probabilities}
    Bayes' Theorem:
    \begin{equation}
    P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
    \end{equation}
    Discuss how to apply this theorem to update probabilities with new evidence.
  \end{block}
\end{frame}


\end{document}