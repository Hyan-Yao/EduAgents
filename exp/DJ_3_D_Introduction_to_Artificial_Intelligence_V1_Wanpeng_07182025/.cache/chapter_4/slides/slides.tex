\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Chapter 4: Neural Networks: Basics]{Chapter 4: Neural Networks: Basics}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks}
    Neural networks are computational models inspired by the human brain, enabling machines to learn from data and make decisions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What Are Neural Networks?}
    \begin{itemize}
        \item Neural networks consist of interconnected layers of nodes (neurons).
        \item They process data, recognize patterns, and improve over time through experience.
    \end{itemize}
    
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Learning from Data}: Improved performance with more data.
            \item \textbf{Flexibility}: Applicable in diverse areas like image recognition and natural language processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Artificial Intelligence}
    Neural networks are a cornerstone of artificial intelligence (AI), enabling complex tasks that require human-like intelligence.

    \begin{enumerate}
        \item \textbf{Image Recognition}:
            \begin{itemize}
                \item Facial recognition and object detection.
                \item \textbf{Key Technology}: Convolutional Neural Networks (CNNs).
            \end{itemize}

        \item \textbf{Natural Language Processing (NLP)}:
            \begin{itemize}
                \item Language translation and chatbots.
                \item \textbf{Key Technology}: Recurrent Neural Networks (RNNs) and Transformers.
            \end{itemize}

        \item \textbf{Game Playing}:
            \begin{itemize}
                \item Learning strategies for games like chess and Go.
                \item Example: AlphaGo's use of deep reinforcement learning.
            \end{itemize}

        \item \textbf{Medical Diagnosis}:
            \begin{itemize}
                \item Predicting diseases through analysis of medical data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Foundational Elements in AI Applications}
    Neural networks serve as a foundational element for many AI applications due to their data-driven approach and generalization capabilities.

    \begin{block}{Fundamental Aspects}
        \begin{itemize}
            \item \textbf{Data-Driven Approach}: Excellent at recognizing patterns from vast amounts of unstructured data.
            \item \textbf{Generalization}: Identify features and draw conclusions not explicitly programmed.
        \end{itemize}
    \end{block}

    \begin{block}{Important Terminology}
        \begin{itemize}
            \item \textbf{Neuron}: The basic unit of a neural network.
            \item \textbf{Layer}: Composed of multiple neurons (input, hidden, output).
            \item \textbf{Activation Function}: Introduces non-linearity into the model.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Understanding neural networks is crucial for exploring artificial intelligence. Their ability to learn from data makes them indispensable.

    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Mimic human brain structures.
            \item Versatile applications across numerous domains.
            \item Learning capability evolves with data.
            \item A strong foundation in neural networks is essential for advanced AI study.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Neural Networks - Overview}
    Neural networks are composed of several critical components that work together to perform tasks such as classification, prediction, and decision-making.
    
    Understanding these components—neurons, layers, and activation functions—is fundamental to grasping how neural networks operate.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Neural Networks - Neurons}
    \begin{block}{1. Neurons}
        \begin{itemize}
            \item \textbf{Definition}: A neuron is the basic unit of a neural network, inspired by biological neurons.
            \item \textbf{Functionality}:
            \begin{itemize}
                \item \textbf{Input}: Receives multiple inputs $x_1, x_2, \ldots, x_n$.
                \item \textbf{Weights}: Adjusts influence of inputs with corresponding weights $w_1, w_2, \ldots, w_n$.
                \item \textbf{Sum}: Computes 
                \begin{equation}
                z = w_1 \cdot x_1 + w_2 \cdot x_2 + \ldots + w_n \cdot x_n + b
                \end{equation}
            \end{itemize}
            \item \textbf{Output}: Generated by an activation function $f(z)$:
            \begin{equation}
            y = f(z)
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Neural Networks - Layers and Activation Functions}
    \begin{block}{2. Layers}
        \begin{itemize}
            \item \textbf{Definition}: Collections of neurons organized into:
                \begin{enumerate}
                    \item Input Layer
                    \item Hidden Layers
                    \item Output Layer
                \end{enumerate}
            \item \textbf{Functionality}:
            \begin{itemize}
                \item Transforms input into new representations.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Activation Functions}
        \begin{itemize}
            \item \textbf{Definition}: Introduces non-linearities allowing the learning of complex patterns.
            \item \textbf{Common Functions}:
            \begin{itemize}
                \item Sigmoid: $f(z) = \frac{1}{1 + e^{-z}}$
                \item ReLU: $f(z) = \max(0, z)$
                \item Softmax: $f(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of Neural Networks - Key Points and Summary}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Neurons process inputs and produce outputs based on weights and biases.
            \item Layers stack neurons to form complex data representations.
            \item Activation Functions allow networks to learn non-linear mappings.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        Neural networks consist of interconnected neurons organized into layers. Each neuron's behavior is controlled by its weights and an activation function, essential for modeling complex relationships.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Overview}
    \begin{block}{Overview of Neural Network Architectures}
        Neural networks come in various architectures, each designed to solve different types of problems effectively. This presentation covers:
        \begin{enumerate}
            \item Feedforward Neural Networks (FNN)
            \item Convolutional Neural Networks (CNN)
            \item Recurrent Neural Networks (RNN)
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Feedforward Neural Networks (FNN)}
    \begin{block}{Definition}
        The simplest type of neural network where connections between nodes do not form cycles. Information moves in one direction—from input to output.
    \end{block}
    \begin{itemize}
        \item \textbf{Structure}: Input layer, hidden layers, output layer.
        \item \textbf{Activation Functions}: Commonly uses ReLU or Sigmoid functions.
        \item \textbf{Use Cases}: Basic classification, regression, and simple pattern recognition.
        \item \textbf{Key Point}: FNNs are straightforward and effective for initial neural network explorations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Convolutional Neural Networks (CNN)}
    \begin{block}{Definition}
        A class of deep neural networks primarily for processing structured arrays of data such as images.
    \end{block}
    \begin{itemize}
        \item \textbf{Structure}: Convolutional layers, pooling layers, and fully connected layers.
        \item \textbf{Illustration}: 
            \begin{itemize}
                \item Input Image $\rightarrow$ Convolution Layer $\rightarrow$ Pooling Layer $\rightarrow$ Fully Connected Layer $\rightarrow$ Output.
            \end{itemize}
        \item \textbf{Use Cases}: Image recognition, classification, and object detection.
        \item \textbf{Key Point}: Captures spatial hierarchies, excelling in visual data processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Recurrent Neural Networks (RNN)}
    \begin{block}{Definition}
        Designed for sequential data processing, RNNs allow information persistence over time through connections that loop back on themselves.
    \end{block}
    \begin{itemize}
        \item \textbf{Structure}: Input layers, recurrent hidden layers, output layers.
        \item \textbf{Illustration}:
            \begin{itemize}
                \item Input Sequence $(t_1, t_2, t_3) \rightarrow$ Recurrent Cells $\rightarrow$ State Transition $(h(t-1) \rightarrow h(t)) \rightarrow$ Output Sequence.
            \end{itemize}
        \item \textbf{Use Cases}: Natural language processing, language modeling, machine translation, and speech recognition.
        \item \textbf{Key Point}: Effective for tasks requiring context from previous inputs, ideal for time-series data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Summary}
    \begin{itemize}
        \item \textbf{Feedforward Networks}: Simple and efficient for straightforward tasks.
        \item \textbf{Convolutional Networks}: Best performance for structured data like images.
        \item \textbf{Recurrent Networks}: Effectively processes sequential data, capturing temporal dependencies.
    \end{itemize}
    Understanding these architectures aids in selecting the appropriate model based on problem requirements for efficient AI solutions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Example Code}
    \begin{block}{Example Code Snippet for FNN in Python}
    \begin{lstlisting}[language=Python]
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(input_dim,)))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    \end{lstlisting}
    \end{block}
    Presenting both theoretical insights and practical examples enhances understanding of neural network architectures.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Process in Neural Networks}
    
    \begin{block}{Introduction to Learning Methodology}
        Neural networks learn from data through a process involving key steps: 
        \textbf{Forward Propagation}, 
        \textbf{Loss Calculation}, 
        and 
        \textbf{Backpropagation}. 
        Understanding these components is crucial for grasping how neural networks optimize performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Forward Propagation}

    \begin{itemize}
        \item \textbf{Definition}: Forward propagation is the process of passing input data through the layers of the neural network to generate an output.
        
        \item \textbf{Process}:
        \begin{enumerate}
            \item \textbf{Input Layer}: Data is fed into the model from the input layer.
            \item \textbf{Hidden Layers}: The data moves through hidden layers where weighted sums are calculated and activation functions are applied.
            \begin{itemize}
                \item \textbf{Activation Function (e.g., ReLU, Sigmoid)}: Introduces non-linearity into the model.
            \end{itemize}
        \end{enumerate}
        
        \item \textbf{Formula}:
        \begin{equation}
        z = w \cdot x + b
        \end{equation}
        \begin{itemize}
            \item Where \( z \) is the weighted input, \( w \) are the weights, \( x \) is the inputs, and \( b \) is the bias.
        \end{itemize}

        \item \textbf{Example}: If the model is predicting house prices, input features could include square footage, number of bedrooms, etc.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Loss Calculation}

    \begin{itemize}
        \item \textbf{Objective}: The loss function quantifies how well the neural network's prediction matches the actual target values.
        
        \item \textbf{Common Loss Functions}:
        \begin{itemize}
            \item Mean Squared Error (MSE) for regression problems.
            \item Cross-Entropy Loss for classification problems.
        \end{itemize}
        
        \item \textbf{Formula for Cross-Entropy}:
        \begin{equation}
        Loss = -\sum (y \cdot \log(\hat{y}))
        \end{equation}
        \begin{itemize}
            \item Where \( y \) is the true label and \( \hat{y} \) is the predicted probability.
        \end{itemize}
        
        \item \textbf{Example}: If a model predicts a house costs $300,000 while the actual price is $350,000, the loss function calculates this difference to guide adjustments in weights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Backpropagation}

    \begin{itemize}
        \item \textbf{Definition}: Backpropagation is the method used to update the weights of the neural network to minimize the loss.
        
        \item \textbf{Process}:
        \begin{enumerate}
            \item Compute the gradient of the loss function with respect to each weight using the chain rule.
            \item Adjust weights in the opposite direction of the gradient to minimize the loss.
        \end{enumerate}
        
        \item \textbf{Key Formula}:
        \begin{equation}
        w = w - \alpha \cdot \frac{\partial Loss}{\partial w}
        \end{equation}
        \begin{itemize}
            \item Where \( \alpha \) is the learning rate, determining the step size of weight updates.
        \end{itemize}
        
        \item \textbf{Example}: If the gradient indicates that increasing a certain weight leads to higher loss, backpropagation will decrease that weight to optimize predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}

    \begin{itemize}
        \item Neural networks learn through a cycle of forward propagation, loss calculation, and backpropagation.
        \item Understanding the roles of activation functions and loss functions is critical for effective training.
        \item The learning process is iterative, relying on weight updates through backpropagation based on loss gradients.
    \end{itemize}

    \begin{block}{Conclusion}
        The learning process in neural networks is foundational for model training and requires a careful balance between forward propagation, loss assessment, and effective weight adjustments through backpropagation. Mastery of these concepts will provide a strong basis for further exploration of complex architectures and algorithms in subsequent chapters.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Commonly Used Algorithms - Overview}
    \begin{block}{Overview of Learning Algorithms in Neural Networks}
        Neural networks rely on various algorithms during training, fundamentally influencing how effectively they learn from data. 
        Understanding these algorithms is crucial for constructing and optimizing neural networks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Commonly Used Algorithms - Gradient Descent}
    \begin{block}{1. Gradient Descent}
        \begin{itemize}
            \item \textbf{Concept}: Primary optimization algorithm used to minimize the loss function.
            \item \textbf{Mechanism}: Calculates the gradient (derivative) of the loss function with respect to each weight in the network. 
            Updates weights in the opposite direction of the gradient to reduce the error.
        \end{itemize}
        
        \begin{equation}
            w_{\text{new}} = w_{\text{old}} - \eta \cdot \nabla L(w)
        \end{equation}
        Where:
        \begin{itemize}
            \item $w$ = weight
            \item $\eta$ = learning rate (step size)
            \item $\nabla L(w)$ = gradient of the loss function
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Commonly Used Algorithms - Variants and Implications}
    \begin{block}{Variants of Gradient Descent}
        \begin{itemize}
            \item \textbf{Stochastic Gradient Descent (SGD)}: Updates weights for each training example, leading to faster iterations and better local minima escape.
            \item \textbf{Mini-batch Gradient Descent}: Updates based on a small random sample of the dataset, balancing efficiency and stability.
        \end{itemize}
    \end{block}
    
    \begin{block}{Adaptive Learning Rate Methods}
        \begin{itemize}
            \item \textbf{Adam (Adaptive Moment Estimation)}: Computes adaptive learning rates for parameters, combining benefits of AdaGrad and RMSProp.
            \begin{itemize}
                \item Handles sparse gradients.
                \item Automatically adjusts learning rates.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Implications of Algorithm Choices}
        \begin{itemize}
            \item \textbf{Learning Rate}: A smaller rate requires more epochs; a larger rate risks divergence.
            \item \textbf{Batch Size}: Affects weight updates; smaller sizes lead to noisier updates but better escape from local minima.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Commonly Used Algorithms - Example and Next Steps}
    \begin{block}{Example}
        Suppose you're training a neural network to classify images. With Adam as your optimization method and a learning rate of 0.001, you may achieve faster convergence and better performance compared to vanilla gradient descent.
    \end{block}

    \begin{block}{Next Steps}
        In the upcoming session, we will have hands-on experience implementing a neural network using Python and TensorFlow, applying these concepts practically.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Hands-On Coding Exercise}
    \begin{block}{Objective}
        This coding exercise aims to reinforce your understanding of neural networks by allowing you to implement a basic architecture using Python and TensorFlow.
        This hands-on experience will help deepen your grasp of the concepts covered in Chapter 4.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts to Review}
    \begin{itemize}
        \item \textbf{Neurons and Layers:} Understand how individual neurons work and how they are organized into layers.
        \item \textbf{Activation Functions:} Learn about the role of activation functions (like ReLU and sigmoid) in introducing non-linearity to the network.
        \item \textbf{Forward Propagation:} Comprehend how data moves through the network during the prediction phase.
        \item \textbf{Loss Function:} Familiarize yourself with loss functions that measure the performance of the neural network.
        \item \textbf{Backpropagation:} Understand how the model adjusts weights based on the loss to improve accuracy during training.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Coding Exercise Steps}
    Follow these steps to implement your neural network:

    \begin{enumerate}
        \item \textbf{Setup Environment:}
        \begin{lstlisting}[language=bash]
pip install tensorflow
        \end{lstlisting}

        \item \textbf{Import Necessary Libraries:}
        \begin{lstlisting}[language=python]
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
        \end{lstlisting}

        \item \textbf{Prepare the Dataset:}
        \begin{lstlisting}[language=python]
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the data
        \end{lstlisting}

        \item \textbf{Define the Model Architecture:}
        \begin{lstlisting}[language=python]
model = keras.Sequential([
    layers.Flatten(input_shape=(28, 28)),       # Flatten the input
    layers.Dense(128, activation='relu'),       # Hidden layer with ReLU
    layers.Dense(10, activation='softmax')      # Output layer with Softmax
])
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Completing the Model}
    Continuing with the coding exercise steps:

    \begin{enumerate}[start=5]
        \item \textbf{Compile the Model:}
        \begin{lstlisting}[language=python]
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
        \end{lstlisting}

        \item \textbf{Train the Model:}
        \begin{lstlisting}[language=python]
model.fit(x_train, y_train, epochs=5)  # Train for 5 epochs
        \end{lstlisting}

        \item \textbf{Evaluate the Model:}
        \begin{lstlisting}[language=python]
test_loss, test_acc = model.evaluate(x_test, y_test)
print('\nTest accuracy:', test_acc)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Understanding the Role of Each Layer:} The first layer flattens the input data, while the hidden layer applies an activation function to introduce non-linearity.
        \item \textbf{Importance of Normalization:} Normalizing input data enhances convergence speed during training.
        \item \textbf{Model Evaluation:} Evaluating accuracy provides insight into the model's performance and areas for improvement.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    By completing this hands-on coding exercise, you will solidify your understanding of fundamental neural network concepts, from architecture design to performance evaluation. 
    This practical implementation will not only solidify your theoretical knowledge but also prepare you for more complex neural network tasks in future applications.
\end{frame}

\begin{frame}
    \frametitle{Additional Resources}
    \begin{itemize}
        \item \url{https://www.tensorflow.org/} 
        \item \url{https://keras.io/guides/} 
        \item \url{http://yann.lecun.com/exdb/mnist/} 
    \end{itemize}
    Feel free to ask questions or seek clarification during this exercise!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Overview}
    \begin{block}{Summary}
        Neural networks have transformed many fields, offering advanced solutions to complex problems. Key applications include:
    \end{block}
    \begin{itemize}
        \item Image Recognition
        \item Natural Language Processing (NLP)
        \item Autonomous Systems
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Image Recognition}
    \begin{block}{Image Recognition}
        Neural networks, particularly Convolutional Neural Networks (CNNs), excel in image processing tasks. 
    \end{block}
    \begin{itemize}
        \item \textbf{Application Example:} Facial Recognition (e.g., Apple’s Face ID)
        \begin{itemize}
            \item Analyzes key features like distance between eyes, nose shape, and jawline.
        \end{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Feature Learning: CNNs learn spatial hierarchies of features automatically.
            \item Real-World Usage: Security systems, social media tagging, medical diagnostics.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - NLP and Autonomous Systems}
    \begin{block}{Natural Language Processing}
        Neural networks revolutionize how machines understand and generate human language.
    \end{block}
    \begin{itemize}
        \item \textbf{Application Example:} Chatbots and Virtual Assistants (e.g., OpenAI's GPT-3)
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Sentiment Analysis: Analyze customer feedback automatically.
            \item Translation Services: Provide accurate translations understanding context.
        \end{itemize}
    \end{itemize}

    \begin{block}{Autonomous Systems}
        Neural networks are critical for self-driving cars and drones.
    \end{block}
    \begin{itemize}
        \item \textbf{Application Example:} Self-Driving Cars (e.g., Tesla)
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Sensor Fusion: Process data from various sensors for environmental understanding.
            \item Path Planning: Make real-time decisions based on detected obstacles and conditions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Key Takeaways}
    \begin{itemize}
        \item Neural networks provide powerful solutions across diverse applications.
        \item Understanding these applications shows the impact of neural networks on everyday life.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Code Snippet}
    \begin{block}{Code Snippet: Image Recognition using CNN}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow.keras import layers, models

# Define a simple CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')  # Assuming 10 classes
])

# Compile and summarize the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Considerations in Neural Networks}
    \begin{itemize}
        \item Key challenges:
        \begin{itemize}
            \item Overfitting
            \item Underfitting
        \end{itemize}
        \item Ethical considerations in AI applications
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Overfitting}
    \begin{block}{Definition}
        Overfitting occurs when a neural network learns to perform exceptionally well on the training data, capturing noise and outliers instead of the underlying data distribution.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Visual Illustration:} Graph showing training vs. validation performance.
        \item \textbf{Example:} Model recognizing cats fails to generalize from specific training images.
        \item \textbf{Prevention Techniques:}
        \begin{itemize}
            \item Regularization (L1, L2)
            \item Dropout
            \item Early Stopping
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Underfitting}
    \begin{block}{Definition}
        Underfitting occurs when a model is too simplistic to capture the underlying trend of the data, failing to learn effectively from the dataset.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Visual Illustration:} High losses on both training and validation datasets.
        \item \textbf{Example:} A linear model fails to fit a complex, nonlinear relationship.
        \item \textbf{Solutions:}
        \begin{itemize}
            \item Increasing Model Complexity
            \item Feature Engineering
            \item Longer Training Time
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Ethical Considerations}
    \begin{block}{Importance}
        As AI technology becomes more integrated into society, ethical considerations are crucial.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Points:}
        \begin{itemize}
            \item Bias in Training Data: AI systems may perpetuate existing biases.
            \item Transparency: Explain model decisions, especially in critical sectors.
            \item Accountability: Establish guidelines for responsible AI use.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Points}
    \begin{itemize}
        \item Overfitting and Underfitting are critical performance barriers.
        \item Ethical considerations in AI applications are vital for fair and accountable use of technology.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Regularization}
    \begin{lstlisting}[language=Python]
from keras.models import Sequential
from keras.layers import Dense
from keras.regularizers import l2

model = Sequential()
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), input_dim=input_shape))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Points Recap}
    \begin{enumerate}
        \item \textbf{Definition of Neural Networks}:
        \begin{itemize}
            \item Computational models inspired by the human brain that recognize patterns through layers of interconnected nodes (neurons).
        \end{itemize}
        
        \item \textbf{Structure of Neural Networks}:
        \begin{itemize}
            \item \textbf{Input Layer}: Receives initial data.
            \item \textbf{Hidden Layers}: Transforms input data; more layers increase capacity and complexity.
            \item \textbf{Output Layer}: Produces final results/outputs.
        \end{itemize}
        
        \item \textbf{Learning Process}:
        \begin{itemize}
            \item Training involves adjusting weights using the backpropagation algorithm based on prediction errors.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Continued}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue from previous enumeration
        \item \textbf{Activation Functions}:
        \begin{itemize}
            \item Functions introducing non-linearity in neural networks.
            \item Common types:
            \begin{itemize}
                \item \textbf{Sigmoid}: Maps outputs between 0 and 1.
                \item \textbf{ReLU (Rectified Linear Unit)}: Outputs 0 for negative inputs, retains positive values.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Common Challenges}:
        \begin{itemize}
            \item \textbf{Overfitting}: Model captures noise rather than the signal.
            \item \textbf{Underfitting}: Model too simple to capture data structure.
        \end{itemize}
        
        \item \textbf{Ethical Considerations}:
        \begin{itemize}
            \item Concerns regarding fairness, accountability, and transparency in AI development.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Engagement and Discussion}
    \textbf{Call to Action for Q\&A:}
    \begin{itemize}
        \item We encourage you to ask questions to clarify any challenging concepts or share practical examples.
    \end{itemize}

    \textbf{Interactive Discussion Points:}
    \begin{enumerate}
        \item Thoughts on addressing overfitting in real-world applications?
        \item How can we ensure ethical practices in AI development?
        \item Can you suggest real-world problems where neural networks might be utilized effectively?
    \end{enumerate}
\end{frame}


\end{document}