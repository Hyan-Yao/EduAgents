\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Logistic Regression}
    \begin{block}{Overview of Logistic Regression}
        \begin{itemize}
            \item \textbf{Definition}: Logistic regression is a statistical method used for predicting binary outcomes, typically represented as 0 and 1 (e.g., yes/no, success/failure).
            \item \textbf{Role in Supervised Learning}: Logistic regression falls under the category of classification algorithms and models the relationship between independent variables and a binary dependent variable.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conceptual Foundations of Logistic Regression}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Binary Classification}: Classifies data points into one of two categories based on predictor variables (e.g., spam or not spam).
            \item \textbf{S-shaped Curve}: Uses the logistic function to map real-valued numbers into values between 0 and 1, indicating probabilities instead of continuous outcomes.
        \end{itemize}
    \end{block}
    \begin{equation}
        P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n)}}
    \end{equation}
    Where:
    \begin{itemize}
        \item $P(Y=1 | X)$ = Probability of the event occurring (class 1)
        \item $\beta_0$ = Intercept
        \item $\beta_1, \beta_2, ..., \beta_n$ = Coefficients of the predictor variables $X_1, X_2, ..., X_n$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Applications}
    \begin{block}{Key Points}
        \begin{enumerate}
            \item \textbf{Binary Output}: Specifically tailored for binary targets.
            \item \textbf{Interpretation of Coefficients}: Represents change in the log odds for a one-unit increase in the predictor variable.
            \item \textbf{Applications}: Used in medical diagnosis, credit scoring, and marketing response prediction.
            \item \textbf{Loss Function}: Utilizes Maximum Likelihood Estimation (MLE) for coefficient optimization.
            \item \textbf{Extension to Multinomial}: Can extend using techniques like One-vs-Rest or Softmax Regression.
        \end{enumerate}
    \end{block}
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Healthcare Example}: Predicting diabetes based on features like age and weight.
            \item \textbf{Marketing Example}: Determining likelihood of a purchase based on browsing history and income.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Logistic Regression? - Definition}
    \begin{block}{Definition of Logistic Regression}
        Logistic regression is a statistical method used for binary classification in supervised learning. It predicts the probability that a given input point belongs to a particular class (e.g., yes/no, pass/fail).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Logistic Regression? - Key Concepts}
    \begin{itemize}
        \item \textbf{Binary Outcomes:} Focuses on binary outcomes with two possible classes. Examples:
            \begin{itemize}
                \item Predicting disease presence: Yes (1) or No (0)
                \item Spam detection: Spam (1) or Not Spam (0)
            \end{itemize}
        
        \item \textbf{Logistic Function:} Converts predicted values into probabilities:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
        
        \item \textbf{Odds and Odds Ratio:} Estimates the log odds of probability:
        \begin{equation}
            \text{log odds} = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic vs Linear Regression}
    \begin{block}{Differences Between Logistic and Linear Regression}
        \begin{itemize}
            \item \textbf{Output Format:}
                \begin{itemize}
                    \item Linear Regression: Continuous values (e.g., prices)
                    \item Logistic Regression: Probabilities (0 to 1)
                \end{itemize}
            
            \item \textbf{Error Measurement:}
                \begin{itemize}
                    \item Linear Regression: Mean Squared Error (MSE)
                    \item Logistic Regression: Likelihood estimation for binary cross-entropy loss
                \end{itemize}
                
            \item \textbf{Assumption:}
                \begin{itemize}
                    \item Linear Regression: Linear relationship
                    \item Logistic Regression: Logistic relationship ideal for binary outcomes
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundation - Overview}
    The logistic function is a mathematical representation central to logistic regression. It predicts probabilities of binary outcomes (e.g., success/failure, yes/no) and constrains these probabilities between 0 and 1.

    \begin{itemize}
        \item Unlike linear regression, which predicts continuous outcomes,
        \item Logistic regression is specifically for binary outcomes.
        \item Ensures predicted probabilities are within the range [0, 1].
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Logistic Function}
    The logistic function is defined as:
    
    \begin{equation}
        f(z) = \frac{1}{1 + e^{-z}}
    \end{equation}
    
    Where:
    \begin{itemize}
        \item \( e \) is Euler's number (approximately 2.71828).
        \item \( z \) is a linear combination of input features:
        \begin{equation}
            z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n
        \end{equation}
        \item \( \beta_0 \): intercept (bias)
        \item \( \beta_1, \beta_2, \ldots, \beta_n \): coefficients for features \( X_1, X_2, \ldots, X_n \)
    \end{itemize}
    
    The function \( f(z) \) outputs a value between 0 and 1, interpreted as the probability of the positive class (e.g., \( P(Y=1|X) \)).
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Logistic Function}
    Consider predicting if a student will pass (1) or fail (0) based on hours studied (X). 

    The logistic regression model yields:
    
    \begin{equation}
        z = -4 + 1.2 \cdot \text{Hours Studied}
    \end{equation}
    
    Example Calculation for 5 hours studied:
    \begin{enumerate}
        \item Calculate \( z \):
        \[
        z = -4 + 1.2 \cdot 5 = 2
        \]
        
        \item Calculate \( P(Y=1|X) \):
        \[
        P(Y=1|X) = f(2) = \frac{1}{1 + e^{-2}} \approx 0.8808
        \]
    \end{enumerate}
    This indicates a high likelihood that the student will pass.
\end{frame}

\begin{frame}[fragile]{Logistic Function Equation}
    \begin{block}{Overview}
        The logistic function equation transforms a linear combination of inputs into a probability, crucial for binary classification problems in supervised learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding the Logistic Function}
    \begin{itemize}
        \item The logistic function outputs values between 0 and 1.
        \item It is used for predicting binary outcomes (e.g., success/failure).
        \item Ideal for supervised learning applications, particularly in logistic regression.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{The Logistic Function Equation}
    \begin{equation}
        P(Y=1|X) = \frac{1}{1 + e^{-z}} \quad \text{where} \quad z = \beta_0 + \beta_1X_1 + \dots + \beta_nX_n 
    \end{equation}
    
    \begin{itemize}
        \item \(P(Y=1|X)\): Predicted probability that outcome \(Y\) is 1 given \(X\).
        \item \(e^{-z}\): Exponential function ensuring output is between 0 and 1.
        \item \(z\): Linear combination of inputs weighted by coefficients.
        \begin{itemize}
            \item \(\beta_0\): Intercept (log odds when all \(X\) are zero).
            \item \(\beta_1, \ldots, \beta_n\): Coefficients influencing the log odds.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example Interpretation}
    \begin{itemize}
        \item Consider:
            \begin{itemize}
                \item \(\beta_0 = -4\)
                \item \(\beta_1 = 0.5\) (influence of \(X_1\))
                \item \(X_1 = 5\)
            \end{itemize}
        \item Calculate \(z\):
        \begin{equation}
            z = -4 + 0.5 \times 5 = -4 + 2.5 = -1.5
        \end{equation}
        
        \item Plug \(z\) into the logistic function:
        \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{1.5}} \approx 0.18
        \end{equation}
        
        \item \textbf{Interpretation:} 18\% probability that the outcome \(Y=1\) given \(X_1 = 5\).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Points to Emphasize}
    \begin{itemize}
        \item The logistic function converts linear combinations into probabilities.
        \item Coefficients \(\beta\) are optimized using Maximum Likelihood Estimation (MLE).
        \item Understanding \(z\) affects interpretations of the logistic model's outputs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Next Steps}
    \begin{block}{Upcoming Content}
        Prepare for the next slide focused on the cost function used in logistic regression, which automates the optimization of coefficients!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cost Function in Logistic Regression - Overview}
    \begin{itemize}
        \item A cost function quantifies how well the model predicts outcomes versus actual values.
        \item The objective in logistic regression is to minimize the cost function to enhance prediction accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Log-Loss Function in Logistic Regression}
    \begin{itemize}
        \item The cost function in logistic regression is known as the \textbf{log-loss function} or \textbf{cross-entropy loss}.
        \item It quantifies the performance of a classification model with outputs as probabilities between 0 and 1.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation of Log-Loss}
    \begin{block}{Log-Loss for a Single Data Point}
        \begin{equation}
        \text{Log Loss}(y, \hat{y}) = - \left(y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})\right)
        \end{equation}
    \end{block}
    \begin{itemize}
        \item Where:
        \begin{itemize}
            \item \(y\) = Actual label (0 or 1)
            \item \(\hat{y}\) = Predicted probability of the positive class
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Total Cost Function}
    \begin{block}{Total Cost for All Predictions}
        For a dataset with \(m\) samples:
        \begin{equation}
        J(\beta) = -\frac{1}{m} \sum_{i=1}^{m} \left(y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Log-Loss}
    \begin{itemize}
        \item \textbf{Sensitive to Differences}: Log-loss penalizes confident mistakes more heavily.
        \item \textbf{Probabilistic Interpretation}: It presents a smooth landscape for optimization, simplifying the minimization process.
        \item As predicted probabilities diverge from the true labels, log-loss can grow towards infinity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Application: Python Code}
    \begin{block}{Log-Loss Calculation in Python}
        \begin{lstlisting}[language=Python]
import numpy as np

def log_loss(y_true, y_pred):
    epsilon = 1e-15  # To prevent log(0)
    y_pred = np.clip(y_pred, epsilon, 1-epsilon)  # Clip predictions
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Example usage:
y_true = np.array([0, 1, 1, 0])
y_pred = np.array([0.1, 0.9, 0.8, 0.2])
print("Log Loss:", log_loss(y_true, y_pred))
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item The log-loss function is vital for evaluating logistic regression models.
        \item Minimizing total cost is essential for effective model training.
        \item A solid understanding of the cost function provides insight into logistic regression in binary classification.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{The Optimization Process - Part 1}
    \frametitle{Understanding Gradient Descent for Logistic Regression}
    
    \begin{block}{1. Introduction to Gradient Descent}
        \begin{itemize}
            \item \textbf{Objective}: Minimize the cost function (log-loss) in logistic regression by finding optimal parameters (weights).
            \item \textbf{Gradient Descent}: An iterative optimization algorithm that adjusts parameters to minimize the cost.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]{The Optimization Process - Part 2}
    \frametitle{How Gradient Descent Works}

    \begin{enumerate}
        \item \textbf{Initialization}:
            \begin{itemize}
                \item Start with random weights, \( w \).
            \end{itemize}
        
        \item \textbf{Compute the Cost}:
            \begin{itemize}
                \item Log-loss function:
                \begin{equation}
                J(w) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_w(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_w(x^{(i)})) \right]
                \end{equation}
                where:
                \begin{itemize}
                    \item \( m \) = number of training examples
                    \item \( h_w(x) \) = hypothesis function (sigmoid function)
                    \item \( y^{(i)} \) = actual label for example \(i\)
                \end{itemize}
            \end{itemize}

        \item \textbf{Calculate the Gradient}: 
            \begin{itemize}
                \item Gradient of cost function:
                \begin{equation}
                \nabla J(w) = \frac{1}{m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) x^{(i)}
                \end{equation}
            \end{itemize}
        
        \item \textbf{Update the Weights}:
            \begin{itemize}
                \item Adjust weights:
                \begin{equation}
                w := w - \alpha \nabla J(w)
                \end{equation}
                where \( \alpha \) is the learning rate.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{The Optimization Process - Part 3}
    \frametitle{Key Points and Conclusion}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Convergence}: Continues until cost function changes are negligible.
            \item \textbf{Learning Rate Selection}:
            \begin{itemize}
                \item Small \( \alpha \): Slow convergence.
                \item Large \( \alpha \): Risks overshooting.
            \end{itemize}
            \item \textbf{Types of Gradient Descent}:
            \begin{itemize}
                \item Batch Gradient Descent: Uses all data points.
                \item Stochastic Gradient Descent: Updates for each training example.
                \item Mini-batch Gradient Descent: Combines both approaches.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Gradient descent is crucial for training logistic regression models, with its understanding essential for effective classification algorithm development.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics Overview}
    \begin{itemize}
        \item Evaluation metrics are crucial for assessing the performance of logistic regression models.
        \item Common metrics include:
        \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
            \item F1-Score
        \end{itemize}
        \item Each metric provides insight into different aspects of model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Accuracy}
    \begin{block}{Definition}
        Accuracy is the proportion of correctly predicted instances among the total instances.
    \end{block}
    \begin{block}{Formula}
        \begin{equation}
        \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \end{equation}
    \end{block}
    Where:
    \begin{itemize}
        \item TP = True Positives
        \item TN = True Negatives
        \item FP = False Positives
        \item FN = False Negatives
    \end{itemize}
    \begin{block}{Example}
        If a test of 100 samples predicts 80 correctly (70 TP and 10 TN), then:
        \[
        \text{Accuracy} = \frac{70 + 10}{100} = 0.80 \text{ or } 80\%
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Precision and Recall}
    \begin{block}{2. Precision}
        \begin{itemize}
            \item \textbf{Definition:} Precision indicates the accuracy of positive predictions.
            \item \textbf{Formula:}
            \begin{equation}
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \end{equation}
        \end{itemize}
        \begin{block}{Example}
            From our previous example, if we have 80 positive predictions (70 TP and 10 FP):
            \[
            \text{Precision} = \frac{70}{70 + 10} = \frac{70}{80} = 0.875 \text{ or } 87.5\%
            \]
        \end{block}
    \end{block}
    
    \begin{block}{3. Recall}
        \begin{itemize}
            \item \textbf{Definition:} Recall measures the model's ability to identify relevant instances (true positives).
            \item \textbf{Formula:}
            \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \end{equation}
        \end{itemize}
        \begin{block}{Example}
            If there are actually 90 true positive cases:
            \[
            \text{Recall} = \frac{70}{70 + 20} = \frac{70}{90} \approx 0.778 \text{ or } 77.8\%
            \]
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. F1-Score}
    \begin{block}{Definition}
        The F1-score is the harmonic mean of precision and recall.
    \end{block}
    \begin{block}{Formula}
        \begin{equation}
        \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{block}
    \begin{block}{Example}
        Using the values computed earlier:
        \[
        \text{F1-Score} = 2 \times \frac{0.875 \times 0.778}{0.875 + 0.778} \approx 0.823 \text{ or } 82.3\%
        \]
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Accuracy can be misleading in imbalanced datasets.
            \item Precision is vital when false positives are costly.
            \item Recall is critical for identifying all positive instances.
            \item F1-Score balances precision and recall effectively.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Logistic Regression - Overview}
    \begin{block}{What is Logistic Regression?}
        Logistic Regression is a statistical model used for binary classification problems. 
        This guide focuses on implementing it using Python's scikit-learn library.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 1: Import Necessary Libraries}
    Before you begin, ensure you have installed the required libraries. 
    You can install them using pip as shown below:
    \begin{lstlisting}[language=Python]
# Install with pip if necessary
# !pip install numpy pandas scikit-learn
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 2: Load Your Dataset}
    Start by loading your dataset. For illustration, the popular Iris dataset is used.
    \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
data = load_iris()
X = data.data[data.target != 0]  # Two classes for binary classification
y = data.target[data.target != 0]
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 3: Pre-process the Data}
    \begin{itemize}
        \item Ensure your data is clean.
        \item Handle missing values, scale features, and transform categorical variables if necessary.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 4: Split the Dataset}
    Divide your dataset into training and testing sets to evaluate the model's performance.
    \begin{lstlisting}[language=Python]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 5: Create the Logistic Regression Model}
    Initialize the Logistic Regression model:
    \begin{lstlisting}[language=Python]
model = LogisticRegression()
model.fit(X_train, y_train)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 6: Make Predictions}
    Use the trained model to make predictions on the test set.
    \begin{lstlisting}[language=Python]
y_pred = model.predict(X_test)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 7: Evaluate the Model}
    Assess the model's performance using:
    \begin{itemize}
        \item Accuracy Score
        \item Confusion Matrix
        \item Classification Report
    \end{itemize}
    \begin{lstlisting}[language=Python]
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Data Preparation:} Quality input data significantly influences model performance.
        \item \textbf{Model Evaluation:} Always use appropriate metrics to ensure reliability.
        \item \textbf{Benefits of Logistic Regression:} Easy interpretation and good performance on binary outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    You now have a foundation to implement Logistic Regression for binary classification tasks. 
    Next, we will explore the assumptions underlying logistic regression models to deepen our understanding.
\end{frame}

\begin{frame}[fragile]{Logistic Regression Assumptions - Overview}
  \textbf{Overview of Logistic Regression:} \\
  Logistic regression is a statistical method used for binary classification problems, predicting the probability that an instance belongs to a particular category. 
  It is particularly popular for its simplicity and interpretability.
\end{frame}

\begin{frame}[fragile]{Logistic Regression Assumptions - Key Assumptions}
  \textbf{Key Assumptions of Logistic Regression:} 
  \begin{enumerate}
    \item \textbf{Binary Outcome:} \\ 
    Designed for binary outcomes (0/1, True/False).
    
    \item \textbf{Linearity Between Features and Log-Odds:} \\ 
    Assumes a linear relationship between independent variables and the log-odds of the dependent variable:
    \begin{equation}
      \text{Log-Odds} = \log\left(\frac{p}{1-p}\right) 
    \end{equation}
    represented as:
    \begin{equation}
      \text{Log-Odds} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
    \end{equation}
    
    \item \textbf{Independence of Observations:} \\ 
    Assumes that observations are independent of one another.
    
    \item \textbf{No Multicollinearity:} \\ 
    Independent variables must not be highly correlated.
    
    \item \textbf{Large Sample Size:} \\ 
    Requires a larger sample size for better parameter estimation.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Logistic Regression Assumptions - Key Points and Practical Considerations}
  \textbf{Key Points to Emphasize:} 
  \begin{itemize}
    \item Understanding these assumptions is crucial for correct application and interpretation of logistic regression results.
    \item Violations can lead to model misfit and inaccurate predictions.
  \end{itemize}
  
  \textbf{Practical Consideration:} 
  \begin{itemize}
    \item Visualize relationships with scatterplots or residual plots to check for linearity.
    \item Check for multicollinearity using statistical tests or variance inflation factors (VIF).
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Logistic Regression Assumptions - Conclusion and Additional Resources}
  \textbf{Conclusion:} \\
  By keeping these assumptions in mind, we can ensure appropriate application of logistic regression, leading to robust insights.

  \textbf{Additional Resources:} 
  \begin{itemize}
    \item Use visualization libraries like Matplotlib or Seaborn in Python for diagnostic plots.
    \item Perform exploratory data analysis (EDA) to validate assumptions before modeling.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Logistic Regression Assumptions - Code Snippet}
  \textbf{Code Snippet (Python):} 
  \begin{lstlisting}
from statsmodels.api import Logit
import pandas as pd

# Example DataFrame
data = pd.DataFrame({
    'hours_studied': [1, 2, 3, 4, 5],
    'passed_exam': [0, 0, 1, 1, 1]
})

# Fitting the logistic regression model
model = Logit(data['passed_exam'], data['hours_studied'])
result = model.fit()

# Summary of the model
print(result.summary())
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Logistic Regression - Introduction}
    \begin{block}{Overview}
        Logistic regression is a powerful statistical method used for binary classification problems, 
        where the outcome variable is dichotomous. 
        This technique is widely utilized to make predictions about binary outcomes based on multiple predictor variables.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Logistic Regression - Industries}
    \begin{enumerate}
        \item \textbf{Healthcare}
            \begin{itemize}
                \item Disease Prediction: Predicting disease likelihood based on factors such as age and BMI.
                \item Clinical Decision-Making: Identifying patients at risk for surgical complications.
            \end{itemize}
        
        \item \textbf{Finance}
            \begin{itemize}
                \item Credit Scoring: Assessing creditworthiness of loan applicants.
                \item Fraud Detection: Identifying potentially fraudulent transactions.
            \end{itemize}
        
        \item \textbf{Marketing}
            \begin{itemize}
                \item Customer Retention: Determining factors contributing to customer churn.
                \item Targeted Advertising: Predicting customer responses to marketing campaigns.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Formula Overview}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Versatility}: Applicable in various domains.
            \item \textbf{Interpretability}: Coefficients provide insights into predictor relationships.
            \item \textbf{Decision-Making Tool}: Aids in informed decisions through quantified probabilities.
        \end{itemize}
    \end{block}
    \begin{block}{Logistic Regression Model}
        The logistic regression model predicts the probability \( P(Y=1|X) \) as follows:
        \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
        where:
        \begin{itemize}
            \item \( e \) is the base of the natural logarithm,
            \item \( \beta_0 \) is the intercept,
            \item \( \beta_1, \beta_2, ... \beta_n \) are the coefficients for predictor variables.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multiclass Logistic Regression}
    Logistic regression is widely used for binary classification, but many real-world problems involve multiple categories.

    \begin{itemize}
        \item Multiclass classification can be handled using:
        \begin{itemize}
            \item One-vs-Rest (OvR)
            \item Softmax Regression
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. One-vs-Rest (OvR) Approach}
    \begin{block}{Concept}
        The One-vs-Rest method creates a separate binary classifier for each class in the dataset.
    \end{block}

    \begin{enumerate}
        \item Train \( K \) classifiers for \( K \) classes.
        \item Each classifier \( C_k \) predicts if a sample belongs to class \( k \).
        \item For a new input, the class with the highest predicted probability is chosen.
    \end{enumerate}

    \begin{block}{Example}
        For classes A, B, and C:
        \begin{itemize}
            \item \( C_A \): Predict A vs. (B, C)
            \item \( C_B \): Predict B vs. (A, C)
            \item \( C_C \): Predict C vs. (A, B)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Softmax Regression (Multinomial Logistic Regression)}
    \begin{block}{Concept}
        Softmax regression generalizes logistic regression for multi-class predictions in a single model.
    \end{block}

    \begin{block}{How it works}
        Given a feature vector \( \mathbf{x} \):
        \[
            P(y = i | \mathbf{x}) = \frac{e^{\theta_i^T \mathbf{x}}}{\sum_{j=1}^{K} e^{\theta_j^T \mathbf{x}}}
        \]
        where \( \theta_i \) are the weights for class \( i \).
    \end{block}

    \begin{block}{Example}
        For classes A, B, and C, with logits:
        \begin{itemize}
            \item For A: 2
            \item For B: 1
            \item For C: 0
        \end{itemize}
        Calculate softmax probabilities:
        \[
            P(A) = \frac{e^2}{e^2 + e^1 + e^0}, \quad P(B) = \frac{e^1}{e^2 + e^1 + e^0}, \quad P(C) = \frac{e^0}{e^2 + e^1 + e^0}
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]{Case Study Example}
  In this slide, we will explore a practical scenario where logistic regression is applied to solve a real-world problem related to predicting customer defaults on loans.
\end{frame}

\begin{frame}[fragile]{Introduction to the Case Study}
  \begin{block}{Case Study: Predicting Customer Default on Loans}
    A financial institution aims to predict whether a customer will default on a loan. 
    Logistic regression will be utilized to handle this binary outcome.
  \end{block}
\end{frame}

\begin{frame}[fragile]{Data Collection}
  \begin{itemize}
    \item \textbf{Input Features:}
      \begin{itemize}
        \item Credit Score
        \item Annual Income
        \item Loan Amount
        \item Employment Status
        \item Age
      \end{itemize}
    \item \textbf{Binary Outcome:}
      \begin{itemize}
        \item Default (1) or No Default (0)
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Logistic Regression Model Overview}
  Logistic regression is used for binary target variables and models the relationship between input features and event probabilities.

  The logistic function transforms a linear combination of inputs into a probability:

  \begin{equation}
    P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
  \end{equation}

  where:
  \begin{itemize}
    \item \(\beta_0\) = Intercept
    \item \(\beta_1, \beta_2,...,\beta_n\) = Coefficients for features
    \item \(X_1, X_2, ..., X_n\) = Input features
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Implementation Steps}
  \begin{enumerate}
    \item \textbf{Data Preparation:}
      \begin{itemize}
        \item Preprocess data (handle missing values, encode categorical variables).
      \end{itemize}

    \item \textbf{Model Training:}
      \begin{itemize}
        \item Split data into training and testing sets.
        \item Fit the model using Python's `scikit-learn`:
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LogisticRegression()
model.fit(X_train, y_train)
        \end{lstlisting}
      \end{itemize}

    \item \textbf{Model Evaluation:}
      \begin{itemize}
        \item Use accuracy, precision, recall, and the confusion matrix:
        \begin{lstlisting}[language=Python]
from sklearn.metrics import classification_report, confusion_matrix

y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
        \end{lstlisting}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Key Takeaways}
  \begin{itemize}
    \item \textbf{Predictive Analysis:} Logistic regression assesses probabilities of customer default based on financial metrics.
    \item \textbf{Interpretability:} Coefficients reveal the impact of each feature.
    \item \textbf{Decision Making:} Enables informed decisions on loan approvals.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Conclusion}
  Logistic regression is a powerful tool in finance for predicting binary outcomes. 
  By effectively implementing and interpreting this model, institutions can enhance risk assessment and improve financial strategies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Overview}
    \begin{block}{Objective}
        Understand the principal challenges and limitations encountered when using logistic regression for predictive modeling.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Assumptions}
    \begin{enumerate}
        \item \textbf{Assumptions of Logistic Regression}
        \begin{itemize}
            \item Logistic regression assumes a linear relationship between independent variables and log odds.
            \item Violation of this assumption leads to inaccurate predictions.
        \end{itemize}
        \begin{block}{Example}
            If the true relationship is quadratic or exponential, logistic regression may struggle to fit the data.
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Multicollinearity and Data Imbalance}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Multicollinearity}
        \begin{itemize}
            \item Occurs when independent variables are highly correlated.
            \item Inflates variance of coefficient estimates, making tests unreliable.
        \end{itemize}
        \begin{block}{Example}
            Including both "age" and "years of experience" might introduce multicollinearity.
        \end{block}
        
        \item \textbf{Data Imbalance}
        \begin{itemize}
            \item Logistic regression may predict the majority class if classes are imbalanced.
            \item Accuracy may be high while sensitivity is poor.
        \end{itemize}
        \begin{block}{Solution}
            Use oversampling, undersampling, or cost-sensitive learning techniques.
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Overfitting and Non-linearity}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Overfitting}
        \begin{itemize}
            \item Model captures noise as signal leading to poor performance on new data.
            \item Occurs when too many predictors or interactions are included.
        \end{itemize}
        \begin{block}{Prevention}
            Use regularization techniques (e.g., L1 or L2 penalties).
        \end{block}
        
        \item \textbf{Non-linearity in Relationships}
        \begin{itemize}
            \item Logistic regression assumes linearity in log-odds; non-linear true relationships lead to suboptimal predictions.
            \item Consider polynomial or interaction terms to fit non-linear relationships.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Formula}
    The logistic regression model is defined as:
    \begin{equation}
        P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n)}}
    \end{equation}
    Where:
    \begin{itemize}
        \item $P(Y=1 | X)$ = Probability that $Y$ equals 1 given inputs $X_1, X_2, \ldots, X_n$.
        \item $\beta_0$ = Intercept.
        \item $\beta_i$ = Coefficients for predictors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Logistic regression is simple and efficient but has notable limitations.
        \item Understanding data characteristics and model assumptions is crucial for effectiveness.
        \item Addressing challenges like multicollinearity, data imbalance, and overfitting can significantly enhance performance.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Being aware of the challenges associated with logistic regression leads to better model diagnostics and improved predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions - Overview}
    \begin{block}{Overview}
        Logistic regression has proven to be a powerful tool in statistical modeling and machine learning. As technology evolves, so do methodologies and applications of logistic regression.
    \end{block}
    \begin{itemize}
        \item Exploring emerging trends and future directions.
        \item Emphasizing advancements that enhance utility and efficiency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions - Integration with Advanced ML Techniques}
    \begin{block}{1. Integration with Advanced Machine Learning Techniques}
        \begin{itemize}
            \item \textbf{Hybrid Models:} Combining logistic regression with ensemble methods for improved accuracy.
            \item Examples: 
                \begin{itemize}
                    \item Initial feature selection with logistic regression.
                    \item Using Random Forest for capturing complex feature interactions.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions - Feature Engineering and Explainability}
    \begin{block}{2. Feature Engineering and Selection}
        \begin{itemize}
            \item \textbf{Automated Feature Engineering:} Trends towards automating feature creation with techniques like deep learning embeddings.
            \item Key Point: Robust feature selection improves model performance and decreases overfitting.
        \end{itemize}
    \end{block}
    
    \begin{block}{4. Explainability and Transparency}
        \begin{itemize}
            \item \textbf{Model Interpretability:} Enhancing interpretability with tools like SHAP.
            \item Key Point: Crucial for applications like healthcare and finance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions - Addressing Imbalanced Data}
    \begin{block}{3. Addressing Imbalanced Data}
        \begin{itemize}
            \item \textbf{Improved Techniques:} Focus on handling class imbalance through methods like SMOTE and cost-sensitive learning.
            \item Illustration: Utilizing SMOTE in fraud detection to train reliable logistic regression models.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions - Application in Novel Domains}
    \begin{block}{5. Application in Novel Domains}
        \begin{itemize}
            \item \textbf{Expansion into New Areas:} Logistic regression's application in various fields.
            \item Examples:
                \begin{itemize}
                    \item \textbf{Healthcare:} Predicting disease presence based on risk factors.
                    \item \textbf{Social Sciences:} Analyzing voting behavior and survey responses.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions - Summary}
    \begin{block}{Summary}
        The future of logistic regression is promising, with advancements focusing on:
        \begin{itemize}
            \item Accuracy
            \item Interpretability
            \item Application in diverse fields
        \end{itemize}
    \end{block}
    \textbf{Key Takeaway:} Ongoing advancements will enrich logistic regression's capabilities, making it a versatile tool in predictive analytics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Overview of Logistic Regression}
    \begin{block}{Overview}
        Logistic Regression is a powerful statistical method used for binary classification problems, helping to predict one of two outcomes based on input variables. 
        It is particularly effective when the dependent variable is categorical (e.g., pass/fail, yes/no).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Key Concepts}
    \begin{enumerate}
        \item \textbf{Logistic Function}:
        \begin{itemize}
            \item Maps predicted values between 0 and 1 using the logistic (sigmoid) function.
            \item \textbf{Formula}:
            \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-z}}
            \end{equation}
            \item Where \( z = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n \).
        \end{itemize}

        \item \textbf{Odds and Odds Ratio}:
        \begin{itemize}
            \item \textbf{Odds}: Ratio of the probability of an event occurring to it not occurring.
            \item \textbf{Odds Ratio}: Indicates change in odds for a one-unit increase in the predictor variable.
            \begin{equation}
            \text{Odds Ratio} = e^{\beta_i}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Interpretation of Coefficients}
    \begin{block}{Interpretation of Coefficients}
        \begin{itemize}
            \item Positive coefficients (\( \beta_i > 0 \)) increase the odds of the outcome.
            \item Negative coefficients (\( \beta_i < 0 \)) decrease the odds.
        \end{itemize}
    \end{block}
    
    \begin{block}{Model Evaluation}
        \begin{itemize}
            \item \textbf{Confusion Matrix}: For performance metrics (accuracy, precision, recall, F1-score).
            \item \textbf{ROC Curve and AUC}: Trade-off between sensitivity and specificity, with AUC indicating overall model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Examples and Conclusions}
    \begin{block}{Examples Illustrating Logistic Regression}
        \begin{itemize}
            \item \textbf{Example 1}: Predicting whether a student passes (1) or fails (0) based on study hours and attendance.
            \item \textbf{Example 2}: Identifying whether an email is spam (1) or not spam (0) based on features like word frequency and sender information.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Logistic Regression can adapt to non-linear relationships through transformations.
            \item Check for multicollinearity among predictors to avoid distortion.
            \item Assumes independence of errors and requires a sufficient sample size.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Logistic regression is a foundational tool in machine learning for binary classification tasks. 
        Mastering its principles and evaluation techniques enables effective real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Q\&A Session - Overview}
  \begin{block}{Overview}
    In this open floor session, we invite you to ask questions and engage in discussions about Logistic Regression, 
    a fundamental supervised learning algorithm used for binary classification tasks.
  \end{block}
\end{frame}

\begin{frame}[fragile]{Q\&A Session - Key Concepts Recap}
  \begin{itemize}
    \item \textbf{Logistic Regression Purpose:} Predicts the probability of a given input belonging to a certain class (0 or 1).
    
    \item \textbf{Sigmoid Function:} The model outputs a probability using the sigmoid function:
    \begin{equation}
      P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n)}}
    \end{equation}
    
    \item \textbf{Cost Function:} It uses the log-likelihood function:
    \begin{equation}
      \text{Cost} = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h(x^{(i)})) + (1-y^{(i)}) \log(1-h(x^{(i)}))]
    \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Q\&A Session - Applications and Discussion Prompts}
  \begin{block}{Applications of Logistic Regression}
    \begin{itemize}
      \item Medical Diagnosis: Predicting disease presence based on test results.
      \item Customer Churn Prediction: Identifying potential service cancellations.
      \item Credit Scoring: Evaluating borrower default risks.
    \end{itemize}
  \end{block}

  \begin{block}{Discussion Prompts}
    \begin{itemize}
      \item What scenarios might logistic regression perform poorly in?
      \item Can you provide examples where logistic regression offers valuable insights?
      \item How could feature selection affect model performance?
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]{Q\&A Session - Interactive Component}
  \begin{block}{Interactive Component}
    \textbf{Practical Exercise:} If time permits, we'll review a dataset and perform logistic regression using Python's Scikit-learn library.
  \end{block}

  \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

# Sample Data Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Model Training
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_pred))
  \end{lstlisting}
\end{frame}


\end{document}