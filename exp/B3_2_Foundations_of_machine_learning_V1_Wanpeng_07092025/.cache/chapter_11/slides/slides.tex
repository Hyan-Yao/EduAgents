\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Dimensionality Reduction Techniques]{Week 11: Dimensionality Reduction Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Dimensionality Reduction Techniques - Overview}
    \begin{block}{What is Dimensionality Reduction?}
        Dimensionality Reduction (DR) refers to the process of reducing the number of random variables or features under consideration, simplifying data while retaining essential information. It is crucial in machine learning for data preprocessing, making high-dimensional datasets easier to visualize and analyze.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Dimensionality Reduction Techniques - Importance}
    \begin{itemize}
        \item \textbf{Simplification of Models:}
            \begin{itemize}
                \item Reduces complexity by condensing data into lower dimensions.
                \item Makes model training faster and more efficient.
            \end{itemize}
        \item \textbf{Improved Interpretability:}
            \begin{itemize}
                \item Simplified data visualization aids understanding of underlying patterns.
                \item Facilitates communication of results to stakeholders who may not be data-savvy.
            \end{itemize}
        \item \textbf{Enhanced Performance:}
            \begin{itemize}
                \item Helps mitigate overfitting by removing noise and irrelevant features.
                \item Can improve model accuracy in some cases.
            \end{itemize}
        \item \textbf{Computational Efficiency:}
            \begin{itemize}
                \item Reduces memory usage and time taken during algorithms training and evaluation.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques in Dimensionality Reduction}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA):}
            \begin{itemize}
                \item Transforms original variables into uncorrelated principal components.
                \item Maximizes variance in lower-dimensional space.
                \item \textbf{Example:} A dataset with 10 features can reduce to 2 principal components with ~90\% variance retained.
                \item \textbf{Formula:}
                \begin{equation}
                    Z = XW
                \end{equation}
                where \( Z \) is the new feature space, \( X \) is the original features, and \( W \) is the eigenvector matrix.
            \end{itemize}
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE):}
            \begin{itemize}
                \item Effective for visualizing high-dimensional data in 2D/3D.
                \item Preserves local structures.
                \item \textbf{Example:} Used for visualizing clusters in datasets with thousands of variables.
            \end{itemize}
        \item \textbf{Linear Discriminant Analysis (LDA):}
            \begin{itemize}
                \item A supervised method for class label separation.
                \item \textbf{Example:} Useful in classifying species based on genetic features.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Overview}
    \begin{block}{Learning Objectives for Dimensionality Reduction Techniques}
        \begin{enumerate}
            \item Understand the Concept of Dimensionality Reduction
            \item Identify Common Dimensionality Reduction Techniques
            \item Apply Dimensionality Reduction Techniques to Real-World Problems
            \item Evaluate the Impact of Dimensionality Reduction on Model Performance
            \item Analyze the Limitations of Dimensionality Reduction
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Understanding Dimensionality Reduction}
    \begin{block}{1. Understand the Concept of Dimensionality Reduction}
        \begin{itemize}
            \item Grasp the purpose and significance in machine learning.
            \item Recognize how it simplifies data for easier visualization.
        \end{itemize}
        \begin{block}{Key Point}
            Dimensionality reduction reduces the number of features in your dataset while preserving important information.
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Techniques Overview}
    \begin{block}{2. Identify Common Dimensionality Reduction Techniques}
        \begin{itemize}
            \item Principal Component Analysis (PCA):
                \begin{itemize}
                    \item Transforms correlated features into uncorrelated variables (principal components).
                \end{itemize}
            \item t-Distributed Stochastic Neighbor Embedding (t-SNE):
                \begin{itemize}
                    \item A non-linear visualization technique for high-dimensional data.
                \end{itemize}
        \end{itemize}
        \begin{block}{Example}
            PCA can reduce a dataset from 50 dimensions to 2, retaining 90% of the variance.
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Applications and Evaluation}
    \begin{block}{3. Apply Dimensionality Reduction Techniques to Real-World Problems}
        \begin{itemize}
            \item Gain experience implementing algorithms using Python.
            \item Utilize Scikit-Learn to apply PCA and t-SNE to datasets.
        \end{itemize}
        \begin{block}{Code Snippet}
            \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=iris.target)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Iris Dataset')
plt.show()
            \end{lstlisting}
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Evaluation and Limitations}
    \begin{block}{4. Evaluate the Impact of Dimensionality Reduction on Model Performance}
        \begin{itemize}
            \item Understand the trade-offs between dimensionality reduction and model complexity.
            \item Assess how dimensionality reduction can enhance model training by reducing overfitting.
        \end{itemize}
        \begin{block}{Key Point}
            Reducing dimensions can improve model accuracy but may result in loss of some data nuances.
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Limitations}
    \begin{block}{5. Analyze the Limitations of Dimensionality Reduction}
        \begin{itemize}
            \item Recognize situations where information loss may occur.
            \item Discuss the importance of domain knowledge for applying these techniques.
        \end{itemize}
        \begin{block}{Example}
            PCA assumes linear relationships and may not perform well with complex, non-linear data structures.
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Conclusion}
    \begin{block}{Conclusion}
        These learning objectives equip you with a solid understanding of dimensionality reduction techniques, empowering you to effectively use and implement these strategies in your machine learning projects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Why Dimensionality Reduction?}
  Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset. This is crucial in machine learning for several reasons:
  \begin{enumerate}
      \item \textbf{High Dimensionality}: Datasets can have an extremely high number of features (e.g., thousands or millions).
      \item \textbf{Curse of Dimensionality}: Increased features lead to sparsity in data, hindering statistical analysis and pattern detection.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Examples of Dimensionality Challenges}
  \begin{itemize}
      \item \textbf{Example of High Dimensionality}: 
      Consider images of handwritten digits:
      \begin{itemize}
          \item 28x28 pixels $\rightarrow$ 784 features
          \item Increasing resolution (e.g., 56x56) $\rightarrow$ 3136 features
      \end{itemize}
      \item \textbf{Curse of Dimensionality}:
      \begin{itemize}
          \item In 2D: Few data points can cover the area adequately.
          \item In 10D: Many regions remain unfilled, complicating clustering and classification.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Practical Aspects}
  \begin{block}{Key Points to Emphasize}
      \begin{itemize}
          \item \textbf{Simplification}: Easier to visualize and analyze.
          \item \textbf{Improved Performance}: Algorithms often perform better with fewer dimensions.
          \item \textbf{Avoid Overfitting}: Focus on significant features to prevent noise learning.
          \item \textbf{Enhanced Visualization}: Lower dimensions help in gaining insights.
      \end{itemize}
  \end{block}
  
  \vspace{1em} % Spacing for the following content
  \textbf{Example Code Snippet for PCA:}
  \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Example dataset (X)
X = np.random.rand(100, 10)  # 100 samples, 10 dimensions
X_scaled = StandardScaler().fit_transform(X)

# Applying PCA
pca = PCA(n_components=2)  # Reduce to 2 dimensions
X_reduced = pca.fit_transform(X_scaled)

print("Original shape:", X.shape)
print("Reduced shape:", X_reduced.shape)
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Dimensionality Reduction Techniques}
    \begin{block}{Overview}
        Dimensionality reduction simplifies models and improves visualization in high-dimensional datasets. We explore three common methods: 
        \begin{itemize}
            \item \textbf{Principal Component Analysis (PCA)}
            \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
            \item \textbf{Linear Discriminant Analysis (LDA)}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Principal Component Analysis (PCA)}
    \begin{itemize}
        \item \textbf{Concept:} Transforms data into uncorrelated Principal Components that capture maximum variance.
        \item \textbf{Methodology:}
            \begin{enumerate}
                \item Standardize the dataset.
                \item Compute covariance matrix.
                \item Calculate eigenvalues and eigenvectors.
                \item Sort eigenvectors by eigenvalues in descending order.
                \item Select the top 'k' eigenvectors to form a new feature space.
            \end{enumerate}
        \item \textbf{Key Formula:}
        \begin{equation}
            Z = XW
        \end{equation}
        where \(Z\) is the reduced dimension matrix, \(X\) is the original data matrix, and \(W\) is the matrix of selected eigenvectors.
        \item \textbf{Example:} Reducing measurements like height and weight into two principal components explaining 95% variance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. t-Distributed Stochastic Neighbor Embedding (t-SNE)}
    \begin{itemize}
        \item \textbf{Concept:} A non-linear technique for visualizing high-dimensional data in 2 or 3 dimensions.
        \item \textbf{Methodology:}
            \begin{enumerate}
                \item Convert similarities into probabilities.
                \item Create pairwise similarity distribution for both high-dimensional and low-dimensional spaces.
                \item Minimize the divergence using gradient descent.
            \end{enumerate}
        \item \textbf{Key Points:}
            \begin{itemize}
                \item Focuses on local structures and visualizes clusters effectively.
                \item Sensitive to parameters like perplexity.
            \end{itemize}
        \item \textbf{Example:} Visualizing handwritten digits where individual digits cluster based on similarity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Linear Discriminant Analysis (LDA)}
    \begin{itemize}
        \item \textbf{Concept:} Supervised technique that maximizes separability among classes.
        \item \textbf{Methodology:}
            \begin{enumerate}
                \item Compute within-class and between-class scatter matrices.
                \item Identify a projection that maximizes the ratio of between-class variance to within-class variance.
            \end{enumerate}
        \item \textbf{Key Formula:}
        \begin{equation}
            S_B w = \lambda S_W w
        \end{equation}
        where \(S_B\) is the between-class scatter matrix, \(S_W\) is the within-class scatter matrix, and \(w\) is the projection vector.
        \item \textbf{Example:} Projecting flower species data in a space that best separates species based on features.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{PCA:} Best for capturing variance using uncorrelated dimensions.
        \item \textbf{t-SNE:} Excellent for visualizing high-dimensional data while preserving local structure.
        \item \textbf{LDA:} Ideal for scenarios focused on class separation using class labels.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Dimensionality reduction techniques such as PCA, t-SNE, and LDA provide powerful tools for simplifying data analysis and visualization in machine learning. They make complex data easier to interpret while retaining the integrity of the underlying information.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Principal Component Analysis (PCA) - Introduction}
  \begin{block}{Concept}
    Principal Component Analysis (PCA) is a dimensionality reduction technique common in statistics and machine learning. 
    Its main goal is to reduce the number of variables in a dataset while preserving as much information as possible.
    PCA achieves this by transforming the original variables into a new set of uncorrelated variables known as principal components.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Principal Component Analysis (PCA) - Methodology}
  \begin{enumerate}
    \item \textbf{Data Standardization:}
      \begin{itemize}
        \item Scale the data such that each feature has a mean of 0 and a standard deviation of 1.
        \item \textbf{Formula:}
        \begin{equation}
          Z = \frac{(X - \mu)}{\sigma}
        \end{equation}
      \end{itemize}
    
    \item \textbf{Covariance Matrix Calculation:}
      \begin{itemize}
        \item Compute covariance to understand variable relationships.
        \item \textbf{Mathematical Representation:}
        \begin{equation}
          Cov(X, Y) = \frac{1}{n-1} \sum (X_i - \bar{X})(Y_i - \bar{Y})
        \end{equation}
      \end{itemize}
    
    \item \textbf{Eigenvalue and Eigenvector Computation:} 
      \begin{itemize}
        \item Calculate eigenvalues (variance magnitude) and eigenvectors (direction).
      \end{itemize}

    \item \textbf{Selecting Principal Components:} 
      \begin{itemize}
        \item Sort eigenvalues and select top \(k\) to form new feature subspace.
      \end{itemize}

    \item \textbf{Transformation of Data:}
      \begin{itemize}
        \item Transform original dataset using selected eigenvectors.
        \item \textbf{Transformation Formula:}
        \begin{equation}
          Y = X \cdot W
        \end{equation}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Principal Component Analysis (PCA) - Example and Key Points}
  \begin{block}{Example}
    Consider a dataset with two features, Height (X1) and Weight (X2). After applying PCA:
    \begin{itemize}
      \item 90\% of the variance can potentially be captured by a single principal component, reducing dimensions from 2D to 1D effectively.
    \end{itemize}
  \end{block}

  \begin{block}{Key Points}
    \begin{itemize}
      \item \textbf{Dimensionality Reduction:} Simplifies datasets with minimal information loss.
      \item \textbf{Variance Maximization:} Seeks to maximize variance in low-dimensional space.
      \item \textbf{Uncorrelated Components:} Principal components are uncorrelated and represent data differently.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA: Steps and Implementation - Overview}
    Principal Component Analysis (PCA) is a powerful technique used for reducing the dimensionality of large datasets, while preserving as much variance as possible. We will walk through the detailed steps necessary to implement PCA.
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA: Step 1 - Data Normalization}
    \begin{block}{Purpose}
    Eliminate biases from features with different scales, ensuring each feature contributes equally to the analysis.
    \end{block}
    
    \begin{block}{Method}
    Standardize the data by centering it (mean = 0) and scaling it (variance = 1).
    \end{block}
    
    \begin{equation}
        z_i = \frac{x_i - \mu}{\sigma}
    \end{equation}
    where:
    \begin{itemize}
        \item \( z_i \) = normalized value
        \item \( x_i \) = original value
        \item \( \mu \) = mean of the feature
        \item \( \sigma \) = standard deviation of the feature
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{PCA: Steps 2 to 6}
    \begin{enumerate}
        \item \textbf{Construct the Covariance Matrix}  
        Purpose: Identify how variables co-vary.  
        \begin{equation}
            C = \frac{1}{n-1} X^T X
        \end{equation}

        \item \textbf{Calculate Eigenvalues and Eigenvectors}  
        Purpose: Determine the principal components which form the new axes.  
        Solve: 
        \begin{equation}
            |C - \lambda I| = 0
        \end{equation}

        \item \textbf{Sort Eigenvalues and Eigenvectors}  
        Purpose: Rank eigenvalues from highest to lowest.

        \item \textbf{Select Principal Components}  
        Choose top \( k \) eigenvectors based on largest eigenvalues.

        \item \textbf{Transform the Original Data}  
        Create a new representation:  
        \begin{equation}
            Y = XW
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example Code}
    \begin{itemize}
        \item Normalization is crucial for meaningful PCA results.
        \item The covariance matrix encapsulates the relationships between variables.
        \item Eigenvalues and eigenvectors are the core components of PCA.
        \item Selecting the right number of components (\( k \)) balances complexity and performance.
    \end{itemize}

    \begin{block}{Example Python Code}
    \begin{lstlisting}[language=Python]
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Assume X is your data matrix
X_normalized = StandardScaler().fit_transform(X)

# Create covariance matrix
cov_matrix = np.cov(X_normalized.T)

# Apply PCA
pca = PCA(n_components=2)  # Reduce to 2 dimensions
X_reduced = pca.fit_transform(X_normalized)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Benefits of PCA - Introduction}
  \begin{block}{What is PCA?}
    Principal Component Analysis (PCA) is a powerful statistical technique used for dimensionality reduction. It transforms a large set of variables into a smaller one while retaining essential features.
  \end{block}
  \begin{block}{Key Benefits of PCA}
    \begin{itemize}
      \item Noise Reduction
      \item Visualization
      \item Feature Extraction
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Benefits of PCA - Key Benefits}
  \begin{enumerate}
    \item \textbf{Noise Reduction}
      \begin{itemize}
        \item PCA filters out noise for clearer insights.
        \item Projects data onto the principal components, minimizing random fluctuations.
        \item \textbf{Example:} In image processing, PCA removes background noise, enhancing image quality.
      \end{itemize}
    
    \item \textbf{Visualization}
      \begin{itemize}
        \item Enables visualization of high-dimensional data in 2D or 3D, facilitating pattern identification.
        \item Reduces dimensionality while preserving variance.
        \item \textbf{Example:} Reducing 50 features to 2 principal components for easier clustering visualization.
      \end{itemize}

    \item \textbf{Feature Extraction}
      \begin{itemize}
        \item Transforms original features into uncorrelated components.
        \item Retains significant aspects of data through linear combinations.
        \item \textbf{Example:} Identifying macroeconomic factors affecting stock prices.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Benefits of PCA - Key Points and Conclusion}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item \textbf{Data Compression:} PCA yields fewer dimensions, enhancing efficiency in storage and processing.
      \item \textbf{Improved Model Performance:} Faster training times and potential reductions in overfitting.
      \item \textbf{Uncovering Patterns:} PCA identifies hidden structures in data not evident from original features.
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    Utilizing PCA can provide considerable benefits across various fields such as data science, finance, and image processing. Understanding these advantages empowers practitioners to enhance their analyses.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitations of PCA - Overview}
  \begin{itemize}
    \item PCA has several limitations that may affect its applicability in different contexts.
    \item Key points to consider include:
      \begin{itemize}
        \item Linearity assumptions
        \item Challenges in interpretability
        \item Sensitivity to scaling
        \item Potential loss of information
        \item Outlier sensitivity
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitations of PCA - Linearity Assumptions}
  \begin{block}{1. Linearity Assumptions}
    \begin{itemize}
      \item PCA assumes linear relationships among data features.
      \item It captures only linear combinations of features; non-linear relationships may be missed.
    \end{itemize}
  \end{block}
  
  \begin{example}
    \begin{itemize}
      \item Consider a dataset with a quadratic relationship between two features:
      \item PCA will struggle to represent the relationship accurately, fitting a linear line instead.
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitations of PCA - Interpretability and Sensitivity}
  \begin{block}{2. Interpretability}
    \begin{itemize}
      \item Principal components are combinations of original variables, making interpretation difficult.
    \end{itemize}
  \end{block}
  
  \begin{example}
    \begin{itemize}
      \item A principal component formed from height and weight may lack a clear contextual meaning.
    \end{itemize}
  \end{example}
  
  \begin{block}{3. Sensitive to Scaling}
    \begin{itemize}
      \item PCA requires data standardization (mean = 0, variance = 1) for optimal results.
      \item Features on different scales can disproportionately influence results.
    \end{itemize}
  \end{block}
  
  \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Example of scaling and applying PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Overview of t-SNE}
  \begin{block}{What is t-SNE?}
    t-Distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualizing high-dimensional data in a lower-dimensional space (2D or 3D). It is particularly effective in maintaining local structures, making it useful for clustering insights.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts of t-SNE}
  \begin{itemize}
    \item \textbf{High-Dimensional Data Visualization}
      \begin{itemize}
        \item Reduces complexity by mapping high-dimensional data to lower-dimensional spaces.
      \end{itemize}
    \item \textbf{Preservation of Local Structures}
      \begin{itemize}
        \item Focuses on keeping similar points together, highlighting clusters.
      \end{itemize}
    \item \textbf{Probability Distributions}
      \begin{itemize}
        \item Converts high-dimensional distances into conditional probabilities using Gaussian and Student's t-distributions.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Mathematical Representation}
  The probability \( p_{ij} \) that point \( j \) picks point \( i \) as a neighbor is given by:
  \begin{equation}
    p_{ij} = \frac{exp(-||x_i - x_j||^2/2\sigma_i^2)}{\sum_{k \neq i} exp(-||x_i - x_k||^2/2\sigma_i^2)}
  \end{equation}
  
  In the low-dimensional space, the corresponding probability \( q_{ij} \) is defined as:
  \begin{equation}
    q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_i - y_k||^2)^{-1}}
  \end{equation}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of t-SNE}
  t-SNE is utilized in various fields including:
  \begin{itemize}
    \item \textbf{Image Processing}
      \begin{itemize}
        \item Visualizing image similarities in datasets like MNIST or ImageNet.
      \end{itemize}
    \item \textbf{Genomics}
      \begin{itemize}
        \item Clustering visualizations for gene expressions.
      \end{itemize}
    \item \textbf{Natural Language Processing}
      \begin{itemize}
        \item Creating embeddings for words and sentences to showcase similarities.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Visualizing Handwritten Digits}
  \begin{block}{Handwritten Digits}
    Using t-SNE on the MNIST dataset allows visualization of digits in 2D space. Each cluster represents a different digit, revealing similarities based on pixel distributions.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item t-SNE is particularly effective for non-linear relationships and emphasizes local structures.
    \item Sensitive to hyperparameters, particularly perplexity, which affects neighbor consideration during embedding.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Code Snippet Example}
  \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Sample Usage
X_embedded = TSNE(n_components=2).fit_transform(high_dimensional_data)

# Plotting the results
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=labels)
plt.title('t-SNE Visualization of High-Dimensional Data')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.show()
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  t-SNE is a powerful technique for visualizing complex high-dimensional datasets, facilitating the understanding of data distributions and the identification of underlying patterns effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Local Dimensionality Reduction Techniques}
    \begin{block}{Introduction}
        Dimensionality reduction reduces the number of features under consideration, capturing the underlying structure of high-dimensional data. 
        Local dimensionality reduction techniques focus on preserving local relationships. This slide introduces **Locally Linear Embedding (LLE)** and compares it to **PCA** and **t-SNE**.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Locally Linear Embedding (LLE)}
    \begin{itemize}
        \item \textbf{Concept}: Preserves neighborhoods, assuming local linearity of data points.
        \item \textbf{Process}:
        \begin{enumerate}
            \item Neighbor Identification: Identify 'k' nearest neighbors for each point.
            \item Reconstruction: Express each data point as a linear combination of its neighbors.
            \item Weight Computation: Minimize reconstruction error to find weights.
            \item Embedding: Solve for lower-dimensional embedding preserving local relationships.
        \end{enumerate}
        \item \textbf{Advantages}: Captures complex structures and is robust to noise.
        \item \textbf{Use Case Example}: Image recognition to enhance model robustness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison to PCA and t-SNE}
    \begin{block}{Key Comparisons}
        \begin{tabular}{|l|l|l|l|}
            \hline
            \textbf{Feature} & \textbf{PCA} & \textbf{t-SNE} & \textbf{LLE} \\
            \hline
            Approach & Global linear transformations & Nonlinear probabilistic approach & Preserves local relationships \\
            \hline
            Data Assumption & Linear structure in global data & Nonlinear structure in high-dimensional space & Local linear structure \\
            \hline
            Output & Components ordered by variance & Probabilistic distance between points & Coordinates maintaining local structure \\
            \hline
            Visualization & Ellipsoidal clusters & Well-suited for visualizing data clusters & Effective for multidimensional manifolds \\
            \hline
        \end{tabular}
    \end{block}
    \begin{block}{Application}
        \begin{itemize}
            \item \textbf{PCA}: Projects data onto principal components, often losing local structures.
            \item \textbf{t-SNE}: Excellent for high-dimensional visualizations, maintaining local relationships.
            \item \textbf{LLE}: Balances local structures and scalability across certain data types.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example Code}
    \begin{itemize}
        \item LLE is useful for data on low-dimensional manifolds.
        \item Preserving local relationships is essential in clustering and feature extraction.
        \item The choice of technique depends on data characteristics and project goals.
    \end{itemize}
    \begin{block}{Example Code Snippet (Python)}
    \begin{lstlisting}[language=Python]
from sklearn.manifold import LocallyLinearEmbedding

# Example data: X is your high-dimensional dataset
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_embedded = lle.fit_transform(X)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction in Practice}
    \begin{block}{Overview of Dimensionality Reduction}
        Dimensionality reduction techniques transform high-dimensional data into a lower-dimensional form while retaining essential characteristics. 
        This practice is crucial for:
        \begin{itemize}
            \item Improving model performance
            \item Mitigating overfitting
            \item Enhancing interpretability
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies Highlighting Impact}
    \begin{enumerate}
        \item \textbf{Image Compression}
        \begin{itemize}
            \item \textbf{Technique Used}: Principal Component Analysis (PCA) 
            \item \textbf{Application}: Improves efficiency in processing and storage of high-dimensional image data.
            \item \textbf{Result}: Key features extracted for compression without significant quality loss.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Image Compression - Example}
    \begin{lstlisting}[language=Python]
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

digits = load_digits()
X = digits.data

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=digits.target)
plt.title("PCA of Digits Dataset")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies Highlighting Impact (Cont'd)}
    \begin{enumerate}[resume]
        \item \textbf{Natural Language Processing}
        \begin{itemize}
            \item \textbf{Technique Used}: t-Distributed Stochastic Neighbor Embedding (t-SNE)
            \item \textbf{Application}: Visualization of word embeddings for semantic representation.
            \item \textbf{Result}: Clustering documents or words enhances topic modeling and sentiment analysis.
        \end{itemize}
        
        \item \textbf{Genomic Data Analysis}
        \begin{itemize}
            \item \textbf{Technique Used}: Uniform Manifold Approximation and Projection (UMAP)
            \item \textbf{Application}: Visualization of complex biological data structures.
            \item \textbf{Result}: Uncovering relationships and clusters related to genetic traits or disease states.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{UMAP Formula}
    While details are complex, key components include:
    \begin{equation}
        \text{UMAP}(X) = \text{minimize} \left\{ \sum_{i \in \text{data}} \sum_{j \in \text{neighbors}} (d_{ij}^2 - Q_{ij})^2 \right\}
    \end{equation}
    where \(d_{ij}\) are distances in the original space and \(Q_{ij}\) are distances in the reduced space.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Improved Performance}: Reduced noise and computational load lead to better model accuracy.
        \item \textbf{Enhanced Interpretability}: Lower dimensions facilitate visualization and understanding of data patterns.
        \item \textbf{Wide Applicability}: Versatile across domains, including images, texts, and genomics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Dimensionality reduction optimizes computational resources and enriches model understanding, making it an indispensable technique in data science and machine learning practices. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction in Preprocessing - Overview}
    Dimensionality reduction is a crucial step in the data preprocessing pipeline for machine learning. 
    \begin{itemize}
        \item Reduces the number of input variables (features) while preserving information.
        \item Enhances model performance and speeds up computations.
        \item Aids in visualization of high-dimensional data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensionality Reduction Techniques}
    \textbf{Key Techniques in Dimensionality Reduction}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA)}:
        \begin{itemize}
            \item Transforms dataset into uncorrelated variables (principal components).
            \item \textbf{Formula}: Given a data matrix \(X\):
            \begin{equation}
                Z = XW
            \end{equation}
            where \(W\) contains the eigenvectors of the covariance matrix of \(X\).
            \item \textbf{Example}: Reduces number of pixels in images while retaining essential patterns.
        \end{itemize}
        
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}:
        \begin{itemize}
            \item Designed for visualizing high-dimensional data in 2 or 3 dimensions.
            \item Retains structure and relationships, ideal for clustering.
        \end{itemize}
        
        \item \textbf{Linear Discriminant Analysis (LDA)}:
        \begin{itemize}
            \item Focuses on preserving class separability for classification tasks.
            \item Used when labeled data is available, e.g., classifying wines.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance and Integration of Dimensionality Reduction}
    \textbf{Importance of Dimensionality Reduction}
    \begin{itemize}
        \item \textbf{Computational Efficiency}: Faster training times for ML algorithms.
        \item \textbf{Overfitting Mitigation}: Helps avoid complexity, improving generalization.
        \item \textbf{Improved Visualizations}: Enhances interpretability and insights into data structure.
    \end{itemize}

    \textbf{Integration in Data Preprocessing Pipeline}
    \begin{enumerate}
        \item Data Cleaning (address missing values, outliers)
        \item Feature Selection (importance assessment and initial cuts)
        \item Dimensionality Reduction (e.g., PCA or t-SNE)
        \item Model Training (fitting ML models with transformed data)
        \item Evaluation (analyzing model performance with relevant metrics)
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Dimensionality Reduction Techniques - Introduction}
    \begin{block}{Introduction to Evaluation}
        Evaluating the effectiveness of dimensionality reduction techniques is crucial for determining how well the method preserves essential information while reducing data complexity. The goal is to create a model that maintains the integrity of the original dataset with fewer dimensions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Criteria}
    When assessing dimensionality reduction techniques, consider the following metrics:
    \begin{itemize}
        \item \textbf{Preservation of Variance:}
            \begin{itemize}
                \item Measure how much variance from the original dataset is retained after transformation.
                \item \textit{Example:} In Principal Component Analysis (PCA), calculate the explained variance ratio of the reduced dimensions.
            \end{itemize}
        \item \textbf{Reconstruction Error:}
            \begin{equation}
                \text{Reconstruction Error} = || \mathbf{X} - \mathbf{X'} ||^2
            \end{equation}
            Where \( \mathbf{X} \) is the original data and \( \mathbf{X'} \) is the reconstructed data.
        \item \textbf{Classification Performance:}
            \begin{itemize}
                \item Assess the impact of reduced dimensions on the performance of downstream machine learning tasks.
                \item \textit{Example:} Compare accuracy, precision, recall, and F1-score of classifiers before and after applying dimensionality reduction techniques.
            \end{itemize}
        \item \textbf{Pairwise Distance Preservation:}
            \begin{itemize}
                \item Analyze how well the distances between data points are maintained.
                \item \textit{Key Point:} If points that were close together in the original space remain close in the reduced space, the technique has performed well.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods of Evaluation}
    To effectively evaluate dimensionality reduction techniques:
    \begin{itemize}
        \item \textbf{Visual Inspection:}
            \begin{itemize}
                \item Use scatter plots or projection techniques to visually assess how well dimensions are preserved.
                \item \textit{Example:} Visualizing PCA components to see cluster formation.
            \end{itemize}
        \item \textbf{Statistical Tests:} Implement tests like MANOVA to quantify differences in group means across reduced dimensions.
        \item \textbf{Cross-Validation:} Apply k-fold cross-validation to ensure that the results are robust and not due to overfitting on a particular dataset.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Techniques}
    Some common dimensionality reduction techniques include:
    \begin{itemize}
        \item \textbf{PCA:} Focuses on maximal variance and linear relationships.
        \item \textbf{t-SNE:} Focuses on preserving local similarities in high-dimensional space, effective for data visualization.
        \item \textbf{UMAP:} Balances local and global structures efficiently and retains more topological structure than t-SNE.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Effectively evaluating dimensionality reduction techniques is critical in selecting the appropriate method for your dataset and machine learning task. By thoroughly assessing variance preservation, reconstruction accuracy, classification performance, and distance relationships, you can better ensure that the reduced dataset adheres to the original data's integrity.

    \begin{block}{Key Takeaway}
        Always evaluate dimensionality reduction methods against your specific objectives in prediction and analysis while considering the trade-offs involved.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations in Dimensionality Reduction Techniques}
  
  \begin{itemize}
    \item Dimensionality reduction simplifies datasets but raises ethical concerns.
    \item Key implications include:
      \begin{itemize}
        \item Data integrity
        \item Privacy
        \item Model interpretability
      \end{itemize}
    \item Emphasizes the importance of transparency and responsible data practices.
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Ethical Implications}
  
  \begin{block}{Data Integrity}
    \begin{itemize}
      \item Reducing dimensions may obscure data structures.
      \item \textbf{Example:} Health data visualizations may overlook critical correlations, leading to poor decisions.
    \end{itemize}
  \end{block}
  
  \begin{block}{Privacy Issues}
    \begin{itemize}
      \item Risk of re-identification in sensitive data after reduction.
      \item \textbf{Example:} Genomic data can potentially violate privacy through cohort identification.
    \end{itemize}
  \end{block}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Interpretability and Transparency}
  
  \begin{block}{Interpretability of Models}
    \begin{itemize}
      \item Reduced dimensions can make models hard to interpret.
      \item \textbf{Example:} Complex features in loan default models can hinder understanding of applicant decisions.
    \end{itemize}
  \end{block}
  
  \begin{block}{Importance of Transparency}
    \begin{itemize}
      \item Document and communicate feature selection/transformation.
      \item Essential for fostering trust among users and stakeholders.
    \end{itemize}
  \end{block}
  
  \begin{block}{Responsible Data Practices}
    \begin{enumerate}
      \item Assess potential impacts of reduced datasets.
      \item Consult stakeholders on acceptable data loss and bias levels.
      \item Conduct post-reduction sensitivity analyses.
    \end{enumerate}
  \end{block}
  
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Recap of Key Points on Dimensionality Reduction Techniques}
        Dimensionality reduction is the process of reducing the number of features in a dataset while retaining essential information. It is crucial for simplifying models, improving visualization, and reducing computational costs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques Covered}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA)}
            \begin{itemize}
                \item \textbf{Concept:} Identifies directions in which data varies the most.
                \item \textbf{Example:} Reduces image features while retaining significant components.
                \item \textbf{Formula:} \[
                \text{Cov}(X) = \frac{1}{n-1} X^T X
                \]
            \end{itemize}
            
        \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}
            \begin{itemize}
                \item \textbf{Concept:} Non-linear method for visualizing high-dimensional data.
                \item \textbf{Example:} Used for visualizing word embeddings in natural language processing.
                \item \textbf{Key Feature:} Preserves local data structure.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{More Techniques and Conclusions}
    \begin{enumerate}[resume]
        \item \textbf{Linear Discriminant Analysis (LDA)}
            \begin{itemize}
                \item \textbf{Concept:} Supervised technique for maximizing class separation.
                \item \textbf{Example:} Differentiating healthy and diseased states in medical diagnostics.
                \item \textbf{Goal:} Maximize the ratio of between-class variance to within-class variance.
            \end{itemize}
            
        \item \textbf{Autoencoders}
            \begin{itemize}
                \item \textbf{Concept:} Neural networks for encoding and decoding data.
                \item \textbf{Example:} Denoising images by reconstructing them post-noise removal.
                \item \textbf{Structure:} Consists of an encoder and a decoder.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Importance of Dimensionality Reduction}
        \begin{itemize}
            \item Improved model performance.
            \item Enhanced data visualization.
            \item Noise reduction and interpretability.
        \end{itemize}
    \end{block}
    
    \begin{block}{Ethical Considerations}
        \begin{itemize}
            \item Bias and interpretability must be considered carefully.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Dimensionality reduction is essential in data science, enabling effective analysis, visualization, and modeling of high-dimensional data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Preparation for Q\&A}
    Consider exploring questions related to:
    \begin{itemize}
        \item Practical applications of these techniques.
        \item Challenges faced during dimensionality reduction.
        \item Ethical implications and how to address them.
    \end{itemize}
    
    Reviewing these points will ensure a thorough understanding of dimensionality reduction techniques and their implications in real-world scenarios.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session}
    Open floor for questions and clarifications regarding the content presented.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Dimensionality Reduction}
    \begin{block}{Description}
        This slide serves as an open platform for students to ask questions and gain clarity on the Dimensionality Reduction Techniques covered in Week 11.
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Dimensionality Reduction (DR)}:
        \begin{itemize}
            \item \textbf{Definition:} A set of techniques used to reduce the number of features (dimensions) in a dataset while retaining essential information.
            \item \textbf{Purpose:} Simplifies data analysis, enhances visualization, and speeds up algorithm performance.
        \end{itemize}
        
        \item \textbf{Common Techniques Discussed}:
        \begin{itemize}
            \item Principal Component Analysis (PCA)
            \item t-Distributed Stochastic Neighbor Embedding (t-SNE)
            \item Linear Discriminant Analysis (LDA)
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Applications of Dimensionality Reduction}
    
    \begin{block}{Example Scenarios for Discussion}
        \begin{enumerate}
            \item \textbf{Example of PCA Usage:}
            \begin{itemize}
                \item Using PCA, a data scientist reduces a dataset of customer buying behaviors to the top 2-3 principal components, revealing major variance.
            \end{itemize}
    
            \item \textbf{Example of t-SNE Application:}
            \begin{itemize}
                \item t-SNE visualizes clustering patterns in a dataset of handwritten digits, demonstrating similarity in a reduced 2D plane.
            \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Know when to apply each technique (linear vs. non-linear).
            \item Understand the trade-offs: simplifying may reduce model fidelity.
            \item Importance of visualizing data before and after DR techniques for better understanding.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Preparation for Q\&A}
    
    \begin{itemize}
        \item Think of specific challenges faced during projects that may relate to dimensionality reduction.
        \item Prepare questions on technical details like component selection in PCA or parameter settings in t-SNE.
        \item Be ready to discuss real-world advantages and limitations of applying these techniques.
    \end{itemize}
    
    \begin{block}{Goal for the Session}
        This session aims to enhance your grasp of dimensionality reduction techniques and how they can be leveraged in various contexts. Feel free to ask any clarifying questions or share your experiences related to these methods!
    \end{block}
\end{frame}


\end{document}