\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 7: Neural Networks Basics]{Week 7: Neural Networks Basics}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Overview}
    \begin{block}{Overview of Neural Networks}
        Neural networks are computational models inspired by the human brain, designed to recognize patterns and solve complex problems. They consist of interconnected nodes or "neurons" that process input data, learn from it, and output predictions or classifications. 
    \end{block}
    \begin{itemize}
        \item Excelling in tasks such as:
        \begin{itemize}
            \item Image recognition
            \item Natural language processing
            \item Game playing
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Importance}
    \begin{block}{Importance in Machine Learning}
        Neural networks have revolutionized the field by providing the ability to learn from vast amounts of data. 
    \end{block}
    \begin{itemize}
        \item \textbf{Versatility}: Applicable to areas like computer vision, speech recognition, and medical diagnosis.
        \item \textbf{Performance}: Superior accuracy compared to traditional algorithms in handling complex, non-linear relationships.
        \item \textbf{Deep Learning}: A subset that uses multiple layers of neurons to enhance performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Chapter Overview}
    \begin{block}{What’s Covered in This Chapter}
        \begin{enumerate}
            \item Definition and Structure: Fundamental components of neural networks.
            \item Functions and Activations: Exploration of activation functions (e.g., Sigmoid, ReLU, Softmax).
            \item Learning Process: Overview of forward propagation, loss functions, and backpropagation.
            \item Types of Neural Networks: Feedforward, CNNs, and RNNs.
            \item Practical Implications: Real-world applications and case studies.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Interconnected Nodes}: Neurons are connected similar to a biological brain, facilitating information sharing.
            \item \textbf{Learning from Data}: Neural networks improve predictions through training on datasets.
            \item \textbf{Relevance to Modern Tech}: Foundation of innovations in AI technology today.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Conclusion}
    \begin{block}{Conclusion}
        Understanding neural networks is critical for anyone interested in machine learning and artificial intelligence. This chapter will provide a solid foundation for engaging with more complex concepts and applications in future discussions.
    \end{block}
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - Definition}
    A \textbf{neural network} is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected nodes or "neurons" that work together to solve complex problems by finding patterns in data.
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - How They Mimic the Human Brain}
    \begin{enumerate}
        \item \textbf{Neurons}:
        \begin{itemize}
            \item In both biological and artificial neural networks, neurons are the fundamental units. Each neuron receives input, processes it, and produces output.
            \item Example: Just as human neurons receive signals from other neurons, artificial neurons receive weighted inputs from the previous layer.
        \end{itemize}
        
        \item \textbf{Connections and Weights}:
        \begin{itemize}
            \item Neurons are connected via edges (similar to synapses in the brain). Each connection has a weight that adjusts as learning proceeds.
            \item Illustration: The connection weight determines the strength of the signal sent from one neuron to another. Higher weights mean a stronger influence on the neuron’s activation.
        \end{itemize}
        
        \item \textbf{Layers}:
        \begin{itemize}
            \item \textbf{Input Layer}: Takes in the initial data.
            \item \textbf{Hidden Layers}: Intermediate processing layers that transform the input into something the output layer can use.
            \item \textbf{Output Layer}: Provides the final output, making predictions or classifications.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - Activation Functions and Purpose}
    \begin{itemize}
        \item \textbf{Activation Functions}:
        \begin{itemize}
            \item Activation functions determine whether a neuron should be activated (fired) based on the input it receives.
            \item Common activation functions include:
            \begin{itemize}
                \item \textbf{Sigmoid}: \( f(x) = \frac{1}{1 + e^{-x}} \) (outputs between 0 and 1)
                \item \textbf{ReLU (Rectified Linear Unit)}: \( f(x) = \max(0, x) \) (outputs 0 for negative inputs and returns the input for positive inputs)
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Purpose in Machine Learning}:
        \begin{itemize}
            \item Used for:
            \begin{itemize}
                \item \textbf{Classification Tasks}: Recognizing patterns in data, such as identifying an email as spam or not.
                \item \textbf{Regression Tasks}: Predicting continuous values, like estimating house prices based on features.
                \item \textbf{Feature Extraction}: Automatically identifying the most relevant features in data.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Neural networks learn from data by adjusting weights based on the error of their predictions during training.
            \item The architecture of a neural network can vary widely (e.g., feedforward, convolutional, recurrent), each suited for different types of tasks.
            \item Neural networks excel in handling unstructured data such as images, audio, and text, making them powerful tools in machine learning.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding neural networks' foundational concepts is crucial for grasping how they function in machine learning. This knowledge sets the stage for exploring their components and architectures in subsequent slides.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Basic Structure of Neural Networks}
  Neural networks consist of several fundamental components that work together to process input and produce output. The core elements include:
  \begin{itemize}
    \item Neurons
    \item Layers
    \item Weights
    \item Biases
    \item Activation Functions
  \end{itemize}
  Understanding these components is essential for grasping how neural networks function.
\end{frame}

\begin{frame}[fragile]
  \frametitle{1. Neurons}
  \begin{block}{Definition}
    Neurons are the basic building blocks of a neural network. Each neuron receives input, processes it, and produces an output.
  \end{block}
  
  \begin{block}{Function}
    A neuron performs a weighted sum of its inputs, adds a bias, and applies an activation function.
  \end{block}

  \begin{example}
    If a neuron has two inputs, \( x_1 \) and \( x_2 \) with weights \( w_1 \) and \( w_2 \), the output \( y \) can be calculated as:
    \begin{equation}
      y = \text{activation}(w_1 \cdot x_1 + w_2 \cdot x_2 + b)
    \end{equation}
    where \( b \) is the bias.
  \end{example}
\end{frame}

\begin{frame}[fragile]
  \frametitle{2. Layers}
  \begin{block}{Structure}
    Neural networks are organized into layers:
    \begin{itemize}
      \item \textbf{Input Layer}: The first layer that receives the initial input data.
      \item \textbf{Hidden Layers}: Intermediate layers where computations are performed. There can be one or multiple hidden layers.
      \item \textbf{Output Layer}: The final layer that produces the network's output.
    \end{itemize}
  \end{block}

  \begin{example}
    A simple neural network can have:
    \begin{itemize}
      \item 1 Input Layer with 3 neurons (features)
      \item 1 Hidden Layer with 4 neurons
      \item 1 Output Layer with 2 neurons (binary classification)
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3. Weights, Biases, and Activation Functions}
  \begin{itemize}
    \item \textbf{Weights}: Represent the strength of the connection between neurons, adjusting during training to minimize error. Higher weights increase the influence of an input on the neuron's output.
    
    \item \textbf{Biases}: A constant added to the weighted sum of inputs, allowing flexibility to adjust the output along with the weighted inputs.
    \begin{equation}
      y = w_1 \cdot x_1 + w_2 \cdot x_2 + b
    \end{equation}
    
    \item \textbf{Activation Functions}: Determine whether a neuron should be activated based on input. Common types include:
    \begin{itemize}
      \item \textbf{Sigmoid}: Outputs values between 0 and 1.
      \begin{equation}
        \sigma(x) = \frac{1}{1 + e^{-x}}
      \end{equation}
      \item \textbf{ReLU}: Outputs the input directly if positive; otherwise, it outputs zero.
      \begin{equation}
        \text{ReLU}(x) = \max(0, x)
      \end{equation}
      \item \textbf{Softmax}: Used for multi-class classification, providing probabilities across classes.
    \end{itemize}
  \end{itemize}
  
  \begin{block}{Summary}
    Neurons process inputs through weights, biases, and activation functions. Layers organize computations into input, hidden, and output stages. Understanding these components provides a solid foundation for exploring more complex neural network architectures.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Introduction}
    % Overview of neural networks
    Neural networks are a set of algorithms inspired by the human brain, designed to recognize patterns. 
    They consist of layers of interconnected neurons and can be categorized based on their architecture and applications. 
    This slide explores three main types of neural networks:
    \begin{itemize}
        \item Feedforward Neural Networks
        \item Convolutional Neural Networks
        \item Recurrent Neural Networks
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Feedforward Neural Networks (FNN)}
    % Description and applications of FNN
    \textbf{Description:}
    \begin{itemize}
        \item The simplest type of artificial neural network with one-way information flow—from input to output.
        \item No cycles or loops are present.
    \end{itemize}

    \textbf{Applications:}
    \begin{itemize}
        \item Classification and regression tasks.
        \item Commonly used in image recognition and financial forecasting.
    \end{itemize}

    \textbf{Key Points:}
    \begin{itemize}
        \item Structure: Composed of input layer, hidden layer(s), and output layer.
        \item Activation Functions: Sigmoid, ReLU, etc., introduce non-linear behavior.
    \end{itemize}

    \textbf{Sample Equation:}
    \begin{equation}
        y = f(W \cdot x + b)
    \end{equation}
    Where: 
    \begin{itemize}
        \item \( W \) = weights
        \item \( x \) = input features
        \item \( b \) = bias
        \item \( f \) = activation function
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Convolutional and Recurrent Neural Networks}
    % Overview of CNN and RNN
    \textbf{2. Convolutional Neural Networks (CNN)}
    \begin{itemize}
        \item Designed for grid-like data, particularly images.
        \item Uses convolutional layers, pooling layers, and fully connected layers.
    \end{itemize}
    \textbf{Applications:}
    \begin{itemize}
        \item Image recognition, video analysis, and medical image analysis.
        \item Widely used in facial recognition and self-driving car perception.
    \end{itemize}

    \textbf{Recall: Convolution Operation}
    \begin{equation}
        (S * I)(x, y) = \sum_{i}\sum_{j} S(i, j) I(x - i, y - j)
    \end{equation}

    \textbf{3. Recurrent Neural Networks (RNN)}
    \begin{itemize}
        \item Suitable for sequence data, where outputs depend on previous inputs.
        \item Allows information to persist through cycles within the network.
    \end{itemize}
    \textbf{Applications:}
    \begin{itemize}
        \item Natural language processing, language translation, and speech recognition.
        \item Effective for predicting time-series data, like stock prices.
    \end{itemize}

    \textbf{Basic Structure:}
    \begin{verbatim}
                h(t-1)  ---->  h(t)
                 ^   |
                 |   v
    Input(t) ----->  h(t) -----> Output(t)
    \end{verbatim}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Neurons and Activation Functions}

  \begin{block}{Understanding Neurons}
    Neurons are the fundamental building blocks of neural networks, simulating how the human brain processes information. Each neuron:
    \begin{itemize}
      \item Receives multiple inputs $x_1, x_2, \ldots, x_n$.
      \item Has weights $w_1, w_2, \ldots, w_n$ associated with each input.
      \item Includes a bias $b$ for improved prediction accuracy.
      \item Applies a non-linear activation function $f$ to produce an output.
    \end{itemize}

    \begin{equation}
      y = f\left(\sum_{i=1}^{n} w_i \cdot x_i + b\right) 
    \end{equation}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Activation Functions}

  Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Below are three popular activation functions:

  \begin{enumerate}
    \item \textbf{Sigmoid Function:}
      \begin{equation}
        f(x) = \frac{1}{1 + e^{-x}} 
      \end{equation}
      \begin{itemize}
        \item Output Range: $(0, 1)$
        \item Produces S-like curve, prone to vanishing gradients.
      \end{itemize}

    \item \textbf{Hyperbolic Tangent Function (tanh):}
      \begin{equation}
        f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} 
      \end{equation}
      \begin{itemize}
        \item Output Range: $(-1, 1)$
        \item Zero-centered, can improve training speed.
      \end{itemize}

    \item \textbf{Rectified Linear Unit (ReLU):}
      \begin{equation}
        f(x) = \max(0, x) 
      \end{equation}
      \begin{itemize}
        \item Output Range: $[0, \infty)$
        \item Reduces vanishing gradient issues, but may lead to "dying ReLU" problem.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}

  \begin{itemize}
    \item Neurons process inputs through weighted sums and activation functions.
    \item Different activation functions, such as Sigmoid, tanh, and ReLU, have distinct characteristics and use cases.
    \item The choice of activation function significantly affects a model's learning capability and overall performance.
  \end{itemize}

  \begin{block}{Visual Aid Suggestion}
    Include a diagram illustrating a neuron with its inputs, weights, bias, and output, along with graphs showing the three activation functions.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedforward Neural Networks - Overview}
    \begin{block}{Definition}
        Feedforward Neural Networks (FNN) are the simplest type of artificial neural network where information flows in one direction—from input to output, without cycles.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Input Layer}: Receives input data.
        \item \textbf{Hidden Layers}: Process inputs using weights and activation functions.
        \item \textbf{Output Layer}: Produces final output based on processed information.
    \end{itemize}
    
    \begin{block}{Activation Functions}
        Each neuron applies an activation function (e.g., sigmoid, ReLU) to introduce non-linearity, enabling FNNs to learn complex patterns.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedforward Neural Networks - Example}
    \begin{block}{Example: Handwritten Digit Classification}
        Consider a simple feedforward neural network used to classify handwritten digits (0-9).
    \end{block}
    
    \begin{itemize}
        \item \textbf{Input Layer}: Each neuron may represent a pixel value (e.g., 784 neurons for a 28x28 image).
        \item \textbf{Hidden Layer}: One hidden layer with 128 neurons receiving weighted inputs from all input neurons.
        \item \textbf{Activation Function}: Using ReLU, output for each neuron is calculated as \( \text{ReLU}(w_1 \cdot x_1 + ... + w_n \cdot x_n + b) \).
        \item \textbf{Output Layer}: 10 neurons representing each digit; the neuron with the highest value indicates the predicted digit.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedforward Neural Networks - Mathematical Representation}
    \begin{block}{Mathematical Expressions}
        \textbf{Weighted Sum}:
        \begin{equation}
            z_j = \sum_{i=1}^{n} w_{ij} x_i + b_j
        \end{equation}
        
        \textbf{Activation Output}:
        \begin{equation}
            a_j = f(z_j)
        \end{equation}
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Points}:
            \begin{itemize}
                \item Unidirectional flow from inputs to outputs.
                \item Versatile applications in classification and regression.
                \item Importance of choosing the correct activation function.
                \item Scalability with varying hidden layers and neurons.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backward Propagation Process}
    \begin{block}{Understanding Backpropagation}
        Backpropagation is a training algorithm for neural networks that computes the gradient of the loss function with respect to each weight by applying the chain rule. This gradient information is then used to update the weights to minimize the loss and improve the model's accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Backpropagation Works}
    \begin{enumerate}
        \item \textbf{Initial Forward Pass}:
        \begin{itemize}
            \item Data is fed into the neural network, layer by layer.
            \item Each neuron applies an activation function to generate outputs.
        \end{itemize}
        
        \item \textbf{Calculating Loss}:
        \begin{itemize}
            \item The output is compared to the true label using a loss function (e.g., Mean Squared Error).
            \item The loss quantifies model performance.
        \end{itemize}
        
        \item \textbf{Backward Pass}:
        \begin{itemize}
            \item Gradients of the loss with respect to each weight are computed, moving from output to input layer.
            \item \begin{equation}
                \frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Backpropagation Works (cont.)}
    \begin{enumerate}[resume]
        \item \textbf{Weight Update}:
        \begin{itemize}
            \item Weights are updated using the calculated gradients:
            \begin{equation}
                w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w}
            \end{equation}
            \item Where \( \eta \) is the learning rate.
        \end{itemize}

        \item \textbf{Iterative Process}:
        \begin{itemize}
            \item Steps 1-4 are repeated for multiple epochs, refining weights to minimize loss.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Role of Learning Rate}:
        A critical hyperparameter influencing weight adjustments. Too high may overshoot minimums; too low may slow convergence.
        
        \item \textbf{Importance of Gradients}:
        Gradients indicate the direction to adjust weights. Understanding their flow through the network is crucial for training.
        
        \item \textbf{Efficiency}:
        Backpropagation is efficient due to the reuse of computations during the forward pass, making it suitable for large networks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Imagine training a simple neural network to classify images of cats and dogs:
    \begin{itemize}
        \item After computing the loss, backpropagation finds how each weight contributed to the error.
        \item If a weight associated with a feature (e.g., pointy ears) indicates that slight adjustments reduce classification error, it is reinforced in future iterations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Backpropagation is essential in training neural networks. It systematically adjusts weights based on prediction errors, laying the foundation for effective AI systems.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Function - Understanding the Concept}
    \begin{block}{Definition}
        A Loss Function (or Cost Function) quantifies how well a neural network's predictions match the actual results. It measures the error between predicted values and the true labels.
    \end{block}

    \begin{block}{Importance in Training Neural Networks}
        \begin{itemize}
            \item \textbf{Performance Metric}: Provides a single number indicating model performance; lower values imply better performance.
            \item \textbf{Guidance for Learning}: Critical for the training process, guiding the optimization algorithm to adjust model weights.
            \item \textbf{Backpropagation}: The gradient of the loss function concerning model parameters updates weights and minimizes error.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Function - Common Examples}
    \begin{block}{Mean Squared Error (MSE)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the average of the squares of the errors—the average squared difference between estimated values and actual value.
            \item \textbf{Formula}:
            \begin{equation}
                \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
            \end{equation}
            Where:
            \begin{itemize}
                \item \( n \) = number of samples
                \item \( \hat{y}_i \) = predicted value for sample \( i \)
                \item \( y_i \) = actual value for sample \( i \)
            \end{itemize}
            \item \textbf{Usage}: Commonly used in regression problems where the target output is continuous.
        \end{itemize}
    \end{block}

    \begin{block}{Cross-Entropy Loss}
        \begin{itemize}
            \item \textbf{Definition}: Measures the difference between predicted probability distribution and actual distribution (ground truth).
            \item \textbf{Formula}:
            \begin{equation}
                \text{Cross-Entropy} = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
            \end{equation}
            Where:
            \begin{itemize}
                \item \( C \) = number of classes
                \item \( y_i \) = binary indicator (0 or 1) if class label \( i \) is the correct classification
                \item \( \hat{y}_i \) = predicted probability for class \( i \)
            \end{itemize}
            \item \textbf{Usage}: Typically used in classification problems, especially for multi-class scenarios.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Function - Key Points and Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The choice of loss function is crucial, influencing the behavior of the learning algorithm.
            \item \textbf{Overfitting and Underfitting}: Poor selection can lead to overfitting (model learns noise) or underfitting (model does not learn enough).
            \item Loss functions can be tailored to specific tasks; understanding their characteristics helps in model selection.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        The loss function is fundamental in training neural networks, acting as a measure that drives learning. By optimizing weights based on loss values, neural networks improve predictions over time. Proper understanding and selection of the loss function is essential for building effective models.
    \end{block}

    \begin{block}{Next Steps}
        Next, we will explore Optimization Algorithms that utilize the loss function to refine the performance of neural networks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Algorithms}
    \begin{block}{Introduction}
        Optimization algorithms are essential in training neural networks, influencing how well the network learns from data by adjusting model parameters to minimize the loss function.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Optimization Algorithms - Part 1}
    \begin{block}{Gradient Descent}
        \begin{itemize}
            \item \textbf{Concept:} Updates weights by moving in the direction of the negative gradient.
            \item \textbf{Mathematical Formula:}
            \begin{equation}
                \theta = \theta - \eta \nabla J(\theta)
            \end{equation}
            \begin{itemize}
                \item $\theta$ = parameters (weights)
                \item $\eta$ = learning rate
                \item $\nabla J(\theta)$ = gradient of the loss function
            \end{itemize}
            \item \textbf{Types:}
            \begin{enumerate}
                \item Batch Gradient Descent
                \item Stochastic Gradient Descent (SGD)
                \item Mini-batch Gradient Descent
            \end{enumerate}
        \end{itemize}
        \textbf{Example:} Gradient Descent iteratively adjusts weights based on the slope of the Mean Squared Error (MSE) curve.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Optimization Algorithms - Part 2}
    \begin{block}{Adam (Adaptive Moment Estimation)}
        \begin{itemize}
            \item \textbf{Concept:} Combines benefits of AdaGrad and RMSProp for adaptive learning rates.
            \item \textbf{Mathematical Formulas:}
            \begin{equation}
                m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
            \end{equation}
            \begin{equation}
                v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
            \end{equation}
            \begin{equation}
                \theta = \theta - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t
            \end{equation}
            \begin{itemize}
                \item $m_t$ = First moment estimate (mean)
                \item $v_t$ = Second moment estimate (variance)
                \item $\beta_1, \beta_2$ = exponential decay rates
                \item $\epsilon$ = small constant to prevent division by zero
            \end{itemize}
            \item \textbf{Advantages:}
            \begin{itemize}
                \item Adaptive learning rates
                \item Faster convergence compared to vanilla gradient descent
            \end{itemize}
        \end{itemize}
        \textbf{Example:} Adam efficiently updates weights in a neural network for image classification by adapting learning rates.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{itemize}
        \item Optimization algorithms are crucial for effective model training.
        \item Gradient Descent and its variants may be slower with large datasets.
        \item Adam provides faster and more reliable convergence.
    \end{itemize}
    \begin{block}{Conclusion}
        Understanding optimization algorithms is vital for improving neural network performance and experimenting with different optimizers can lead to better predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Understanding the Concepts}
    
    \begin{block}{Definitions}
        \begin{itemize}
            \item \textbf{Overfitting:}
            \begin{itemize}
                \item Occurs when a model learns both the underlying patterns and the noise in training data.
                \item Results in poor generalization to unseen data.
                \item Visual indicator: Training loss decreases while validation loss increases.
            \end{itemize}
            \item \textbf{Underfitting:}
            \begin{itemize}
                \item Happens when a model is too simple to capture the data's underlying trend.
                \item Results in poor performance on both training and validation data.
                \item Visual representation: Model fails to fit well even on training data.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Key Points and Examples}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Balance is Key:} The goal is to generalize well to new, unseen data by balancing fitting and learning patterns without absorbing noise.
        \end{itemize}
    \end{block}

    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Overfitting Example:} High-degree polynomial regression to a small dataset leads to a model capturing noise.
            \item \textbf{Underfitting Example:} Linear regression applied to a quadratic relationship fails to represent the data well.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques}

    \begin{block}{Techniques for Regularization}
        \begin{itemize}
            \item \textbf{L1 Regularization (Lasso):} 
            \begin{itemize}
                \item Adds penalty based on the absolute value of coefficients.
                \item Formula: \( J(\theta) = \text{Loss} + \lambda \sum |\theta_j| \)
            \end{itemize}
            \item \textbf{L2 Regularization (Ridge):}
            \begin{itemize}
                \item Adds penalty equal to the square of coefficients.
                \item Formula: \( J(\theta) = \text{Loss} + \lambda \sum \theta_j^2 \)
            \end{itemize}
            \item \textbf{Dropout:} Randomly ignores neurons during training to prevent co-adaptation.
            \item \textbf{Early Stopping:} Monitors performance on a validation set and stops training when performance degrades.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        Correctly identifying and addressing overfitting and underfitting is crucial for building effective neural networks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Evaluating Neural Networks - Overview}
  Evaluating the performance of neural networks is crucial for understanding their effectiveness in solving tasks. Here are the key metrics to assess a model appropriately:

  \begin{enumerate}
      \item **Accuracy**
      \item **Precision**
      \item **Recall**
      \item **F1 Score**
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Evaluating Neural Networks - Metrics Definitions}
  
  \begin{block}{Accuracy}
      \begin{itemize}
          \item **Definition**: Proportion of true results (TP + TN) out of total cases.
          \item **Formula**: 
          \[
          \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
          \]
          \item **Example**: If a model predicts 90 out of 100 instances correctly, accuracy is 90\%.
      \end{itemize}
  \end{block}
  
  \begin{block}{Precision}
      \begin{itemize}
          \item **Definition**: Ratio of correctly predicted positives to total predicted positives.
          \item **Formula**: 
          \[
          \text{Precision} = \frac{TP}{TP + FP}
          \]
          \item **Example**: If 10 out of 15 positive predictions were correct, precision is \( \frac{10}{15} \approx 0.67 \) or 67\%.
      \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Evaluating Neural Networks - More Metrics}
  
  \begin{block}{Recall (Sensitivity)}
      \begin{itemize}
          \item **Definition**: Ratio of correctly predicted positives to all actual positives.
          \item **Formula**: 
          \[
          \text{Recall} = \frac{TP}{TP + FN}
          \]
          \item **Example**: If a model identifies 30 true positives out of 50 actual positives, recall is \( \frac{30}{50} = 0.6 \) or 60\%.
      \end{itemize}
  \end{block}
  
  \begin{block}{F1 Score}
      \begin{itemize}
          \item **Definition**: Harmonic mean of precision and recall; provides balance.
          \item **Formula**: 
          \[
          F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
          \]
          \item **Example**: If precision is 0.67 and recall is 0.6, F1 Score is \( 0.63 \).
      \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Introduction}
    \begin{block}{Overview}
        Neural networks are powerful computational models that learn complex patterns from data, inspired by the human brain. This slide explores their real-world applications across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Key Applications}
    \begin{enumerate}
        \item \textbf{Image Recognition}
        \begin{itemize}
            \item Crucial for analyzing visual data using Convolutional Neural Networks (CNNs).
            \item Example: Facial recognition in social media platforms.
            \item Key Point: Significant improvements in image classification tasks.
        \end{itemize}

        \item \textbf{Natural Language Processing (NLP)}
        \begin{itemize}
            \item Powers tasks like understanding and processing human language.
            \item Example: Chatbots and language translation using RNNs and transformers (BERT, GPT).
            \item Key Point: Generates human-like responses and translates languages effectively.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - More Key Applications}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Healthcare}
        \begin{itemize}
            \item Used for medical image analysis and disease prediction.
            \item Example: Analyzing medical scans (MRI, X-rays) for diagnoses.
            \item Key Point: Learning from historical data for personalized treatment plans.
        \end{itemize}

        \item \textbf{Finance}
        \begin{itemize}
            \item Assists in predicting stock trends and detecting fraud.
            \item Example: RNNs analyzing time series data for stock price forecasts.
            \item Key Point: Fraud detection systems recognize unusual transaction patterns.
        \end{itemize}
        
        \item \textbf{Autonomous Vehicles}
        \begin{itemize}
            \item Essential for processing sensor data in self-driving cars.
            \item Example: Identifying objects and making driving decisions.
            \item Key Point: Real-time processing critical for safe operation.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Conclusion}
    \begin{block}{Summary}
        Neural networks have transformed industries by providing innovative solutions to complex problems, enhancing applications in image recognition, healthcare, finance, and autonomous driving.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Additional Notes}
    \begin{itemize}
        \item Performance can be assessed with metrics such as accuracy and precision.
        \item Example of a simple CNN model in TensorFlow:
        \begin{lstlisting}[language=Python]
        model = tf.keras.Sequential([
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)),
            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
        ])
        \end{lstlisting}
        \item Consider including diagrams of neural networks and their applications in various fields.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks - Overview}
    Neural networks have made significant strides in various applications, but working with them comes with its own set of challenges. Understanding these challenges is crucial for effectively designing, training, and deploying neural network models. Here, we will explore three primary challenges: 
    \begin{enumerate}
        \item Data Requirements
        \item Computational Power
        \item Interpretability
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks - Data Requirements}
    \begin{block}{Explanation}
        Neural networks thrive on large datasets to learn patterns and make predictions. However, acquiring high-quality labeled data can be resource-intensive and time-consuming.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Quantity:} Neural networks often require thousands to millions of examples to achieve high performance.
        \item \textbf{Quality:} Data should be clean, representative, and free of bias to avoid overfitting or misleading results.
    \end{itemize}
    
    \begin{block}{Example}
        In image recognition tasks, a convolutional neural network might be trained on thousands of labeled images (like cats and dogs) to learn distinguishing features.
    \end{block}
    
    \begin{lstlisting}[language=Python]
# Sample Data Augmentation in Python for Image Data
from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(rotation_range=40, 
                             width_shift_range=0.2, 
                             height_shift_range=0.2, 
                             shear_range=0.2, 
                             zoom_range=0.2)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks - Computational Power}
    \begin{block}{Explanation}
        Training complex neural networks is computationally intensive. Accessing high-performance computing resources becomes a necessity.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Hardware Requirements:} Training a deep network may require GPUs or specialized hardware like TPUs.
        \item \textbf{Time Consumption:} Depending on the size of the data and network architecture, training can take hours or even days.
    \end{itemize}
    
    \begin{block}{Example}
        Training a deep learning model for natural language processing on a dataset like the Common Crawl might require parallel processing on multiple GPUs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks - Interpretability}
    \begin{block}{Explanation}
        Neural networks are often described as "black boxes" because understanding their decision-making process can be challenging. Interpretability becomes crucial in applications like healthcare or finance.
    \end{block}

    \begin{itemize}
        \item \textbf{Model Transparency:} Understanding how inputs affect outputs can drive trust and adoption in sensitive domains.
        \item \textbf{Techniques for Interpretability:} Methods like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) can help in elucidating model decisions.
    \end{itemize}
    
    \begin{block}{Example}
        Using LIME to explain the predictions made by a neural network on loan approval might reveal which factors (like credit score or income level) influenced the decision.
    \end{block}
    
    \begin{lstlisting}[language=Python]
# Sample LIME Explanation
from lime.lime_text import LimeTextExplainer

explainer = LimeTextExplainer()
exp = explainer.explain_instance(text_data, model.predict_proba, num_features=10)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks - Conclusion}
    Addressing data requirements, computational power, and interpretability is essential for successful neural network deployment. As advancements in technology and methods emerge, these challenges may evolve, but understanding them lays the groundwork for building robust neural network models.

    By grasping these challenges, we start to appreciate the complexities involved in machine learning, paving the way for effective solutions and innovations in the field.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recent Advancements in Neural Networks}
    \begin{block}{Overview}
        Recent advancements in neural networks have revolutionized the field of artificial intelligence, enabling significant progress in various applications such as image recognition, natural language processing, and more. This overview covers two main areas: \textbf{Deep Learning} and \textbf{Transfer Learning}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Deep Learning}
    \begin{block}{Explanation}
        Deep learning is a subset of machine learning that utilizes multi-layered neural networks to automatically learn patterns from large amounts of data. This makes it powerful for high-level abstraction tasks such as recognizing speech or images.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Architecture:} Utilizes architectures like Convolutional Neural Networks (CNNs) for image data and Recurrent Neural Networks (RNNs) for sequential data.
        \item \textbf{Non-linearity:} Each layer employs non-linear activation functions (e.g., ReLU, Sigmoid) to capture complex patterns.
    \end{itemize}
    
    \begin{block}{Example}
        In a deep learning model for image classification, a CNN takes an image and identifies features like edges and textures in the first layers, then higher-level patterns like shapes and objects in deeper layers.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Transfer Learning}
    \begin{block}{Explanation}
        Transfer learning reuses a neural network developed for a particular task as the starting point for a model on a second task, especially useful with limited data for the latter task.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Pre-trained Models:} Models like VGG16, ResNet, and BERT are pre-trained on extensive datasets (e.g., ImageNet, Wikipedia) and can be fine-tuned on smaller datasets.
        \item \textbf{Reduced Training Time:} Transfer learning reduces training time and computational resources by leveraging knowledge from pre-trained models.
    \end{itemize}
    
    \begin{block}{Example}
        An image classifier trained to recognize many objects can be fine-tuned to identify a few specific classes using a small dataset.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        Advancements in deep learning and transfer learning enhance neural network applications, allowing them to learn from less data and perform exceptionally well across domains. They address challenges in traditional machine learning methods and improve model generalization capabilities.
    \end{block}
    
    \begin{itemize}
        \item Deep learning leverages multi-layer networks to learn complex patterns from data.
        \item Transfer learning enhances efficiency and effectiveness by reusing pre-trained models.
        \item These advancements help overcome challenges in traditional approaches.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Future Directions - Key Points Covered}
    \begin{enumerate}
        \item \textbf{Neural Networks Overview}
        \begin{itemize}
            \item Computational models inspired by the human brain
            \item Recognize patterns and make decisions
            \item Layers: input, hidden, and output
            \item Neurons process input through activation functions
        \end{itemize}
        
        \item \textbf{Advancements in Neural Networks}
        \begin{itemize}
            \item \textit{Deep Learning:} Many-layered networks for hierarchical data representation
            \item \textit{Transfer Learning:} Pretrained models fine-tuned for specific tasks
        \end{itemize}
        
        \item \textbf{Applications of Neural Networks}
        \begin{itemize}
            \item Used across industries: NLP, computer vision, autonomous systems
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Future Directions - Potential Future Trends}
    \begin{enumerate}
        \setcounter{enumi}{3} % To continue numbering from the previous frame
        \item \textbf{Explainable AI (XAI)}
        \begin{itemize}
            \item Need for transparency in decision-making processes
            \item Research on interpreting neural network decisions for trust and accountability
        \end{itemize}

        \item \textbf{Incorporation of Reinforcement Learning (RL)}
        \begin{itemize}
            \item Merging with RL for advancements in gaming and robotics
            \item Learning optimal strategies through environmental interaction
        \end{itemize}
        
        \item \textbf{Continual Learning}
        \begin{itemize}
            \item Ability to learn continuously without forgetting previous knowledge
            \item Essential for adapting to changing environments
        \end{itemize}

        \item \textbf{Sustainability in AI Research}
        \begin{itemize}
            \item Reducing computational power for training neural networks
            \item Addressing environmental impact of energy consumption
        \end{itemize}

        \item \textbf{Federated Learning}
        \begin{itemize}
            \item Training across decentralized devices while maintaining data privacy
            \item Secure and efficient training on sensitive information
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Future Directions - Impact on Machine Learning}
    \begin{itemize}
        \item \textbf{Increased Accessibility to AI}
        \begin{itemize}
            \item User-friendly tools democratizing access to neural network technology
            \item Empowering individuals and small enterprises for innovation
        \end{itemize}

        \item \textbf{Enhanced Predictive Analytics}
        \begin{itemize}
            \item Improving decision-making across sectors: healthcare, finance, logistics
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Future Directions - Example Code Snippet}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

# Define a simple neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=64, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=output_dim, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusions and Future Directions - Final Thoughts}
    This chapter has provided foundational insights into neural networks, their advancements, 
    and evolving future directions. Understanding these concepts will prepare students to explore 
    more complex systems and their applications in machine learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q \& A Session - Overview}
    \begin{block}{Overview of Neural Networks: Key Concepts}
        Before we dive into your questions, let's quickly recap the crucial concepts we've discussed throughout the week:
    \end{block}
    \begin{enumerate}
        \item \textbf{Neural Networks Defined}: Computational models inspired by the human brain for tasks like classification and regression.
        \item \textbf{Architecture}:
        \begin{itemize}
            \item \textbf{Input Layer}: Receives the input data.
            \item \textbf{Hidden Layers}: Perform computations and learning.
            \item \textbf{Output Layer}: Produces the final prediction.
        \end{itemize}
        \item \textbf{Activation Functions}: Functions like ReLU, Sigmoid, and Tanh that introduce non-linearity.
        \item \textbf{Training Process}:
        \begin{itemize}
            \item \textbf{Forward Propagation}: Data is processed to produce an output.
            \item \textbf{Backpropagation}: Adjusting weights based on error using optimization methods.
        \end{itemize}
        \item \textbf{Loss Function}: Measures performance (e.g., Mean Squared Error, Cross-Entropy Loss).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q \& A Session - Examples}
    \begin{block}{Examples to Illustrate Concepts}
        \begin{itemize}
            \item \textbf{Activation Function: ReLU}
            \begin{equation}
                f(x) =
                \begin{cases}
                    x & \text{if } x > 0 \\
                    0 & \text{if } x \leq 0
                \end{cases}
            \end{equation}
            \item \textbf{Loss Function: Cross-Entropy}
            \begin{equation}
                L = -\frac{1}{N}\sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q \& A Session - Encouraging Participation}
    \begin{block}{Encouraging Participation}
        Let’s get started with your questions! Consider these prompts:
    \end{block}
    \begin{enumerate}
        \item \textbf{Questions about Concepts}:
        \begin{itemize}
            \item What aspects of the training process were unclear?
            \item How do different activation functions affect learning?
        \end{itemize}
        \item \textbf{Real-Life Applications}:
        \begin{itemize}
            \item How do you see neural networks impacting industries you're interested in?
        \end{itemize}
        \item \textbf{Hypothetical Scenarios}:
        \begin{itemize}
            \item If faced with a dataset with missing values, how would you preprocess your data?
        \end{itemize}
    \end{enumerate}
    \begin{block}{Conclusion}
        This is a great opportunity for collaborative learning and to clarify any doubts you may have.
    \end{block}
\end{frame}


\end{document}