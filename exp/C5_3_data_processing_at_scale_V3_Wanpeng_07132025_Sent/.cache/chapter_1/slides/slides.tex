\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 1: Data Processing]{Week 1: Introduction to Data Processing Concepts}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Processing Concepts}
    Overview of the chapter objectives and the significance of data processing in large-scale operations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Chapter Objectives}
    \begin{enumerate}
        \item \textbf{Understanding Data Processing}:
        \begin{itemize}
            \item Define data processing and its importance in today’s data-driven environment.
            \item Explore different types of data processing methods:
            \begin{itemize}
                \item Batch processing
                \item Real-time processing
                \item Distributed processing
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Identification of Key Components}:
        \begin{itemize}
            \item Identify core components and technologies used in data processing platforms, including databases, data warehouses, and data lakes.
        \end{itemize}

        \item \textbf{Importance of Scalability and Efficiency}:
        \begin{itemize}
            \item Discuss how data processing scales to handle large volumes of data and the need for efficiency in operational processes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data Processing in Large-Scale Operations}
    Data processing is pivotal for organizations managing vast amounts of information. It transforms raw data into meaningful insights, which supports decision-making processes and enhances operational efficiency. Key points include:
    \begin{itemize}
        \item \textbf{Enhanced Decision-Making}:
        \begin{itemize}
            \item Example: Retail companies analyze sales data to adjust inventory levels in real-time.
        \end{itemize}
        
        \item \textbf{Operational Efficiency}:
        \begin{itemize}
            \item Example: Manufacturing plants leverage data processing to monitor equipment, predict maintenance needs, and minimize downtime.
        \end{itemize}
        
        \item \textbf{Data Integrity and Accuracy}:
        \begin{itemize}
            \item Implementing proper data processing protocols ensures data is accurate, consistent, and timely—crucial for compliance and reporting.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Data Processing}
    \begin{enumerate}
        \item \textbf{Batch Processing}:
        \begin{itemize}
            \item A method of processing large volumes of data at once, often at scheduled intervals.
            \item Example: End-of-day processing of transactions in banking.
        \end{itemize}

        \item \textbf{Real-Time Processing}:
        \begin{itemize}
            \item Continuous input and immediate output of data, enabling instant feedback and actions.
            \item Example: Processing online transactions during e-commerce operations.
        \end{itemize}

        \item \textbf{Distributed Processing}:
        \begin{itemize}
            \item Utilizes multiple systems (computers) to process data simultaneously, improving speed and efficiency.
            \item Example: Cloud computing environments where resources are pooled from various locations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Data processing is essential for converting data into actionable insights.
        \item Different methodologies cater to various operational needs based on volume, speed, and complexity.
        \item Understanding the underlying architecture of data processing systems enhances a firm's ability to leverage data effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Diagrams}
    \begin{itemize}
        \item \textbf{Data Processing Flow Diagram}: Illustrate the steps from data collection through processing to output generation.
        \item \textbf{Architecture of a Data Processing System}: Simple layout showing databases, processing servers, and users.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Processing - Part 1}
    \begin{block}{1. Understanding Data Processing}
        \textbf{Definition:} \\
        Data processing refers to the collection, manipulation, and transformation of raw data into valuable information that can be used to make informed decisions in a range of industries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Processing - Part 2}
    \begin{block}{2. Why Data Processing is Critical}
        \begin{itemize}
            \item \textbf{Decision-Making:}
            \begin{itemize}
                \item \textbf{Insight Generation:} Processed data helps organizations understand market trends, consumer preferences, and operational efficiencies.
                \item \textbf{Examples:}
                \begin{itemize}
                    \item A retail company uses sales data analysis to determine which products to stock for upcoming seasons.
                    \item Financial institutions analyze transaction data to assess loan applications and detect fraud.
                \end{itemize}
            \end{itemize}

            \item \textbf{Operational Efficiency:}
            \begin{itemize}
                \item \textbf{Streamlining Processes:} Automating data processing tasks reduces operational costs and minimizes human error.
                \item \textbf{Examples:}
                \begin{itemize}
                    \item A manufacturing company utilizes sensor data from machinery to predict maintenance needs.
                    \item Logistics companies optimize delivery routes using real-time traffic and shipment data.
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Processing - Part 3}
    \begin{block}{3. Real-World Impact}
        \begin{itemize}
            \item \textbf{Case Study: Healthcare}
            \begin{itemize}
                \item Hospitals utilize patient data to enhance treatment plans, streamline operations, and manage resources effectively.
                \item Patient records processed with advanced analytics help in predicting patient admissions, thereby optimizing staffing levels.
            \end{itemize}
            \item \textbf{Case Study: E-commerce}
            \begin{itemize}
                \item Online retailers process user behavior data for personalized recommendations and targeted advertising, increasing sales conversions.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Processing - Part 4}
    \begin{block}{4. Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Data-Driven Culture:} Organizations that embrace data processing often outperform competitors as decisions are based on quantitative insights.
            \item \textbf{Adaptability:} Efficient data processing allows businesses to adapt quickly to market changes, enhancing agility and responsiveness.
            \item \textbf{Scalability:} The capability to process large volumes of data effectively enables sustained performance and innovation as businesses grow.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{5. Conclusion}
        In conclusion, effective data processing acts as the backbone of modern operational strategies. 
        It enables organizations not only to understand their current position but also to predict future trends, ultimately driving success in today's competitive landscape.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{References}
    \begin{itemize}
        \item \textbf{Data Processing Techniques,} [Online Resource]
        \item \textbf{The Role of Data in Decision-Making,} Harvard Business Review
        \item \textbf{Operational Efficiency: A Guide,} MIT Sloan Management Review
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Data Processing - Overview}
    Understanding core concepts is essential for efficient data processing. Key concepts include:
    \begin{itemize}
        \item Data Ingestion
        \item Data Transformation
        \item Data Storage
        \item Data Integration
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Data Processing - Data Ingestion}
    \textbf{1. Data Ingestion} \\
    Definition: The process of collecting and importing data for immediate use or storage.
    \begin{block}{Types of Data Ingestion}
        \begin{itemize}
            \item \textbf{Batch Processing:} Collected and processed in groups at scheduled intervals.
            \item \textbf{Streaming Processing:} Data ingested continuously in real-time.
        \end{itemize}
    \end{block}
    \textbf{Key Points:}
    \begin{itemize}
        \item Method choice depends on data velocity and processing needs.
        \item Batch is efficient for large volumes; streaming is essential for real-time insights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Data Processing - Data Transformation}
    \textbf{2. Data Transformation} \\
    Definition: Converting raw data into a usable format, including cleaning and aggregating. 
    \begin{block}{Common Transformation Tasks}
        \begin{itemize}
            \item Data Cleansing: Correcting inaccuracies or errors (e.g., removing duplicates).
            \item Data Normalization: Standardizing formats (e.g., currency conversion).
            \item Aggregation: Summarizing data (e.g., daily total sales).
        \end{itemize}
    \end{block}
    \textbf{Key Points:}
    \begin{itemize}
        \item Improves data quality for analysis.
        \item Prepares data for specific analytical needs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Data Processing - Data Storage}
    \textbf{3. Data Storage} \\
    Definition: Methods of holding data in accessible repositories for future use. 
    \begin{block}{Storage Solutions}
        \begin{itemize}
            \item \textbf{Relational Databases:} Structured storage using tables (e.g., SQL databases).
            \item \textbf{NoSQL Databases:} Flexible storage for unstructured data (e.g., MongoDB).
            \item \textbf{Data Lakes:} Stores large amounts of raw data in native format (e.g., AWS S3).
        \end{itemize}
    \end{block}
    \textbf{Key Points:}
    \begin{itemize}
        \item Ensure efficient data retrieval and analysis.
        \item Choose storage based on data structure and retrieval needs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Data Processing - Data Integration}
    \textbf{4. Data Integration} \\
    Definition: Combining data from different sources into a unified view essential for analysis. 
    \begin{block}{Common Integration Techniques}
        \begin{itemize}
            \item \textbf{ETL:} Extract, Transform, Load process for data from multiple sources.
            \item \textbf{Data Virtualization:} Creating a single view without physical consolidation.
        \end{itemize}
    \end{block}
    \textbf{Key Points:}
    \begin{itemize}
        \item Helps create a holistic view of organizational data.
        \item Important for accurate reporting and informed decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Data Processing - Conclusion}
    Understanding these core concepts—data ingestion, transformation, storage, and integration—is fundamental. Each component plays a crucial role in ensuring that data is handled efficiently, leading to:
    \begin{itemize}
        \item Better insights
        \item Enhanced decision-making capabilities
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion Techniques - Overview}
    \begin{block}{Overview}
        Data ingestion is the process of acquiring data from different sources and preparing it for processing. It is a critical first step in the data processing workflow, transferring data into a system for subsequent transformation and storage.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Ingestion Techniques - Key Methods}
    \begin{enumerate}
        \item \textbf{Batch Processing}
            \begin{itemize}
                \item \textbf{Definition:} Data collected and processed as a single batch; no immediate processing required.
                \item \textbf{Characteristics:}
                    \begin{itemize}
                        \item Scheduled processing (e.g., hourly, daily).
                        \item Processes large volumes of data.
                        \item Less resource-intensive.
                    \end{itemize}
                \item \textbf{Use Case:} Retail business compiles daily sales transactions overnight.
                \item \textbf{Illustration:} Like a bucket collecting rain, pouring out all at once.
            \end{itemize}

        \item \textbf{Streaming (Real-time Processing)}
            \begin{itemize}
                \item \textbf{Definition:} Continuous input of data for real-time processing.
                \item \textbf{Characteristics:}
                    \begin{itemize}
                        \item Data ingested and processed in small increments.
                        \item Low latency for instant results.
                        \item High resource utilization.
                    \end{itemize}
                \item \textbf{Use Case:} Stock market applications tracking price changes in real-time.
                \item \textbf{Illustration:} Like a tap running water continuously.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Batch vs. Streaming - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Latency:} 
                \begin{itemize}
                    \item Batch has more latency; processed in intervals.
                    \item Streaming provides low latency for immediate insights.
                \end{itemize}
            \item \textbf{Volume Handling:} 
                \begin{itemize}
                    \item Batch handles larger datasets effectively.
                    \item Streaming excels in processing continuous streams of smaller data.
                \end{itemize}
            \item \textbf{Choosing the Right Method:}
                \begin{itemize}
                    \item Batch processing for tasks tolerating delays (e.g., end-of-month reports).
                    \item Streaming for time-sensitive applications (e.g., fraud detection).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Transformation}
    Data transformation is the process of converting data from its original format into a format that is suitable for analysis and processing. This can involve several operations, including:
    
    \begin{itemize}
        \item \textbf{Data Cleansing}: Removing inaccuracies and correcting errors in the data.
        \item \textbf{Data Aggregation}: Summarizing data to provide insights (e.g., calculating averages).
        \item \textbf{Data Normalization}: Adjusting data to a common scale without distorting differences in values.
        \item \textbf{Data Enrichment}: Enhancing data by appending additional data from external sources.
    \end{itemize}
    
    \textbf{Example:} Consider a retail dataset with sales data. You might clean the data to remove duplicates, aggregate monthly sales figures, and normalize product names for consistency (e.g., `T-Shirt` vs. `tshirt`).
\end{frame}

\begin{frame}[fragile]
    \frametitle{ETL Processes: An Overview}
    ETL stands for Extract, Transform, Load. It is a critical process in data warehousing and analytics, consisting of three main phases:

    \begin{enumerate}
        \item \textbf{Extract}: Data is extracted from various sources such as databases, flat files, or web APIs.
        \item \textbf{Transform}: Here, the data is cleansed, normalized, and transformed through various operations to ensure quality and suitability for analysis.
        \item \textbf{Load}: After transformation, the data is loaded into a destination data warehouse or data store.
    \end{enumerate}
    
    \begin{block}{ETL Flow Diagram}
    \begin{center}
    [Data Source 1] $\rightarrow$ [Extract] $\rightarrow$ [Data Source 2] $\rightarrow$ [Transform] $\rightarrow$ [Data Warehouse] $\rightarrow$ [Load]
    \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of ETL in Data Processing Workflows}
    \begin{itemize}
        \item \textbf{Quality Data}: Ensures that only high-quality processed data is available for analysis.
        \item \textbf{Data Integration}: Combines disparate data sources into a single, cohesive dataset.
        \item \textbf{Efficiency}: Automates the process of data handling, reducing errors and saving time for analysts.
        \item \textbf{Scalability}: Adapts to growing data volumes and complexities, essential for big data environments.
    \end{itemize}
    
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Data transformation impacts data quality and usability.
        \item ETL processes facilitate efficient data integration crucial for analytics.
        \item Businesses leverage ETL for accurate and comprehensive data for strategic decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load data
data = pd.read_csv('sales_data.csv')

# Transform: Clean and Aggregate
data['Product'] = data['Product'].str.lower() # Normalization
data_cleaned = data.drop_duplicates() # Cleaning
monthly_sales = data_cleaned.groupby(data_cleaned['Date'].dt.to_period('M')).agg({'Sales': 'sum'}) # Aggregation

# Save to new CSV
monthly_sales.to_csv('monthly_sales.csv')
    \end{lstlisting}
    
    \textbf{Summary:} Understanding data transformation and ETL processes allows for improved data handling and analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - Overview}
    \begin{block}{Understanding Data Storage Options}
        In the realm of big data, choosing the right storage solution is crucial. Two primary types of databases are:
        \begin{itemize}
            \item \textbf{SQL (Structured Query Language) Databases}
            \item \textbf{NoSQL (Not Only SQL) Databases}
        \end{itemize}
        Understanding their characteristics and use cases is essential for effective data architecture.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - SQL Databases}
    \begin{block}{SQL Databases}
        \textbf{Definition:} Relational databases storing data in structured formats with predefined schemas.
        
        \textbf{Key Characteristics:}
        \begin{itemize}
            \item Schema-based: Adheres to defined schemas.
            \item ACID Compliance: Ensures reliability and data integrity.
            \item Efficient for structured data: Best for defined relationships among data entities.
        \end{itemize}
        
        \textbf{Common SQL Databases:}
        \begin{itemize}
            \item MySQL
            \item PostgreSQL
            \item Microsoft SQL Server
        \end{itemize}
        
        \textbf{When to Use SQL Databases:}
        \begin{itemize}
            \item Complex queries and transactions (e.g., banking systems).
            \item High data integrity and consistency.
            \item Structured data with predefined relationships.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - SQL Example}
    \begin{block}{Example: SQL Query}
        Here is an example of a SQL query to retrieve customer data:
        \begin{lstlisting}[language=SQL]
SELECT customer_name, order_total 
FROM orders 
WHERE order_date >= '2023-01-01'
ORDER BY order_total DESC;
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - NoSQL Databases}
    \begin{block}{NoSQL Databases}
        \textbf{Definition:} Non-relational databases designed for flexible schemas and scalability.
        
        \textbf{Key Characteristics:}
        \begin{itemize}
            \item Schema-less: Facilitates flexible data models.
            \item High scalability: Optimized for large volumes of unstructured data.
            \item Eventual consistency: Ensures higher availability over strict consistency.
        \end{itemize}
        
        \textbf{Common NoSQL Databases:}
        \begin{itemize}
            \item MongoDB (document store)
            \item Cassandra (wide-column store)
            \item Redis (key-value store)
        \end{itemize}
        
        \textbf{When to Use NoSQL Databases:}
        \begin{itemize}
            \item Large volumes of rapidly changing data.
            \item Unstructured or frequently changing data.
            \item Applications requiring high scalability and availability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - NoSQL Example}
    \begin{block}{Example: MongoDB Document}
        Below is an example of a document in MongoDB format:
        \begin{lstlisting}[language=json]
{
  "customer_name": "Jane Doe",
  "order_total": 150.00,
  "order_date": "2023-01-15"
}
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - Key Points}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item SQL databases are ideal for structured data requiring complex transactions.
            \item NoSQL databases excel in handling unstructured data with flexible schemas.
            \item Choose the appropriate database based on application needs: scalability, consistency, and data structure.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - Conclusion}
    \begin{block}{Conclusion}
        Understanding the differences between SQL and NoSQL databases is fundamental for data engineers and data scientists. 
        Assessing data nature and application requirements aids in making informed decisions regarding the appropriate storage solution for big data needs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Storage Solutions for Big Data - Additional Resources}
        \begin{itemize}
            \item “SQL vs NoSQL: Which is the Right Database for You?” - Tutorial
            \item Database Architecture Best Practices - Article
        \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Introduction to Data Processing Tools}
  \begin{block}{Overview of Data Processing Tools}
    Data processing tools are essential in the management and analysis of large datasets, particularly in big data environments. 
    Two of the most widely used tools are \textbf{Apache Hadoop} and \textbf{Apache Spark}. 
    They differ in architecture, processing capabilities, and use cases.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache Hadoop}
  \begin{itemize}
    \item \textbf{Key Features:}
      \begin{itemize}
        \item Distributed Storage \& Processing
        \item Batch Processing
        \item Scalability
      \end{itemize}
      
    \item \textbf{Example Use Case:} 
      Retail company analyzing sales data to identify trends.
      
    \item \textbf{Components:}
      \begin{enumerate}
        \item \textbf{HDFS:}
          \begin{itemize}
            \item Stores data across multiple nodes.
            \item Ensures data redundancy through replication.
          \end{itemize}
        \item \textbf{MapReduce:}
          \begin{itemize}
            \item Process model for large datasets.
            \item \texttt{Map Function} produces key-value pairs.
          \end{itemize}
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache Hadoop - Code Example}
  \begin{block}{Example of a Simple MapReduce Job in Python}
  \begin{lstlisting}[language=Python]
def map_function(data):
    for item in data:
        yield (item.key, item.value)

def reduce_function(key, values):
    return sum(values)
  \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache Spark}
  \begin{itemize}
    \item \textbf{Key Features:}
      \begin{itemize}
        \item In-Memory Processing
        \item Real-Time Stream Processing
        \item Unified Engine for multiple workloads
      \end{itemize}

    \item \textbf{Example Use Case:} 
      Social media platform analyzing user interactions in real time.
      
    \item \textbf{Core Components:}
      \begin{enumerate}
        \item \textbf{Spark Core:} Supports RDDs (Resilient Distributed Datasets).
        \item \textbf{Spark SQL:} Executes SQL queries on large datasets.
        \item \textbf{Spark Streaming:} Processes live data streams.
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Apache Spark - Code Example}
  \begin{block}{Sample Spark Code to Read and Process a Text File}
  \begin{lstlisting}[language=Python]
from pyspark import SparkContext

sc = SparkContext("local", "TextFileProcessing")
lines = sc.textFile("hdfs://path/to/data.txt")
words = lines.flatMap(lambda line: line.split())
wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
  \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Selection:} 
      Choose Hadoop for batch processing and scalability; choose Spark for performance and real-time processing.
    \item \textbf{Ecosystem Integration:} 
      Both integrate well with data storage solutions and have vast ecosystems.
    \item \textbf{Use Cases:} 
      Familiarize with scenarios where each tool excels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Summary}
  Understanding the strengths and use cases of Apache Hadoop and Apache Spark is crucial for planning data processing strategies in today's data-driven landscape. 
  Next, we will explore the design principles underlying scalable architectures for big data applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Principles for Designing Scalable Data Architectures}
    As modern data processing demands increase, establishing a scalable architecture is crucial for the effective handling of large datasets. Below are the core principles to consider for designing a scalable architecture that meets these demands.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Horizontal Scalability}
    \begin{itemize}
        \item \textbf{Definition:} The ability to increase capacity or performance by adding more nodes to the system (e.g., servers).
        \item \textbf{Example:} In a distributed database, if the load increases, you can add more servers to share the workload rather than upgrading a single server (vertical scaling).
        \item \textbf{Key Concept:} Distributing the data and the processing across multiple machines enhances performance and reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decoupling Components}
    \begin{itemize}
        \item \textbf{Definition:} Designing components of the system to operate independently.
        \item \textbf{Example:} Use microservices architecture where data ingestion, processing, and storage are handled by different services that communicate via APIs.
        \item \textbf{Key Concept:} Flexibility and maintenance are improved when services are loosely coupled.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Partitioning (Sharding) and Load Balancing}
    \begin{block}{Data Partitioning (Sharding)}
        \begin{itemize}
            \item \textbf{Definition:} Dividing your dataset into smaller, more manageable pieces.
            \item \textbf{Example:} In a user database, you can shard the data by user ID ranges.
            \item \textbf{Key Concept:} Reduces load on individual nodes and increases parallel processing capabilities.
        \end{itemize}
    \end{block}

    \begin{block}{Load Balancing}
        \begin{itemize}
            \item \textbf{Definition:} Distributing workloads evenly across resources in the architecture.
            \item \textbf{Example:} Use a load balancer to direct incoming data processing requests to various servers.
            \item \textbf{Key Concept:} Ensures no single node becomes a bottleneck.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Caching Mechanisms and Asynchronous Processing}
    \begin{block}{Caching Mechanisms}
        \begin{itemize}
            \item \textbf{Definition:} Storing frequently accessed data for quick retrieval.
            \item \textbf{Example:} Use in-memory databases (like Redis) to cache results of expensive computations.
            \item \textbf{Key Concept:} Improves data retrieval speed and lowers load on primary data stores.
        \end{itemize}
    \end{block}

    \begin{block}{Asynchronous Processing}
        \begin{itemize}
            \item \textbf{Definition:} Initiating data processes independent of the main application flow.
            \item \textbf{Example:} Using message queues (like Apache Kafka) to handle data streaming.
            \item \textbf{Key Concept:} Enhances system responsiveness and efficiency.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monitoring and Analytics}
    \begin{itemize}
        \item \textbf{Definition:} Implementing tools to track system performance and resource utilization.
        \item \textbf{Example:} Use monitoring tools like Prometheus or Grafana to visualize potential scaling needs.
        \item \textbf{Key Concept:} Proactive monitoring allows timely adjustments to the architecture.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Architecture Layout}
    \begin{center}
        \begin{verbatim}
+--------------------+                     +--------------------+
|  Data Ingestion    |                     |    Data Storage    |
| (e.g., Kafka)      | --- Load Balancer --> |   (e.g., HDFS)     |
+--------------------+                     +--------------------+
         |                                         |
         |                                         |
         |                                         |
+--------------------+                     +--------------------+
|  Processing Layer   |                     |   Caching Layer    |
| (e.g., Spark Jobs)  |                     | (e.g., Redis)     |
+--------------------+                     +--------------------+
        \end{verbatim}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Scalability is critical in modern data processing.
        \item Employing horizontal scaling and microservices promotes resilience and adaptability.
        \item Effective scaling requires a well-balanced architecture.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Integrating APIs in Data Processing}
    \begin{block}{Overview}
        Integrating Application Programming Interfaces (APIs) is crucial for fostering seamless communication between various data processing systems. APIs enable the flow of data across different platforms, making it easier to collect, process, and analyze data efficiently.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{What is an API?}
    \begin{itemize}
        \item \textbf{Definition}: An API is a set of rules and protocols that allow different software applications to communicate with each other.
        \item \textbf{Functionality}: Acts as an intermediary for systems to request and exchange data without requiring knowledge of the underlying implementation.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Importance of API Integration in Data Processing}
    \begin{itemize}
        \item \textbf{Interoperability}: Facilitates integration between disparate systems, including databases, cloud services, and analytics platforms.
        \item \textbf{Real-time Data Access}: Enables applications to request data on-demand, ensuring data is always up-to-date.
        \item \textbf{Automation}: Streamlines workflows by allowing systems to trigger actions in response to data changes automatically.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Examples of API Integration}
    \begin{enumerate}
        \item \textbf{Web API:} Fetching live weather data from a web service.
        \begin{itemize}
            \item *Use Case*: A mobile app calls a weather API to provide real-time updates.
        \end{itemize}
        \item \textbf{REST API:} Interacting with databases using HTTP methods (GET, POST, PUT, DELETE).
        \begin{itemize}
            \item *Use Case*: A data processing pipeline retrieves user information from a CRM for analysis.
        \end{itemize}
        \item \textbf{Streaming APIs:} Processing data in real-time.
        \begin{itemize}
            \item *Use Case*: A financial application integrating a streaming API to track stock prices.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Ease of Integration}: APIs simplify the connection between systems, reducing the need for complex coding.
        \item \textbf{Scalability}: Well-designed APIs can handle increased loads, essential for scalable data architectures.
        \item \textbf{Security}: Using secure APIs (like OAuth) ensures safe data access and transfer.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Tips for API Integration}
    \begin{itemize}
        \item \textbf{Documentation}: Review API documentation for endpoints, request formats, and authentication methods.
        \item \textbf{Error Handling}: Implement robust error handling to manage API call failures effectively.
        \item \textbf{Testing}: Use tools like Postman for testing API calls before integration.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    Here is a simple Python example to illustrate how to call a REST API and process its response:
    \begin{lstlisting}[language=Python]
import requests

# Define the API endpoint
url = "https://api.example.com/data"

# Make a GET request to the API
response = requests.get(url)

# Check the status code
if response.status_code == 200:
    data = response.json()  # Parse JSON response
    print("Data retrieved successfully:", data)
else:
    print("Error fetching data:", response.status_code)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Integrating APIs into data processing systems is essential for creating cohesive data flows. Understanding APIs empowers developers to harness data from various sources efficiently, leading to better decision-making and streamlined operations. As we explore performance optimization strategies in the next slide, consider how API integration can enhance overall system efficiency.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Optimization Strategies - Overview}
    \begin{block}{Overview}
        Optimizing performance is crucial in data processing to enhance efficiency and reduce resource consumption. 
        In this slide, we explore \textbf{parallel processing} and \textbf{cloud-based solutions} as key strategies for performance optimization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Optimization Strategies - Parallel Processing}
    \begin{block}{1. Parallel Processing}
        \textbf{Definition:} Parallel processing involves dividing a computational task into smaller sub-tasks that can be processed simultaneously across multiple CPU cores or machines.
        
        \textbf{Benefits:}
        \begin{itemize}
            \item \textbf{Increased Speed:} Tasks complete faster due to concurrent execution.
            \item \textbf{Efficient Resource Utilization:} Maximizes the use of available CPUs and memory.
        \end{itemize}

        \textbf{Example:} Consider a large dataset (e.g., millions of customer records) needing analysis. Instead of processing them sequentially (one after another), parallel processing could divide the dataset into chunks. Each chunk is processed independently by different cores, drastically reducing processing time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Optimization Strategies - Cloud-Based Solutions}
    \begin{block}{2. Cloud-Based Solutions}
        \textbf{Definition:} Cloud-based solutions leverage remote servers hosted on the internet to store, manage, and process data, offering scalability and flexibility.

        \textbf{Benefits:}
        \begin{itemize}
            \item \textbf{Scalability:} Easily adjust resources based on demand (e.g., increase processing power during peak times).
            \item \textbf{Cost-Effectiveness:} Pay-as-you-go pricing models reduce upfront costs for hardware.
            \item \textbf{High Availability:} Redundant cloud services ensure that data processing is available and reliable.
        \end{itemize}

        \textbf{Example:} A retail company may use a cloud-based platform like Amazon Web Services (AWS) to handle spikes in data during holiday sales. This allows them to scale effortlessly without maintaining extensive on-premise infrastructure.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Performance optimization is essential for handling large volumes of data efficiently.
            \item Parallel processing can significantly reduce processing time by utilizing available resources more effectively.
            \item Cloud-based solutions provide flexibility and scalability to meet varying data processing needs without the overhead of physical infrastructure.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Implementing performance optimization strategies such as parallel processing and cloud-based solutions is crucial in modern data processing ecosystems. These methods not only enhance efficiency but also provide the scalability required in today's data-driven environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Overview}
    \begin{block}{Overview}
        As we delve into data processing, it is crucial to maintain a strong ethical framework. 
        This involves understanding the implications of handling data responsibly, particularly concerning privacy and security. 
        Ethical data processing not only protects individuals but also reinforces trust in systems and institutions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Key Concepts}
    \begin{itemize}
        \item \textbf{Data Privacy}
            \begin{itemize}
                \item \textbf{Definition}: Proper handling, processing, storage, and usage of personal information.
                \item \textbf{Importance}: Preventing identity theft and unauthorized data exposure, foundation of trust.
            \end{itemize}
        \item \textbf{Data Security}
            \begin{itemize}
                \item \textbf{Definition}: Measures to protect data from unauthorized access, corruption, or theft.
                \item \textbf{Practices}:
                    \begin{itemize}
                        \item Access Controls: User authentication and role-based access controls.
                        \item Encryption: Securing data both at rest and in transit.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations and Examples}
    \begin{itemize}
        \item \textbf{Informed Consent}
            \begin{itemize}
                \item Individuals should be made aware of what data is being collected and how it will be used.
            \end{itemize}
        \item \textbf{Data Minimization}
            \begin{itemize}
                \item Collect only necessary data, reducing the risk of misuse.
            \end{itemize}
        \item \textbf{Accountability}
            \begin{itemize}
                \item Organizations must take responsibility for data management practices.
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Case of Facebook-Cambridge Analytica}: Highlighted lapses in ethical data management.
            \item \textbf{GDPR Enforcement}: Enforces strict guidelines on data protection and privacy for individuals in the EU.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Trust Matters}: Ethical data practices enhance customer trust and loyalty.
        \item \textbf{Legal Compliance}: Adhering to laws like GDPR is both a legal and ethical obligation.
        \item \textbf{Proactive Approach}: Implement ethical guidelines proactively to prevent breaches.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Understanding the ethics in data processing is a fundamental responsibility for professionals in the field. 
        By prioritizing data privacy and security, we protect individuals’ rights and promote a trustworthy digital environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies of Successful Implementations}
    \begin{block}{Introduction}
        Data processing is fundamental across various industries, enabling organizations to derive insights from large sets of data efficiently. This section explores case studies showcasing effective data processing strategies and their transformational impact on operations and decision-making processes at scale.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies}
    \begin{itemize}
        \item \textbf{Netflix: Personalized Recommendations Engine}
            \begin{itemize}
                \item \textbf{Description:} Analyzes user behavior in real-time.
                \item \textbf{Techniques:} Big Data Analytics, Machine Learning Algorithms.
                \item \textbf{Impact:} Enhanced user engagement and retention.
            \end{itemize}
        
        \item \textbf{Amazon: Supply Chain Optimization}
            \begin{itemize}
                \item \textbf{Description:} Optimizes inventory management and logistics.
                \item \textbf{Techniques:} Predictive Analytics, Real-Time Data Processing.
                \item \textbf{Impact:} Reduced shipping costs and improved delivery times.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies Continued}
    \begin{itemize}
        \item \textbf{Airbnb: Dynamic Pricing Model}
            \begin{itemize}
                \item \textbf{Description:} Enables pricing recommendations based on demand.
                \item \textbf{Techniques:} Data Mining, Statistical Analysis.
                \item \textbf{Impact:} Increased revenue for hosts and enhanced user trust.
            \end{itemize}

        \item \textbf{Spotify: Music Recommendation and Discovery}
            \begin{itemize}
                \item \textbf{Description:} Curates personalized playlists based on user preferences.
                \item \textbf{Techniques:} Collaborative Filtering, Data Clustering.
                \item \textbf{Impact:} Significant increase in user engagement and tailored content.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Real-time Processing:} Enables immediate insights and better decisions.
            \item \textbf{Scalability:} Frameworks must handle increasing data loads without losing performance.
            \item \textbf{Integration of Technologies:} Combining data processing techniques enhances the generation of insights.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        These case studies illustrate that successful data processing implementations are integral to gaining a competitive advantage in today’s data-driven world. By harnessing these technologies, organizations can streamline operations and create value for their customers, resulting in improved satisfaction and loyalty.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Key Learnings Summary}
  
  \begin{enumerate}
      \item \textbf{Understanding Data Processing}  
        \begin{itemize}
            \item Data processing refers to techniques and systems used to collect, organize, and manipulate data for generating meaningful insights.
        \end{itemize}

      \item \textbf{Data Processing Lifecycle}  
        \begin{itemize}
            \item Consists of stages: \textbf{Collection}, \textbf{Storage}, \textbf{Processing}, \textbf{Analysis}, and \textbf{Visualization}.
        \end{itemize}

      \item \textbf{Types of Data Processing}  
        \begin{itemize}
            \item \textbf{Batch Processing} - Processing large blocks of data.
            \item \textbf{Stream Processing} - Continuous input and output of data.
        \end{itemize}

      \item \textbf{Tools and Technologies}  
        \begin{itemize}
            \item Tools include databases (e.g., SQL, NoSQL) and processing frameworks (e.g., Apache Spark, Hadoop).
        \end{itemize}

      \item \textbf{Real-World Applications}  
        \begin{itemize}
            \item Case studies show successful implementations in various industries.
        \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Upcoming Actions}
  
  \begin{enumerate}
      \item \textbf{Deepen Your Knowledge}  
        \begin{itemize}
            \item We will explore specific data architectures and frameworks.
        \end{itemize}

      \item \textbf{Hands-On Learning}  
        \begin{itemize}
            \item Practical exercises with real datasets, including:
                \begin{itemize}
                    \item Implementation of Data Pipelines.
                    \item API Utilization for data querying.
                \end{itemize}
        \end{itemize}

      \item \textbf{Discussion and Questions}  
        \begin{itemize}
            \item Allocate time for clarifying concepts and solving challenges.
        \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Remember}
  
  \begin{itemize}
      \item Data processing is an iterative and ongoing process, fundamental to data-oriented decision-making.
      \item Familiarity with both theoretical and practical aspects enhances understanding and capabilities.
      \item Engage in discussions to aid learning and bridge gaps in understanding.
  \end{itemize}

  \begin{block}{Note}
    Feel free to bring your questions and insights to the next class as we explore more about data processing!
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Discussion and Questions - Introduction}
  In this session, we will open the floor for questions and discussions about the key concepts of data processing that we covered in Week 1. 
  This is a valuable opportunity to clarify ideas, break down complex topics, and share personal insights with your peers.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Discussion and Questions - Topics for Discussion}
  \begin{enumerate}
    \item \textbf{Understanding Data Processing} 
      \begin{itemize}
        \item \textbf{Definition:} Transformation, organization, and manipulation of raw data into a useful format.
        \item \textbf{Example:} Raw data from a social media platform can be processed to determine user engagement metrics.
      \end{itemize}
    
    \item \textbf{Key Steps in Data Processing}
      \begin{itemize}
        \item Data Collection
        \item Data Cleaning
        \item Data Transformation
        \item Data Analysis
        \item Data Visualization
      \end{itemize}
      \textbf{Key Point:} Each step is crucial for ensuring the integrity and usability of data.
    
    \item \textbf{Importance of Data Processing Platforms}
      \begin{itemize}
        \item Integrate architecture and tools for seamless data processing.
        \item \textbf{Example Platforms:} Apache Hadoop, Apache Spark, AWS Redshift, Google BigQuery.
      \end{itemize}
    
    \item \textbf{Real-World Applications of Data Processing}
      \begin{itemize}
        \item Case studies across sectors like finance, healthcare, and marketing.
        \item \textbf{Example:} Retail companies using customer data for personalized marketing strategies.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Discussion and Questions - Key Points and Reflection}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Data processing enables effective decision-making by providing actionable insights.
      \item Understanding the architecture and integration of data processing platforms is crucial.
      \item Real-life applications illustrate the relevance of strong data processing skills.
    \end{itemize}
  \end{block}
  
  \textbf{Reflection and Sharing:}
  \begin{itemize}
    \item Encourage students to share challenges faced in data processing.
    \item Discuss how differences in processing methods can lead to varied outcomes.
  \end{itemize}
  
  \textbf{Questions to Stimulate Discussion:}
  \begin{itemize}
    \item What are the main challenges you've experienced when working with datasets?
    \item How can we ensure that our data processing methods are ethical and respectful of privacy?
    \item What advancements in data processing do you think will shape the future of the industry?
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Discussion and Questions - Conclusion}
  Your contributions during this discussion will enrich the learning experience for everyone. 
  Don’t hesitate to ask questions and share insights as we navigate through the complexities of data processing together!
\end{frame}


\end{document}