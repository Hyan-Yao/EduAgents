\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}

% Title Page Information
\title[Chapter 13]{Chapter 13: Introduction to Neural Networks and Deep Learning}
\author[Your Name]{Your Name}
\institute[Your Institution]{Your Department, Your Institution}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Overview}
    \begin{block}{Overview of Neural Networks}
        Neural networks are a pivotal component of modern machine learning, mimicking how the human brain processes information. They recognize patterns, make decisions, and learn from data, making them versatile for various applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Key Concepts}
    \begin{enumerate}
        \item \textbf{Structure of Neural Networks}:
        \begin{itemize}
            \item \textbf{Neurons}: Fundamental units that receive inputs, process them, and produce output.
            \item \textbf{Layers}:
            \begin{itemize}
                \item \textit{Input Layer}: Accepts initial data.
                \item \textit{Hidden Layers}: Process inputs through weights and activation functions.
                \item \textit{Output Layer}: Produces the final prediction or classification.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Learning Process}
    \begin{enumerate}
        \setcounter{enumi}{1} % Continue from the previous numbering
        \item \textbf{Learning Process}:
        \begin{itemize}
            \item \textit{Feedforward}: Data moves in one direction from input to output.
            \item \textit{Backpropagation}: Adjusting weights based on prediction errors to help the network learn from mistakes.
        \end{itemize}
        \item \textbf{Activation Functions}:
        \begin{itemize}
            \item \textbf{ReLU}: Outputs positive values, fostering sparse representations.
            \item \textbf{Sigmoid}: Outputs values between 0 and 1, useful for binary classification.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Significance}
    \begin{block}{Significance in Machine Learning}
        \begin{itemize}
            \item \textbf{Pattern Recognition}: Excels at recognizing complex patterns in data.
            \item \textbf{Scalability}: Handles large datasets efficiently.
            \item \textbf{Versatility}: Used in various fields:
            \begin{itemize}
                \item Healthcare: Diagnosing diseases from medical images.
                \item Finance: Fraud detection in banking transactions.
                \item Automotive: Enabling self-driving cars.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Example}
    \begin{block}{Example}
        Consider a neural network designed to recognize handwritten digits (0-9). 
        \begin{itemize}
            \item Input Layer: Takes pixel values of the image.
            \item Hidden Layers: Capture patterns like curves and lines.
            \item Output Layer: Predicts the digit.
        \end{itemize}
        With enough training, the network can classify digits with high accuracy, illustrating its ability to learn from raw data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Future Directions}
    \begin{block}{Future Directions}
        The field of neural networks is evolving with architectures like:
        \begin{itemize}
            \item \textbf{Transformers}
            \item \textbf{U-Nets}
            \item \textbf{Diffusion Models}
        \end{itemize}
        These advancements enhance the performance of machine learning tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Key Takeaways}
    \begin{itemize}
        \item Neural networks are inspired by the human brain and serve as powerful tools in machine learning.
        \item They learn from data through feedforward and backpropagation.
        \item Applications span various industries, showcasing their flexibility and capability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        As we delve deeper into neural networks, we will explore their architecture, advanced forms like deep learning, and their profound impact on machine learning capabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Deep Learning?}
    
    \begin{block}{Definition of Deep Learning}
        Deep Learning is a subset of machine learning, a branch of artificial intelligence (AI). It utilizes neural networks with multiple layers to process and interpret complex patterns in large datasets. Unlike traditional machine learning, which depends on manual feature extraction, deep learning automatically learns hierarchical feature representations from raw data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relationship with Traditional Machine Learning}
    
    \begin{itemize}
        \item \textbf{Machine Learning (ML):}
        \begin{itemize}
            \item Algorithms learn from data to make predictions or decisions.
            \item Requires manual feature engineering; experts define relevant features.
            \item Examples: Decision Trees, Support Vector Machines, Linear Regression.
        \end{itemize}
        
        \item \textbf{Deep Learning (DL):}
        \begin{itemize}
            \item Uses multi-layered neural networks to learn directly from raw data.
            \item Eliminates the need for extensive feature engineering; the model discovers features autonomously.
            \item Example: Convolutional Neural Networks (CNNs) for image recognition.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Deep Learning vs. Traditional ML Example}
    
    \begin{itemize}
        \item \textbf{Traditional ML Approach:} 
        \begin{itemize}
            \item For image classification, human experts select features (e.g., edges, corners).
            \item Models require these features for training.
        \end{itemize}

        \item \textbf{Deep Learning Approach:} 
        \begin{itemize}
            \item A deep learning model (e.g., CNN) takes a raw image as input.
            \item Processes data through multiple layers and learns to identify relevant features.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Automation of Learning:}
        \begin{itemize}
            \item Deep learning models can adapt and improve with more data, leading to better performance on complex tasks.
        \end{itemize}
        
        \item \textbf{Applications:}
        \begin{itemize}
            \item Powers advanced applications like image and speech recognition (e.g., Google Photos, Siri).
            \item Utilized in natural language processing (e.g., chatbots, translation services).
            \item Essential for autonomous driving systems (e.g., Tesla's Autopilot).
        \end{itemize}
        
        \item \textbf{Scalability:}
        \begin{itemize}
            \item Deep learning excels with large datasets, handling intricate structures of high-dimensional data effectively.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engagement Questions}
    
    \begin{itemize}
        \item How could a deep learning model change the way we interact with technology in our daily lives?
        \item In what scenarios do you think manual feature engineering could be more beneficial than a deep learning approach?
    \end{itemize}
    
    \begin{block}{Conclusion}
        Deep learning represents a significant advancement in AI, allowing machines to learn from vast amounts of data with minimal human intervention, unlocking new capabilities across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Introduction}
    \begin{block}{Introduction to Neural Networks}
        Neural networks are computational models inspired by human brains. 
        They consist of interconnected nodes (neurons) that process data and learn from it.
    \end{block}
    \begin{itemize}
        \item Understanding the architecture helps in grasping how neural networks function.
        \item Recognizing potential applications of deep learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Key Components}
    \begin{block}{Key Components}
        \begin{enumerate}
            \item \textbf{Layers}:
                \begin{itemize}
                    \item \textbf{Input Layer}: Receives data (e.g., pixels in image recognition tasks).
                    \item \textbf{Hidden Layers}: Perform processing through weighted connections (e.g., feature detection).
                    \item \textbf{Output Layer}: Produces final output (e.g., detection of cat vs. dog).
                \end{itemize}
            \item \textbf{Neurons}:
                \begin{itemize}
                    \item Each neuron processes inputs with weights and biases, outputting via an activation function.
                \end{itemize}
            \item \textbf{Activation Functions}:
                \begin{itemize}
                    \item Introduce non-linearity, enabling learning of complex patterns.
                    \item Examples: Sigmoid, ReLU, Softmax.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Conclusion}
    \begin{block}{Conclusion}
        The architecture consists of layers, neurons, and activation functions that work together to enable learning.
        Understanding these components allows us to appreciate various applications of neural networks.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Points:}
            \begin{itemize}
                \item Mimicking human brain functionalities.
                \item Layers transforming inputs to outputs.
                \item Importance of activation functions in capturing non-linear relationships.
            \end{itemize}
        \item \textbf{Engaging Questions:}
            \begin{itemize}
                \item How would the architecture change for image vs. text tasks?
                \item What is the importance of different activation functions?
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Overview}
    \begin{itemize}
        \item Neural networks are computational models inspired by the human brain.
        \item They are essential in modern machine learning applications.
        \item This presentation covers:
        \begin{itemize}
            \item Feedforward Neural Networks (FNN)
            \item Convolutional Neural Networks (CNN)
            \item Recurrent Neural Networks (RNN)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedforward Neural Networks (FNN)}
    \begin{block}{Definition}
        FNNs are the simplest type of artificial neural networks with no cycles in connections.
    \end{block}
    
    \begin{itemize}
        \item Data flows in one direction: input to output.
        \item Each layer connects to the next.
        \item \textbf{Example Application:} Image classification (e.g., distinguishing between cats and dogs).
    \end{itemize}

    \begin{block}{Illustration}
        Visualize layers of nodes where pixel data flow through hidden layers to produce output predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Convolutional Neural Networks (CNN)}
    \begin{block}{Definition}
        CNNs are specialized for grid-like data such as images.
    \end{block}

    \begin{itemize}
        \item Utilizes convolutional layers to capture spatial hierarchies.
        \item Includes pooling layers for down-sampling while retaining critical features.
        \item \textbf{Example Application:} Facial recognition systems processing pixel patterns.
    \end{itemize}

    \begin{block}{Illustration}
        Envision a 3D cube of an image being partitioned to focus on details like edges and textures.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNN)}
    \begin{block}{Definition}
        RNNs excel with sequential data and maintain information over time.
    \end{block}

    \begin{itemize}
        \item Connections can create cycles, enabling memory of previous inputs.
        \item Processes inputs of varying lengths, useful for language tasks.
        \item \textbf{Example Application:} Language translation, understanding contextual relationships between words.
    \end{itemize}

    \begin{block}{Illustration}
        Imagine data flowing in a sequence, with the network updating its state based on previous elements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{itemize}
        \item \textbf{FNNs} are suitable for static tasks.
        \item \textbf{CNNs} are ideal for visual data processing.
        \item \textbf{RNNs} are perfect for understanding sequences and context.
    \end{itemize}
    
    \begin{block}{Final Thoughts}
        Consider your data type and problem nature when selecting a neural network architecture.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Introduction}
    \begin{block}{Overview}
        Activation functions are essential components of neural networks that introduce non-linearity, enabling models to learn complex patterns in data.
    \end{block}
    \begin{itemize}
        \item Introduce non-linearity to neuron outputs
        \item Help in learning complex relationships
        \item Fundamental for building effective neural networks
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Common Types}
    \begin{enumerate}
        \item \textbf{ReLU (Rectified Linear Unit)}
        \begin{itemize}
            \item \textbf{Formula:} \( f(x) = \max(0, x) \)
            \item Outputs input directly if positive; otherwise, zero.
            \item \textbf{Key Points:}
            \begin{itemize}
                \item Avoids vanishing gradient problem.
                \item Computationally efficient and induces sparsity.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Sigmoid}
        \begin{itemize}
            \item \textbf{Formula:} \( f(x) = \frac{1}{1 + e^{-x}} \)
            \item Maps input to a range of 0 to 1, useful for binary classification.
            \item \textbf{Key Points:}
            \begin{itemize}
                \item Prone to vanishing gradients for extreme values.
                \item Ideal for probabilistic outputs.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Softmax and Summary}
    \begin{itemize}
        \item \textbf{Softmax}
        \begin{itemize}
            \item \textbf{Formula:} \( f(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}} \)
            \item Converts raw logits into probabilities for multi-class classification.
            \item \textbf{Key Points:}
            \begin{itemize}
                \item Outputs a probability distribution.
                \item Sensitive to input scale and useful in final classifier layer.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \begin{block}{Summary}
        \begin{itemize}
            \item Each activation function serves a unique purpose.
            \item Understanding these functions is key to effective neural network design.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Introduction}
    \begin{block}{Overview}
        Training neural networks is a critical process that allows models to learn from data. 
        There are two main mechanisms involved:
        \begin{itemize}
            \item \textbf{Forward Propagation}
            \item \textbf{Backpropagation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Forward Propagation}
    \begin{block}{What is Forward Propagation?}
        It's the process by which input data is passed through the network to generate an output.
    \end{block}

    \begin{block}{How Does it Work?}
        \begin{itemize}
            \item Each layer transforms its input using weights, biases, and an activation function.
            \item The output of one layer becomes the input to the next layer until the final output is produced.
        \end{itemize}
    \end{block}

    \begin{block}{Illustration}
    \begin{center}
    \texttt{Input Layer $\rightarrow$ Hidden Layers (with activation functions) $\rightarrow$ Output Layer}
    \end{center}
    \end{block}

    \begin{block}{Example}
        Given an input of pixel values from an image, forward propagation computes the network's prediction (e.g., cat vs. dog).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Backpropagation}
    \begin{block}{What is Backpropagation?}
        Backpropagation is the process used to update the network weights based on the error of the output compared to the actual label.
    \end{block}

    \begin{block}{How Does it Work?}
        \begin{itemize}
            \item Compute the loss (error) using a loss function.
            \item Use the chain rule of calculus to calculate gradients of the loss with respect to each weight.
            \item Adjust the weights to reduce the loss.
        \end{itemize}
    \end{block}

    \begin{block}{Illustration}
    \begin{center}
    \texttt{Output Error $\rightarrow$ Gradient Calculation $\rightarrow$ Weight Updates}
    \end{center}
    \end{block}

    \begin{block}{Example}
        If the predicted output is 0.8 for a cat and the actual label is 1 (cat), backpropagation adjusts the weights to reduce future errors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Summary and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Forward propagation is about making predictions.
            \item Backpropagation is essential for learning and refining the model.
            \item The combination minimizes errors in predictions.
        \end{itemize}
    \end{block}

    \begin{block}{Mnemonic for Training Process}
        \textbf{F}orward pass brings in \textbf{P}redictions, while \textbf{B}ackward pass brings in \textbf{E}rrors correction.
    \end{block}

    \begin{block}{Conclusion}
        Understanding these processes is foundational for grasping how neural networks learn from data,
        improving their performance on unseen data over training iterations called epochs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Functions - Introduction}
    \begin{block}{What is a Loss Function?}
        A **loss function** (or cost function) measures how well a neural network is performing by quantifying the difference between predicted outputs and actual target values. The main goal of training is to minimize this loss, thereby improving model accuracy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Functions - Importance}
    \begin{itemize}
        \item \textbf{Guidance for Optimization:} Loss functions provide feedback to update the model's weights during training.
        \item \textbf{Performance Metric:} They allow for evaluation and comparison of different models, aiding in informed decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Loss Functions}
    \begin{enumerate}
        \item \textbf{Mean Squared Error (MSE)}
            \begin{itemize}
                \item \textbf{Definition:} Average squared difference between predicted values and actual values.
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
                \end{equation}
                where \( n \) is the number of samples, \( y_i \) is the actual value, and \( \hat{y}_i \) is the predicted value.
                \item \textbf{Use Case:} Common in regression tasks (e.g., predicting house prices).
            \end{itemize}
        
        \item \textbf{Binary Cross-Entropy (BCE)}
            \begin{itemize}
                \item \textbf{Definition:} Measures performance for models predicting a value between 0 and 1.
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{BCE} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
                \end{equation}
                \item \textbf{Use Case:} Used in binary classification tasks (e.g., email spam detection).
            \end{itemize}

        \item \textbf{Categorical Cross-Entropy}
            \begin{itemize}
                \item \textbf{Definition:} Used for multi-class classification problems.
                \item \textbf{Use Case:} Identifying objects in images (e.g., image classification).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways & Discussion Questions}
    \begin{itemize}
        \item Loss functions are vital for providing feedback during the learning process.
        \item Choosing the appropriate loss function is essential for task success.
        \item Regularly monitoring loss can help identify model issues like overfitting or underfitting.
    \end{itemize}
    
    \begin{block}{Engaging Questions}
        \begin{itemize}
            \item How might selecting a different loss function influence model performance?
            \item Can you think of a scenario where MSE might not be appropriate?
            \item What other metrics would you consider alongside loss functions to evaluate model performance?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Overview}
    \begin{block}{Introduction to Optimization}
        Optimization is crucial in training neural networks, as it helps us find the best parameters to minimize the loss function. This iterative process improves model performance and enhances generalization to new data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Gradient Descent}
    \begin{block}{Concept}
        Gradient Descent is a fundamental algorithm that updates model parameters in the direction opposite to the gradient of the loss function in order to minimize loss.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Learning Rate ($\eta$):} A hyperparameter dictating step size. Too large risks overshooting, too small slows convergence.
        \item \textbf{Update Rule:} 
            \begin{equation}
            \theta = \theta - \eta \nabla J(\theta)
            \end{equation}
        \end{itemize}
        
        \begin{block}{Examples}
            \begin{itemize}
                \item \textbf{Mini-batch Gradient Descent:} Uses small batches for updates, strikes a balance between speed and accuracy.
                \item \textbf{Stochastic Gradient Descent (SGD):} Updates more frequently, leading to faster convergence but introduces noise to updates.
            \end{itemize}
        \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Adam (Adaptive Moment Estimation)}
    \begin{block}{Concept}
        Adam enhances the training process by adapting the learning rate for each parameter individually, drawing from Momentum and RMSProp.
    \end{block}

    \begin{itemize}
        \item \textbf{Adaptive Learning Rate:} Utilizes estimates of the first and second moments of gradients ($m_t$ and $v_t$).
    \end{itemize}

    \begin{block}{Parameter Updates}
        \begin{equation}
        m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla J(\theta)
        \end{equation}
        \begin{equation}
        v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla J(\theta))^2
        \end{equation}
        \begin{equation}
        \theta = \theta - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t
        \end{equation}
    \end{block}

    \begin{block}{Examples}
        Frequently used for its efficiency in training models with sparse gradients, like transformers and U-nets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Overview}
    \begin{block}{Understanding Overfitting and Underfitting}
        \begin{itemize}
            \item **Overfitting**: Model learns training data too well, including noise, resulting in poor performance on unseen data.
            \item **Underfitting**: Model is too simple to capture data structure, leading to poor performance on both training and test datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Examples}
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Overfitting Example:} 
            \begin{itemize}
                \item Teaching a child to identify cats with unique pictures leads to memorization instead of understanding.
            \end{itemize}
            
            \item \textbf{Underfitting Example:} 
            \begin{itemize}
                \item Showing only a black silhouette of a cat prevents adequate learning of the concept.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mitigating Overfitting and Underfitting}
    \begin{block}{Strategies}
        \begin{enumerate}
            \item **Regularization**
                \begin{itemize}
                    \item Adds a penalty to the loss function.
                    \item \textbf{L1 (Lasso)}: Absolute value penalty.
                    \item \textbf{L2 (Ridge)}: Square of coefficients penalty.
                    \begin{equation}
                        J(\theta) = \text{Loss} + \lambda \sum_{i=1}^{n} \theta_i^2
                    \end{equation}
                \end{itemize}
            \item **Cross-Validation**: Evaluates model performance on different datasets.
            \item **Early Stopping**: Halts training when performance stops improving.
            \item **Simplifying the Model**: Reducing complexity by lowering layers or neurons.
            \item **Data Augmentation**: Creates variations in training data to improve generalization.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reflection Questions}
    \begin{block}{Discussion Points}
        \begin{itemize}
            \item How can identifying overfitting and underfitting at an early stage improve your model-building process?
            \item What is your experience with regularization, and which methods have you found most effective?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks}

    Neural networks are computational models that mimic the way the human brain processes information. 
    They consist of interconnected layers of nodes or "neurons," enabling them to learn from data and make predictions 
    or decisions without being explicitly programmed for the specific task.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications of Neural Networks}

    \begin{enumerate}
        \item \textbf{Image Recognition}
        \begin{itemize}
            \item Neural networks, especially Convolutional Neural Networks (CNNs), excel in identifying and classifying objects within images.
            \item \textit{Example:} Facial recognition in social media platforms.
        \end{itemize}

        \item \textbf{Natural Language Processing (NLP)}
        \begin{itemize}
            \item Neural networks enable machines to understand and generate human language.
            \item \textit{Example:} Chatbots analyzing user queries.
        \end{itemize}

        \item \textbf{Healthcare Diagnostics}
        \begin{itemize}
            \item Analyzing medical images to identify diseases more accurately than human radiologists.
            \item \textit{Example:} Detection of cancerous cells in biopsy images.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continuing Applications of Neural Networks}

    \begin{enumerate}
        \setcounter{enumi}{3} % Starts enumeration from 4
        \item \textbf{Autonomous Vehicles}
        \begin{itemize}
            \item Processing data from sensors for real-time driving decisions.
            \item \textit{Example:} Tesla's Autopilot features.
        \end{itemize}

        \item \textbf{Finance and Trading}
        \begin{itemize}
            \item Predicting stock market trends through historical data analysis.
            \item \textit{Example:} Algorithmic trading systems.
        \end{itemize}

        \item \textbf{Creative Arts}
        \begin{itemize}
            \item Generating creative content like music and artwork.
            \item \textit{Example:} OpenAI's DALL-E for image creation.
        \end{itemize}

        \item \textbf{Recommendation Systems}
        \begin{itemize}
            \item Personalizing user experiences on online platforms.
            \item \textit{Example:} Netflix's recommendations based on viewing history.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize and Conclusion}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Versatility:} Neural networks adapt across sectors.
            \item \textbf{Impact on Society:} Substantial influence on daily life, from healthcare to creative arts.
            \item \textbf{Continuous Evolution:} Rapid developments like Transformers and U-nets pushing boundaries.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Neural networks offer powerful solutions to complex problems, creating innovative opportunities across various fields.
        Understanding these applications highlights the future implications of AI and its role in society.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction to Convolutional Neural Networks (CNNs)}
    Convolutional Neural Networks are deep learning algorithms designed for processing structured grid data, particularly images. They excel in image classification, object detection, and segmentation through their hierarchical feature learning.
\end{frame}

\begin{frame}
    \frametitle{Key Concepts of CNNs}
    \begin{itemize}
        \item \textbf{Convolution:} Core operation involving sliding a filter over the image to produce feature maps.
        \item \textbf{Pooling:} Technique for down-sampling feature maps, retaining essential information (e.g., Max Pooling, Average Pooling).
        \item \textbf{Activation Functions:} Functions like ReLU introduce non-linearity, allowing CNNs to learn complex patterns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{CNNs in Image Processing}
    CNNs transform raw pixel data into abstract representations, enabling various levels of feature extraction:
    \begin{enumerate}
        \item \textbf{Low-Level Features:} Detecting simple patterns like edges or colors.
        \item \textbf{Mid-Level Features:} Learning complex shapes such as textures or parts of objects.
        \item \textbf{High-Level Features:} Recognizing entire objects for classification tasks.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Examples and Applications}
    \begin{itemize}
        \item \textbf{Image Classification:} Tagging images (e.g., "cat," "dog").
        \item \textbf{Object Detection:} Identifying and localizing objects (e.g., finding a person in a photo).
        \item \textbf{Facial Recognition:} Recognizing individuals by facial features.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Efficiency:} Fewer parameters compared to fully connected layers, enhancing efficiency for image tasks.
        \item \textbf{Technological Advancement:} Supported by powerful hardware (e.g. GPUs) and rich datasets (like ImageNet).
    \end{itemize}
    \begin{block}{Convolution Operation Pseudocode}
        \begin{lstlisting}
for each position (i, j) in input_image:
    output_feature_map[i][j] = sum(input_image[i+m][j+n] * filter[m][n] for m,n in filter_shape)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Convolutional Neural Networks have revolutionized image processing and computer vision, enabling applications from self-driving cars to facial recognition systems by mimicking how our brain perceives visual information.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNNs) - Introduction}
    Recurrent Neural Networks (RNNs) are a type of artificial neural network specifically designed for processing sequential data. Unlike traditional neural networks that assume independent data inputs, RNNs consider the order of data points, making them suitable for tasks where context and sequence matter. 
    \begin{itemize}
        \item Applications: Time series analysis, natural language processing, speech recognition.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNNs) - Key Concepts}
    \begin{itemize}
        \item \textbf{Sequential Data}: Data where the order is important. Examples include:
        \begin{itemize}
            \item Sentences in a text.
            \item Time-stamped weather data.
            \item Stock prices over time.
        \end{itemize}
        \item \textbf{Hidden States}: RNNs maintain a hidden state that captures the information from previous inputs, enabling them to remember context across sequences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNNs) - How They Work}
    The working process of RNNs involves:
    \begin{enumerate}
        \item \textbf{Input Sequence}: The input is a sequence \( x_1, x_2, \ldots, x_T \).
        \item \textbf{Hidden State Update}:
        \[
        h_t = \text{activation}(W_h \cdot h_{t-1} + W_x \cdot x_t + b)
        \]
        where \( h_t \) is the hidden state at time \( t \), \( W_h \) and \( W_x \) are weight matrices, and \( b \) is a bias vector.
        \item \textbf{Output Generation}: Output \( y_t \) can be produced at each time step or just at the end of the sequence.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNNs) - Applications}
    \begin{itemize}
        \item \textbf{Natural Language Processing (NLP)}: E.g., sentiment analysis of tweets.
        \item \textbf{Speech Recognition}: Transcribing spoken language into text.
        \item \textbf{Time Series Prediction}: E.g., predicting future stock prices based on historical data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNNs) - Key Points}
    \begin{itemize}
        \item RNNs can handle inputs of arbitrary lengths.
        \item They learn patterns in sequential data, making them suitable for applications in language understanding and temporal data analysis.
        \item Challenges: Vanishing gradients can occur during training, leading to advanced architectures like LSTM and GRU.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recurrent Neural Networks (RNNs) - Conclusion}
    Recurrent Neural Networks represent a significant development in neural networks by addressing the importance of sequence and context. Their robust framework is applicable in various fields, ranging from language processing to financial forecasting.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recent Advances in Neural Network Designs}
    \begin{block}{Overview of Innovative Architectures}
        Neural networks have evolved rapidly, leading to several cutting-edge models transforming various domains, including natural language processing, computer vision, and generative modeling. This presentation explores three notable architectures: Transformers, U-Nets, and Diffusion Models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transformers}
    \begin{itemize}
        \item \textbf{Concept}: Transformers utilize self-attention mechanisms for processing data sequences, enabling parallel processing unlike RNNs.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Self-Attention}: Weighs the significance of words in a sentence irrespective of their position.
            \item \textbf{Positional Encoding}: Adds sequence information to input embeddings to maintain order.
        \end{itemize}
        \item \textbf{Example in Action}: BERT and GPT models excel in understanding and generating human-like text.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{U-Nets and Diffusion Models}
    \begin{block}{U-Net}
        \begin{itemize}
            \item \textbf{Concept}: Developed for biomedical image segmentation, features a contracting path for context and a symmetric expanding path for precise localization.
            \item \textbf{Key Features}:
            \begin{itemize}
                \item \textbf{Skip Connections}: Utilize precise features from the encoder in the decoder.
                \item \textbf{Symmetric Structure}: Captures both global and local features effectively.
            \end{itemize}
            \item \textbf{Example in Action}: Widely used in medical imaging, such as tumor segmentation in MRI scans.
        \end{itemize}
    \end{block}
    
    \begin{block}{Diffusion Models}
        \begin{itemize}
            \item \textbf{Concept}: Generate data by reversing a diffusion process, learning to recover original data from noise.
            \item \textbf{Key Features}:
            \begin{itemize}
                \item \textbf{Generative Process}: Can produce high-quality images from noise.
                \item \textbf{Applications}: Outperform GANs in generating realistic samples.
            \end{itemize}
            \item \textbf{Example in Action}: DALL-E 2 creates images from textual descriptions using diffusion principles.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Engagement}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item These innovative architectures demonstrate diverse methodologies tailored to specific tasks.
            \item Understanding these models aids practical applications and inspires future research in AI.
            \item Encourage exploration of how these models can solve real-world problems or inspire new technological solutions.
        \end{itemize}
    \end{block}

    \begin{block}{Engagement Questions}
        \begin{itemize}
            \item How might Transformers change the future of machine translation?
            \item In which new fields could U-Nets be utilized beyond healthcare?
            \item What limitations might diffusion models have compared to traditional generative models?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Deep Learning}
    \begin{block}{Introduction to Future Trends}
        As deep learning evolves, several exciting trends are anticipated to shape its development and application across various fields. Understanding these trends helps stay updated with technologies and inspires innovative thinking.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Increased Model Efficiency and Optimization}
    \begin{itemize}
        \item \textbf{Concept:} Need for models achieving higher performance with less computational power and data.
        \item \textbf{Example:} 
            \begin{itemize}
                \item \textit{Model Pruning:} Removing unnecessary weights.
                \item \textit{Quantization:} Reducing the precision of weights.
            \end{itemize}
            Lightweight models are particularly useful for mobile devices and edge computing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Autonomous Machine Learning (AutoML) and Multimodal Learning}
    \begin{enumerate}
        \item \textbf{AutoML}
            \begin{itemize}
                \item \textbf{Concept:} Automating machine learning processes (model selection, hyperparameter tuning).
                \item \textbf{Example:} Google's AutoML allows users to build high-quality models with minimal coding knowledge.
            \end{itemize}
        
        \item \textbf{Multimodal Learning}
            \begin{itemize}
                \item \textbf{Concept:} Integrating different data types (text, images, audio) for enhanced understanding.
                \item \textbf{Example:} Analyzing videos by combining visual, audio, and subtitles for accurate interpretations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Explainable AI (XAI) and Self-Supervised Learning}
    \begin{enumerate}
        \item \textbf{Explainable AI (XAI)}
            \begin{itemize}
                \item \textbf{Concept:} Models must explain reasoning behind decisions.
                \item \textbf{Example:} Tools like LIME generate explanations for individual predictions, crucial in sectors like healthcare and finance.
            \end{itemize}
        
        \item \textbf{Self-Supervised Learning}
            \begin{itemize}
                \item \textbf{Concept:} Models learn from unlabeled data by predicting input parts from others.
                \item \textbf{Example:} Models like BERT and GPT-3 use this method in natural language processing to understand context.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-time Learning and Key Points}
    \begin{enumerate}
        \item \textbf{Real-time and Online Learning}
            \begin{itemize}
                \item \textbf{Concept:} Models learn and adapt as new data arrives.
                \item \textbf{Example:} Autonomous vehicles use online learning to adjust to changing road conditions.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Adaptation:} Importance of adaptable systems in the trend towards efficiency and real-time learning.
            \item \textbf{Accessibility:} Tools like AutoML democratize AI technology.
            \item \textbf{Transparency:} Explainability is crucial for ethical usage.
            \item \textbf{Integration:} Combining different modalities will enhance understanding and content generation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Thoughts}
    By keeping an eye on these trends, we can prepare to innovate and leverage deep learning technologies effectively. The future is bright for those ready to embrace these advancements! 

    \begin{block}{Additional Information}
        Explore open-source implementations of the mentioned concepts for practical understanding.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in AI and Machine Learning}
    \begin{block}{Understanding Ethical Considerations}
        As AI and Machine Learning technologies evolve, they ignite important ethical discussions. Addressing these issues is crucial to ensure the responsible use of AI in society.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations - Part 1}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item Definition: AI systems can reflect or even exacerbate biases present in the training data.
                \item Example: Facial recognition software misidentifies individuals from minority groups at higher rates.
                \item Key Point: Ensure diverse and representative datasets to minimize bias.
            \end{itemize}

        \item \textbf{Transparency}
            \begin{itemize}
                \item Definition: The operation of AI systems can be opaque; users may not understand decision processes.
                \item Example: Applicants should understand reasons behind loan application rejections based on AI models.
                \item Key Point: Develop algorithms that provide understandable explanations to build trust.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations - Part 2}
    \begin{enumerate}[resume]
        \item \textbf{Accountability}
            \begin{itemize}
                \item Definition: Responsibility when AI systems cause harm or make mistakes.
                \item Example: In an accident involving a self-driving car, who is liable—the manufacturer, the developer, or the driver?
                \item Key Point: Establish clear guidelines regarding accountability in AI-related incidents.
            \end{itemize}

        \item \textbf{Privacy Concerns}
            \begin{itemize}
                \item Definition: AI technologies often require personal data access.
                \item Example: Health apps may track sensitive metrics without clear privacy policies.
                \item Key Point: Prioritize user consent and data protection.
            \end{itemize}
        
        \item \textbf{Employment Impact}
            \begin{itemize}
                \item Definition: AI systems may displace jobs and cause economic shifts.
                \item Example: Automation can lead to job losses in manufacturing but may create tech-oriented opportunities.
                \item Key Point: Consider retraining programs for affected workers.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reflection \& Discussion Questions}
    \begin{itemize}
        \item How can we implement fairness measures in AI systems effectively?
        \item What role should developers play in ensuring transparency and accountability?
        \item In what ways can society adapt to changes brought about by AI in the workforce?
    \end{itemize}
    \begin{block}{Conclusion}
        By examining these ethical considerations, we aim to develop AI technologies that benefit all members of society while mitigating potential harms. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Points Recap}
    
    \begin{enumerate}
        \item \textbf{Introduction to Neural Networks}:
        \begin{itemize}
            \item Computational models inspired by the human brain's structure.
            \item Process data through interconnected nodes (neurons).
        \end{itemize}
        
        \item \textbf{Deep Learning}:
        \begin{itemize}
            \item A subset of machine learning with deeper networks enabling better feature extraction.
            \item Breakthroughs in image recognition and natural language processing.
        \end{itemize}
        
        \item \textbf{Types of Architectures}:
        \begin{itemize}
            \item \textit{Feedforward Neural Networks (FNNs)}: Data moves one direction—input to output.
            \item \textit{Convolutional Neural Networks (CNNs)}: Specialized for images.
            \item \textit{Recurrent Neural Networks (RNNs)}: Designed for sequential data.
        \end{itemize}
        
        \item \textbf{Training Neural Networks}:
        \begin{itemize}
            \item Backpropagation and optimization to minimize loss function.
        \end{itemize}

        \item \textbf{Recent Advances}:
        \begin{itemize}
            \item \textit{Transformers}, \textit{U-Nets}, and \textit{Diffusion Models}.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Examples and Applications}
    
    \begin{itemize}
        \item \textbf{Image Classification}: Utilizing CNNs for tasks like object identification (e.g., classifying cats vs. dogs).
        
        \item \textbf{Language Translation}: Employing Transformers in applications like Google Translate.
        
        \item \textbf{Medical Diagnosis}: Analyzing medical images with deep learning models to assist in identifying conditions (e.g., tumors).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Discussion Points}
    
    \begin{block}{Engaging Questions for Discussion}
        \begin{itemize}
            \item How might advancements in neural networks change industries like education, healthcare, and entertainment?
            \item What ethical considerations do we need to take into account with deep learning technologies?
            \item In what areas would you personally like to apply neural network techniques?
        \end{itemize}
    \end{block}
    
    \textbf{Conclusion:} Neural networks and deep learning are pivotal technologies shaping our future. Their versatility holds the potential to transform countless fields. Explore these concepts further!

    \textbf{Q\&A Session:} Open the floor for questions or thoughts and encourage discussions on recent technologies and personal interests in neural networks.
\end{frame}


\end{document}