\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{What is Data Preprocessing?}
        Data preprocessing is a crucial step in the machine learning pipeline that transforms raw data into a clean and usable format. It involves various techniques to prepare data for analysis and model training, ensuring that models can learn effectively and make accurate predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Data Preprocessing Important?}
    \begin{itemize}
        \item \textbf{Data Quality}: Poor quality data leads to unreliable models due to inconsistencies and errors.
        \item \textbf{Model Performance}: Clean data often results in enhanced model performance, leading to faster convergence and higher accuracy.
        \item \textbf{Handling Missing Values}: Missing data can skew results; imputation techniques can fill these gaps, ensuring sufficient information for learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Example}
    Consider a dataset containing information about houses (size, location, price). If some house prices are missing or incorrectly recorded (e.g., 'N/A', 'unknown'), a model trained on this data might predict house prices inaccurately. 
    \begin{block}{Strategy}
        Use techniques like imputation to replace missing prices with the average price of similar houses or drop records with too many missing values.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Preprocessing Enhances Quality}: Clean data improves the accuracy of machine learning models significantly.
        \item \textbf{Adaptability}: Different data types (numerical, categorical, text) may require different preprocessing techniques.
        \item \textbf{No One-Size-Fits-All}: Each dataset might need a unique combination of preprocessing steps based on its characteristics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques Overview}
    \begin{enumerate}
        \item \textbf{Data Cleaning}: Remove duplicates, correct errors, handle missing values.
        \item \textbf{Data Transformation}: Normalize or standardize data to ensure all features contribute equally.
        \item \textbf{Feature Engineering}: Create new features from existing ones to improve model performance.
        \item \textbf{Data Reduction}: Reduce dimensionality using techniques like PCA to eliminate irrelevant features.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Data preprocessing is the backbone of effective machine learning. By investing time into proper data cleaning, transformation, and management, we lay the foundation for building robust and accurate predictive models.
    \begin{block}{Note}
        Remember: Well-preprocessed data = Better predictions!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Concept Overview}
    \begin{block}{Overview}
        Data quality is critical for the success of machine learning (ML) models. High-quality data—characterized by accuracy, completeness, consistency, and relevance—ensures that models learn effectively and produce reliable predictions. Conversely, poor-quality data can lead to misleading results and costly errors.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Key Aspects}
    \begin{enumerate}
        \item \textbf{Accuracy}: Data must reflect true values.
        \item \textbf{Completeness}: All necessary data should be present.
        \item \textbf{Consistency}: Data should be uniform across datasets.
        \item \textbf{Relevance}: Data must align with the problem being solved.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Impact on Models}
    \begin{itemize}
        \item \textbf{Bias and Variance}: Poor data quality can introduce biases affecting predictions.
        \item \textbf{Model Robustness}: High-quality datasets enable models to generalize effectively.
        \item \textbf{Error Propagation}: Errors in training data propagate, leading to larger forecasting errors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Examples}
    \begin{itemize}
        \item \textbf{Spam Detection Model}: Incorrectly labeled training data leads to misclassifying legitimate emails.
        \item \textbf{House Price Predictions}: Missing values for key features yield unreliable predictions and poor decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Key Takeaways}
    \begin{itemize}
        \item \textbf{Invest in Data Quality}: Time spent on cleaning and preprocessing data enhances ML performance.
        \item \textbf{Data Quality Checks}: Regular validation steps maintain high data quality standards.
        \item \textbf{Feedback Loops}: Utilize model predictions to identify and address data quality issues.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Data Types}
    \begin{block}{Introduction to Data Types}
        Understanding the various types of data is crucial for successful data preprocessing, which directly impacts the performance of machine learning models. 
    \end{block}
    In this slide, we will explore three primary categories of data:
    \begin{itemize}
        \item Structured
        \item Unstructured
        \item Semi-structured
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Structured Data}
    \begin{itemize}
        \item \textbf{Definition}: Highly organized and easily searchable data in fixed fields within records, typically represented in tabular formats.
        \item \textbf{Characteristics}:
            \begin{itemize}
                \item Stored in predefined models (e.g., rows and columns)
                \item Well-defined data types (e.g., integer, string, date)
            \end{itemize}
        \item \textbf{Examples}:
            \begin{itemize}
                \item Databases: SQL databases (MySQL, Oracle)
                \item Spreadsheets: Excel files
            \end{itemize}
        \item \textbf{Key Point}: Easy to process and analyze due to its predictable format.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Unstructured Data}
    \begin{itemize}
        \item \textbf{Definition}: Lacks a predefined format or organization, making it complex to collect, process, and analyze.
        \item \textbf{Characteristics}:
            \begin{itemize}
                \item No set format or model; data can come in various forms
                \item Requires advanced techniques (e.g., text mining, NLP) for analysis
            \end{itemize}
        \item \textbf{Examples}:
            \begin{itemize}
                \item Text Data: Emails, social media posts, articles
                \item Media Files: Images, videos, audio recordings
            \end{itemize}
        \item \textbf{Key Point}: Abundant but poses significant challenges for processing and analysis due to its variability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Semi-Structured Data}
    \begin{itemize}
        \item \textbf{Definition}: Does not conform to a strict schema but still contains organizational properties for easier analysis than unstructured data.
        \item \textbf{Characteristics}:
            \begin{itemize}
                \item Contains tags or markers to separate data elements
                \item A blend of structured and unstructured characteristics
            \end{itemize}
        \item \textbf{Examples}:
            \begin{itemize}
                \item JSON Files: Common in web applications for data transmission
                \item XML Files: Markup languages for data storage and sharing
            \end{itemize}
        \item \textbf{Key Point}: Provides flexibility for dynamic systems where structured data is not feasible.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item \textbf{Structured Data}: Organized and easy to analyze.
        \item \textbf{Unstructured Data}: Diverse formats; valuable but challenging to process.
        \item \textbf{Semi-Structured Data}: A hybrid retaining elements of both structured and unstructured data.
    \end{itemize}
    \begin{block}{Engagement Question}
        Think about the data you interact with daily—can you identify which type it belongs to? 
        How might knowing its type influence how you process or analyze it?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues - Introduction}
    \begin{block}{Introduction to Common Data Issues}
        Data preprocessing is crucial in data analysis. The presence of various issues can severely impact the quality of insights derived from the data. Understanding these common data issues helps us to clean and prepare our datasets effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues - Types}
    \begin{itemize}
        \item \textbf{Noise:}
        \begin{itemize}
            \item \textit{Definition:} Random errors or variations not reflecting true values.
            \item \textit{Example:} Temperature data with faulty readings, like -50°C or 150°C.
        \end{itemize}
        
        \item \textbf{Outliers:}
        \begin{itemize}
            \item \textit{Definition:} Data points lying significantly outside the range of other observations.
            \item \textit{Example:} Income data with most entries between \$30,000 and \$100,000, but a few exceeding \$1,000,000.
        \end{itemize}
        
        \item \textbf{Inconsistencies:}
        \begin{itemize}
            \item \textit{Definition:} Conflicting data entries leading to ambiguity.
            \item \textit{Example:} Using "NY" in one dataset and "New York" in another for the same state.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Techniques}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Impact of Data Issues:} Quality affects insights and model performance.
            \item \textbf{Identification is Key:} Identifying issues through visualization and summary statistics is critical.
        \end{itemize}
    \end{block}
    
    \begin{block}{Techniques for Detection}
        \begin{itemize}
            \item \textbf{Box Plot for Outliers:} Visualizes data distribution and highlights outliers.
            \item \textbf{Histogram for Noise Identification:} Shows frequency distribution for identifying irregularities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example for Visualization}
    \begin{block}{Simple Python Code for Box Plot}
        \begin{lstlisting}[language=Python]
import pandas as pd
import matplotlib.pyplot as plt

# Example to visualize outliers using a box plot
data = pd.read_csv('your_data.csv')
plt.boxplot(data['income'])
plt.title('Income Distribution with Outliers')
plt.ylabel('Income')
plt.show()
        \end{lstlisting}
    \end{block}
    
    \textit{This code snippet allows the visualization of outliers in a dataset through a box plot.}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues - Conclusion}
    \begin{block}{Conclusion}
        Understanding and addressing these common data issues is essential for ensuring the accuracy and reliability of data analysis. The next step will focus on effectively handling missing values, which often accompany the challenges discussed.
    \end{block}
    
    By highlighting these concepts, we can foster a deeper appreciation for data integrity and its implications for effective analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values - Overview}
    \begin{itemize}
        \item **Definition**: Missing values occur when no data is stored for a variable in an observation.
        \item **Importance**: Missing data can bias results and reduce the statistical power of analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values - Techniques}
    \begin{block}{Key Approaches}
        \begin{itemize}
            \item **Deletion Methods**
            \item **Imputation Methods**
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values - Deletion Methods}
    \begin{itemize}
        \item **Listwise Deletion**: 
            \begin{itemize}
                \item Removes entire records with missing values.
                \item \textbf{Example}: 100 records, 10 missing, analyze 90.
                \item \textbf{Pros}: Simple.
                \item \textbf{Cons}: Can lead to significant data loss.
            \end{itemize}
        \item **Pairwise Deletion**: 
            \begin{itemize}
                \item Omits missing values for specific calculations only.
                \item \textbf{Example}: Analyze two variables using only available data for those.
                \item \textbf{Pros}: Retains more data.
                \item \textbf{Cons}: Inconsistent sample sizes across analyses.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values - Imputation Methods}
    \begin{itemize}
        \item **Mean Imputation**: Replaces missing values with the mean.
            \begin{itemize}
                \item \textbf{Example}: Average height to fill in missing height data.
            \end{itemize}
        \item **Median Imputation**: Uses the median for skewed data.
            \begin{itemize}
                \item \textbf{Example}: Median salary for better central tendency.
            \end{itemize}
        \item **Mode Imputation**: For categorical data, replaces with the most frequent category.
            \begin{itemize}
                \item \textbf{Example}: Replace missing favorite color with the most common.
            \end{itemize}
        \item **Model-Based Imputation**: Predictive models estimate missing values.
            \begin{itemize}
                \item \textbf{Example}: Regression model to predict and fill missing ages.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values - Key Points}
    \begin{itemize}
        \item \textbf{Method Selection}: Depends on dataset size, pattern of missingness.
        \item \textbf{Caution}: Over-imputation can introduce bias.
        \item \textbf{Sensitivity Analysis}: Essential to assess method impacts on results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values - Conclusion and Exercise}
    \begin{itemize}
        \item Effective handling of missing values is crucial for data integrity.
        \item Engage in practical exercises with real datasets to apply these methods.
        \item \textbf{Optional Exercise}: Explore a dataset with missing values; apply deletion and imputation methods, and compare results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Imputation Techniques - Overview}
    \begin{block}{Overview of Imputation Techniques}
        Imputation techniques are methods used to replace missing data values with substituted values. This process is vital in preparing a dataset for analysis, as missing values can lead to inaccurate model predictions and analyses. Here we cover four primary imputation methods:
    \end{block}
    \begin{enumerate}
        \item Mean Imputation
        \item Median Imputation
        \item Mode Imputation
        \item Model-Based Imputation
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mean Imputation}
    \begin{block}{Definition}
        Replaces missing values with the mean (average) of all non-missing values in that feature.
    \end{block}
    
    \begin{block}{How It Works}
        Calculate the mean:
        \begin{equation}
            \text{Mean} = \frac{\sum{\text{value}}}{n}
        \end{equation}
        where \( n \) is the number of non-missing values.
    \end{block}
    
    \begin{block}{Example}
        Given: [2, 3, NaN, 5, 6] \\
        **Mean Calculation**: 
        \begin{equation}
            \text{Mean} = \frac{2 + 3 + 5 + 6}{4} = 4
        \end{equation}
        **Imputed Data**: [2, 3, 4, 5, 6]
    \end{block}

    \begin{block}{Key Point}
        Use when data is normally distributed; otherwise, it may skew results.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Median and Mode Imputation}
    \begin{block}{Median Imputation}
        \begin{itemize}
            \item **Definition**: Replaces missing values with the median value of non-missing values.
            \item **How It Works**: The median is the middle value when data is sorted.
            \item **Example**: Given: [1, 2, NaN, 3, 4] \\
            Sorted: [1, 2, 3, 4] \\
            **Median Calculation**: 2 \\
            **Imputed Data**: [1, 2, 2, 3, 4]
            \item **Key Point**: More robust than mean when data contains outliers.
        \end{itemize}
    \end{block}
    
    \begin{block}{Mode Imputation}
        \begin{itemize}
            \item **Definition**: Replaces missing values with the mode, which is the most frequently occurring value.
            \item **Example**: Given: [1, 1, 2, NaN, 3] \\
            **Mode Calculation**: 1 \\
            **Imputed Data**: [1, 1, 2, 1, 3]
            \item **Key Point**: Use for categorical data to provide sensible imputation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model-Based Imputation}
    \begin{block}{Model-Based Imputation}
        \begin{itemize}
            \item **Definition**: Uses algorithms to predict and fill missing values based on other available data features.
            \item **How It Works**: Train a machine learning model (e.g., regression, decision trees) on the non-missing data to predict missing values.
            \item **Example**: Use regression to estimate missing values based on correlations with other features.
            \item **Key Point**: This method can provide higher accuracy; however, it requires additional computational resources and careful model selection.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Choosing the right imputation technique depends on the data's distribution, the nature of the dataset, and the amount of missing data. 
    \begin{block}{Engagement Question}
        *Engage with your data: Ask yourself - Which method do you think is the most suitable for your dataset? Why?*
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Normalization}
    \begin{block}{What is Data Normalization?}
        Data normalization is the process of adjusting values in a dataset to bring them into a common scale without distorting the differences in their ranges. This technique is crucial in data preprocessing for machine learning algorithms that rely on distance calculations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Normalization}
    \begin{enumerate}
        \item \textbf{Consistency in Scale:} Ensures that different features contribute equally to distance calculations in algorithms.
        
        \item \textbf{Improving Convergence Time:} Normalized data can lead to faster convergence in optimization algorithms like gradient descent.
        
        \item \textbf{Enhancing Model Performance:} Algorithms like neural networks can achieve better accuracy with normalized data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Normalization Techniques}
    \begin{itemize}
        \item \textbf{Min-Max Scaling:}
        \[
        X' = \frac{X - X_{min}}{X_{max} - X_{min}}
        \]
        
        \item \textbf{Z-score Normalization:}
        \[
        Z = \frac{X - \mu}{\sigma}
        \]
        where \( \mu \) is the mean and \( \sigma \) is the standard deviation.
        
        \item \textbf{Robust Scaling:}
        \[
        X' = \frac{X - \text{median}(X)}{IQR}
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Normalization}
    \begin{block}{Without Normalization}
        In a dataset where one feature is "age" (0-100) and another is "income" (0-100,000), the "income" feature can overshadow the "age" feature.
    \end{block}

    \begin{block}{With Normalization}
        After applying Min-Max normalization, both features contribute equally to model training by setting their ranges to [0, 1].
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Data normalization is essential for ensuring balanced feature contributions in machine learning model training, improving performance, and accelerating the learning process. Understanding various techniques will prepare you for effective data preprocessing.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques - Overview}
    \begin{block}{Overview}
        Data normalization is essential in machine learning to ensure that features contribute equally to distance metrics and model performance. 
        By transforming features into a common scale, we enhance the learning process of algorithms, which often assumes all input features are on a similar scale.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques - Key Techniques}
    \begin{enumerate}
        \item \textbf{Min-Max Scaling}
        \begin{itemize}
            \item \textbf{Description:} Rescales features to a fixed range, typically [0, 1].
            \item \textbf{Formula:} 
            \begin{equation}
            X' = \frac{X - X_{min}}{X_{max} - X_{min}}
            \end{equation}
            \item \textbf{When to Use:} Ideal for distance-based algorithms like K-means, K-NN.
            \item \textbf{Example:} 
                Original: [50, 55, 75, 100]
                Scaled: [0, 0.05, 0.25, 1]
        \end{itemize}

        \item \textbf{Z-score Normalization (Standardization)}
        \begin{itemize}
            \item \textbf{Description:} Normalizes data based on mean and standard deviation.
            \item \textbf{Formula:} 
            \begin{equation}
            Z = \frac{X - \mu}{\sigma}
            \end{equation}
            \item \textbf{When to Use:} Useful with Gaussian distribution, for algorithms like Logistic Regression, Neural Networks.
            \item \textbf{Example:} 
                Original: [15, 20, 25]
                Z-scored: [-1, 0, 1]
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization Techniques - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Robust Scaling}
        \begin{itemize}
            \item \textbf{Description:} Uses median and interquartile range (IQR) for scaling, making it robust to outliers.
            \item \textbf{Formula:} 
            \begin{equation}
            X' = \frac{X - Q_{50}}{Q_{75} - Q_{25}}
            \end{equation}
            \item \textbf{When to Use:} Best for datasets with many outliers.
            \item \textbf{Example:} 
                Original: [1, 2, 3, 100]
                Scaled: [-1, 0, 1, 65]
        \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Normalization techniques improve the performance of machine learning models.
            \item Choice depends on dataset characteristics and chosen ML algorithm.
            \item Every technique has advantages and optimal scenarios.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Encoding Categorical Variables}
    \begin{block}{What Are Categorical Variables?}
        \begin{itemize}
            \item Categorical variables represent discrete values or groups.
            \item Examples: 
            \begin{itemize}
                \item \textbf{Color:} Red, Blue, Green
                \item \textbf{Animal Type:} Dog, Cat, Bird
            \end{itemize}
            \item Often incompatible with machine learning models that require numerical input.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Encode Categorical Variables?}
    \begin{block}{Importance of Encoding}
        \begin{itemize}
            \item Most machine learning algorithms operate on numerical data.
            \item Non-numeric types cannot be processed directly.
            \item Proper encoding enables models to understand and make predictions based on categorical variables.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Techniques for Encoding}
    \begin{enumerate}
        \item \textbf{Label Encoding}
            \begin{itemize}
                \item Converts categories into unique integers.
                \item Example: 
                \begin{itemize}
                    \item Red $\rightarrow$ 0
                    \item Blue $\rightarrow$ 1
                    \item Green $\rightarrow$ 2
                \end{itemize}
                \item Use case: Suitable for ordinal data.
                \begin{lstlisting}[language=Python]
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
color = ['Red', 'Blue', 'Green']
encoded_color = le.fit_transform(color) # Outputs: [2, 0, 1]
\end{lstlisting}
            \end{itemize}
        
        \item \textbf{One-Hot Encoding}
            \begin{itemize}
                \item Creates binary columns for each category.
                \item Example:
                \begin{itemize}
                    \item Original: [Red, Blue, Green]
                    \item One-Hot: 
                    \begin{itemize}
                        \item Red: [1, 0, 0]
                        \item Blue: [0, 1, 0]
                        \item Green: [0, 0, 1]
                    \end{itemize}
                \end{itemize}
                \item Use case: Preferred for nominal data.
                \begin{lstlisting}[language=Python]
import pandas as pd

df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})
one_hot_encoded = pd.get_dummies(df, columns=['Color'])
\end{lstlisting}
            \end{itemize}

        \item \textbf{Binary Encoding}
            \begin{itemize}
                \item Combines label and one-hot encoding benefits.
                \item Example:
                \begin{itemize}
                    \item Red $\rightarrow$ 0 $\rightarrow$ 00
                    \item Blue $\rightarrow$ 1 $\rightarrow$ 01
                    \item Green $\rightarrow$ 2 $\rightarrow$ 10
                \end{itemize}
                \item Each binary digit results in a new column.
                \begin{lstlisting}[language=Python]
!pip install category_encoders  # Make sure to install category_encoders package

import category_encoders as ce
df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})
encoder = ce.BinaryEncoder(cols=['Color'])
binary_encoded_df = encoder.fit_transform(df)
\end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Reflection}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Choose encoding technique based on the nature of categorical variables (ordinal vs nominal).
            \item Beware of excessive dimensions from one-hot encoding; use binary encoding as an alternative when needed.
        \end{itemize}
    \end{block}
    
    \begin{block}{Questions for Reflection}
        \begin{itemize}
            \item How do different encoding techniques affect machine learning model performance?
            \item In what scenarios might label encoding introduce bias or misinterpretation?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Feature Engineering}
    \begin{block}{Importance of Feature Selection and Extraction in Preprocessing}
        Feature Engineering is essential in preprocessing, as it helps improve model performance through careful selection and transformation of features.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{What is Feature Engineering?}
    Feature Engineering involves:
    \begin{enumerate}
        \item \textbf{Feature Selection}: Identifying relevant features and removing those that are irrelevant or redundant.
        \item \textbf{Feature Extraction}: Transforming data into a lower-dimensional space while retaining important information.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Why is Feature Engineering Important?}
    \begin{itemize}
        \item \textbf{Model Performance}: Better features can improve accuracy and generalization.
        \item \textbf{Reduced Overfitting}: Irrelevant features contribute to model complexity, making overfitting less likely.
        \item \textbf{Improved Interpretability}: Efforts in feature selection help clarify model predictions.
        \item \textbf{Efficiency}: Fewer features accelerate training times for quicker experiments.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Techniques in Feature Selection}
    \begin{enumerate}
        \item \textbf{Filter Methods}: Use statistics to evaluate feature importance (e.g., chi-square test).
        \item \textbf{Wrapper Methods}: Assess feature quality using predictive models (e.g., recursive feature elimination).
        \item \textbf{Embedded Methods}: Perform selection during model training (e.g., Lasso regression).
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Techniques in Feature Extraction}
    \begin{enumerate}
        \item \textbf{Principal Component Analysis (PCA)}: Transforms features into orthogonal components explaining maximum variance.
        \item \textbf{t-SNE}: Visualizes high-dimensional data by mapping it to lower dimensions.
        \item \textbf{Feature Creation}: Develops new features from existing data (e.g., deriving "age" from a "date of birth").
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet for Feature Selection}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif

# Load dataset
data = pd.read_csv('ecommerce_data.csv')

# Features and target variable
X = data.drop('sales', axis=1)
y = data['sales']

# Feature selection using ANOVA F-value
selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(X, y)

print("Selected Features: ", selector.get_feature_names_out())
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item Effective feature engineering significantly impacts model success.
        \item Use tailored techniques based on your data and problem context.
        \item Collaborate with stakeholders to identify impactful features.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Data - Introduction}
    Data splitting is a crucial step in the data preprocessing process. By partitioning your dataset into distinct subsets, you can validate model performance and ensure that your findings are robust. The primary splits are into \textbf{training}, \textbf{validation}, and \textbf{testing} sets.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Data - Why Split Data?}
    \begin{itemize}
        \item \textbf{Model Training:} The model learns from the training set to identify patterns and features.
        \item \textbf{Hyperparameter Tuning:} The validation set is used to fine-tune model parameters, providing insights into model performance without overfitting.
        \item \textbf{Final Evaluation:} The test set gives the final evaluation of the fully trained model, simulating how it will perform on unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Data - Common Methods}
    \begin{enumerate}
        \item \textbf{Simple Random Sampling}
        \begin{itemize}
            \item Randomly select a subset of data for each set.
            \item \textbf{Example:} For 1,000 data points, use 70\% for training, 15\% for validation, and 15\% for testing.
            \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

# Load your dataset
X, y = load_your_data()

# Split the data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Stratified Sampling}
        \begin{itemize}
            \item Maintains class proportions in all subsets, crucial for imbalanced datasets.
            \item \textbf{Example:} In a binary classification (70\% class A, 30\% class B), this method ensures similar class distribution in all splits.
            \begin{lstlisting}[language=Python]
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{K-Fold Cross-Validation}
        \begin{itemize}
            \item Divides the dataset into 'k' parts. Train on k-1 parts and test on the remaining.
            \item \textbf{Example:} For 100 samples and k=5, each fold has 20 samples for testing.
            \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold

kf = KFold(n_splits=5)
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    # Train and test your model here
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Data - Key Points}
    \begin{itemize}
        \item Always maintain a separate test set to avoid overfitting and ensure a fair model evaluation.
        \item Use stratified sampling for classification problems to preserve class distributions.
        \item K-Fold Cross-Validation allows robust model evaluation by utilizing the entire dataset across multiple iterations.
    \end{itemize}
    
    \textbf{Conclusion:} Effective data splitting helps in building reliable machine learning models that generalize well to unseen data.
    
    \textbf{Transition:} Next, we will explore \textbf{Data Visualization}, an essential tool in the preprocessing phase.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Visualization}
    % Overview of data visualization and its significance for preprocessing.
    Data visualization is the graphical representation of information and data. It utilizes visual elements like charts, graphs, and maps to reveal trends, outliers, and patterns in datasets, facilitating better understanding and analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Visualization}
    \begin{itemize}
        \item \textbf{Identify Data Issues:} Visualizations can uncover anomalies, missing values, or outliers that may be difficult to detect in raw data.
        \item \textbf{Aid in Preprocessing Decisions:} Visualizing relationships and distributions guides informed decisions on necessary preprocessing steps, such as normalization, transformation, or imputation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques for Data Visualization}
    \begin{enumerate}
        \item \textbf{Histograms:} Understanding single variable distributions, e.g., housing prices reveal skewness.
        \item \textbf{Box Plots:} Identify outliers and compare distributions, e.g., salaries across departments.
        \item \textbf{Scatter Plots:} Exploring relationships between two variables, e.g., experience vs. salary.
        \item \textbf{Heatmaps:} Visualizing correlations between variables, guiding feature selection in datasets.
        \item \textbf{Pair Plots:} Exploring multi-dimensional relationships, showing interactions among multiple features.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Choose the Right Visualization:} Different visualizations serve distinct purposes; select accordingly.
        \item \textbf{Context Matters:} Interpret visualizations based on the dataset's context and specific questions.
        \item \textbf{Iterate:} Visualization is an iterative process; update visuals as your data preprocessing evolves.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Incorporating data visualization into preprocessing enhances your understanding of data and empowers informed decision-making, leading to effective modeling outcomes. Remember, a clear picture is worth a thousand data points!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Preprocessing Pipeline Overview}
    \begin{block}{Building a Structured Data Preprocessing Pipeline}
        A preprocessing pipeline is a systematic series of tasks that prepares data for machine learning. Its significance lies in:
        \begin{itemize}
            \item Ensuring data is clean and organized.
            \item Affecting model performance and efficiency.
            \item Maintaining reproducibility in workflows.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Pipeline Components}
    \begin{enumerate}
        \item \textbf{Data Collection:} Gathering raw data from various sources (e.g., APIs, CSV files).
        \item \textbf{Data Integration:} Merging data from different sources into one unified dataset.
        \item \textbf{Data Cleaning:} Addressing missing values, duplicates, and correcting errors.
        \item \textbf{Data Transformation:} Adjusting data format and scaling for algorithm requirements.
        \item \textbf{Feature Selection:} Identifying and selecting the most relevant features for model training.
        \item \textbf{Data Splitting:} Dividing the dataset into training, validation, and test sets.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Pipeline Implementation}
    \begin{block}{Python Code Example}
        The following code demonstrates how to set up a preprocessing pipeline in Python using Scikit-learn:
        \begin{lstlisting}[language=Python]
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of a Preprocessing Pipeline}
    \begin{block}{Key Benefits}
        Maintaining a structured preprocessing pipeline is crucial because it:
        \begin{itemize}
            \item Ensures reproducibility of strategies.
            \item Reduces the risk of data leakage.
            \item Automates repeated tasks for enhanced efficiency.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item A well-defined preprocessing pipeline can significantly improve model performance.
        \item Each step should be tailored to the specific dataset and problem at hand.
        \item Continuous monitoring and adjustments are necessary as data characteristics change over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Preprocessing Techniques}
    \begin{block}{Overview}
        Measuring the impact of different preprocessing techniques on model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Data Preprocessing?}
    Data preprocessing involves transforming raw data into an understandable format. 
    This step is crucial in preparing data for a machine learning model, as it often 
    directly affects the model's performance. 

    \begin{itemize}
        \item Common preprocessing techniques include:
        \begin{itemize}
            \item Normalization
            \item Encoding categorical variables
            \item Handling missing values
            \item And more...
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Evaluate Preprocessing Techniques?}
    Understanding how different preprocessing techniques impact model performance 
    enables you to select the best approach for your specific dataset and model type. 
    This can lead to:
    
    \begin{itemize}
        \item Better predictive accuracy
        \item More robust models
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics}
    \begin{enumerate}
        \item \textbf{Model Accuracy}: Proportion of correct predictions made by the model.
        \item \textbf{Precision and Recall}: Used in classification tasks to understand the quality of predictions.
            \begin{itemize}
                \item \textbf{Precision}: 
                $$ \text{Precision} = \frac{\text{True Positives}}{\text{True Positives + False Positives}} $$
                \item \textbf{Recall}: 
                $$ \text{Recall} = \frac{\text{True Positives}}{\text{True Positives + False Negatives}} $$
            \end{itemize}
        \item \textbf{F1 Score}: Harmonic mean of precision and recall, useful for imbalanced datasets.
        \item \textbf{Cross-Validation}: Technique to assess how results generalize to unseen data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Process}
    \begin{enumerate}
        \item \textbf{Select a Dataset}:
            \begin{itemize}
                \item Example: Iris Flower Dataset.
            \end{itemize}
        \item \textbf{Choose Preprocessing Techniques}:
            \begin{itemize}
                \item \textbf{Normalization}: Scale features to a range (0, 1).
                \item \textbf{One-Hot Encoding}: Convert categorical variable into binary columns.
            \end{itemize}
        \item \textbf{Compare Models}:
            \begin{itemize}
                \item Train models (e.g., Logistic Regression, Decision Tree) using different preprocessing techniques.
                \item Record metrics (accuracy, precision, etc.) for each model setup.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Results}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|l|c|c|c|c|}
            \hline
            \textbf{Technique} & \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
            \hline
            No Preprocessing & Logistic Reg & 85\% & 0.83 & 0.84 & 0.83 \\
            Normalization + One-Hot & Decision Tree & 90\% & 0.88 & 0.89 & 0.88 \\
            \hline
        \end{tabular}
        \caption{Performance Comparison of Preprocessing Techniques}
    \end{table}
    This comparison illustrates how preprocessing can improve model performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Tailored Approach}: Different datasets require different preprocessing techniques for optimal performance.
        \item \textbf{Iterative Testing}: Always evaluate various techniques and combinations to identify the best fit for your use case.
        \item \textbf{Importance of Cross-Validation}: Avoid overfitting by validating techniques on unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Evaluating preprocessing techniques is not just a best practice; it is an essential component of building effective machine learning models. 
    By systematically measuring the impact of different preprocessing methods, we can significantly enhance the performance and reliability of our models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: The Impact of Data Preprocessing}
    \begin{block}{Introduction}
        Data preprocessing is a critical step in the machine learning workflow, significantly influencing the performance of predictive models. 
        This case study illustrates the tangible benefits of effective data preprocessing through a real-world example: predicting housing prices.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Housing Price Prediction}
    \begin{enumerate}
        \item \textbf{Overview of the Problem}
        \begin{itemize}
            \item A real estate company aimed to predict house prices to improve pricing strategies.
            \item Dataset features included:
                \begin{itemize}
                    \item Size of the house (sq. ft.)
                    \item Number of bedrooms and bathrooms
                    \item Location attributes (city, neighborhood)
                    \item Year built
                    \item Market conditions (e.g., interest rates)
                \end{itemize}
        \end{itemize}
        
        \item \textbf{Initial Data Quality Issues}
        \begin{itemize}
            \item Missing Values: Several entries had missing data.
            \item Outliers: Some data points were outside the normal range.
            \item Categorical Variables: Location needed conversion to numerical format.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Steps}
    \begin{enumerate}
        \item \textbf{Handling Missing Values}
            \begin{itemize}
                \item Used mean/mode imputation for numerical and categorical features.
                \item Example: Average size used to fill missing entries.
            \end{itemize}
        
        \item \textbf{Removing Outliers}
            \begin{itemize}
                \item Implemented z-score method to remove deviations.
                \item Formula: 
                \[
                z = \frac{(X - \mu)}{\sigma}
                \]
                where \(X\) = value, \(\mu\) = mean, \(\sigma\) = standard deviation.
            \end{itemize}
        
        \item \textbf{Encoding Categorical Variables}
            \begin{itemize}
                \item Used One-Hot Encoding for location features.
                \item Example: "New York", "Los Angeles", "Chicago" became binary columns.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Outcome and Model Performance}
    \begin{enumerate}
        \item With preprocessing steps executed, the dataset became cleaner and ready for model training.
        \item The real estate company used a linear regression model:
        \begin{itemize}
            \item \textbf{Before Preprocessing:} Model accuracy was around 65\%.
            \item \textbf{After Preprocessing:} Model accuracy increased to 85\%.
        \end{itemize}
    \end{enumerate}
    \begin{block}{Key Takeaway}
        Effective data preprocessing enhances model accuracy and ensures reliable predictions, driving better business decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Questions}
    \begin{itemize}
        \item Data quality impacts model performance significantly.
        \item Simple preprocessing techniques can lead to considerable improvements in predictive accuracy.
        \item Preprocessing is an essential step that should not be overlooked.
    \end{itemize}
    \begin{block}{Questions for Reflection}
        \begin{itemize}
            \item What challenges could arise if inadequate preprocessing were applied?
            \item How might these techniques vary in different industries?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Key Takeaways}
  
  \begin{enumerate}
    \item \textbf{Importance of Data Quality:} 
      Data preprocessing is essential for improving the accuracy and performance of machine learning models.
    \item \textbf{Common Preprocessing Techniques:}
      \begin{itemize}
        \item \textbf{Data Cleaning:} Removing duplicates and handling missing values to ensure reliability.
        \item \textbf{Data Transformation:} Normalization and standardization for comparability.
        \item \textbf{Feature Engineering:} Creating new features from existing data to enhance performance.
        \item \textbf{Data Splitting:} Properly dividing data into training, validation, and test sets to evaluate performance.
      \end{itemize}
  \end{enumerate}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Examples}
  
  \begin{block}{Data Cleaning Example}
    A dataset with missing customer reviews can skew sentiment analysis results. Imputing or removing these entries helps maintain integrity.
  \end{block}
  
  \begin{block}{Data Transformation Example}
    Normalizing income data to a scale between 0 and 1 allows for better comparison against other features, crucial for algorithms like KNN.
  \end{block}

  \begin{block}{Feature Engineering Example}
    From time stamps, we derive features such as “hour of the day” or “day of the week,” leading to refined predictions.
  \end{block}

  \begin{block}{Data Splitting Tip}
    A common split is 70\% for training, 15\% for validation, and 15\% for testing.
  \end{block}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Steps: Model Implementation}

  As we transition into the next chapter, we will focus on:
  
  \begin{itemize}
    \item \textbf{Selecting a Suitable Model:} Understand various algorithms based on data nature.
    \item \textbf{Model Training and Evaluation:} Learn to train on preprocessed data and validate performance.
    \item \textbf{Real-World Application:} Investigate case studies demonstrating practical application of learned techniques.
  \end{itemize}

  \begin{block}{Inspiring Question}
    How can mastering data preprocessing enhance the models we build and the insights we can derive from data?
  \end{block}

  By concluding this chapter, we prepare ourselves for effective model deployment in the machine learning lifecycle.
  
\end{frame}


\end{document}