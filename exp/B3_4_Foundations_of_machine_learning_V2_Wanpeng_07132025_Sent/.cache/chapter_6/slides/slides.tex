\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \maketitle
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Techniques}
    \begin{block}{Overview of Regression Methods}
        Regression is a statistical method used to model the relationship between a dependent variable (outcome) and one or more independent variables (predictors).
    \end{block}
    \begin{itemize}
        \item **What is Regression?** It enables predictions about future outcomes by modeling relationships.
        \item **Importance in Machine Learning:**
        \begin{itemize}
            \item Predict numeric outcomes (e.g., house prices).
            \item Analyze relationships for informed decision-making.
            \item Optimize resources based on historical data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Types of Regression Techniques}
    \begin{itemize}
        \item **Linear Regression:** Models the relationship as a straight line.
        \begin{block}{Example}
            Predicting a student's test score based on study hours.
        \end{block}
        
        \item **Polynomial Regression:** Uses nth degree polynomials for relationships.
        \begin{block}{Example}
            Modeling the trajectory of a rocket.
        \end{block}
        
        \item **Ridge and Lasso Regression:** Introduces penalties to avoid overfitting.
        \begin{block}{Example}
            Predicting stock prices using many indicators.
        \end{block}
        
        \item **Logistic Regression:** Models probability for binary classification.
        \begin{block}{Example}
            Determining if a customer will buy a product.
        \end{block}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item **Real-world Applications:** Widely used in industries, providing actionable insights.
        \item **Model Interpretation:** Crucial for understanding predictor impacts on outcomes.
        \item **Avoiding Common Pitfalls:** Be aware of overfitting and underfitting risks.
    \end{itemize}

    \begin{block}{Concluding Thought}
        How can regression techniques transform our approach to data analysis and decision-making?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Regression? - Definition}
    \begin{block}{Definition}
        \textbf{Regression} is a statistical method used for predicting and analyzing the relationships between variables. It specifically helps us estimate the value of a dependent variable based on one or more independent variables.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Regression? - Purpose & Key Points}
    \begin{block}{Purpose of Regression}
        The primary purpose of regression is to identify and quantify the relationship between variables, enabling predictions about the dependent variable when the values of independent variables are known.
    \end{block}

    \begin{itemize}
        \item \textbf{Prediction and Estimation:} E.g., predicting sales based on advertising spend.
        \item \textbf{Understanding Relationships:} How changes in independent variables correlate with changes in the dependent variable.
        \item \textbf{Data-Driven Decision Making:} Informed decisions based on insights from regression analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Regression? - Example Scenario}
    \begin{block}{Example Scenario}
        Imagine a farmer wants to predict the yield of his crop based on two factors: the amount of water given to the crops and the hours of sunlight they receive.
    \end{block}

    \begin{itemize}
        \item \textbf{Dependent Variable (Y):} Crop yield (measured in tons)
        \item \textbf{Independent Variables (X1, X2):} 
            \begin{itemize}
                \item $X_1 = \text{Amount of water (liters)}$
                \item $X_2 = \text{Hours of sunlight (hours per day)}$
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Regression Equation}
        The regression model could look like: 
        \[
        Y = a + b_1 X_1 + b_2 X_2
        \]
        Where:
        \begin{itemize}
            \item $Y$ = crop yield
            \item $a$ = intercept (base yield)
            \item $b_1$, $b_2$ = coefficients for water and sunlight
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - Introduction}
    \begin{block}{Overview}
        Regression techniques are powerful statistical tools used to understand relationships between variables and predict future outcomes based on these relationships. 
    \end{block}
    \begin{itemize}
        \item Understanding various regression types is crucial for selecting the appropriate method for analysis.
        \item Each regression type has unique applications suitable for specific scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - 1. Linear Regression}
    \begin{block}{Concept}
        Establishes a relationship between a dependent variable and one or more independent variables using a straight line.
    \end{block}
    \begin{itemize}
        \item \textbf{Example}: Predicting a person's weight based on their height.
        \item \textbf{Key Equation}:
        \begin{equation}
            Y = a + bX
        \end{equation}
        Where:
        \begin{itemize}
            \item $Y$ = dependent variable
            \item $X$ = independent variable
            \item $a$ = y-intercept (constant)
            \item $b$ = slope (coefficient of $X$)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - 2. Multiple Regression}
    \begin{block}{Concept}
        Similar to linear regression but involves multiple independent variables.
    \end{block}
    \begin{itemize}
        \item \textbf{Example}: Predicting house prices based on location, size, and number of bedrooms.
        \item \textbf{Key Equation}:
        \begin{equation}
            Y = a + b_1X_1 + b_2X_2 + \ldots + b_nX_n
        \end{equation}
        Where $X_1, X_2, \ldots, X_n$ are different predictors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - 3. Polynomial Regression}
    \begin{block}{Concept}
        A form of regression that models the relationship as an $n$th degree polynomial.
    \end{block}
    \begin{itemize}
        \item \textbf{Example}: Analyzing the growth of a plant over time.
        \item \textbf{Key Equation}:
        \begin{equation}
            Y = a + b_1X + b_2X^2 + \ldots + b_nX^n
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - 4. Logistic Regression}
    \begin{block}{Concept}
        Used when the dependent variable is categorical (especially binary outcomes).
    \end{block}
    \begin{itemize}
        \item \textbf{Example}: Predicting whether a student will pass or fail based on study hours.
        \item \textbf{Key Expression}:
        \begin{equation}
            P(Y=1) = \frac{1}{1 + e^{-(a + bX)}}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - 5. Ridge and Lasso Regression}
    \begin{block}{Ridge Regression}
        \begin{itemize}
            \item Concept: Used when there is multicollinearity in the regression model.
            \item \textbf{Key Adjustment}: Adds a penalty to the loss function: 
            \begin{equation}
                \text{Loss Function} = \text{SE} + \lambda \sum b_j^2
            \end{equation}
        \end{itemize}
    \end{block}
    \begin{block}{Lasso Regression}
        \begin{itemize}
            \item Concept: Similar to ridge regression but can reduce feature coefficients to zero.
            \item \textbf{Key Adjustment}: 
            \begin{equation}
                \text{Loss Function} = \text{SE} + \lambda \sum |b_j|
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - Summary}
    \begin{itemize}
        \item Consider the nature of your dependent variable (continuous vs categorical) when choosing a technique.
        \item Simpler models often perform better and are easier to interpret.
    \end{itemize}
    \begin{block}{Conclusion}
        Through understanding these types of regression, you will be equipped to analyze data effectively and derive insights that drive decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Overview}
    \begin{block}{Understanding Linear Regression}
        Linear Regression is a fundamental statistical method used to model relationships between:
        \begin{itemize}
            \item \textbf{Dependent Variable (Y):} The outcome we want to predict (e.g., sales).
            \item \textbf{Independent Variable (X):} The predictor(s) (e.g., advertising expenditure).
        \end{itemize}
        It helps us understand how changes in independent variables affect the dependent variable.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Model Formulation}
    \begin{block}{Model Equation}
        The linear regression model can be expressed with the equation:
        \begin{equation}
            Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon 
        \end{equation}
        Where:
        \begin{itemize}
            \item \(Y\) = Predicted dependent variable
            \item \(X_i\) = Independent variables
            \item \(\beta_0\) = Intercept of the model
            \item \(\beta_i\) = Coefficients for each independent variable
            \item \(\epsilon\) = Error term
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        Predicting a student's final score:
        \[
        \text{Final Score} = \beta_0 + \beta_1 (\text{Hours Studied}) + \epsilon 
        \]
        If \(\beta_1 = 5\), this indicates each additional hour studied increases the score by 5 points.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{enumerate}
            \item \textbf{Prediction Capability:} Linear regression predicts outcomes from data relationships.
            \item \textbf{Assumptions:}
            \begin{itemize}
                \item Linearity
                \item Independence
                \item Homoscedasticity
                \item Normality
            \end{itemize}
            \item \textbf{Applications:} Utilized in various fields like economics and biology for data-driven decisions.
            \item \textbf{Simplification:} Starts with simple linear regression (one independent variable) before expanding.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Conclusion}
        Linear regression is a powerful tool laying the groundwork for more complex modeling techniques. 
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Linear Regression - Overview}
    \begin{itemize}
        \item Linear regression is a statistical method to model relationships between variables.
        \item It is widely used for predictions and understanding trends in data.
        \item This presentation outlines a step-by-step guide using a sample dataset.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Dataset & Libraries}
    \begin{itemize}
        \item \textbf{Step 1: Choose a Dataset}
        \begin{itemize}
            \item Example: Dataset of house prices including features like:
            \begin{itemize}
                \item Square footage
                \item Number of bedrooms
                \item Age of the house
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Step 2: Import Required Libraries}
        \begin{lstlisting}[language=Python]
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Steps 3 to 9}
    \begin{enumerate}
        \item \textbf{Step 3: Load the Data}
        \begin{lstlisting}[language=Python]
data = pd.read_csv('house_prices.csv')
        \end{lstlisting}
        
        \item \textbf{Step 4: Explore the Data}
        \begin{lstlisting}[language=Python]
print(data.head())
        \end{lstlisting}
        Example output:
        \begin{center}
            \begin{tabular}{|c|c|c|c|}
                \hline
                Square Footage & Bedrooms & Age & Price \\
                \hline
                1500 & 3 & 10 & 300000 \\
                2000 & 4 & 5 & 500000 \\
                1200 & 2 & 15 & 250000 \\
                \hline
            \end{tabular}
        \end{center}
        
        \item \textbf{Step 5: Prepare the Data}
        \begin{lstlisting}[language=Python]
X = data[['Square Footage', 'Bedrooms', 'Age']]
y = data['Price']
        \end{lstlisting}

        \item \textbf{Steps 6 to 9: Model Training and Evaluation}
        \begin{itemize}
            \item Split data into training and testing sets
            \item Create and train the model
            \item Make predictions and evaluate performance
        \end{itemize}
        
        Example evaluation:
        \begin{lstlisting}[language=Python]
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'MSE: {mse}, R²: {r2}')
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Introduction}
    \begin{itemize}
        \item Linear regression predicts relationships between dependent and independent variables.
        \item Certain key assumptions must be met for valid results.
        \item Understanding these assumptions ensures the integrity of the model and the accuracy of predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Key Assumptions}
    \begin{enumerate}
        \item \textbf{Linearity}
            \begin{itemize}
                \item Changes in independent variable(s) correspond to proportional changes in the dependent variable.
                \item \textit{Example}: Salary increases consistently with each additional year of experience.
            \end{itemize}
            
        \item \textbf{Independence}
            \begin{itemize}
                \item Residuals must be independent, meaning one observation's error should not influence another's.
                \item \textit{Example}: Responses from one individual in a study shouldn't affect another's.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Continued}
    \begin{enumerate}[resume]
        \item \textbf{Homoscedasticity}
            \begin{itemize}
                \item Variance of residuals is constant across all levels of independent variable(s).
                \item \textit{Example}: Variability in sales should not change with advertising budget.
            \end{itemize}

        \item \textbf{Normality of Residuals}
            \begin{itemize}
                \item Residuals should be normally distributed for valid hypothesis testing.
                \item \textit{Example}: Test scores producing a bell-shaped residual distribution signify normality.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Assess these assumptions before interpreting regression results.
            \item Violating assumptions leads to biased estimates and inaccurate predictions.
            \item Use diagnostic tools like residual plots and Q-Q plots for validity checks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Regression - Introduction}
    \begin{block}{Introduction to Limitations}
        While linear regression is a widely used and powerful statistical method for predicting outcomes, it is essential to recognize its limitations. Addressing these limitations will help in selecting appropriate models for different scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Regression - Key Points}
    \begin{enumerate}
        \item \textbf{Assumption of Linearity}
            \begin{itemize}
                \item Assumes a straight-line relationship between variables.
                \item \textit{Example}: Linear model on a parabolic dataset leads to inaccurate predictions.
            \end{itemize}
        \item \textbf{Sensitivity to Outliers}
            \begin{itemize}
                \item One extreme value can significantly distort results.
                \item \textit{Example}: An exceptionally low exam score can skew results in a study hours vs. scores dataset.
            \end{itemize}
        \item \textbf{Overfitting and Underfitting}
            \begin{itemize}
                \item Overfitting: Model learns noise.
                \item Underfitting: Model too simple to capture trends.
                \item \textit{Illustration}: Fitting a line through circularly arranged points causes underfitting.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Regression - Continued}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue from last point
        \item \textbf{Multicollinearity}
            \begin{itemize}
                \item Highly correlated independent variables lead to unreliable coefficient estimates.
                \item \textit{Example}: Size and number of rooms in predicting house prices may provide redundant information.
            \end{itemize}
        \item \textbf{Homoscedasticity}
            \begin{itemize}
                \item Assumes constant variance of errors; violation leads to invalid hypotheses and confidence intervals.
                \item \textit{Example}: Incomes affecting spending variance can mislead results.
            \end{itemize}
        \item \textbf{Normality of Errors}
            \begin{itemize}
                \item Residuals should be normally distributed for valid hypothesis testing.
                \item \textit{Example}: Sales prediction errors might deviate from normality due to seasonality.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Regression - Conclusion}
    \begin{block}{Conclusion}
        Recognizing these limitations is crucial for effectively using linear regression. Always validate assumptions, check for outliers, and consider alternative models when appropriate.
    \end{block}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Linear regression limitations pertain to assumptions, sensitivity, and model fitting challenges.
            \item Understanding these aids in better modeling choices and improved analytical outcomes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Introduction}
    \begin{block}{What is Logistic Regression?}
        Logistic Regression is a statistical method used for binary classification, predicting the probability of a binary outcome based on predictor variables.
    \end{block}
    
    \begin{itemize}
        \item Useful in scenarios with two possible outcome values, such as:
            \begin{itemize}
                \item Email Spam Detection (spam or not spam)
                \item Disease Diagnosis (positive or negative)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Working Mechanism}
    
    \begin{block}{How Does It Work?}
        Logistic regression uses the logistic (sigmoid) function to model a binary outcome:
        \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
        Where:
        \begin{itemize}
            \item $P(Y=1|X)$: Predicted probability of outcome 1.
            \item $e$: Euler's number.
            \item $\beta_0$: Intercept and $\beta_1, \beta_2, ...$: Coefficients of predictor variables.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Provides outputs confined between 0 and 1 (probabilities).
            \item Typically uses Maximum Likelihood Estimation (MLE) for coefficient estimation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Example and Conclusion}
    
    \begin{block}{Example: Predicting Online Fraud}
        Consider predicting fraudulent online transactions (1) vs legitimate ones (0) based on:
        \begin{itemize}
            \item Transaction Amount
            \item User's Past Behavior
            \item Device Used
        \end{itemize}
        Logistic regression helps model relationships and enhances decision-making.
    \end{block}
    
    \begin{block}{Conclusion}
        Logistic regression is a foundational technique in statistics and machine learning for binary classification, widely applicable across various fields including healthcare and finance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Model - Introduction}
    \begin{block}{Understanding the Logistic Function}
        Logistic Regression is a statistical method used for binary classification, predicting the likelihood of an event occurring based on predictor variables.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Model - The Logistic Function}
    \begin{block}{The Logistic Function}
        The logistic function, also known as the sigmoid function, transforms outputs of linear regression into probabilities between 0 and 1:
        \begin{equation}
            f(z) = \frac{1}{1 + e^{-z}}
        \end{equation}
        Where \( z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n \) and \( e \) is Euler's number (approximately 2.71828).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Model - Formulation and Example}
    \begin{block}{Model Formulation}
        The probability of an event is given by:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
        \end{equation}
        Here, \( P(Y=1 | X) \) represents the probability of a positive outcome.
    \end{block}
    \begin{block}{Practical Example}
        \textbf{Scenario:} Predicting if a student will pass (1) or fail (0) an exam based on hours studied (X) and previous grades (Y):
        \[
        P(Y=1 | X) = \frac{1}{1 + e^{-(-3 + 0.5 \cdot \text{Hours Studied} + 0.7 \cdot \text{Average Grade})}}
        \]
        For example, if a student studies for 8 hours and has an average grade of 75, inputting these values gives us a probability of passing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Model - Key Points}
    \begin{enumerate}
        \item Logistic Regression is mainly for binary outcomes but can be extended to multi-class problems.
        \item Understanding the logistic function is essential for interpreting probabilities correctly.
        \item Proper evaluation metrics like confusion matrix, accuracy, precision, and recall are crucial for assessing models.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression Model - Summary}
    \begin{block}{Conclusion}
        Logistic regression is a powerful tool for binary classification, turning linear combinations of features into probabilistic outcomes applicable in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementing Logistic Regression}
    \begin{block}{Introduction to Logistic Regression}
        Logistic regression is a powerful method for binary classification. It estimates the probability of an input belonging to a certain category using one or more predictor variables.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Steps to Implement Logistic Regression - Part 1}
    \begin{enumerate}
        \item \textbf{Define the Problem}
        \begin{itemize}
            \item Determine whether the outcome is binary (e.g., yes/no).
            \item \textit{Example:} Predict if a customer will purchase a product based on demographic information.
        \end{itemize}

        \item \textbf{Collect \& Prepare Data}
        \begin{itemize}
            \item Gather a dataset with features and a binary target variable.
            \item Clean the data: handle missing values, encode categorical variables, and normalize numerical values.
            \item \textit{Example:} Dataset with customer ages, income levels, and purchase history.
        \end{itemize}

        \item \textbf{Choose the Model}
        \begin{itemize}
            \item Use libraries like \texttt{scikit-learn} in Python.
            \item \texttt{Code Example:}
            \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Steps to Implement Logistic Regression - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue from the previous frame
        \item \textbf{Split the Dataset}
        \begin{itemize}
            \item Divide the data into training and testing sets (e.g., 80\% training, 20\% testing).
            \item \texttt{Code Example:}
            \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.features, data.target, test_size=0.2, random_state=42)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Train the Model}
        \begin{itemize}
            \item Fit the logistic regression model to the training data.
            \item \texttt{Code Example:}
            \begin{lstlisting}[language=Python]
model.fit(X_train, y_train)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Make Predictions}
        \begin{itemize}
            \item Use the model to make predictions on test data.
            \item \texttt{Code Example:}
            \begin{lstlisting}[language=Python]
predictions = model.predict(X_test)
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Steps to Implement Logistic Regression - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{6} % Continue from the previous frame
        \item \textbf{Evaluate Model Performance}
        \begin{itemize}
            \item Use metrics like accuracy, precision, and recall.
            \item \texttt{Example Metric Calculation:}
            \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        - Output of logistic regression is a probability between 0 and 1.
        - Coefficients indicate the change in log-odds for a one-unit increase in predictor.
        - The logistic function formula: 
        \[
        P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \]
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Practical Example for Reinforcement}
    \begin{block}{Example Scenario}
        - Predict if a customer will buy a product based on age and income.
        - Model finds: Customers above 30 years old with over \$50,000 income have a 75\% purchase probability.
        - Insights can guide targeted marketing strategies.
    \end{block}

    \begin{block}{Engagement}
        - Explore logistic regression with datasets from platforms like Kaggle or UCI Machine Learning Repository.
        - Gain hands-on experience and deepen your understanding!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Regression Models - Overview}
    \begin{block}{Overview of Evaluation Metrics}
        When assessing the efficacy of regression models, it is crucial to utilize relevant evaluation metrics. These metrics provide insight into how well your model performs in predicting outcomes. Common metrics include \textbf{Accuracy}, \textbf{Precision}, and \textbf{Recall}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Regression Models - Accuracy}
    \begin{block}{1. Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
            \end{equation}
            \item \textbf{Example}: In a binary classification task with 70 correct predictions out of 100, the accuracy is \(70\%\).
            \item \textbf{Relevance}: Useful in balanced datasets where classes are evenly distributed.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Regression Models - Precision and Recall}
    \begin{block}{2. Precision}
        \begin{itemize}
            \item \textbf{Definition}: Precision quantifies the number of true positive predictions made out of all positive predictions.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \end{equation}
            \item \textbf{Example}: If 30 positive cases are identified, with 20 true positives, then precision is \( \frac{20}{30} = 66.67\%\).
            \item \textbf{Relevance}: High precision is critical when the cost of false positives is high, such as fraud detection.
        \end{itemize}
    \end{block}

    \begin{block}{3. Recall}
        \begin{itemize}
            \item \textbf{Definition}: Recall measures the ability of a model to identify all relevant instances (true positives).
            \item \textbf{Formula}:
            \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \end{equation}
            \item \textbf{Example}: If there are 50 actual positive cases and 40 are correctly identified, recall is \( \frac{40}{50} = 80\%\).
            \item \textbf{Relevance}: High recall is vital in contexts where missing positive cases is critical, such as medical diagnoses.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Regression Models - Summary and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Context Matters}: The choice of metrics depends on the problem context; trade-offs may exist.
            \item \textbf{Not Just Accuracy}: Particularly in imbalanced datasets, accuracy alone can be misleading. 
            \item \textbf{Holistic Approach}: Understanding the balance among metrics is essential for fitting specific applications.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding and utilizing these evaluation metrics is essential in developing robust regression models that perform well in real-world applications. Consider the implications of these metrics in your predictive modeling efforts!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression}
    \begin{block}{Overview}
        Regression techniques are powerful statistical tools used to analyze the relationship between a dependent variable and one or more independent variables. By modeling these relationships, we can predict outcomes, inform decisions, and optimize processes in various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Regression in Different Fields}
    \begin{enumerate}
        \item \textbf{Finance}
            \begin{itemize}
                \item \textbf{Credit Scoring}: Regression analysis assesses creditworthiness by predicting loan default probability based on borrower data.
                \item \textbf{Example}: Logistic regression can categorize borrowers as 'high risk' or 'low risk' using variables such as income and credit history.
            \end{itemize}

        \item \textbf{Healthcare}
            \begin{itemize}
                \item \textbf{Predicting Disease Outcomes}: Regression identifies risk factors associated with health outcomes and disease progression.
                \item \textbf{Example}: Multiple regression may determine how factors like age and diet affect diabetes incidence.
            \end{itemize}

        \item \textbf{Marketing}
            \begin{itemize}
                \item \textbf{Sales Forecasting}: Regression models predict sales based on marketing activities.
                \item \textbf{Example}: Linear regression analyzes how advertising spend influences sales revenue.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Formulas}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Regression techniques are versatile and applicable in numerous domains.
            \item They convert data into actionable insights for better decision-making.
            \item Choose the regression model based on the dependent variable: 
                \begin{itemize}
                    \item Linear regression for continuous outcomes.
                    \item Logistic regression for binary outcomes.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Example Formulas}
        \begin{equation}
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
        \end{equation}
        \begin{equation}
        P(y=1) = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n\right)}}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Understanding regression's applications across industries emphasizes its significance in data analysis. This knowledge encourages us to explore how we can implement regression techniques in our fields, such as enhancing healthcare, optimizing marketing strategies, or assessing financial risks for data-driven decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Linear vs Logistic Regression}
    Regression techniques are crucial in data analysis, enabling us to model relationships between variables. This case study examines the differences between linear regression and logistic regression through practical applications, emphasizing their strengths and limitations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Linear Regression}
    \begin{itemize}
        \item \textbf{Definition}: A statistical method to model the relationship between a dependent variable (Y) and one or more independent variables (X).
        \item \textbf{Equation}:
        \begin{equation}
            Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \varepsilon
        \end{equation}
        \begin{itemize}
            \item $Y$ = Predicted value
            \item $\beta_0$ = Y-intercept
            \item $\beta_n$ = Coefficients of independent variables
            \item $\varepsilon$ = Error term
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Logistic Regression}
    \begin{itemize}
        \item \textbf{Definition}: A regression type used when the dependent variable is categorical (often binary) to predict the probability of an event occurring.
        \item \textbf{Equation}:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \ldots + \beta_nX_n)}}
        \end{equation}
        \begin{itemize}
            \item $P(Y=1 | X)$ = Probability of the event occurring
            \item $e$ = Base of the natural logarithm
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study Example: Predicting Outcomes}
    \textbf{Scenario:} A healthcare organization wants to predict whether patients will develop diabetes based on age, weight, and physical activity level.

    \textbf{Application of Linear Regression:}
    \begin{itemize}
        \item \textbf{Use Case}: Predicting blood sugar levels (continuous outcome).
        \item \textbf{Interpretation}: 
        \begin{itemize}
            \item E.g., "For each additional year of age, blood sugar increases by 0.5 mg/dL."
        \end{itemize}
    \end{itemize}

    \textbf{Application of Logistic Regression:}
    \begin{itemize}
        \item \textbf{Use Case}: Predicting whether a patient will develop diabetes (binary outcome).
        \item \textbf{Interpretation}:
        \begin{itemize}
            \item E.g., "A patient with a probability score of 0.7 is more likely to develop diabetes."
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Nature of Outcomes}:
        \begin{itemize}
            \item Linear regression predicts continuous outputs.
            \item Logistic regression is for categorical outcomes.
        \end{itemize}
        \item \textbf{Interpretability}:
        \begin{itemize}
            \item Coefficients in linear regression offer direct averages.
            \item Probabilities in logistic regression must be translated from odds.
        \end{itemize}
        \item \textbf{Modeling Purpose}:
        \begin{itemize}
            \item Choose linear regression for predicting quantities.
            \item Choose logistic regression for predicting occurrences.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding both linear and logistic regression is essential as they cater to different types of predictive questions. This case study illustrates how to choose between these techniques based on the nature of the dependent variable, thus empowering informed decision-making in various contexts, particularly in healthcare.
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    Recent advancements in regression techniques illustrate the intersection of traditional statistical methods and cutting-edge artificial intelligence (AI) technologies. 
    This discussion highlights:
    \begin{itemize}
        \item Key developments in regression methods
        \item Integration of AI and its impact on predictive analytics
        \item Enhancement of decision-making in various fields
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Enhanced Interpretability}
    Modern machine learning models like Random Forests and Gradient Boosting Machines provide enhanced predictive power but can obscure interpretability. 
    Key methods improving interpretability include:
    \begin{itemize}
        \item \textbf{SHAP} (SHapley Additive exPlanations)
        \item \textbf{LIME} (Local Interpretable Model-agnostic Explanations)
    \end{itemize}
    
    \begin{block}{Example}
        In predicting house prices, SHAP values highlight how features (like square footage, number of bedrooms) contribute to the prediction.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Integration of Neural Networks}
    Neural networks have greatly influenced regression tasks, especially in high-dimensional data areas like image and speech processing. 
    Significant advancements include:
    \begin{itemize}
        \item \textbf{Transformers} for improved sequence modeling
        \item \textbf{U-Nets} for feature extraction in images
    \end{itemize}
    
    \begin{block}{Example}
        In healthcare, U-Nets are used for image segmentation to predict diseases, converting pixel data into quantifiable outputs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Use of Conformal Prediction}
    Conformal prediction offers frameworks for reliable predictive intervals instead of point estimates, quantifying uncertainty in predictions.
    \begin{itemize}
        \item Especially valuable in finance and healthcare
    \end{itemize}
    
    \begin{block}{Example}
        Rather than a point prediction of \$100 for a stock price, a conformal prediction might suggest a range from \$90 to \$110.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Incorporation of Causal Inference}
    Current trends in regression techniques focus on understanding causal relationships alongside predictions. 
    Prominent methods:
    \begin{itemize}
        \item \textbf{Propensity Score Matching}
        \item \textbf{Instrumental Variables}
    \end{itemize}
    
    \begin{block}{Example}
        Researchers often use causal regression methods when evaluating a new medication to accurately measure its effects, accounting for confounding variables.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points}
    \begin{itemize}
        \item \textbf{Combining Techniques:} Melding traditional statistical models with modern machine learning enhances performance and interpretability.
        \item \textbf{Focus on Interpretability:} Techniques like SHAP and LIME make complex models understandable.
        \item \textbf{Exploration of Causality:} Advancements in causal inference provide deeper insights beyond correlation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The integration of AI with regression techniques is evolving rapidly, uncovering innovative applications and methodologies. This development promises:
    \begin{itemize}
        \item Enhanced data analysis capabilities
        \item Improved decision-making across industries 
    \end{itemize}
    The future of regression techniques is compelling and essential.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions for Reflection}
    \begin{itemize}
        \item How might you use interpretability techniques like SHAP in your projects?
        \item In what domains do you see the greatest application of conformal prediction methods?
        \item What challenges do you foresee when integrating AI with traditional regression models in your work?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Regression Techniques}
    \begin{block}{Introduction}
        Regression techniques, while powerful, are prone to challenges that can impact their effectiveness. Recognizing these challenges is crucial for improving model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges}
    \begin{itemize}
        \item \textbf{Assumptions Violation}
        \item \textbf{Multicollinearity}
        \item \textbf{Overfitting}
        \item \textbf{Outliers}
        \item \textbf{Data Quality}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Assumptions Violation}
    \begin{itemize}
        \item \textbf{Description:} Foundations like linearity and homoscedasticity may be violated.
        \item \textbf{Example:} Non-linear relationship in housing price models.
        \item \textbf{Strategy:}
        \begin{itemize}
            \item Transform features (e.g., logarithmic).
            \item Use Generalized Additive Models (GAMs).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Multicollinearity}
    \begin{itemize}
        \item \textbf{Description:} High correlation among predictors leads to unstable coefficient estimates.
        \item \textbf{Example:} Correlated TV and radio advertising in marketing models skew results.
        \item \textbf{Strategy:}
        \begin{itemize}
            \item Use Variance Inflation Factor (VIF) to detect.
            \item Employ regularization (Ridge or Lasso).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Overfitting}
    \begin{itemize}
        \item \textbf{Description:} A model that fits training data well but performs poorly on new data.
        \item \textbf{Example:} Complex polynomial regressions overfitting training data.
        \item \textbf{Strategy:}
        \begin{itemize}
            \item Simplify models and reduce features.
            \item Use cross-validation for assessment.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Outliers}
    \begin{itemize}
        \item \textbf{Description:} Extreme values distort regression analysis.
        \item \textbf{Example:} Demand spikes due to promotions affecting sales predictions.
        \item \textbf{Strategy:}
        \begin{itemize}
            \item Identify using Z-scores or box plots.
            \item Use robust regression methods like RANSAC.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Data Quality}
    \begin{itemize}
        \item \textbf{Description:} Inaccurate or missing data leads to poor outcomes.
        \item \textbf{Example:} Missing vital sales data affects trend analysis.
        \item \textbf{Strategy:}
        \begin{itemize}
            \item Implement data cleansing techniques for missing values.
            \item Use visualization to detect inaccuracies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Always check that regression assumptions hold.
        \item Aim for the simplest model to prevent overfitting.
        \item Ensure high-quality, clean data for accurate predictions.
    \end{itemize}
    \begin{block}{Conclusion}
        Addressing challenges in regression enhances performance and fosters advanced analytical techniques in AI and machine learning contexts.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Summary of Key Points}
    
    \begin{enumerate}
        \item \textbf{Understanding Regression:} 
        \begin{itemize}
            \item Statistical analysis to model relationships between variables.
            \item Predict outcomes based on predictors.
        \end{itemize}
        
        \item \textbf{Types of Regression:} 
        \begin{itemize}
            \item \textbf{Linear Regression:} Models linear relationships (e.g., house prices).
            \item \textbf{Multiple Regression:} Uses multiple predictors (e.g., advertising spend).
            \item \textbf{Polynomial Regression:} Captures non-linear trends (e.g., growth trends).
            \item \textbf{Logistic Regression:} Used for binary outcomes (e.g., customer purchase decisions).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Challenges and Metrics}
    
    \begin{block}{Challenges in Regression}
        \begin{itemize}
            \item \textbf{Overfitting:} Complex models capturing noise.
            \begin{itemize}
                \item \textit{Solution:} Employ cross-validation and regularization.
            \end{itemize}
            \item \textbf{Underfitting:} Oversimplified models failing to capture data structure.
            \begin{itemize}
                \item \textit{Solution:} Increase model complexity or add relevant features.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Evaluation Metrics}
        \begin{itemize}
            \item Importance of Mean Squared Error (MSE) and R-squared.
            \item \textbf{MSE Formula:} 
            \begin{equation}
                MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
            \end{equation}
            \item R-squared indicates variance explained by the model.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Implications and Key Takeaways}

    \begin{block}{Implications for Machine Learning}
        \begin{itemize}
            \item \textbf{Versatility:} Foundational in diverse fields (economics, social sciences).
            \item \textbf{Guidance:} Offers insights into feature selection, model validation, and tuning.
            \item \textbf{Thought-Provoking Questions:}
            \begin{itemize}
                \item How can you improve your regression model's predictive power?
                \item What potential variables are currently overlooked?
                \item How does the choice of regression affect result interpretation?
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Mastering regression equips data scientists for real-world challenges.
            \item Collaboration of techniques fosters a deeper understanding of data.
            \item Be mindful of data limitations and ethical implications.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}