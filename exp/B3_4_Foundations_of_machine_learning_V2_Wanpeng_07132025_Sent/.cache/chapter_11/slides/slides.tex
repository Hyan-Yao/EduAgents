\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Clustering Methods]{Chapter 11: Clustering Methods}
\author{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
}

\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Clustering Methods}
    \begin{block}{Overview of Clustering Techniques}
        Clustering is a fundamental unsupervised machine learning technique used to group a set of objects in such a way that:
        \begin{itemize}
            \item Objects in the same group (or cluster) are more similar to each other than to those in other groups.
            \item This process helps in understanding patterns, categorizing data, and reducing complexity.
        \end{itemize}        
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relevance of Clustering in Machine Learning}
    Clustering techniques are highly relevant in various fields:
    \begin{itemize}
        \item \textbf{Exploratory Data Analysis:} Helps visualize data by discovering inherent structures.
        \item \textbf{Customer Segmentation:} Identifies distinct customer segments for targeted marketing strategies.
        \item \textbf{Image Segmentation:} Aids in segmenting images into recognizable parts for further analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Clustering Techniques}
    \begin{enumerate}
        \item \textbf{K-Means Clustering}
        \begin{itemize}
            \item \textit{How it Works:} Partitions the dataset into K clusters by minimizing variance within each cluster.
            \item \textit{Example:} Grouping different types of plants based on features such as height and leaf size.
        \end{itemize}

        \item \textbf{Hierarchical Clustering}
        \begin{itemize}
            \item \textit{How it Works:} Builds a tree of clusters by merging or splitting based on distance.
            \item \textit{Example:} Organizing academic disciplines based on research topics and interconnections.
        \end{itemize}

        \item \textbf{DBSCAN (Density-Based Spatial Clustering of Applications with Noise)}
        \begin{itemize}
            \item \textit{How it Works:} Identifies clusters based on the density of data points, discovering clusters of varied shapes.
            \item \textit{Example:} Identifying geographical locations with high concentrations of customers or events.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Clustering?}
    Clustering is a method of unsupervised learning that involves grouping a set of objects such that objects in the same group (or cluster) are more similar to each other than to those in other groups. This similarity can be identified based on various criteria, such as distance, density, or connection. Clustering helps in discovering inherent patterns within data and is widely used in various fields.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Clustering}
    \begin{itemize}
        \item \textbf{Unsupervised Learning:} Clustering is a form of unsupervised learning as it does not rely on labeled data but identifies natural groupings in a dataset.
        \item \textbf{Groups (Clusters):} Each cluster comprises data points that share common characteristics or features.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering}
    \begin{enumerate}
        \item \textbf{Market Segmentation:}
            \begin{itemize}
                \item Businesses use clustering to identify different customer segments based on purchasing behavior, facilitating targeted marketing strategies.
                \item \textit{Example:} Grouping customers by shopping habits for personalized marketing campaigns.
            \end{itemize}
        \item \textbf{Image Segmentation:}
            \begin{itemize}
                \item Clustering techniques like K-means separate image regions based on pixel intensity or color.
                \item \textit{Example:} Clustering neighboring pixels for object recognition in images.
            \end{itemize}
        \item \textbf{Social Network Analysis:}
            \begin{itemize}
                \item Clustering identifies communities within social networks.
                \item \textit{Example:} Algorithms suggest friends or groups based on users' connections.
            \end{itemize}
        \item \textbf{Anomaly Detection:}
            \begin{itemize}
                \item Clustering helps identify rare data points that do not fit well in any cluster.
                \item \textit{Example:} In fraud detection, unusual transactions can be flagged if they do not match typical spending patterns.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering - Overview}
    \begin{block}{Overview}
        Clustering groups data points based on similarities and can be classified into:
        \begin{itemize}
            \item \textbf{Hierarchical Clustering}
            \item \textbf{Non-Hierarchical Clustering}
        \end{itemize}
        Understanding these methods is crucial for effective data analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering - Hierarchical}
    \begin{block}{1. Hierarchical Clustering}
        \begin{itemize}
            \item \textbf{Definition}: Builds a tree of clusters for visualization.
            \item \textbf{Types}:
                \begin{itemize}
                    \item \textbf{Agglomerative (Bottom-Up)}: Starts with individual points and merges them.
                    \item \textbf{Divisive (Top-Down)}: Starts with all data in one cluster and splits recursively.
                \end{itemize}
            \item \textbf{Example}: Animals grouped progressively from individual types to broader categories (cats, dogs, rabbits).
            \item \textbf{Key Point}: Represented as a \textbf{dendrogram}.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Clustering - Non-Hierarchical}
    \begin{block}{2. Non-Hierarchical Clustering}
        \begin{itemize}
            \item \textbf{Definition}: Divides data into distinct clusters without hierarchy; commonly known as partitional clustering.
            \item \textbf{Method}: \textbf{k-Means Clustering}
            \item \textbf{Example}: Randomly selects centroids and assigns nearby data points to clusters.
            \item \textbf{Key Point}: Generally faster than hierarchical methods, but requires prior knowledge of the number of clusters (k).
        \end{itemize}
    \end{block}

    \begin{block}{Code Snippet Example (k-Means)}
        \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans

# Sample data points (features)
data = [[1, 2], [1, 4], [1, 0], 
        [4, 2], [4, 0], [4, 4]]

# Initialize the k-means model with 2 clusters
kmeans = KMeans(n_clusters=2)

# Fit model on the data
kmeans.fit(data)

# Get cluster labels
labels = kmeans.labels_
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to k-Means Clustering - Definition}
    \begin{itemize}
        \item k-Means Clustering is a popular unsupervised machine learning algorithm.
        \item It partitions a dataset into distinct groups (clusters) based on feature similarities.
        \item The algorithm assumes a predetermined number of clusters (k).
        \item It iteratively refines the clustering until the optimal configuration is found.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to k-Means Clustering - How It Works}
    \begin{enumerate}
        \item \textbf{Initialization}:
            \begin{itemize}
                \item Randomly select k initial centroids from the data points.
            \end{itemize}
        \item \textbf{Assignment Step}:
            \begin{itemize}
                \item Each data point is assigned to the nearest centroid, forming k clusters.
            \end{itemize}
        \item \textbf{Update Step}:
            \begin{itemize}
                \item Calculate new centroids as the mean of all points in each cluster.
                \item Repeat until centroids change insignificantly or a maximum number of iterations is reached.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to k-Means Clustering - Example and Key Points}
    \textbf{Example}: Consider a dataset of fruits characterized by weight and sweetness level.
    \begin{itemize}
        \item \textbf{Data Points}:
            \begin{itemize}
                \item Apple (150g, 8)
                \item Banana (120g, 7)
                \item Cherry (50g, 9)
            \end{itemize}
        \item Assuming \( k = 2 \):
            \begin{itemize}
                \item Initial centroids might be Apple and Cherry.
                \item Apples and Bananas may cluster together; Cherries would form a second cluster.
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Scalability}: Efficient with large datasets.
            \item \textbf{Simplicity}: Easily implemented; ideal for beginners.
            \item \textbf{Limitations}: The choice of k influences results and can struggle with varied cluster shapes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How k-Means Works - Overview}
    \begin{block}{Overview of the k-Means Algorithm}
        k-Means is a popular clustering algorithm used to partition a dataset into K distinct, non-overlapping groups (clusters) based on feature similarity. 
        Its simplicity and efficiency make it widely used in various applications, from market segmentation to image compression.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How k-Means Works - Step-by-Step}
    \begin{enumerate}
        \item \textbf{Initialization}
        \begin{itemize}
            \item \textbf{Objective}: Choose K initial cluster centroids.
            \item \textbf{Methods}:
            \begin{itemize}
                \item Randomly select K data points from the dataset.
                \item Use smarter techniques like K-means++ for better centroid spread.
            \end{itemize}
            \item \textbf{Example}: With customer ages, select initial centroids like 22, 35, 48.
        \end{itemize}

        \item \textbf{Assignment Step}
        \begin{itemize}
            \item \textbf{Objective}: Assign each data point to the nearest cluster centroid.
            \item \textbf{Distance Formula}:
            \begin{equation}
                d(x_i, C_j) = \sqrt{\sum_{k=1}^{n}(x_{ik} - C_{jk})^2}
            \end{equation}
            \item \textbf{Action}: Assign \(x_i\) to cluster \(j\) with minimum distance.
            \item \textbf{Example}: For a customer age of 30, assign to the closest centroid, e.g., 35.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How k-Means Works - Continued}
    \begin{enumerate}[resume]
        \item \textbf{Update Step}
        \begin{itemize}
            \item \textbf{Objective}: Recalculate centroids based on current assignments.
            \item \textbf{New Centroid Formula}:
            \begin{equation}
                C_j = \frac{1}{N_j} \sum_{x_i \in Cluster_j} x_i
            \end{equation}
            where \(N_j\) is the count of points in cluster \(j\).
            \item \textbf{Action}: Shift centroid to new position.
            \item \textbf{Example}: Points [22, 25, 27] give new centroid \((22 + 25 + 27) / 3 = 24.67\).
        \end{itemize}

        \item \textbf{Iterations}
        \begin{itemize}
            \item Repeat assignment and update until:
            \begin{itemize}
                \item Centroids stabilize or }
                \item Maximum iterations reached.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right k - Introduction}
    When using k-means clustering, selecting the optimal number of clusters (k) is crucial for accurate data representation. 
    \begin{itemize}
        \item If k is too small, distinct groups may be merged.
        \item If k is too large, noise may be emphasized.
    \end{itemize}
    We will explore two methods to determine the right k:
    \begin{itemize}
        \item The Elbow Method
        \item The Silhouette Score
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right k - The Elbow Method}
    \textbf{Concept:}
    The Elbow Method helps visualize the trade-off between the number of clusters and the variability explained by the clusters (inertia). 
    \begin{itemize}
        \item As k increases, inertia decreases.
        \item The “elbow” point marks a balance between complexity and performance.
    \end{itemize}

    \textbf{Steps:}
    \begin{enumerate}
        \item Calculate the inertia for a range of k values (e.g., k = 1 to 10).
        \item Plot k vs. inertia on a graph.
        \item Locate the elbow point.
    \end{enumerate}
    
    \textbf{Example:}
    If inertia decreases until k=4 and then tapers off, k=4 is the optimal choice.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right k - The Silhouette Score}
    \textbf{Concept:}
    The Silhouette Score measures how similar an object is to its own cluster compared to other clusters.
    \begin{itemize}
        \item Scores range from -1 to +1:
        \begin{itemize}
            \item +1: Samples are well clustered.
            \item 0: Overlapping clusters.
            \item -1: Samples may be in the wrong cluster.
        \end{itemize}
    \end{itemize}

    \textbf{Steps:}
    \begin{enumerate}
        \item Calculate the average Silhouette Score for a range of k values.
        \item Select the k with the highest average score.
    \end{enumerate}
    
    \textbf{Example:}
    For k options from 2 to 10, if k=3 shows a score of 0.7 and others are below 0.5, k=3 is likely best.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right k - Key Points}
    \begin{itemize}
        \item \textbf{Balance:} Selecting k is about balancing model complexity and generalization.
        \item \textbf{Visualization:} Use graphs to provide an intuitive feel for cluster quality.
        \item \textbf{Iterative Approach:} It may require trying multiple methods to validate k.
    \end{itemize}

    \textbf{Conclusion:}
    Choosing the right number of clusters is essential in k-means clustering. Using methods like the Elbow Method and Silhouette Score provides systematic ways to determine the optimal k, enhancing the clarity and efficacy of our clustering objectives.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of k-Means Clustering}
    
    k-Means clustering is widely used in unsupervised machine learning to group similar data points together. It helps in identifying patterns and extracting valuable insights from data.
    
    \begin{itemize}
        \item Simple and easy to implement
        \item Scalable to large datasets
        \item Flexible with various data types
        \item Easy interpretability of clusters
        \item Fast convergence for real-time applications
        \item Best suited for spherical clusters
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simplicity and Ease of Implementation}
    
    \begin{block}{Overview}
        k-Means is straightforward for beginners in data analysis, consisting of:
        \begin{itemize}
            \item Initializing centroids
            \item Assigning clusters to data points
            \item Updating centroids
        \end{itemize}
    \end{block}
    
    \begin{exampleblock}{Example}
        A marketing team segments customers into categories like 'frequent buyers' and 'occasional shoppers' using k-Means.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Scalability and Flexibility}
    
    \begin{block}{Scalability}
        k-Means handles large datasets efficiently with linear complexity, O(n * k * i):
        \begin{itemize}
            \item $n$ = number of data points
            \item $k$ = number of clusters
            \item $i$ = number of iterations
        \end{itemize}
        This allows k-Means to process millions of entries quickly.
    \end{block}
    
    \begin{block}{Flexibility}
        k-Means can work with various data types:
        \begin{itemize}
            \item Numerical data directly
            \item Categorical data via techniques like one-hot encoding
        \end{itemize}
        It is applicable in fields such as finance and biology.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-Means Clustering - Introduction}
    \begin{block}{Introduction}
        K-means clustering is a popular method for partitioning data into distinct groups (clusters).
        While it has several advantages, it also presents various limitations that can affect its effectiveness and applicability. 
        Understanding these limitations is crucial for selecting the right clustering method for your data analysis needs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-Means Clustering - Part 1}
    \begin{enumerate}
        \item \textbf{Sensitivity to Initial Conditions}  
        \begin{itemize}
            \item K-means is highly sensitive to the initial placement of centroids.
            \item Different initializations can lead to different clustering results.
            \item \textbf{Example:} Running k-means several times with different starting points may yield significantly different results.
        \end{itemize}
        
        \item \textbf{Fixed Number of Clusters (k)}
        \begin{itemize}
            \item The requirement to specify the number of clusters (k) beforehand can be challenging.
            \item Poor choice of k can lead to inaccurate representation of data structure.
            \item \textbf{Illustration:} Analyzing a dataset with 4 clusters but choosing k=3 may lose crucial data characteristics.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-Means Clustering - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Assumption of Spherical Clusters}  
        \begin{itemize}
            \item K-means assumes clusters are spherical and of similar sizes.
            \item This limits effectiveness for elongated or irregularly shaped distributions.
            \item \textbf{Example:} K-means struggles with two intertwined spirals, failing to create meaningful separation.
        \end{itemize}

        \item \textbf{Sensitivity to Outliers}  
        \begin{itemize}
            \item Outliers can disproportionately influence k-means results, skewing centroids.
            \item \textbf{Example:} An outlier far from a concentrated cluster will pull the centroid closer, misrepresenting the cluster.
        \end{itemize}

        \item \textbf{Need for Feature Scaling}  
        \begin{itemize}
            \item K-means assumes equal contribution of features to the distance calculation.
            \item Features with larger ranges can dominate, distorting the clustering.
            \item \textbf{Illustration:} Features in thousands (e.g., income) may overshadow features in units (e.g., age).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of k-Means Clustering - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{5} % Continue numbering from previous frame
        \item \textbf{Iterative Nature and Computation Time}  
        \begin{itemize}
            \item The algorithm requires several iterations to converge.
            \item This can be time-consuming for large datasets.
            \item \textbf{Key Point:} Performance issues may arise with large datasets, making k-means less efficient compared to faster models.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Conclusion}
        While k-means clustering can be a powerful tool for data segmentation, it's essential to be aware of its limitations. 
        Practitioners should consider these challenges when assessing whether k-means is the right choice for their data analysis tasks. 
        Always explore and compare different clustering methods and validate your results.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering Overview - Introduction}
    \begin{block}{Overview}
        Hierarchical clustering is a powerful technique in data analysis that groups data points into clusters based on similarity.
    \end{block}
    \begin{itemize}
        \item Builds a hierarchy of clusters visualized as a dendrogram.
        \item Unlike fixed cluster methods, it illustrates how clusters relate.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Key Concepts}
    \begin{itemize}
        \item \textbf{What is Hierarchical Clustering?}
            \begin{itemize}
                \item Creates a hierarchy of clusters through merging or splitting.
            \end{itemize}
        \item \textbf{Types of Hierarchical Clustering:}
            \begin{itemize}
                \item \textbf{Agglomerative Clustering:} 
                    \begin{itemize}
                        \item Starts with each data point as a single cluster.
                        \item Merges closest pairs iteratively.
                    \end{itemize}
                \item \textbf{Divisive Clustering:} 
                    \begin{itemize}
                        \item Begins with one cluster containing all data points.
                        \item Recursively splits into smaller clusters.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Examples and Key Points}
    \begin{block}{Examples}
        \begin{itemize}
            \item \textbf{Agglomerative Example:} 
                \begin{itemize}
                    \item Start with points A, B, C, D, E, merge iteratively.
                \end{itemize}
            \item \textbf{Divisive Example:} 
                \begin{itemize}
                    \item Begin with all points and split to separate clusters.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize:}
            \begin{itemize}
                \item Useful for data too complex for fixed cluster methods.
                \item Dendrogram provides visual representation of clustering.
                \item Applicable in various fields (bioinformatics, segmentation).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Hierarchical Clustering - Overview}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Agglomerative clustering is a bottom-up approach to clustering.
            \item It merges smaller clusters into larger ones based on similarity.
            \item Uses distance metrics to determine cluster proximity.
            \item Versatile with different linkage criteria.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Hierarchical Clustering - Process}
    \begin{enumerate}
        \item \textbf{Initialization:} Each data point is a distinct cluster.
        \item \textbf{Cluster Merging:} 
        \begin{itemize}
            \item Compute distances between all cluster pairs.
            \item Merge the two closest clusters.
        \end{itemize}
        \item \textbf{Repetition:} Repeat until one cluster remains or a specific count is reached.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agglomerative Hierarchical Clustering - Example}
    \begin{block}{Example Data Points}
        \begin{itemize}
            \item A (1, 2)
            \item B (2, 3)
            \item C (5, 6)
            \item D (8, 8)
            \item E (9, 10)
        \end{itemize}
    \end{block}
    \begin{block}{Steps to Clustering}
        \begin{enumerate}
            \item Start with clusters: \{A\}, \{B\}, \{C\}, \{D\}, \{E\}.
            \item Merge closest clusters, e.g., \{A\} and \{B\}.
            \item Update distances and repeat merges until all points are clustered.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Divisive Hierarchical Clustering - Overview}
    \begin{itemize}
        \item Divisive Hierarchical Clustering is a \textbf{top-down approach} for classifying data into distinct groups (clusters).
        \item Unlike agglomerative clustering, which builds clusters from individual points, divisive clustering starts with a \textbf{single large cluster} containing all data.
        \item The method involves \textbf{recursively splitting} that single cluster into smaller clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Divisive Hierarchical Clustering - Key Concepts}
    \begin{enumerate}
        \item \textbf{Top-Down Approach}
            \begin{itemize}
                \item Begins with one cluster containing all data points.
                \item Iteratively divides clusters based on criteria like dissimilarity or distance.
            \end{itemize}
        \item \textbf{Splitting Process}
            \begin{itemize}
                \item A selected cluster is divided into two or more clusters.
                \item Continued until a stopping criterion is met (e.g., desired number of clusters).
            \end{itemize}
        \item \textbf{Distance Measures}
            \begin{itemize}
                \item Various metrics can be used such as:
                    \begin{itemize}
                        \item Euclidean distance
                        \item Manhattan distance
                        \item Cosine similarity
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Divisive Hierarchical Clustering - Example and Key Points}
    \begin{block}{Example}
        \begin{itemize}
            \item Dataset: Various fruits characterized by features (color, size, weight)
            \item Start: All fruits in one cluster
            \item First Split: Small fruits (berries) and large fruits (apples)
            \item Subsequent Splits: Grouping small fruits (blueberries vs. strawberries), larger fruits (citrus vs. non-citrus)
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Hierarchical structure helps visualize relationships.
            \item Allows exploration of complex data relationships.
            \item Useful in fields like biology, marketing, and image processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Divisive Hierarchical Clustering - Conclusion}
    \begin{itemize}
        \item Divisive hierarchical clustering uncovers underlying patterns in data, progressing from broad understanding to specific insights.
        \item It provides a flexible intuition for clustering strategies, adapting to dataset characteristics.
        \item Understanding this method aids in tackling real-world problems involving complex data structuring.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrograms in Hierarchical Clustering}
    A **dendrogram** is a tree-like diagram that visually represents the arrangement and relationships of clusters formed during the hierarchical clustering process.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Dendrogram?}
    \begin{block}{Definition}
        A dendrogram shows how data points are grouped together at various levels of similarity or dissimilarity.
    \end{block}
    
    \begin{itemize}
        \item **Hierarchical Structure**: Depicts a hierarchy of clusters.
        \item **Height Interpretation**: Indicates level of dissimilarity between clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Dendrograms?}
    \begin{itemize}
        \item **Visual Clarity**: Intuitive visualization of relationships between data points and clusters.
        \item **Determining Number of Clusters**: "Cut" the dendrogram to obtain distinct clusters based on similarity levels.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Dendrogram}
    Consider a dataset with animals based on various features (e.g., size, habitat, diet).

    \begin{enumerate}
        \item **Data Points**: Animals: {Dog, Cat, Lion, Shark, Dolphin}
        \item **Dendrogram Representation**: 
            \begin{itemize}
                \item Start with all species as individual clusters.
                \item Merging visualized in the dendrogram:
                \begin{itemize}
                    \item (Dog, Cat) merged at a low height.
                    \item (Shark, Dolphin) merged at a higher point.
                \end{itemize}
            \end{itemize}
    \end{enumerate}
    \begin{block}{Visual Depiction}
        Imagine a tree where 'Dog' and 'Cat' meet at a lower branch indicating closeness, while 'Shark' and 'Dolphin' meet higher, signifying a broader category.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item **Interpretability**: Dendrograms provide a straightforward way to interpret clustering results.
        \item **Flexibility**: Different distance calculation methods affect the dendrogram's shape.
        \item **Use Cases**: Commonly used in biology, marketing, and data mining.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Dendrograms serve as a powerful tool in hierarchical clustering, transforming complex relationships into an easily understandable format. 

    Whether exploring animal taxonomy or segmenting customers, understanding dendrograms is vital for effective data interpretation.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Hierarchical Clustering - Overview}
    \begin{block}{Overview}
        Hierarchical clustering is a powerful method used to group similar data points into clusters based on characteristics. 
        It provides detailed insights into data structure and is particularly appreciated for its intuitive approach.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Hierarchical Clustering - Key Benefits}
    \begin{enumerate}
        \item \textbf{No Predefined Number of Clusters} 
        \begin{itemize}
            \item Does not require specifying the number of clusters in advance.
            \item Adaptable to datasets where the number of clusters is unknown.
            \item \textit{Example:} Grouping animal species based on traits.
        \end{itemize}
        
        \item \textbf{Visual Representation with Dendrograms}
        \begin{itemize}
            \item Dendrograms provide a step-by-step visual summary of the clustering process.
            \item Helps determine the appropriate number of clusters.
            \item \textit{Illustration:} Visualizing species mating behavior.
        \end{itemize}
        
        \item \textbf{Ability to Capture Nested Structures}
        \begin{itemize}
            \item Identifies sub-clusters within clusters.
            \item \textit{Example:} Market segmentation reveals distinct customer segments.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Hierarchical Clustering - Continued}
    \begin{enumerate}
        \setcounter{enumi}{3} % set counter to continue from previous frame
        \item \textbf{Flexibility in Distance Metrics}
        \begin{itemize}
            \item Various distance metrics can be used (e.g., Euclidean, Manhattan).
            \item \textit{Example:} Using Haversine distance for geographical clustering.
        \end{itemize}

        \item \textbf{Handling Different Types of Data}
        \begin{itemize}
            \item Applicable to both numerical and categorical data.
            \item \textit{Illustration:} Clustering customer data by age and purchasing categories.
        \end{itemize}

        \item \textbf{No Assumption of Cluster Shape}
        \begin{itemize}
            \item Does not assume clusters are spherical or of similar size, useful for real-world data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Situations Where Hierarchical Clustering is Preferred}
    \begin{itemize}
        \item \textbf{Exploratory Data Analysis}
        \begin{itemize}
            \item Useful for identifying natural groupings without assumptions.
        \end{itemize}
        \item \textbf{Small to Medium-sized Datasets}
        \begin{itemize}
            \item Ideal for smaller datasets due to computational efficiency.
        \end{itemize}
        \item \textbf{Detailed Analysis Required}
        \begin{itemize}
            \item Provides valuable insights through visual interpretations.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Hierarchical clustering is a robust technique suitable for various applications, emphasizing:
    \begin{itemize}
        \item Flexibility
        \item Visual interpretation
        \item Capturing complex data relationships
    \end{itemize}
    By understanding these benefits, hierarchical clustering can be effectively leveraged in exploratory analyses and beyond.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Hierarchical Clustering - Overview}
    Hierarchical clustering is a method for grouping similar data points based on their features.
    
    However, it presents numerous challenges and limitations that can affect its performance, particularly with large datasets.
    
    In this section, we'll explore the primary limitations of hierarchical clustering.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Hierarchical Clustering - Key Limitations}
    \begin{enumerate}
        \item \textbf{Scalability Issues}
            \begin{itemize}
                \item The agglomerative approach requires computation of distances between all pairs of data points.
                \item \textbf{Example:} For a dataset with \(N\) points, the time complexity is O(\(N^2\)), making it inefficient for large datasets.
                \item Calculating pairwise distances for 1,000 data points involves nearly 499,500 calculations.
            \end{itemize}
        
        \item \textbf{Memory Consumption}
            \begin{itemize}
                \item Storing all pairwise distances can lead to high memory usage, limiting feasibility for very large datasets.
            \end{itemize}
        
        \item \textbf{Sensitivity to Noise and Outliers}
            \begin{itemize}
                \item Hierarchical clustering is affected by noise and outliers, which can distort data structure.
                \item \textbf{Illustration:} An outlier can be misclassified alongside similar points, yielding misleading results.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Hierarchical Clustering - More Key Limitations}
    \begin{enumerate}[resume]
        \item \textbf{Lacks a Clear Objective Function}
            \begin{itemize}
                \item Unlike k-means, hierarchical clustering lacks a clear optimization objective, making cluster selection subjective.
            \end{itemize}
        
        \item \textbf{Dendrogram Interpretation}
            \begin{itemize}
                \item Dendrograms visually represent clustering processes, but interpretation can be complex.
                \item \textbf{Key Point:} An incorrectly chosen cut can lead to poor clustering assignments.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Summary}
        Hierarchical clustering is effective for exploratory data analysis but limited by scalability, sensitivity to outliers, and high memory demands.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Hierarchical Clustering - Key Takeaway}
    \begin{itemize}
        \item When considering hierarchical clustering, evaluate the dataset's size, noise presence, and analysis goals.
        \item Exploring alternative methods, such as k-means or DBSCAN, may be more suitable for certain applications.
        
        \item By understanding these limitations, we can better appreciate when hierarchical clustering is advantageous and when to consider other approaches.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Overview}
    \begin{block}{Overview}
        Clustering is a powerful statistical and machine learning technique used across multiple industries to find patterns and group similar data points. 
        By organizing data into manageable clusters, companies can derive insights that lead to better decision-making.
    \end{block}
    \begin{block}{Key Industries Utilizing Clustering}
        \begin{itemize}
            \item Healthcare
            \item Marketing
            \item Finance
            \item Social Media
            \item Telecommunications
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Key Industries}
    \begin{itemize}
        \item \textbf{Healthcare}
            \begin{itemize}
                \item Patient Segmentation 
                \item Drug Discovery
            \end{itemize}
        \item \textbf{Marketing}
            \begin{itemize}
                \item Customer Segmentation 
                \item Targeted Advertising
            \end{itemize}
        \item \textbf{Finance}
            \begin{itemize}
                \item Risk Assessment 
                \item Fraud Detection
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Clustering - Examples and Impact}
    \begin{itemize}
        \item \textbf{Social Media}
            \begin{itemize}
                \item Community Detection 
                \item Influencer Identification
            \end{itemize}
        \item \textbf{Telecommunications}
            \begin{itemize}
                \item Network Optimization 
                \item Churn Prediction
            \end{itemize}
    \end{itemize}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Clustering reveals hidden patterns and trends in data.
            \item Applications enhance efficiency and decision-making across multiple sectors.
            \item Understanding specific clusters allows organizations to optimize strategies and improve customer satisfaction.
        \end{itemize}
    \end{block}
    \begin{block}{Closing Thought}
        Consider how you can apply clustering concepts to identify opportunities in your field of interest.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 1}

  \textbf{Clustering Methods Recap:}

  Clustering methods are pivotal in data analysis, especially when dealing with complex datasets that have no predefined labels. These methods allow us to group similar data points together, uncovering patterns and structures that might not be immediately apparent. 

  \begin{block}{Key Concepts}
      \begin{enumerate}
          \item \textbf{Definition of Clustering:}
          Clustering is an unsupervised learning technique that focuses on grouping a set of objects (data points) so that objects in the same group (cluster) are more similar to each other than to those in other groups.

          \item \textbf{Common Clustering Algorithms:}
          \begin{itemize}
              \item \textbf{K-Means Clustering:} Partitions data into K clusters by minimizing variance, ideal for large datasets with well-defined spherical shapes.
              \item \textbf{Hierarchical Clustering:} Builds a hierarchy of clusters through either agglomerative (bottom-up) or divisive (top-down) approaches.
              \item \textbf{DBSCAN:} Groups dense areas of data points and identifies outliers, effective for data with noise and varying density.
          \end{itemize}
      \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 2}
  
  \begin{block}{Importance in Data Analysis}
      \begin{itemize}
          \item \textbf{Pattern Recognition:} Helps in identifying natural groupings, such as customer segments in marketing or anomalies in fraud detection.
          \item \textbf{Data Summarization:} Provides a simplified representation of large datasets, making them more manageable.
          \item \textbf{Feature Engineering:} Clusters can serve as additional features in predictive modeling, thus improving model accuracy.
      \end{itemize}
  \end{block}

  \begin{block}{Real-World Examples}
      \begin{itemize}
          \item In \textbf{E-commerce}, clustering can analyze customer behaviors for targeted marketing strategies.
          \item In \textbf{Healthcare}, it identifies patient subgroups for personalized treatments.
          \item In \textbf{Social Networks}, it discovers communities and user behavior dynamics.
      \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 3}

  \textbf{Key Takeaways:}
  \begin{itemize}
      \item Clustering is a foundational tool in data science for exploring and understanding complex data.
      \item It supports decision-making in various fields by providing insights into data structure and relationships.
      \item Understanding different clustering techniques enables effective data analysis.
  \end{itemize}

  \textbf{Questions to Consider:}
  \begin{itemize}
      \item How can we use clustering to improve customer experiences in service industries?
      \item In what ways might different clustering algorithms yield distinct results on the same dataset? 
  \end{itemize}

  By using clustering methods effectively, we harness the power of our data to make informed decisions and drive innovation across sectors.
\end{frame}


\end{document}