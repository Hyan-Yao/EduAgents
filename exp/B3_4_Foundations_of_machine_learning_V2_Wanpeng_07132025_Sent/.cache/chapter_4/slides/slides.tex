\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to AI and Data Quality - Overview}
    \begin{block}{Overview}
        In this section, we will explore the relationship between Artificial Intelligence (AI), particularly supervised learning, and the crucial role of data quality in achieving effective AI applications. Understanding how these components work together is vital for anyone looking to leverage AI technologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to AI and Data Quality - Key Concepts}
    \begin{itemize}
        \item \textbf{Artificial Intelligence (AI):}
            \begin{itemize}
                \item AI simulates human intelligence in machines.
                \item Tasks include visual perception, speech recognition, decision-making, and language translation.
            \end{itemize}
        
        \item \textbf{Supervised Learning:}
            \begin{itemize}
                \item A machine learning approach using labeled data.
                \item Models learn to associate inputs with correct outputs.
                \item Example: Email spam detection.
            \end{itemize}
        
        \item \textbf{Importance of Data Quality:}
            \begin{itemize}
                \item Directly impacts AI model performance and reliability.
                \item \textbf{Key Dimensions of Data Quality:}
                    \begin{itemize}
                        \item \textbf{Accuracy}: Correctness of data values.
                        \item \textbf{Completeness}: Presence of all required data.
                        \item \textbf{Consistency}: Uniformity across datasets.
                        \item \textbf{Timeliness}: Up-to-date data relevance.
                        \item \textbf{Relevance}: Pertinence of data to questions asked.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to AI and Data Quality - Examples and Takeaways}
    \begin{block}{Examples to Consider}
        \textbf{Scenario:} Building an AI model to predict housing prices.
        \begin{itemize}
            \item \textbf{Training Data:} Inaccurate or incomplete information about past home sales leads to miscalculations.
            \item \textbf{Outcome:} High-quality data yields better predictions; poor quality results in flawed outcomes.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Understand supervised learning and its reliance on quality data.
            \item Recognizing data quality's importance is essential for effective AI applications.
            \item Focus on improving data quality leads to robust, accurate, and valuable AI solutions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Definition}
    \begin{block}{Definition}
        Supervised learning is a type of machine learning where a model is trained on a labeled dataset. This means that:
        \begin{itemize}
            \item For each training example, input data is paired with the correct output.
            \item The objective is for the algorithm to learn the relationship between input and output to predict outcomes for new, unseen data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Role}
    \begin{block}{Role in Machine Learning}
        Supervised learning plays a crucial role in automating decision-making across various domains. Examples include:
        \begin{itemize}
            \item Spam detection
            \item Image recognition
            \item Medical diagnosis
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Key Concepts}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Labeled Data:}
            \begin{itemize}
                \item Requires a dataset with input-output pairs (e.g., features vs. target).
                \item \textit{Example:} Predicting email spam with features like subject line and body text, labeled as "spam" or "not spam".
            \end{itemize}
        
            \item \textbf{Training Phase:}
            \begin{itemize}
                \item The model learns from labeled data, adjusting parameters to minimize errors.
                \item \textit{Illustration:} Teaching a child with labeled pictures to recognize different animals.
            \end{itemize}
        
            \item \textbf{Testing Phase:}
            \begin{itemize}
                \item The model is evaluated on a separate test set to determine prediction accuracy.
                \item \textit{Example:} Testing a spam detection model on unseen emails.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Importance of Data Quality:} The effectiveness of supervised learning is highly dependent on the quality of labeled data.
            \item \textbf{Types of Problems:}
            \begin{itemize}
                \item Classification: Output is a category (e.g., "spam" vs. "not spam").
                \item Regression: Output is a continuous value (e.g., house prices).
            \end{itemize}
            \item \textbf{Real-World Applications:}
            \begin{itemize}
                \item Credit scoring systems
                \item Medical diagnosis models based on patient data
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Supervised Learning - Summary}
    \begin{block}{Summary}
        Supervised learning is a fundamental approach in machine learning relying on labeled data to train models. Its applications range from:
        \begin{itemize}
            \item Everyday technology like email filters
            \item Complex systems in healthcare
        \end{itemize}
        Understanding the concepts and importance of data quality is crucial for successful AI development.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Supervised Learning}
    \begin{block}{Introduction}
        Supervised learning is a type of machine learning using labeled data to train models, predicting outcomes for new data. We will cover:
        \begin{itemize}
            \item Decision Trees
            \item k-Nearest Neighbors (k-NN)
            \item Support Vector Machines (SVM)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Decision Trees}
    \begin{block}{Concept}
        Decision Trees represent decisions by branching flowchart-like structures. Each node tests an attribute, branches show outcomes, and leaf nodes represent class labels.
    \end{block}
    
    \begin{block}{Example}
        \begin{itemize}
            \item Root node asks, "Is the weather sunny?"
            \item Branches lead to questions like, "Is it a weekend?"
            \item Leaves indicate decisions (e.g., go to the beach or stay home).
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Pros: Easy to interpret, good for numerical and categorical data.
            \item Cons: Prone to overfitting, can become complex.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. k-Nearest Neighbors (k-NN)}
    \begin{block}{Concept}
        k-NN classifies points by examining the 'k' closest labeled neighbors in the feature space, predicting the most common class.
    \end{block}
    
    \begin{block}{Example}
        Imagine classifying a new fruit based on color and weight. The algorithm checks the closest 'k' fruits, and if most are apples, the new fruit is classified as an apple.
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Pros: Simple to implement with no training phase.
            \item Cons: Slower with large datasets; performance depends on 'k'.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Support Vector Machines (SVM)}
    \begin{block}{Concept}
        SVMs find the optimal hyperplane separating different class data points, maximizing the margin to nearest points (support vectors).
    \end{block}
    
    \begin{block}{Example}
        Visualize a plot of two flower types (red and blue). SVM finds the furthest line separating the two types.
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Pros: Effective in high-dimensional spaces; handles complex datasets.
            \item Cons: High computational cost; hard to interpret.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Engaging Questions}
    \begin{block}{Conclusion}
        Understanding these algorithms is essential for effective supervised learning. Each has strengths and weaknesses, and the choice depends on the problem and data.
    \end{block}
    
    \begin{block}{Engaging Questions}
        \begin{itemize}
            \item How would you choose an algorithm for a new dataset?
            \item Can you identify a real-world application for each algorithm?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Data Types}
    \begin{block}{1. Understanding Data Types}
        In the realm of AI and ML, the type of data utilized is crucial for model performance and efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Structured Data}
    \begin{itemize}
        \item \textbf{Definition:} Highly organized and formatted for easy search and analysis.
        \item \textbf{Examples:}
        \begin{itemize}
            \item \textbf{Databases:} Structured tables in SQL with variable columns and record rows.
            \item \textbf{Spreadsheets:} Excel files with defined column headers (e.g., "Product Name", "Sales").
            \item \textbf{Sensor Data:} Consistent readings from devices (e.g., temperature measurements).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Unstructured Data}
    \begin{itemize}
        \item \textbf{Definition:} Lacks predefined format, making it complex to analyze.
        \item \textbf{Examples:}
        \begin{itemize}
            \item \textbf{Text Data:} Articles, social media posts, emails, conversation transcripts.
            \item \textbf{Media Files:} Images, audio, and videos requiring processing for understanding.
            \item \textbf{Web Content:} HTML pages, blogs, user-generated content online.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relevance of Data Types in ML}
    \begin{itemize}
        \item \textbf{Structured Data in ML:}
        \begin{itemize}
            \item Traditional ML algorithms excel with structured data (e.g., classification, regression).
            \item \textbf{Example:} Decision tree model classifying patient disease based on symptoms.
        \end{itemize}
        
        \item \textbf{Unstructured Data in ML:}
        \begin{itemize}
            \item Deep learning allows meaningful information extraction from unstructured data.
            \item \textbf{Example:} Sentiment analysis of social media text using NLP techniques.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Reflection}
    \begin{itemize}
        \item The choice of data type significantly impacts model complexity and performance.
        \item Structured data is easier to manipulate but may miss insights from complex phenomena.
        \item Unstructured data can reveal deeper insights that structured data cannot.
    \end{itemize}

    \begin{block}{Questions to Reflect On}
        \begin{itemize}
            \item How can combining structured and unstructured data enhance AI applications?
            \item What challenges arise when preprocessing unstructured data for ML?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Introduction}
    \begin{itemize}
        \item Data quality is crucial for AI and machine learning effectiveness.
        \item The phrase "garbage in, garbage out" highlights the importance of data quality.
        \item Explore the influence of data quality on AI outcomes and ML model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Key Concepts}
    \begin{enumerate}
        \item \textbf{Data Quality Defined}:
        \begin{itemize}
            \item \textbf{Accuracy}: Data must accurately represent the real-world scenario.
            \item \textbf{Completeness}: All necessary data fields must be present.
            \item \textbf{Consistency}: Data should be harmonious across datasets.
        \end{itemize}

        \item \textbf{AI Outcomes Influenced by Data Quality}:
        \begin{itemize}
            \item Poor data quality can lead to inaccurate predictions and inefficiencies.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Case Examples}
    \begin{block}{Healthcare Use Case}
        An AI model predicting patient outcomes was trained on a dataset with missing values, leading to incorrect predictions and mismanagement of treatments.
    \end{block}
    
    \begin{block}{Finance Use Case}
        A financial institution used accurate data to assess creditworthiness, reducing loan defaults significantly. In contrast, a limited dataset led to many misassessments of high-risk borrowers.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Conclusion}
    \begin{itemize}
        \item High data quality is essential for leveraging the potential of AI and ML.
        \item Organizations investing in data quality see higher ROI in AI initiatives.
        \item Achieving high data quality is an ongoing process requiring regular evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Introduction}
    \begin{block}{Overview}
        Data preprocessing is critical in any data analysis or machine learning workflow. It transforms raw data into a clean, usable format, ensuring algorithms can operate effectively.
    \end{block}
    \begin{itemize}
        \item Importance of integrity in input data
        \item Impact on model performance
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Key Techniques}
    \begin{enumerate}
        \item Handling Missing Values
        \item Data Normalization
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values}
    \begin{block}{Explanation}
        Missing values can skew data analysis and model performance. Addressing them is essential.
    \end{block}
    \begin{itemize}
        \item \textbf{Techniques to Handle Missing Values:}
        \begin{itemize}
            \item \textbf{Deletion}
            \begin{itemize}
                \item Row Deletion: Removes rows with missing data.
                \item Column Deletion: Removes entire columns when most values are missing.
            \end{itemize}
            \item \textbf{Imputation}
            \begin{itemize}
                \item Mean/Median/Mode: Replaces missing values with appropriate statistics.
                \item Predictive: Uses models to estimate missing values.
                \item \textbf{Example:} Average height can replace missing height values.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Normalization - Techniques}
    \begin{block}{Explanation}
        Normalization scales data to a common range, preserving differences in value distributions.
    \end{block}
    \begin{itemize}
        \item \textbf{Common Techniques:}
        \begin{itemize}
            \item \textbf{Min-Max Normalization}
            \begin{itemize}
                \item Rescales data to a fixed range (e.g., 0 to 1).
                \item \textbf{Formula:} 
                \[
                X' = \frac{X - \text{min}(X)}{\text{max}(X) - \text{min}(X)}
                \]
                \item \textbf{Example:} A value of 50 in the range 10 to 100 becomes 0.5.
            \end{itemize}
            \item \textbf{Z-Score Normalization}
            \begin{itemize}
                \item Converts data to a distribution with mean 0 and standard deviation 1.
                \item \textbf{Formula:} 
                \[
                Z = \frac{X - \mu}{\sigma}
                \]
                \item Where $\mu$ is the mean and $\sigma$ is the standard deviation.
                \item \textbf{Example:} A value of 60 with mean 50 and stddev 10 has a Z-score of 1.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Conclusion}
    \begin{itemize}
        \item Data preprocessing is essential for model accuracy; it prevents garbage in, garbage out scenarios.
        \item Each preprocessing technique has its pros and cons; context matters for method selection.
        \item Normalization is vital when feature scales vary significantly.
    \end{itemize}
    \begin{block}{Conclusion}
        Effective data preprocessing enhances model performance, improves data integrity, and ensures reliable AI outcomes. Investing time in preprocessing leads to substantial long-term benefits in data analysis and machine learning projects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Machine Learning Models - Overview}
    \begin{itemize}
        \item Supervised learning models enable predictions based on labeled training data.
        \item This section covers key steps and practical tips for effective implementation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Implementing Supervised Learning Models - Part 1}
    \begin{enumerate}
        \item \textbf{Define the Problem}
            \begin{itemize}
                \item Identify the problem to solve (e.g., predicting house prices).
                \item Determine the type of output required: label (classification) or value (regression).
            \end{itemize}
        
        \item \textbf{Select a Dataset}
            \begin{itemize}
                \item Choose a dataset relevant to the chosen problem.
                \item Ensure high quality and diversity in the dataset.
            \end{itemize}
        
        \item \textbf{Data Preprocessing}
            \begin{itemize}
                \item \textbf{Cleaning}: Handle missing values.
                \item \textbf{Normalization}: Scale features uniformly.
                \item \textbf{Encoding}: Convert categorical variables to numerical.\\ 
                \textit{Example: One-hot encoding for "Location".}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Implementing Supervised Learning Models - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{4} % Continue the numbering
        \item \textbf{Choose a Machine Learning Algorithm}
            \begin{itemize}
                \item Classification: Logistic Regression, Decision Trees, SVM.
                \item Regression: Linear Regression, Random Forests.
                \item \textit{Example: Use Decision Tree for spam classification.}
            \end{itemize}

        \item \textbf{Split the Data}
            \begin{itemize}
                \item Divide data into training and testing sets (e.g., 80\% train, 20\% test).
                \item Code Snippet:
                \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Train the Model}
            \begin{itemize}
                \item Train model using the training dataset.
                \item Code Snippet:
                \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Implementing Supervised Learning Models - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{7} % Continue the numbering
        \item \textbf{Evaluate the Model}
            \begin{itemize}
                \item Assess model performance using metrics like accuracy, precision, and recall.
                \item Visualize performance: confusion matrices, ROC curves.
                \item \textit{Example: Use a confusion matrix for classification analysis.}
            \end{itemize}

        \item \textbf{Iterate and Improve}
            \begin{itemize}
                \item Tweak parameters, try different algorithms, or enhance features.
            \end{itemize}

        \item \textbf{Deploy the Model}
            \begin{itemize}
                \item Prepare the model for real-world applications to make predictions on new data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Tips for Implementing Machine Learning Models}
    \begin{itemize}
        \item \textbf{Start Simple}: Begin with simpler models (e.g., logistic regression).
        \item \textbf{Use Libraries}: Leverage libraries like Scikit-Learn, TensorFlow, or PyTorch.
        \item \textbf{Understand Trade-offs}: More complex models may not always yield better performance.
        \item \textbf{Documentation and Versioning}: Ensure good documentation and use version control.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Implementing machine learning models is a structured process.
        \item Continuous evaluation and adaptation are key to successful model implementation.
        \item Emphasize experimentation and learning throughout the journey.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance}
    \begin{block}{Understanding Evaluation Metrics}
        When assessing the effectiveness of a machine learning model, it is crucial to use appropriate evaluation metrics. These metrics help us quantify how well our model performs in making predictions. Common metrics include:
        \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy}
    \begin{block}{Definition}
        Accuracy measures the proportion of correct predictions made by the model out of all predictions. It is straightforward but may not always provide a complete picture, especially with imbalanced class distributions.
    \end{block}

    \begin{block}{Formula}
        \begin{equation}
        \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Predictions}}
        \end{equation}
    \end{block}

    \begin{block}{Example}
        Total emails: 100 \\
        Correctly classified as spam (True Positives): 40 \\
        Correctly classified as not spam (True Negatives): 50 \\
        Incorrectly classified: 10 (False Positives + False Negatives) \\
        \text{Accuracy} = $\frac{40 + 50}{100} = 0.90$ or 90\%
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall}
    \begin{block}{Precision}
        \begin{itemize}
            \item \textbf{Definition}: Precision indicates the correctness of the positive predictions, crucial when the cost of a false positive is high.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Example}: For the email example, if 50 emails are predicted as spam and only 40 are actually spam:
            \[
            \text{Precision} = \frac{40}{40 + 10} = \frac{40}{50} = 0.80 \text{ or } 80\%
            \]
        \end{itemize}
    \end{block}

    \begin{block}{Recall}
        \begin{itemize}
            \item \textbf{Definition}: Recall measures how effectively the model identifies true positives and is vital when missing a positive instance is costly.
            \item \textbf{Formula}:
            \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Example}: If there are 60 actual spam emails and the model identifies 40:
            \[
            \text{Recall} = \frac{40}{40 + 20} = \frac{40}{60} = 0.67 \text{ or } 67\%
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Next Steps}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item High accuracy does not always indicate a good model; consider precision and recall together.
            \item Understand your application's context to prioritize metrics appropriately (e.g., recall in disease detection).
            \item Visualize performance using confusion matrices.
        \end{itemize}
    \end{block}
    
    \begin{block}{Quick Tips}
        \begin{itemize}
            \item Evaluate models with multiple metrics for a comprehensive performance understanding.
            \item Explore F1 Score: the harmonic mean of precision and recall:
            \begin{equation}
            \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
        \end{itemize}
    \end{block}

    \begin{block}{Next Steps}
        In the following slide, we will discuss real-world case studies that highlight the importance of data quality in AI applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies in AI and Data Quality - Introduction}
    \begin{block}{Overview}
        In the realm of Artificial Intelligence (AI), the quality of data is paramount. This presentation highlights real-world case studies that illustrate the reliance on high-quality data in AI applications.
    \end{block}
    \begin{itemize}
        \item Importance of data quality
        \item Direct impact on model performance
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: IBM Watson for Oncology}
    \begin{itemize}
        \item \textbf{Overview:} 
        In 2016, IBM Watson for Oncology aimed to assist oncologists in diagnosing cancer using extensive datasets.
        
        \item \textbf{Data Quality Issues:}
        \begin{itemize}
            \item Inconsistent data led to inaccurate recommendations.
            \item Bias in training data limited diverse understanding.
        \end{itemize}
        
        \item \textbf{Outcome:} 
        Contradictions with experienced doctors caused skepticism, highlighting the need for reliable datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Google's Image Recognition}
    \begin{itemize}
        \item \textbf{Overview:} 
        Google's image recognition software relies on data from the internet for high accuracy.
        
        \item \textbf{Data Quality Issues:}
        \begin{itemize}
            \item Inconsistent labeling resulted in misclassifications.
            \item Overfitting due to noisy data affected real-world performance.
        \end{itemize}
        
        \item \textbf{Outcome:} 
        Improved data collection led to better labeling and performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 3: Tesla's Autonomous Vehicles}
    \begin{itemize}
        \item \textbf{Overview:} 
        Tesla's self-driving cars use camera and sensor data for navigation.
        
        \item \textbf{Data Quality Issues:}
        \begin{itemize}
            \item Sensor misalignment caused poor situational awareness.
            \item Data drift from changing environments affected accuracy.
        \end{itemize}
        
        \item \textbf{Outcome:} 
        Continuous model updates and data validation improved robustness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Data Integrity:} High-quality data must be accurate, complete, and timely.
        \item \textbf{Diversity:} Datasets should reflect diverse scenarios to avoid biases.
        \item \textbf{Continuous Improvement:} Regular updates and validation are essential.
    \end{itemize}
    \begin{block}{Conclusion}
        These case studies illustrate the vital link between data quality and AI application success. Prioritizing data quality is crucial for building reliable AI solutions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues in AI}
    In the realm of Artificial Intelligence (AI), the quality of data is paramount for achieving accurate and reliable model performance. 
    Data issues can lead to models that underperform or provide biased outcomes. This presentation explores common data issues and their solutions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues - Incomplete Data}
    \begin{itemize}
        \item \textbf{Definition:} Missing values can lead to incomplete analyses and skewed results.
        \item \textbf{Example:} 
            \begin{itemize}
                \item A healthcare dataset missing patient age data can mispredict treatment efficacy for certain age groups.
            \end{itemize}
        \item \textbf{Solution:} 
            \begin{itemize}
                \item Impute missing values with mean substitution, median for ordinal data, or predictive models.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues - Noisy and Biased Data}
    \begin{itemize}
        \item \textbf{Noisy Data:}
            \begin{itemize}
                \item \textbf{Definition:} Contains errors, inconsistencies, or outliers that can confuse learning algorithms.
                \item \textbf{Example:} A typo in annual income can significantly influence loan approval predictions.
                \item \textbf{Solution:} Clean data by filtering out outliers or applying z-score thresholds for anomaly detection.
            \end{itemize}
        \item \textbf{Biased Data:}
            \begin{itemize}
                \item \textbf{Definition:} Fails to appropriately represent the population, leading to biased models.
                \item \textbf{Example:} A facial recognition dataset dominated by a specific ethnic group yields poor performance for others.
                \item \textbf{Solution:} Ensure diverse representation by strategically gathering data from multiple demographics.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues - Irrelevant Data and Key Takeaways}
    \begin{itemize}
        \item \textbf{Irrelevant Data:}
            \begin{itemize}
                \item \textbf{Definition:} Features that do not contribute to the predicted outcome.
                \item \textbf{Example:} Including door color when predicting house prices detracts from meaningful trends.
                \item \textbf{Solution:} Use feature selection techniques (e.g., correlation analysis) to identify and retain relevant features.
            \end{itemize}
        \item \textbf{Key Takeaways:}
            \begin{itemize}
                \item The integrity of AI models is heavily influenced by data quality.
                \item Regular audits and preprocessing can mitigate data issues.
                \item Implement feedback loops for continuous improvement of data quality.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engagement and Conclusion}
    \begin{itemize}
        \item \textbf{Engage with the Following Questions:}
            \begin{itemize}
                \item How might an AI application fail due to one of the issues discussed?
                \item What steps can you take in your current projects to ensure high data quality?
                \item How can data biases affect real-world decision-making?
            \end{itemize}
        \item \textbf{Conclusion:} 
            \begin{itemize}
                \item Addressing common data issues significantly enhances model reliability and trust.
                \item High-quality data is crucial for successful AI applications.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in AI and Data Processing}
    \begin{block}{Overview}
        Exploring current trends and emerging technologies in AI and their impact on data quality is crucial for anticipating future applications. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding the Future of AI}
    \begin{itemize}
        \item AI is rapidly evolving due to:
            \begin{itemize}
                \item Advancements in computational power
                \item Algorithmic innovations
                \item Increased data availability
            \end{itemize}
        \item The evolution of AI impacts the quality of data, which is essential for reliable AI applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Emerging Trends in AI}
    \begin{enumerate}
        \item \textbf{Enhanced Neural Networks}
            \begin{itemize}
                \item \textbf{Transformers}: Effective for NLP; improve context understanding via parallel processing.
                \item \textbf{Diffusion Models}: Generate high-quality images, marking a shift towards generative AI.
                \item \textit{Example}: Google’s Vision Transformer (ViT) enhances image processing tasks.
            \end{itemize}

        \item \textbf{Automated Data Cleaning}
            \begin{itemize}
                \item Machine learning algorithms identify and correct data quality issues automatically.
                \item \textit{Example}: Tools like Trifacta and DataRobot enhance data quality through automated detection.
            \end{itemize}

        \item \textbf{Synthetic Data Generation}
            \begin{itemize}
                \item Artificial data mimicking real-world patterns allows for training while preserving privacy.
                \item \textit{Example}: Synthetic Data Vault creates datasets that replicate real-world statistical properties.
            \end{itemize}

        \item \textbf{Federated Learning}
            \begin{itemize}
                \item Decentralized model training that protects data privacy.
                \item \textit{Example}: Google’s keyboard prediction model processes user input without direct data access.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact on Data Quality}
    The innovations in AI reshape data usage while emphasizing the importance of data quality:
    \begin{itemize}
        \item \textbf{Robustness}: Ensuring models can handle incomplete or noisy data.
        \item \textbf{Diversity}: Addressing biases in training data for fairer AI systems.
        \item \textbf{Transparency}: Clear documentation for reproducibility and trust.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Change is Continuous}: AI technologies evolve constantly; agility in data practices is necessary.
        \item \textbf{Quality and Ethics Go Hand-in-Hand}: Maintaining data quality is critical for fair outcomes.
        \item \textbf{Stay Informed}: Updating knowledge on AI trends is vital for project success.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion on Ethical Considerations}
    \begin{block}{Overview}
        This presentation highlights the ethical implications of data quality in AI applications, focusing on fairness, accountability, and transparency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Bias and Its Implications}
    \begin{itemize}
        \item \textbf{Definition}: Data bias occurs when AI training data is skewed or unrepresentative.
        \item \textbf{Example}: AI models for job recruitment favor specific demographics, disadvantaging others.
        \item \textbf{Key Point}: Diverse and representative datasets are critical for mitigating bias.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transparency and Explainability}
    \begin{itemize}
        \item \textbf{Definition}: Users must understand AI decision-making processes.
        \item \textbf{Example}: In healthcare, algorithms need to clarify how factors like age affect predictions.
        \item \textbf{Key Point}: Clear documentation fosters trust and accountability in AI systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Privacy and Protection}
    \begin{itemize}
        \item \textbf{Definition}: Respect for individuals' privacy rights is essential in data collection and usage.
        \item \textbf{Example}: Facial recognition systems must avoid exploitation and unjust surveillance.
        \item \textbf{Key Point}: Strong data governance policies build public trust in AI.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Data Quality on Social Justice}
    \begin{itemize}
        \item \textbf{Definition}: Poor data quality can perpetuate inequalities.
        \item \textbf{Example}: Predictive policing may lead to excessive policing in certain neighborhoods.
        \item \textbf{Key Point}: Ethical data practices can ensure AI contributes positively to society.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Reflection}
    \begin{block}{Conclusion}
        Navigating the ethical landscape of data quality in AI is crucial for developing responsible technologies.
    \end{block}

    \begin{block}{Quick Reflection Questions}
        \begin{itemize}
            \item How might bias in data affect real-life AI applications?
            \item Why is transparency vital in AI development?
            \item How can we improve data collection practices for enhanced quality?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interactive Q\&A - Overview}
    \begin{block}{Objective}
        Foster an engaging dialogue with students about the concepts covered in Chapter 4, focusing on the introduction to AI and the significance of data quality.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interactive Q\&A - Key Discussion Points}
    \begin{enumerate}
        \item \textbf{Introduction to AI}
            \begin{itemize}
                \item Definition: The capability of machines to imitate intelligent human behavior (learning, reasoning, problem-solving).
                \item Examples: Virtual assistants, image recognition systems, recommendation systems.
            \end{itemize}
        \item \textbf{Importance of Data Quality}
            \begin{itemize}
                \item Definition: Accuracy, completeness, reliability, and relevance of data.
                \item Consequences of poor data: Incorrect insights, misdiagnoses in healthcare, and biased recommendations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interactive Q\&A - Engagement Strategies}
    \begin{itemize}
        \item \textbf{Questions to Students:}
            \begin{itemize}
                \item What challenges do you think AI faces in ensuring data quality?
                \item Can you think of industries where the impact of data quality is critical?
                \item How might improving data quality change the outcome of an AI system?
            \end{itemize}
        \item \textbf{Interactive Elements:}
            \begin{itemize}
                \item Utilize polling tools (e.g., Slido, Mentimeter) to gather opinions.
                \item Invite students to share personal experiences with AI related to data quality.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interactive Q\&A - Real-World Implications}
    \begin{block}{Real-World Context}
        Discuss examples such as: 
        - Backlash faced by companies like Facebook or Google due to data misuse (e.g., misinformation, privacy violations).
    \end{block}
    
    \begin{block}{Discussion Prompt}
        Reflect on how to assess data quality in AI applications. What metrics or strategies would you propose?
    \end{block}
    
    \begin{block}{Concluding Thoughts}
        Emphasize the integral relationship between understanding AI and ensuring high-quality data, along with its ethical implications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Takeaways - Part 1}
    \begin{block}{1. Introduction to Supervised Learning}
        \begin{itemize}
            \item \textbf{Definition:} Supervised learning is a type of machine learning where an algorithm is trained on labeled data.
            \item \textbf{Key Features:}
                \begin{itemize}
                    \item \textbf{Training Phase:} The algorithm learns the mapping from inputs to outputs using historical data.
                    \item \textbf{Prediction Phase:} Once trained, it can predict the output for new, unseen data.
                \end{itemize}
            \item \textbf{Example:} A model categorizing emails as "spam" or "not spam," with each email labeled to guide learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Takeaways - Part 2}
    \begin{block}{2. Importance of Data Quality}
        \begin{itemize}
            \item \textbf{Definition:} Data quality refers to the reliability and suitability of data for its intended purpose.
            \item \textbf{Key Aspects:}
                \begin{itemize}
                    \item \textbf{Accuracy:} Is the data correct and free from errors?
                    \item \textbf{Completeness:} Is all necessary data present?
                    \item \textbf{Consistency:} Is the data consistent across different datasets?
                \end{itemize}
            \item \textbf{Impact on Supervised Learning:} Poor data quality can lead to inaccurate models due to mislabeled training sets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Takeaways - Part 3}
    \begin{block}{3. Strategies to Improve Data Quality}
        \begin{itemize}
            \item \textbf{Data Preprocessing:} Involves cleaning data by removing duplicates, correcting errors, and addressing missing values.
            \item \textbf{Validation Techniques:} Implement validation steps during data collection to catch errors early.
            \item \textbf{Continuous Monitoring:} Regularly assess datasets to ensure they remain accurate and relevant over time.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Supervised learning is powerful and essential for AI, relying heavily on data quality.
            \item Understanding data quality is crucial for building effective AI models and enhancing analytics skills.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Learning - Overview}
    As we progress through this course on Artificial Intelligence (AI), here’s what you can expect to explore in the upcoming chapters:
    \begin{enumerate}
        \item \textbf{Deep Dive into Supervised Learning}
        \item \textbf{Exploring Unsupervised Learning}
        \item \textbf{The Role of Data Quality in AI}
        \item \textbf{Evaluating AI Models}
        \item \textbf{Ethical Considerations in AI}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Learning - Chapter Insights}
    \begin{block}{1. Deep Dive into Supervised Learning}
        \begin{itemize}
            \item \textbf{What to Expect:} Algorithms learn from labeled data.
            \item \textbf{Example:} Training a model to recognize images of cats vs. dogs using labeled pictures.
        \end{itemize}
    \end{block}
    
    \begin{block}{2. Exploring Unsupervised Learning}
        \begin{itemize}
            \item \textbf{What to Expect:} Clustering and association algorithms.
            \item \textbf{Example:} Grouping customer profiles without prior labels, like Netflix recommendations.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Learning - Further Insights}
    \begin{block}{3. The Role of Data Quality in AI}
        \begin{itemize}
            \item \textbf{What to Expect:} Criteria for good data quality (accuracy, completeness, timeliness).
            \item \textbf{Example:} Impacts of outdated allergies in hospital records.
        \end{itemize}
    \end{block}

    \begin{block}{4. Evaluating AI Models}
        \begin{itemize}
            \item \textbf{What to Expect:} Evaluation metrics (accuracy, precision, recall, F1-score).
            \item \textbf{Example:} Balancing false positives and negatives in spam detection.
        \end{itemize}
    \end{block}

    \begin{block}{5. Ethical Considerations in AI}
        \begin{itemize}
            \item \textbf{What to Expect:} Focus on fairness and transparency.
            \item \textbf{Example:} Case studies on data biases affecting decisions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resources for Further Study - Introduction}
    \begin{block}{Understanding AI and Data Quality}
        To enhance your understanding of AI, it is essential to recognize the interconnection between the technology itself and the quality of the data that drives it.
        Here are various types of resources to further your learning experience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resources for Further Study - Recommended Readings}
    \begin{enumerate}
        \item \textbf{"Artificial Intelligence: A Guide to Intelligent Systems" by Michael Negnevitsky}
        \begin{itemize}
            \item Balanced introduction to AI principles and algorithms.
            \item Illustrative examples simplify complex topics.
        \end{itemize}
        
        \item \textbf{"Data Quality: The Accuracy Dimension" by Jack E. Olson}
        \begin{itemize}
            \item Focus on factors affecting data accuracy.
            \item Practical insights into effective data management practices.
        \end{itemize}
        
        \item \textbf{"The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling" by Ralph Kimball}
        \begin{itemize}
            \item Key understanding of data modeling's importance.
            \item Essential for maintaining data quality in AI-related tasks.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resources for Further Study - Online Resources}
    \begin{enumerate}
        \item \textbf{Coursera - "AI For Everyone" by Andrew Ng}
        \begin{itemize}
            \item Non-technical course covering AI fundamentals.
            \item Practical view of AI's organizational impact without heavy mathematics.
        \end{itemize}
        
        \item \textbf{Kaggle - Datasets and Competitions}
        \begin{itemize}
            \item Engage with real-world data problems.
            \item Hands-on experience in a community-driven environment.
        \end{itemize}
        
        \item \textbf{Towards Data Science on Medium}
        \begin{itemize}
            \item Articles and case studies on AI advancements.
            \item User-friendly insights into complex data quality issues.
        \end{itemize}
    \end{enumerate}
\end{frame}


\end{document}