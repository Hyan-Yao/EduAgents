\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Title Page Information
\title[Chapter 8]{Chapter 8: Evaluation Metrics for Machine Learning Models}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Evaluation Metrics}
    \begin{block}{Overview}
        Evaluation metrics are essential for assessing the performance of machine learning models. They provide quantitative measures that help us understand how well a model predicts or classifies new data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Evaluation Metrics}
    \begin{itemize}
        \item **Performance Insight:** Identify strengths and weaknesses of the model.
        \item **Model Comparison:** Compare different models to choose the best-performing one.
        \item **Decision Making:** Enable stakeholders to make informed choices impacting product development.
        \item **Guidance for Improvement:** Pinpoint areas needing enhancement for targeted improvements.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Evaluation Metrics}
    \begin{enumerate}
        \item **Accuracy:** Proportion of correct predictions. Example: 80 out of 100 correct gives 80\%.
        \item **Precision:** Quality of positive predictions. Example: 10 correct out of 15 predicted positives results in precision = 10/(10+5) = 0.67.
        \item **Recall (Sensitivity):** Correctly identified actual positives. Example: Recall = 10/15 = 0.67.
        \item **F1-Score:** Balance between precision and recall. Example: If both are 0.67, F1-Score = 2 * (0.67 * 0.67) / (0.67 + 0.67).
        \item **AUC-ROC:** Measures model's effectiveness in separating positive and negative classes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Choosing the right metric depends on the problem's specific nature and objectives.
        \item No single metric provides a complete picture; use multiple metrics for comprehensive understanding.
        \item Always consider context: What does success look like? What are the costs of false positives vs. false negatives?
    \end{itemize}

    \begin{block}{Conclusion}
        Evaluation metrics are crucial in validating model effectiveness and driving improvements. Selecting the right metrics is key to achieving success in machine learning projects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Evaluation Metrics?}
    \begin{block}{Definition}
        Evaluation metrics are quantitative measures used to assess the performance of machine learning models. They serve as benchmarks that help to understand how well a model is making predictions compared to actual outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Evaluation Metrics}
    \begin{itemize}
        \item \textbf{Performance Assessment}: Provides insights into the predictive capabilities of a model.
        \item \textbf{Model Comparison}: Enables comparison between different models to determine which performs best.
        \item \textbf{Guiding Improvements}: Tracks changes during model training and tuning to identify areas for improvement.
        \item \textbf{Application Suitability}: Helps in choosing the appropriate metric based on the problem type (e.g., classification vs. regression).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Evaluation Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item Definition: Proportion of correctly predicted instances over total instances.
                \item Example: If a model correctly predicts 80 out of 100 instances, accuracy = 80\%.
            \end{itemize}
        \item \textbf{Precision and Recall}
            \begin{itemize}
                \item \textbf{Precision}: Proportion of true positive predictions over all positive predictions.
                \item \textbf{Recall}: Proportion of true positive predictions over all actual positive instances.
                \item Example: In a medical diagnosis model, if 70 true positives and 30 false positives, then \\ 
                Precision = \( \frac{70}{70 + 30} = 0.7 \) or 70\%.
            \end{itemize}
        \item \textbf{F1 Score}
            \begin{itemize}
                \item Balance between precision and recall, useful for imbalanced classes. 
                \item Formula: 
                \begin{equation}
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} 
                \end{equation}
                \item Example: For Precision = 0.7 and Recall = 0.8, 
                \\ \( F1 = 2 \times \frac{0.7 \times 0.8}{0.7 + 0.8} = 0.74 \).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Discussion}
    \begin{itemize}
        \item Evaluation metrics are crucial for understanding model performance.
        \item The choice of metric should align with the specific goals of the model, as not all metrics are created equal.
        \item Visualization and reporting of multiple metrics provide a holistic view of model performance.
    \end{itemize}
    
    \textbf{Application Question:} Consider a scenario where you are building a fraud detection system: which evaluation metric(s) would you prioritize and why?
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy Metric - What is Accuracy?}
    \begin{itemize}
        \item Accuracy is a fundamental metric to evaluate classification models in machine learning.
        \item It measures the proportion of correct predictions from the total predictions made.
        \item \textbf{Key Concept:} 
        \begin{itemize}
            \item Accuracy tells us how often the classifier is correct, but lacks details on the types of errors made.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy Metric - How is Accuracy Calculated?}
    \begin{block}{Formula for Accuracy}
        \[
        \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}
        \]
    \end{block}
    \begin{itemize}
        \item \textbf{Definitions:} 
        \begin{itemize}
            \item TP (True Positives): Correctly predicted positive cases.
            \item TN (True Negatives): Correctly predicted negative cases.
            \item FP (False Positives): Incorrectly predicted positive cases.
            \item FN (False Negatives): Incorrectly predicted negative cases.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy Metric - Example and Limitations}
    \begin{itemize}
        \item \textbf{Example:}
        \begin{itemize}
            \item Binary classification for emails: "Spam" or "Not Spam".
            \item Predictions:
                \begin{itemize}
                    \item 70 classified as Spam (40 true, 30 false).
                    \item 30 classified as Not Spam (20 true, 10 false).
                \end{itemize}
            \item \textbf{Calculating Accuracy:}
            \[
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{40 + 20}{40 + 20 + 30 + 10} = \frac{60}{100} = 0.6 \text{ or } 60\%
            \]
        \end{itemize}
        
        \item \textbf{When to Use Accuracy?}
        \begin{itemize}
            \item Best for balanced classes: Accurate when instances are approximately equal across classes.
            \item Limitations with imbalanced datasets: High accuracy doesn't always indicate good performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision Metric - Understanding Precision}
    \begin{block}{Definition}
        Precision is a critical evaluation metric for classification models, particularly for imbalanced datasets or when false positives incur high costs.
    \end{block}
    
    \begin{itemize}
        \item Quantifies accuracy of positive predictions.
        \item Essential in applications like spam detection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision Metric - Formula for Precision}
    \begin{block}{Formula}
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
        \end{equation}
    \end{block}
    
    \begin{itemize}
        \item **True Positives (TP)**: Instances correctly predicted as positive.
        \item **False Positives (FP)**: Instances incorrectly predicted as positive.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision Metric - Importance and Example}
    \begin{itemize}
        \item \textbf{Why is Precision Important?}
            \begin{itemize}
                \item Focuses on the quality of positive predictions.
                \item Particularly useful in imbalanced class scenarios.
            \end{itemize}

        \item \textbf{Example: Email Classification}
            \begin{itemize}
                \item **True Positives (TP)**: 80 spam emails correctly identified.
                \item **False Positives (FP)**: 20 legitimate emails incorrectly classified as spam.
            \end{itemize}
        
        \item \textbf{Calculation}
            \begin{equation}
                \text{Precision} = \frac{80}{80 + 20} = 0.8
            \end{equation}
            This indicates 80\% reliability for spam classifications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall Metric - Definition}
    \begin{block}{What is Recall?}
        Recall, also known as Sensitivity or True Positive Rate, is a metric used to assess the effectiveness of a classification model. It measures the model's ability to identify all relevant instances of the positive class.
    \end{block}
    \begin{itemize}
        \item Recall answers the question: How good is the model at finding the 'yes' cases?
        \item Essential for applications where missing positive instances can have severe consequences.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall Metric - Calculation}
    \begin{block}{Formula for Recall}
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
        \end{equation}
    \end{block}
    \begin{itemize}
        \item **True Positives (TP)**: Correctly identified positive instances (e.g., detecting a disease).
        \item **False Negatives (FN)**: Positive instances that were incorrectly classified as negative (e.g., failing to detect a disease).
    \end{itemize}
    \begin{block}{Example Calculation}
        Imagine a disease test where:
        \begin{itemize}
            \item 80 TP: correctly detected
            \item 20 FN: missed cases
        \end{itemize}
        \begin{equation}
            \text{Recall} = \frac{80}{80 + 20} = \frac{80}{100} = 0.8 \text{ (or 80\%)}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall Metric - Applications}
    \begin{block}{When is Recall Particularly Useful?}
        \begin{enumerate}
            \item **Medical Diagnosis**: High recall is essential in disease detection (e.g., cancer).
            \item **Fraud Detection**: Crucial in catching fraudulent transactions in finance.
            \item **Search and Rescue Operations**: Identifying potential locations for missing persons.
        \end{enumerate}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item High Recall vs. Low Precision: Trade-off is often acceptable.
            \item Recall's Relationship with Precision: Both metrics provide a better understanding of model performance, especially in imbalanced datasets.
        \end{itemize}
    \end{block}
    \begin{block}{Summary}
        Recall helps ensure critical positive instances are not overlooked, vital in scenarios where identifying the positive class matters most.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - What is the F1 Score?}
    The F1 Score is a crucial evaluation metric in machine learning that combines two important aspects of model performance: 
    \textbf{Precision} and \textbf{Recall}. It is particularly useful when dealing with imbalanced datasets, where one class significantly outnumbers the other.

    \begin{itemize}
        \item \textbf{Precision}: Measures the accuracy of positive predictions.
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
        \end{equation}

        \item \textbf{Recall}: Measures the ability of a model to find all relevant cases (true positives).
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - How to Calculate the F1 Score?}
    The F1 Score is the harmonic mean of Precision and Recall, balancing the two metrics to give a single score that reflects both. 

    \begin{block}{F1 Score Formula}
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{block}

    \textbf{Importance of the F1 Score}:
    \begin{itemize}
        \item Offers a balance between Precision and Recall.
        \item Essential in contexts with imbalanced data, ensuring that both types of errors are minimized.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Example Scenario}
    Imagine a medical screening test for a rare disease:
    \begin{itemize}
        \item \textbf{True Positives (TP)}: 80 people tested positive and actually have the disease.
        \item \textbf{False Positives (FP)}: 20 people tested positive but do not have the disease.
        \item \textbf{False Negatives (FN)}: 10 people tested negative but actually have the disease.
    \end{itemize}

    Calculating Precision and Recall:
    
    \textbf{Precision}:
    \[
    \text{Precision} = \frac{80}{80 + 20} = 0.8
    \]
    
    \textbf{Recall}:
    \[
    \text{Recall} = \frac{80}{80 + 10} = 0.888
    \]

    \textbf{F1 Score Calculation}:
    \[
    \text{F1 Score} = 2 \times \frac{0.8 \times 0.888}{0.8 + 0.888} \approx 0.842
    \]
    
    \textit{Key Points to Emphasize:} \\
    Higher F1 Score = Better balance between Precision and Recall.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Overview}
    A **confusion matrix** is a fundamental tool used to evaluate the performance of classification models in machine learning. It provides a visual representation of a model's predictions compared to actual outcomes, enabling us to see:
    \begin{itemize}
        \item Where our model is making correct predictions.
        \item Where our model is making incorrect predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Components}
    A confusion matrix typically has four components:
    \begin{enumerate}
        \item **True Positive (TP)**: Correctly predicted positive cases.
        \item **True Negative (TN)**: Correctly predicted negative cases.
        \item **False Positive (FP)**: Incorrectly predicted positive cases (Type I Error).
        \item **False Negative (FN)**: Incorrectly predicted negative cases (Type II Error).
    \end{enumerate}
    This can be visualized as follows:
    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & Predicted Positive & Predicted Negative \\
        \hline
        Actual Positive & TP & FN \\
        \hline
        Actual Negative & FP & TN \\
        \hline
    \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Example and Key Metrics}
    Consider a scenario where a model predicts whether an email is spam:
    \begin{itemize}
        \item TP: 70 (emails correctly identified as spam)
        \item TN: 50 (emails correctly identified as not spam)
        \item FP: 10 (emails incorrectly identified as spam)
        \item FN: 5 (emails incorrectly identified as not spam)
    \end{itemize}
    Visual representation:
    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & Predicted Spam & Predicted Not Spam \\
        \hline
        Actually Spam & 70 (TP) & 5 (FN) \\
        \hline
        Actually Not Spam & 10 (FP) & 50 (TN) \\
        \hline
    \end{tabular}
    \end{center}
    Key metrics derived:
    \begin{itemize}
        \item \textbf{Accuracy}: $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$
        \item \textbf{Precision}: $\text{Precision} = \frac{TP}{TP + FP}$
        \item \textbf{Recall}: $\text{Recall} = \frac{TP}{TP + FN}$
        \item \textbf{F1 Score}: $F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is the ROC Curve?}
    \begin{itemize}
        \item \textbf{Definition}: The Receiver Operating Characteristic (ROC) curve is a graphical representation of a binary classifier's diagnostic ability as its discrimination threshold varies.
        \item \textbf{Purpose}: Assesses model performance across different thresholds, providing insight into class distinction.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of the ROC Curve}
    \begin{itemize}
        \item \textbf{Axes}:
        \begin{itemize}
            \item \textbf{X-Axis}: False Positive Rate (FPR) – Ratio of negatives incorrectly classified as positives. \(\text{FPR} = \frac{FP}{FP + TN}\)
            \item \textbf{Y-Axis}: True Positive Rate (TPR) – Ratio of positives correctly identified. \(\text{TPR} = \frac{TP}{TP + FN}\)
        \end{itemize}
        \item \textbf{Curve}: Plots TPR against FPR at various threshold settings.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting the ROC Curve}
    \begin{itemize}
        \item \textbf{Perfect Classifier}: TPR = 1, FPR = 0 (top left corner).
        \item \textbf{Random Classifier}: A model with no distinction ability lies along the diagonal line (TPR = FPR).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is AUC?}
    \begin{itemize}
        \item \textbf{Definition}: Area Under the Curve (AUC) quantifies overall classifier performance, providing a single metric for class discrimination.
        \item \textbf{Value Range}:
        \begin{itemize}
            \item \textbf{AUC = 1}: Perfect model
            \item \textbf{AUC = 0.5}: No discrimination ability (random)
            \item \textbf{AUC < 0.5}: Model performs worse than random chance
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Consider a model predicting spam emails (positive class) vs. non-spam (negative class):
    \begin{enumerate}
        \item Plot the ROC curve using thresholds:
        \begin{itemize}
            \item \textbf{Threshold of 0.3}: TPR = 85\%, FPR = 10\%
            \item \textbf{Threshold of 0.6}: TPR = 75\%, FPR = 5\%
        \end{itemize}
        \item Visualize trade-offs between sensitivity (TPR) and specificity (1 - FPR).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Comparison Tool}: Evaluate and compare multiple classifiers effectively.
        \item \textbf{Threshold Independence}: Independent of class distribution or operational thresholds.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the ROC Curve and AUC is critical for evaluating binary classification models. 
    \begin{itemize}
        \item Enhances informed decisions on model selection.
        \item Provides insights into performance beyond metrics like accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Overview}
    \begin{itemize}
        \item Selecting the right performance metric is crucial for evaluating machine learning models.
        \item The appropriate metric provides insight and guides decision-making based on the project's context.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Guidelines}
    \begin{enumerate}
        \item \textbf{Understand Your Objective}:
            \begin{itemize}
                \item \textbf{Classification vs. Regression}:
                    \begin{itemize}
                        \item Classification: e.g., spam detection (metrics: accuracy, precision, recall, F1-score).
                        \item Regression: e.g., house prices (metrics: MAE, MSE, R-squared).
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Consider the Nature of Your Data}:
            \begin{itemize}
                \item Class Imbalance: Use precision, recall, or F1-score to evaluate model performance in imbalanced datasets.
            \end{itemize}
        
        \item \textbf{Decision Threshold}: 
            \begin{itemize}
                \item Explore thresholds using the ROC curve to balance sensitivity and specificity.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Business Impact & Validation}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Business Impact}:
            \begin{itemize}
                \item Evaluate the cost of false positives and negatives (e.g., criticality in medical diagnoses).
                \item Align metrics with business goals (e.g., prioritize recall if catching positives is vital).
            \end{itemize}
        
        \item \textbf{Validation Strategy}:
            \begin{itemize}
                \item Ensure metrics align with the evaluation methodology (e.g., consistency across k-fold cross-validation).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Examples}
    \begin{itemize}
        \item \textbf{Scenario 1: Email Spam Detection}
            \begin{itemize}
                \item \textbf{Objective}: Reduce spam but also catch spam effectively.
                \item \textbf{Chosen Metric}: F1-Score.
            \end{itemize}

        \item \textbf{Scenario 2: House Price Prediction}
            \begin{itemize}
                \item \textbf{Objective}: Accurate pricing for a real estate company.
                \item \textbf{Chosen Metric}: Mean Absolute Error (MAE).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Summary}
    \begin{itemize}
        \item Align metrics with objectives, data nature, and cost of errors.
        \item Use a combination of metrics for a holistic view of model performance.
        \item Contextual understanding is key; what works for one problem may not fit another.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Formulas}
    \begin{block}{F1 Score}
        \[
        \text{F1 Score} = \frac{2 \times (\text{Precision} \times \text{Recall})}{\text{Precision} + \text{Recall}}
        \]
    \end{block}
    
    \begin{block}{Mean Absolute Error (MAE)}
        \[
        \text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Conclusion}
    \begin{itemize}
        \item Choosing the right evaluation metric is integral to the success of a machine learning project.
        \item Always consider the broader context, including business objectives and data characteristics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Application: Case Studies - Introduction}
    \begin{itemize}
        \item Understanding evaluation metrics is crucial for assessing the performance of machine learning models.
        \item This presentation covers case studies that illustrate the application of various evaluation metrics in real-world scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Email Spam Detection}
    \begin{itemize}
        \item \textbf{Context}: An email service provider classifies emails as 'Spam' or 'Not Spam'.
        \item \textbf{Model Applied}: Binary classification model (e.g., Logistic Regression).
        \item \textbf{Metrics Used}:
            \begin{itemize}
                \item \textbf{Accuracy}: Overall correctness of the model.
                \item \textbf{Precision}: The proportion of predicted Spam emails that are actually Spam.
                \item \textbf{Recall}: The proportion of actual Spam emails that were identified.
            \end{itemize}
        \item \textbf{Importance}:
            \begin{itemize}
                \item High recall is essential to avoid compromising user experience or security.
                \item Example: With a recall of 0.80 (80% correct identification of Spam), 20 spam emails would be missed.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Loan Default Prediction}
    \begin{itemize}
        \item \textbf{Context}: A financial institution identifies borrowers likely to default on loans.
        \item \textbf{Model Applied}: Decision Tree Classifier.
        \item \textbf{Metrics Used}:
            \begin{itemize}
                \item \textbf{F1 Score}: Balances precision and recall.
                \item \textbf{ROC-AUC}: Evaluates model differentiation between defaulting and non-defaulting borrowers.
            \end{itemize}
        \item \textbf{Importance}:
            \begin{itemize}
                \item A high F1 score is crucial due to the significant impact of false positives and false negatives.
                \item A ROC-AUC of 0.9 indicates an effective model, accurately classifying borrower types.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 3: Customer Sentiment Analysis}
    \begin{itemize}
        \item \textbf{Context}: A retail company analyzes customer feedback to classify sentiments.
        \item \textbf{Model Applied}: A multi-class classification model (e.g., neural network).
        \item \textbf{Metrics Used}:
            \begin{itemize}
                \item \textbf{Accuracy}: Overall performance measure.
                \item \textbf{Confusion Matrix}: Insights into true positives, false positives, and false negatives across classes.
            \end{itemize}
        \item \textbf{Importance}:
            \begin{itemize}
                \item Examining customer feedback nuances is crucial for enhancing products and customer service.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{enumerate}
        \item \textbf{Context Matters}: Metric selection depends on problem context and error costs.
        \item \textbf{Multiple Metrics}: A combination of metrics provides a comprehensive evaluation.
        \item \textbf{Stakeholder Impact}: Consider how metrics affect stakeholders aesthetically and functionally.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Case studies demonstrate that metrics are critical for model performance understanding.
        \item The choice of metrics should align with specific application needs.
        \item Tailored evaluation strategies drive meaningful outcomes in real-world scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Evaluation Metrics}
    \begin{block}{Understanding Evaluation Metrics}
        Evaluation metrics are essential for assessing the performance of machine learning models. However, they have limitations that must be understood to avoid making faulty conclusions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Limitations of Evaluation Metrics - Part 1}
    \begin{itemize}
        \item \textbf{A. Metric Sensitivity to Class Distribution}
        \begin{itemize}
            \item Some metrics, like accuracy, can be misleading with imbalanced datasets.
            \item Example: In a dataset of 100 instances (95 cats, 5 dogs), predicting all as cats results in 95\% accuracy, neglecting the minority class.
        \end{itemize}
        
        \item \textbf{B. Overfitting to Specific Metrics}
        \begin{itemize}
            \item Tuning models for a specific metric may hinder generalization.
            \item Example: A model optimized for F1 score may lack precision when high precision is critical, as in medical diagnoses.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Limitations of Evaluation Metrics - Part 2}
    \begin{itemize}
        \item \textbf{C. Lack of Contextual Relevance}
        \begin{itemize}
            \item Some metrics ignore business requirements; high precision might not be necessary when recall is key.
            \item Example: In fraud detection, low recall can cause significant losses despite high precision.
        \end{itemize}

        \item \textbf{D. Subjectivity in Metric Selection}
        \begin{itemize}
            \item Metric selection varies by domain, making it subjective.
            \item Example table:
            \end{itemize}
            \begin{block}{Key Metrics by Domain}
                \begin{tabular}{|l|l|}
                    \hline
                    \textbf{Domain} & \textbf{Key Metric} \\
                    \hline
                    Email Filtering & Precision \\
                    Medical Diagnosis & Recall \\
                    Image Classification & F1 Score \\
                    \hline
                \end{tabular}
            \end{block}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Use Multiple Metrics}: Combine various metrics (e.g., precision, recall, F1 score) for a holistic model evaluation.
        \item \textbf{Consider Business Goals}: Align metric selection with specific application goals and challenges.
        \item \textbf{Account for Context}: Evaluate models in their practical context to ensure metrics reflect real-world implications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        While evaluation metrics provide insights into model performance, understanding their limitations is crucial. Combine metrics tailored to context and business needs for informed decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Introduction}
    \begin{block}{Introduction to Evaluation Metrics}
        In machine learning, choosing the right evaluation metric is crucial for understanding how well our model performs. Different metrics provide insights from various perspectives, depending on the specific characteristics of the problem we're trying to solve. This slide will compare commonly-used evaluation metrics, showing their unique strengths and use cases through real examples.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Key Metrics}
    \begin{block}{Key Metrics for Comparison}
        \begin{enumerate}
            \item \textbf{Accuracy}
            \begin{itemize}
                \item Definition: Proportion of true results (both true positives and true negatives) among the total number of cases examined.
                \item Use Case: Effective for balanced datasets.
                \item Example: A spam filter where 90 out of 100 emails are classified correctly. Accuracy = $\frac{90}{100} = 90\%$.
            \end{itemize}

            \item \textbf{Precision}
            \begin{itemize}
                \item Definition: The ratio of true positive predictions to the total predicted positives.
                \item Use Case: Important when the cost of false positives is high (e.g., fraud detection).
                \item Example: In a medical diagnosis, if 8 out of 10 cases are correctly diagnosed, Precision = $\frac{8}{10} = 80\%$.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Key Metrics (Cont.)}
    \begin{block}{Key Metrics for Comparison (Continued)}
        \begin{enumerate}
            \setcounter{enumi}{2}
            \item \textbf{Recall (Sensitivity)}
            \begin{itemize}
                \item Definition: The ratio of true positive predictions to the actual positives.
                \item Use Case: Useful when false negatives are costly (e.g., in cancer diagnosis).
                \item Example: If 15 out of 20 positive cases are detected, Recall = $\frac{15}{20} = 75\%$.
            \end{itemize}

            \item \textbf{F1 Score}
            \begin{itemize}
                \item Definition: The harmonic mean of Precision and Recall.
                \item Use Case: Useful in scenarios with imbalanced classes.
                \item Example: If Precision = 0.80 and Recall = 0.75, then F1 Score = $2 \times \frac{0.80 \times 0.75}{0.80 + 0.75} \approx 0.775$ or 77.5\%.
            \end{itemize}

            \item \textbf{ROC-AUC}
            \begin{itemize}
                \item Definition: Receiver Operating Characteristic - Area Under Curve.
                \item Use Case: Helpful for binary classification and imbalanced datasets.
                \item Example: An AUC score close to 1 indicates good model performance in distinguishing classes.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Key Points & Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Context Matters: The choice of metric can significantly affect model evaluation. 
            \item Trade-offs: Many situations involve trade-offs; improving one metric might worsen another.
            \item Visualizations: Tools like ROC curves assist in understanding model performance at various thresholds.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        To summarize, understanding the comparative advantages and applications of these metrics will guide us in selecting appropriate measures for our machine learning models. The right evaluation metric can be the difference between a successful model and a misleading one!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools for Evaluating Models - Overview}
    \begin{block}{Importance of Model Evaluation}
        Evaluating machine learning models is crucial for determining their effectiveness and suitability for specific tasks. Various tools and libraries provide essential metrics and visualizations to assess model performance.
    \end{block}
    \begin{block}{Focus: Scikit-learn}
        This presentation places special emphasis on **scikit-learn**, a widely used library for model evaluation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools for Evaluating Models - Key Libraries}
    \begin{enumerate}
        \item \textbf{Scikit-learn}
          \begin{itemize}
              \item Provides evaluation tools for classification, regression, and clustering.
              \item Key functions: 
                \begin{itemize}
                    \item `accuracy\_score`
                    \item `confusion\_matrix`
                    \item `classification\_report`
                    \item `mean\_squared\_error`
                \end{itemize}
          \end{itemize}
          
        \item \textbf{TensorFlow \& Keras}
          \begin{itemize}
              \item Built-in evaluation functions for deep learning models.
              \item Functions: 
                \begin{itemize}
                    \item `model.evaluate()`
                    \item `tf.keras.metrics`
                \end{itemize}
          \end{itemize}
          
        \item \textbf{PyTorch}
          \begin{itemize}
              \item Allows flexibility in calculating custom metrics.
              \item Often used with scikit-learn for evaluation.
          \end{itemize}

        \item \textbf{MLflow}
          \begin{itemize}
              \item Manages the machine learning lifecycle, including evaluation.
              \item Features include logging models and tracking metrics.
          \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Scikit-learn - Example Code}
    \begin{block}{Example Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

# Load sample data
data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)

# Train a model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, predictions))
print("Confusion Matrix:\n", confusion_matrix(y_test, predictions))
print("Classification Report:\n", classification_report(y_test, predictions))
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Homework/Practice Activity: Evaluating Machine Learning Models}
    \begin{block}{Objective}
        Gain hands-on experience in calculating key evaluation metrics for machine learning models.
        \pause
        By the end, you should be able to interpret these metrics and understand their significance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dataset}
    You will be using a sample dataset provided on the course platform. This dataset contains:
    \begin{itemize}
        \item \textbf{Feature Variables}: Characteristics or attributes of the data.
        \item \textbf{Target Variable}: The true labels (e.g., '0' for negative, '1' for positive class).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics to Compute}
    Using Python and Scikit-learn, compute the following metrics:
    \begin{enumerate}
        \item \textbf{Accuracy}:
        \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \end{equation}
        \pause
        \item \textbf{Precision}:
        \begin{equation}
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
        \end{equation}
        \pause
        \item \textbf{Recall (Sensitivity)}:
        \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
        \end{equation}
        \pause
        \item \textbf{F1 Score}:
        \begin{equation}
            F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
        \pause
        \item \textbf{Confusion Matrix}:
        \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
            \hline
            \textbf{Actual Positive} & TP & FN \\
            \hline
            \textbf{Actual Negative} & FP & TN \\
            \hline
        \end{tabular}
        \end{center}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Instructions}
    Follow these steps to complete the activity:
    \begin{enumerate}
        \item Download the sample dataset from the course platform.
        \item Load the dataset using Pandas and split it into training and testing sets.
        \item Fit a classification model using scikit-learn (e.g., Logistic Regression, Decision Tree).
        \item Make predictions on the test set and compute the metrics listed above.
        \item Document your findings, including the confusion matrix and interpretation of each metric.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    Focus on:
    \begin{itemize}
        \item The importance of each metric in different scenarios (e.g., imbalanced datasets).
        \item The trade-offs between precision and recall (e.g., in medical diagnosis).
        \item How these metrics inform model improvement and selection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    This practical activity will:
    \begin{itemize}
        \item Reinforce your understanding of evaluation metrics in machine learning.
        \item Help you reflect on how accurately measuring performance impacts decision-making and model refinements in real-world applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Overview}
    In this chapter, we explored the significance of evaluation metrics in machine learning (ML). Metrics allow us to judge the effectiveness of our models. 
    Selecting the right metrics is crucial for assessing model performance and ensuring ML applications meet desired objectives.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Key Points}
    \begin{enumerate}
        \item \textbf{Different Types of Metrics}:
            \begin{itemize}
                \item \textbf{Classification Metrics}:
                    \begin{itemize}
                        \item \textbf{Accuracy}: Percentage of correct predictions.
                        \item \textbf{Precision}: Proportion of true positives in predicted positives.
                        \item \textbf{Recall (Sensitivity)}: Measures how well the model finds all relevant cases.
                        \item \textbf{F1 Score}: Harmonic mean of precision and recall.
                    \end{itemize}
                
                \item \textbf{Regression Metrics}:
                    \begin{itemize}
                        \item \textbf{Mean Absolute Error (MAE)}: Average absolute differences between predicted and actual values.
                        \item \textbf{Root Mean Square Error (RMSE)}: Sensitive to larger errors.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Selection of Metrics}:
            Understanding the problem domain is crucial for choosing the right metrics based on application specifics.
        \item \textbf{Communication of Results}:
            Clear communication is vital for stakeholders and using visual aids like confusion matrices and ROC curves.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Conclusion}
    \begin{block}{Conclusion}
        Understanding evaluation metrics is fundamental in machine learning. It helps fine-tune models and align outputs with user expectations.
    \end{block}
    \textbf{Next Steps:} Engage in an open discussion about your experiences with evaluation metrics and raise any questions you might have!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussion - Introduction}
    \begin{block}{Introduction}
        As we wrap up our exploration of evaluation metrics for machine learning models, this slide is dedicated to fostering an interactive dialogue. 
        It's a space for you to voice your thoughts, ask for clarifications, and delve deeper into any topics we've covered. 
        Remember, understanding these metrics is crucial for effectively assessing and improving model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussion - Key Concepts Recap}
    \begin{itemize}
        \item \textbf{Evaluation Metrics Importance}:
            \begin{itemize}
                \item Helps in determining model performance and effectiveness.
                \item Aids in comparing different models and selecting the best one.
            \end{itemize}
        
        \item \textbf{Common Metrics to Discuss}:
            \begin{itemize}
                \item \textbf{Accuracy}: Proportion of true results among total cases.
                \item \textbf{Precision}: Ratio of correctly predicted positives to total predicted positives.
                \item \textbf{Recall (Sensitivity)}: Ratio of correctly predicted positives to actual positives.
                \item \textbf{F1 Score}: Harmonic mean of precision and recall for imbalanced datasets.
                \item \textbf{ROC-AUC}: Graphical representation of true positive rate vs false positive rate.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussion - Example Scenario}
    \begin{block}{Example Scenario for Discussion}
        Consider a binary classification problem where we want to identify whether an email is spam or not. 
        Using various evaluation metrics:
        \begin{itemize}
            \item If our model has an accuracy of 90\%, does that automatically mean it’s good? 
            \item Discuss real scenarios—if 90\% of emails are not spam, the model could be correctly classifying most emails, yet failing to catch spam effectively.
            \item Here, measuring precision and recall could provide deeper insights into model quality.
        \end{itemize}
    \end{block}

    \begin{block}{Questions to Spark Dialogue}
        \begin{itemize}
            \item What challenges have you faced when choosing the right evaluation metric for your specific problem?
            \item How do you think metrics might differ in importance across various domains (e.g., healthcare vs. finance)?
            \item Can you think of a scenario where high accuracy might mask a model's poor performance? 
            \item How might advancements in neural network designs change how we evaluate models?
        \end{itemize}
    \end{block}
\end{frame}


\end{document}