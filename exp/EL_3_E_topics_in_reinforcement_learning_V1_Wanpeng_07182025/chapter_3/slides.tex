\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Dynamic Programming - Overview}
    \begin{block}{Overview of Dynamic Programming}
        Dynamic Programming (DP) is a computational method for solving complex problems by:
        \begin{itemize}
            \item Breaking them down into simpler subproblems.
            \item Storing solutions to these subproblems to avoid redundancy.
        \end{itemize}
        This process significantly enhances efficiency as it is especially useful in scenarios with overlapping subproblems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Dynamic Programming - MDPs}
    \begin{block}{Relevance in Solving Markov Decision Processes (MDPs)}
        MDPs provide a framework for decision-making in uncertain environments and consist of:
        \begin{itemize}
            \item \textbf{States (S)}: Situations requiring decisions.
            \item \textbf{Actions (A)}: Possible choices in each state.
            \item \textbf{Transition Probabilities (P)}: Likelihood of moving between states given an action.
            \item \textbf{Rewards (R)}: Benefits received on transitioning from one state to another.
            \item \textbf{Policies ($\pi$)}: Strategies that determine action in each state.
        \end{itemize}
        Dynamic programming techniques for solving MDPs include:
        \begin{enumerate}
            \item \textbf{Value Iteration}: Updates state values based on expected future rewards.
            \item \textbf{Policy Iteration}: Alternates evaluation and improvement of policies until the optimal policy is found.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Dynamic Programming - Key Objectives}
    \begin{block}{Key Objectives of the Session}
        \begin{enumerate}
            \item Understand the concepts and principles of Dynamic Programming.
            \item Explore applications of dynamic programming in MDPs.
            \item Discuss and implement algorithms such as Value Iteration and Policy Iteration.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example Problem}
        \textbf{Consider a simple MDP with states} $S = \{s_0, s_1, s_2\}$:
        \begin{itemize}
            \item Move from state $s_0$ to $s_1$: reward = +5.
            \item Move from state $s_1$ to $s_2$: reward = +10.
            \item Transition from $s_2$ to $s_0$: reward = 0.
        \end{itemize}
        Using dynamic programming, we can calculate expected total rewards from states, informing optimal policy.
    \end{block}

    \begin{lstlisting}[language=Python]
# Pseudocode for Value Iteration
# Initialize Value function V(s) for all states
V = {s_0: 0, s_1: 0, s_2: 0} 

# Iterate until convergence
for each state in S:
    V[state] = max(expected_reward(state, action, V))
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{What is Dynamic Programming? - Overview}
    \begin{block}{Definition}
        Dynamic Programming (DP) is a powerful algorithmic technique used to solve complex problems efficiently by breaking them down into simpler, overlapping subproblems. It is particularly effective in optimizing problems involving decisions over time.
    \end{block}
    
    \begin{block}{Fundamental Principle}
        The core idea of DP is to divide a problem into smaller, manageable subproblems, solve each subproblem once, and store their solutions to avoid redundant computations.
    \end{block}
\end{frame}

\begin{frame}[fragile]{What is Dynamic Programming? - Key Concepts}
    \begin{itemize}
        \item \textbf{Optimal Substructure:} The optimal solution of the problem can be constructed from the optimal solutions of its subproblems.
        \item \textbf{Overlapping Subproblems:} The problem can be broken into subproblems which are reused several times.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{What is Dynamic Programming? - Example: Fibonacci Sequence}
    \begin{block}{Fibonacci Sequence}
        A classic example of DP is the computation of Fibonacci numbers. The naive recursive solution recalculates Fibonacci(n) multiple times. Using DP, we can store previously computed values:
    \end{block}

    \begin{itemize}
        \item \textbf{Recursive Approach (Inefficient):}
        \begin{equation*}
            \text{Fibonacci}(n) = \text{Fibonacci}(n-1) + \text{Fibonacci}(n-2)
        \end{equation*}
        
        \item \textbf{Dynamic Programming Approach (Efficient):}
        \begin{lstlisting}[language=Python]
def fibonacci(n):
    fib = [0] * (n + 1)
    fib[1] = 1
    for i in range(2, n + 1):
        fib[i] = fib[i - 1] + fib[i - 2]
    return fib[n]
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{What is Dynamic Programming? - Key Points}
    \begin{itemize}
        \item \textbf{Performance:} DP reduces the time complexity from exponential in naive recursive implementations to polynomial time in most cases.
        \item \textbf{Applications:} Widely used in various domains, including computer science, economics, and bioinformatics for problems like shortest paths, knapsack problem, and sequence alignment.
        \item \textbf{Strategies:} The two primary strategies in DP are:
        \begin{itemize}
            \item Memoization (top-down approach)
            \item Tabulation (bottom-up approach)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{What is Dynamic Programming? - Conclusion}
    \begin{block}{Conclusion}
        Dynamic Programming is a vital concept that provides powerful techniques to solve optimization problems efficiently. Understanding its principles of optimal substructure and overlapping subproblems is crucial as we delve deeper into applications, including reinforcement learning and algorithm design.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Dynamic Programming in Reinforcement Learning}
    \begin{block}{Overview}
        Dynamic Programming (DP) is integral to the field of Reinforcement Learning (RL), providing powerful tools for optimizing decision-making processes in uncertain environments. DP enhances our ability to understand and model the decision-making policies of agents by breaking down complex problems into manageable subproblems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Basics}
    \begin{itemize}
        \item In RL, an agent learns to make decisions by interacting with an environment to maximize cumulative rewards over time.
        \item The agent observes states, takes actions, and receives rewards based on its decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Dynamic Programming}
    \begin{block}{Dynamic Programming Techniques}
        \begin{itemize}
            \item DP facilitates the evaluation and improvement of policies.
            \item Techniques: Policy Evaluation and Policy Iteration.
            \item Efficiently identifies optimal policies in Markov Decision Processes (MDPs).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Applications of Dynamic Programming}
    \begin{enumerate}
        \item \textbf{Policy Evaluation}
            \begin{itemize}
                \item Assessing each state value under a given policy using the Bellman equation:
                \begin{equation}
                    V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s,a)\left[R(s,a,s') + \gamma V^\pi(s')\right]
                \end{equation}
                \item Here, \( V^\pi(s) \) is the expected return from state \( s \) under policy \( \pi \), and \( \gamma \) is the discount factor.
            \end{itemize}

        \item \textbf{Policy Improvement}
            \begin{itemize}
                \item Developing a better policy by leveraging the value estimates.
                \item If a state’s value indicates a more rewarding action, the policy is updated, refining the decision-making process.
            \end{itemize}

        \item \textbf{Value Iteration}
            \begin{itemize}
                \item A method to converge to the optimal policy by iteratively updating the value function.
                \item Implementation:
                \begin{lstlisting}
                for s in states:
                    V[s] = max(sum(P(s'|s,a) * (R(s,a,s') + gamma * V[s']) for s' in states) for a in actions)
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of Using Dynamic Programming in RL}
    \begin{itemize}
        \item \textbf{Efficiency:} 
            \begin{itemize}
                \item DP allows systematic exploration of state-action spaces, avoiding redundant calculations.
                \item Improves convergence times.
            \end{itemize}
        \item \textbf{Optimality:} 
            \begin{itemize}
                \item Guarantees that the results, such as the learned policy, are optimal under the MDP framework.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Dynamic Programming significantly enhances decision-making processes in Reinforcement Learning. It equips agents with the ability to evaluate, refine, and optimize their policies, essential for effective learning in complex, dynamic environments. Understanding these applications lays the groundwork for comprehending more advanced topics in RL.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in MDPs - Overview}
    \begin{block}{What is an MDP?}
        A Markov Decision Process (MDP) is a mathematical framework used to describe decision-making in scenarios where outcomes are partly random and partly under the control of a decision-maker.
    \end{block}
    \begin{itemize}
        \item Underpins many algorithms in dynamic programming and reinforcement learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in MDPs - Components}
    \begin{enumerate}
        \item \textbf{States (S)}: Represents a specific situation for a decision-maker.
        \item \textbf{Actions (A)}: Decisions made by the agent to change the state.
        \item \textbf{Rewards (R)}: The immediate payoff from transitioning between states.
        \item \textbf{Transition Probabilities (P)}: Likelihood of transitioning from one state to another given an action.
        \item \textbf{Policies ($\pi$)}: A strategy defining the actions to take in each state.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in MDPs - Detailed Components}
    \begin{block}{1. States (S)}
        \begin{itemize}
            \item \textbf{Example:} Each arrangement of pieces in a chess game.
            \item \textbf{Key Point:} Must encapsulate all information to make decisions.
        \end{itemize}
    \end{block}

    \begin{block}{2. Actions (A)}
        \begin{itemize}
            \item \textbf{Example:} Moving directions in navigation tasks.
            \item \textbf{Key Point:} Actions depend on the current state.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in MDPs - Rewards and Transition Probabilities}
    \begin{block}{3. Rewards (R)}
        \begin{itemize}
            \item \textbf{Example:} Scoring points in a game after a successful move.
            \item \textbf{Key Point:} Aim is to maximize total rewards over time.
        \end{itemize}
    \end{block}

    \begin{block}{4. Transition Probabilities (P)}
        \begin{itemize}
            \item \textbf{Example:} In a weather model, transitioning from "Rainy" to "Dry."
            \item \textbf{Key Point:} Understanding transition probabilities aids in future state predictions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in MDPs - Policies and Summary}
    \begin{block}{5. Policies ($\pi$)}
        \begin{itemize}
            \item \textbf{Example:} If traffic light is red, stop; if green, go.
            \item \textbf{Key Point:} The optimal policy maximizes expected rewards.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        \begin{itemize}
            \item MDPs provide a structured way to model decision scenarios in uncertain environments.
            \item The interaction between components is crucial for optimizing policies through methods like value iteration.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Overview}
    \begin{equation}
        V(s) = E \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) | s_0 = s \right]
    \end{equation}
    \begin{itemize}
        \item Where $E$ denotes expectation, $\gamma$ the discount factor ($0 < \gamma < 1$), and $R$ is the reward function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming vs. Other Techniques - Overview}
    \begin{itemize}
        \item Dynamic Programming (DP), Monte Carlo (MC) methods, and Temporal Difference (TD) learning are key techniques in reinforcement learning.
        \item Understanding their differences will help in selecting the right approach for solving specific problems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming}
    \begin{block}{Definition}
        Dynamic Programming is a method for solving complex problems by breaking them down into simpler subproblems, utilizing the principle of optimality.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Characteristics:}
            \begin{itemize}
                \item Requires a complete model of the environment (transition probabilities and rewards).
                \item Uses a systematic approach for calculating value functions and policies.
            \end{itemize}
        \item \textbf{Example:} Value Iteration, where the value of states is updated iteratively until convergence.
    \end{itemize}
    \begin{equation}
        V(s) = \max_a \sum_{s'} P(s'|s,a)(R(s,a,s') + \gamma V(s'))
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Monte Carlo Methods and Temporal Difference Learning}
    \begin{block}{Monte Carlo Methods}
        \begin{itemize}
            \item \textbf{Definition:} Monte Carlo methods rely on random sampling to obtain numerical results.
            \item \textbf{Key Characteristics:}
                \begin{itemize}
                    \item Does not require knowledge of the environment's model.
                    \item Useful for episodic tasks where complete episodes can be observed.
                \end{itemize}
            \item \textbf{Example:} Estimating the value of a state by averaging returns from multiple episodes.
        \end{itemize}
        \begin{equation}
            V(s) \approx \frac{1}{N} \sum_{i=1}^N G_i
        \end{equation}
    \end{block}
    \begin{block}{Temporal Difference Learning}
        \begin{itemize}
            \item \textbf{Definition:} Combines ideas from DP and MC methods by learning value estimates based on other learned estimates.
            \item \textbf{Key Characteristics:}
                \begin{itemize}
                    \item Updates value estimates after each step using bootstrapping.
                    \item Requires less data than MC and can work with ongoing tasks.
                \end{itemize}
            \item \textbf{Example:} Q-Learning, where the action-value function is learned directly from experience.
        \end{itemize}
        \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Introduction}
    \begin{block}{Overview}
        The Bellman Equations are foundational in dynamic programming and reinforcement learning, describing the relationship between the value of a state and the future states' values.
    \end{block}
    They provide a recursive approach for computing optimal policies in decision-making processes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Forms}
    \begin{itemize}
        \item \textbf{Bellman Expectation Equation}
          \begin{equation}
              V_\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a)\left[R(s, a, s') + \gamma V_\pi(s')\right]
          \end{equation}
          \begin{itemize}
              \item $V_\pi(s)$: Value of state $s$ under policy $\pi$.
              \item $P(s'|s, a)$: State transition probability.
              \item $R(s, a, s')$: Immediate reward.
              \item $\gamma$: Discount factor.
          \end{itemize}

        \item \textbf{Bellman Optimality Equation}
          \begin{equation}
              V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a)\left[R(s, a, s') + \gamma V^*(s')\right]
          \end{equation}
          \begin{itemize}
              \item $V^*(s)$: Optimal value of state $s$ maximizing future rewards.
          \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Key Concepts}
    \begin{itemize}
        \item \textbf{Value Iteration}:
            \begin{itemize}
                \item An iterative method to compute optimal values using the Bellman Optimality equation until convergence.
            \end{itemize}
        \item \textbf{Policy Iteration}:
            \begin{itemize}
                \item Involves policy evaluation and improvement:
                    \begin{itemize}
                        \item Evaluate the current policy.
                        \item Improve the policy based on the value function.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
    
    \begin{block}{Significance}
        Bellman equations are crucial for algorithms in dynamic programming, aiding in optimal decision-making in uncertain environments, relevant in fields like robotics, finance, and AI.
    \end{block}
    
    \begin{block}{Example}
        In a grid world scenario, the Bellman Equations help evaluate values of positions based on future rewards, guiding the agent towards optimal paths.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Algorithm - Overview}
    \begin{block}{Definition}
        Value Iteration is a fundamental algorithm in dynamic programming used for determining the optimal policy in Markov Decision Processes (MDPs). It repeatedly updates the value estimates of each state until they converge to the optimal values.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Algorithm - Key Concepts}
    \begin{itemize}
        \item \textbf{State Value Function (V)}: Represents the expected long-term return for each state. $V(s)$ denotes the value of state $s$.
        \item \textbf{Bellman Optimality Equation}: 
        \begin{equation}
            V(s) = \max_{a} \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V(s')]
        \end{equation}
        where $P(s'|s, a)$ is the transition probability, $R(s, a, s')$ is the reward, and $\gamma$ is the discount factor.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration - Steps and Pseudo-code}
    \begin{enumerate}
        \item \textbf{Initialization}: Choose an arbitrary value function $V_0(s)$ for all states $s$.
        \item \textbf{Value Update Loop}:
        \begin{equation}
            V_{k+1}(s) = \max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V_k(s') \right]
        \end{equation}
        \item \textbf{Convergence Check}: Repeat until:
        \begin{equation}
            |V_{k+1}(s) - V_k(s)| < \epsilon
        \end{equation}
        \item \textbf{Extract Optimal Policy}:
        \begin{equation}
            \pi^*(s) = \arg\max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V(s') \right]
        \end{equation}
    \end{enumerate}
    \begin{block}{Pseudo-code}
        \begin{lstlisting}
Initialize V(s) arbitrarily for all states s
Repeat
    Δ ← 0
    For each state s do
        v ← V(s)
        V(s) ← max_a Σ P(s'|s, a) [R(s, a, s') + γV(s')]
        Δ ← max(Δ, |v - V(s)|)
    End For
Until Δ < θ (small threshold for convergence)

For each state s do
    π(s) ← argmax_a Σ P(s'|s, a) [R(s, a, s') + γV(s')]
End For
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Flowchart of Value Iteration}
    \begin{itemize}
        \item Start
        \item Initialize Values
        \item Update Values
        \item Check Convergence
        \item (Yes: Stop) / (No: Repeat)
        \item Extract Policy
        \item End
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration - Key Points and Applications}
    \begin{itemize}
        \item *Convergence*: Value iteration guarantees convergence to the optimal value function provided $\gamma < 1$.
        \item *Efficiency*: Value iteration can find optimal policies, but may require many iterations in large state spaces.
        \item *Comparison*: Value iteration updates value functions directly, while policy iteration alternates between policy evaluation and improvement.
    \end{itemize}
    \begin{block}{Example Application}
        Imagine an MDP for a robot navigating a grid. Each cell represents a state with variable rewards based on actions (up, down, left, right). Value iteration computes the optimal navigation strategy to maximize expected rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Algorithm - Introduction}
    The Policy Iteration Algorithm is a dynamic programming technique used to find the optimal policy for Markov Decision Processes (MDPs). Unlike Value Iteration, which computes the value function directly, Policy Iteration improves a given policy iteratively until it reaches optimality.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Policy}: A policy \( \pi \) is a mapping from states to actions, guiding the decision-making process in an MDP.
        \item \textbf{Value Function}: For a given policy \( \pi \), the value function \( V^\pi(s) \) represents the expected return (cumulative reward) from state \( s \) when following policy \( \pi \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of Policy Iteration}
    \begin{enumerate}
        \item \textbf{Initialize the Policy}: Start with an arbitrary policy \( \pi_0 \) for all states.
        
        \item \textbf{Policy Evaluation}:
        \begin{equation}
        V^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V^\pi(s')
        \end{equation}
        This step continues until \( V^\pi \) converges.
        
        \item \textbf{Policy Improvement}:
        \begin{equation}
        \pi(s) = \arg\max_a \sum_{s'} P(s'|s, a) V^\pi(s')
        \end{equation}
        If the policy did not change, the algorithm has converged.
        
        \item \textbf{Iterate}: Repeat the evaluation and improvement steps until the policy is stable.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: A Simple Gridworld}
    \begin{itemize}
        \item \textbf{States}: Cells in a 3x3 grid.
        \item \textbf{Actions}: Move Up, Down, Left, Right.
        \item \textbf{Rewards}: Positive reward for reaching the goal state; negative for hitting walls.
        \item \textbf{Iterations}: Start with an arbitrary policy, evaluate its value, improve the policy based on the value until the optimal policy is found.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Policy Iteration}
    \begin{itemize}
        \item \textbf{Guaranteed Convergence}: Always converges to the optimal policy.
        \item \textbf{Efficiency}: May require fewer iterations than Value Iteration depending on the system dynamics.
        \item \textbf{Clarity}: Separates the evaluation and improvement steps, making it easier to understand and implement.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Execution Process Diagram}
    \begin{enumerate}
        \item \textbf{Initialization}: Start with an initial policy.
        \item \textbf{Evaluation Cycle}: Compute the value function for the current policy.
        \item \textbf{Improvement Cycle}: Update the policy based on the computed values.
        \item \textbf{Convergence Check}: Check if the policy has changed; if not, stop.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Points}
    \begin{itemize}
        \item Policy Iteration is an effective method for determining optimal policies in MDPs.
        \item It emphasizes clarity through distinct evaluation and improvement phases, ultimately leading to policy convergence.
        \item Remember to clearly define your policy and value functions.
        \item Be patient with convergence; some policies may take longer to evaluate than others.
        \item Use examples to visualize and reinforce understanding of the algorithm's steps.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Dynamic Programming in MDPs}
    \begin{itemize}
        \item Dynamic Programming (DP) is a technique for solving complex problems.
        \item It breaks problems into simpler subproblems.
        \item In Markov Decision Processes (MDPs), DP is used for optimal decision-making under uncertainty.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{block}{Markov Decision Process (MDP)}
        An MDP is defined by:
        \begin{itemize}
            \item A set of states $S$
            \item A set of actions $A$
            \item A transition model $P(s' | s, a)$
            \item A reward function $R(s, a)$
        \end{itemize}
    \end{block}
    
    \begin{block}{Value Function}
        Represents maximum expected future rewards:
        \[
        V_\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right]
        \]
        where $\gamma$ is the discount factor.
    \end{block}
    
    \begin{block}{Optimal Policy}
        A policy that maximizes the value function for all states.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration and Policy Iteration}
    \begin{itemize}
        \item \textbf{Value Iteration}:
        \begin{enumerate}
            \item Initialize $V(s) = 0$ for all states $s$.
            \item Update as follows:
            \[
            V(s) \leftarrow \max_{a \in A}\left( R(s, a) + \gamma \sum_{s'}P(s'|s,a)V(s') \right)
            \]
            \item Repeat until convergence.
        \end{enumerate}
    
        \item \textbf{Policy Iteration}:
        \begin{enumerate}
            \item Initialize random policy $\pi$.
            \item Calculate $V^\pi(s)$ for policy evaluation.
            \item Update policy:
            \[
            \pi'(s) = \arg\max_{a \in A}\left( R(s, a) + \gamma \sum_{s'} P(s'|s,a)V^\pi(s') \right)
            \]
            \item Repeat until no change.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Grid World Problem}
    \begin{itemize}
        \item A simple 3x3 grid where the objective is to reach the bottom right corner while avoiding obstacles.
        \item Rewards are assigned for reaching the goal and penalties for obstacles.
          
        \item \textbf{Using Value Iteration}:
        \begin{itemize}
            \item Initialize all values to 0.
            \item Update values based on actions until stabilization.
        \end{itemize}

        \item \textbf{Using Policy Iteration}:
        \begin{itemize}
            \item Start with a random policy (e.g., always move right).
            \item Evaluate the value function for the current policy.
            \item Update the policy based on calculated values.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item DP provides a structured approach for decision-making under uncertainty.
        \item Understanding Value and Policy Iteration aids in finding optimal solutions.
        \item Convergence of the algorithms depends on the discount factor $\gamma$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Dynamic Programming is crucial for solving MDPs.
        \item It systematically computes strategies through iterative updates.
        \item These processes can be applied to various real-world scenarios such as robotics and finance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming}
    \begin{block}{Overview of Dynamic Programming}
        Dynamic Programming (DP) is a powerful algorithmic technique used to solve complex problems by breaking them down into simpler subproblems. It is particularly effective for optimization problems where the solution can be constructed efficiently by combining solutions to overlapping subproblems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in Applying Dynamic Programming}
    \begin{enumerate}
        \item \textbf{State Space Complexity}
        \item \textbf{Computational Demands}
        \item \textbf{Memory Limitations}
        \item \textbf{Complex Transition Functions}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space Complexity}
    \begin{itemize}
        \item \textbf{Definition}: The total number of distinct states that a DP algorithm may need to explore, which can grow exponentially with the input size.
        \item \textbf{Example}: 
            \begin{itemize}
                \item Fibonacci sequence calculation reduces time complexity from exponential to linear.
                \item Naive tabulation for large \( n \) may run into memory issues.
            \end{itemize}
        \item \textbf{Formula}: 
            \[
            F(n) = F(n-1) + F(n-2)
            \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Computational Demands}
    \begin{itemize}
        \item \textbf{Definition}: Extensive computation and longer execution times, especially with large states and choices per state.
        \item \textbf{Example}:
            \begin{itemize}
                \item The Knapsack Problem (time complexity: \( O(n \times W) \)).
                \item Large computational overhead as \( n \) and \( W \) increase.
            \end{itemize}
        \item \textbf{Illustration}:
            \begin{itemize}
                \item Item values: \([v_1, v_2, \ldots, v_n]\)
                \item Weights: \([w_1, w_2, \ldots, w_n]\)
                \item Maximum Weight: \( W \)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Memory Limitations and Transition Functions}
    \begin{itemize}
        \item \textbf{Memory Limitations}:
            \begin{itemize}
                \item DP approaches may require significant memory, leading to issues in constrained environments.
                \item Example: Edit Distance calculations can become large if both strings are lengthy.
            \end{itemize}
        \item \textbf{Complex Transition Functions}:
            \begin{itemize}
                \item Defining accurate transitions can complicate implementation.
                \item Example: Shortest Path problem complexity increases with graph edges and costs.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Balance Utilization vs. Complexity}: Optimize space and time complexity.
        \item \textbf{Iterative vs. Recursive Implementation}: Choose bottom-up or top-down approaches based on the problem.
        \item \textbf{Optimal Substructure}: Ensure the problem exhibits this property for DP applicability.
    \end{itemize}
    \begin{block}{Conclusion}
        Recognizing challenges early can lead to better DP strategies and improved efficiency in real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming in Reinforcement Learning}
    \begin{block}{Introduction}
        Dynamic Programming (DP) provides a structured methodology for solving decision-making processes in Reinforcement Learning (RL), particularly in the context of Markov Decision Processes (MDPs). 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming Techniques}
    \begin{itemize}
        \item \textbf{Value Iteration}:
        \begin{enumerate}
            \item Initialize \( V(s) \) arbitrarily for all states \( s \).
            \item Update using:
            \begin{equation}
            V_{new}(s) = \max_a \sum_{s'} P(s' | s, a) \left[R(s, a, s') + \gamma V(s')\right]
            \end{equation}
            \item Continue until \( V_{new}(s) \) converges.
        \end{enumerate}
        
        \item \textbf{Policy Iteration}:
        \begin{enumerate}
            \item Start with an arbitrary policy.
            \item Evaluate \( V_\pi(s) \) for the current policy.
            \item Update policy: 
            \begin{equation}
            \pi'(s) = \arg\max_a Q(s, a)
            \end{equation}
            \item Repeat until stabilization.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration with Other Learning Algorithms}
    \begin{itemize}
        \item DP can improve Monte Carlo and Temporal Difference (TD) learning.
        \item \textbf{Examples of Integrated Approaches}:
        \begin{itemize}
            \item \textbf{Q-Learning}:
            \begin{equation}
            Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
            \end{equation}
            \item \textbf{SARSA}:
            Updates action-value based on actions taken, balancing exploration and exploitation.
        \end{itemize}
        
        \item \textbf{Advantages of DP in RL}:
        \begin{itemize}
            \item Guarantees optimality if conditions are met.
            \item Systematic policy improvement.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Key Concepts}
    \begin{enumerate}
        \item \textbf{Dynamic Programming (DP)}:
        \begin{itemize}
            \item Breaks complex problems into simpler subproblems.
            \item Stores solutions for future use, leveraging overlapping subproblems and optimal substructure.
        \end{itemize}
        
        \item \textbf{Markov Decision Processes (MDPs)}:
        \begin{itemize}
            \item Models decision-making with partly random and partly controlled outcomes.
            \item Key components:
            \begin{itemize}
                \item \textbf{States (S)}: All possible situations.
                \item \textbf{Actions (A)}: Possible actions the agent can take.
                \item \textbf{Transition Function (P)}: Probability of reaching the next state.
                \item \textbf{Rewards (R)}: Immediate gain from state transitions.
                \item \textbf{Discount Factor ($\gamma$)}: Present value of future rewards.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Importance of DP in Reinforcement Learning}:
        \begin{itemize}
            \item Techniques like Value Iteration and Policy Iteration are fundamental for computing optimal policies and value functions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Example}
    \begin{block}{Value Iteration Algorithm}
        1. Initialize the value function arbitrarily.
        2. Update using the Bellman equation:
        \begin{equation}
        V_{k+1}(s) = R(s) + \gamma \sum_{s'} P(s' | s, a)V_k(s')
        \end{equation}
        where:
        \begin{itemize}
            \item $s$: the current state
            \item $R(s)$: reward for state $s$
            \item $P(s' | s, a)$: transition probability to next state $s'$
        \end{itemize}
        3. Repeat until convergence.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Summary}
    \begin{itemize}
        \item DP is crucial for efficient algorithms solving optimal policies in MDPs, impacting AI and machine learning.
        \item DP complements other techniques like Monte Carlo and temporal difference learning for scalability.
        \item Applications extend to robotics, finance, and operations research.
    \end{itemize}
    
    \textbf{Conclusion:} Dynamic programming enables effective management of uncertainties in decision-making, paving the way for advanced AI topics and intelligent agent development.
\end{frame}


\end{document}