\frametitle{Value Iteration - Steps and Pseudo-code}
    \begin{enumerate}
        \item \textbf{Initialization}: Choose an arbitrary value function $V_0(s)$ for all states $s$.
        \item \textbf{Value Update Loop}:
        \begin{equation}
            V_{k+1}(s) = \max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V_k(s') \right]
        \end{equation}
        \item \textbf{Convergence Check}: Repeat until:
        \begin{equation}
            |V_{k+1}(s) - V_k(s)| < \epsilon
        \end{equation}
        \item \textbf{Extract Optimal Policy}:
        \begin{equation}
            \pi^*(s) = \arg\max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V(s') \right]
        \end{equation}
    \end{enumerate}
    \begin{block}{Pseudo-code}
        \begin{lstlisting}
Initialize V(s) arbitrarily for all states s
Repeat
    Δ ← 0
    For each state s do
        v ← V(s)
        V(s) ← max_a Σ P(s'|s, a) [R(s, a, s') + γV(s')]
        Δ ← max(Δ, |v - V(s)|)
    End For
Until Δ < θ (small threshold for convergence)

For each state s do
    π(s) ← argmax_a Σ P(s'|s, a) [R(s, a, s') + γV(s')]
End For
        \end{lstlisting}
    \end{block}
