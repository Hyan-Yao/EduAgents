\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Week 10: Policy Gradient Methods]{Week 10: Policy Gradient Methods}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Policy Gradient Methods?}
    \begin{block}{Definition}
        Policy Gradient Methods are algorithms in Reinforcement Learning (RL) that optimize the policy directly through gradient ascent, rather than learning a value function to derive a policy.
    \end{block}
    
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Policy}: A mapping from states to actions (e.g., \( \pi(a|s; \theta) \)).
            \item \textbf{Gradient Ascent}: Updating policy parameters \( \theta \).
            \item \textbf{Return}: Cumulative reward over time, represented as \( R_t \).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Policy Gradient Methods}
    \begin{enumerate}
        \item \textbf{Direct Optimization}:
            \begin{itemize}
                \item Learn the policy directly, beneficial in high-dimensional action spaces and complex environments.
            \end{itemize}
        
        \item \textbf{Handling Continuous Action Spaces}:
            \begin{itemize}
                \item Effective in environments with continuous actions, applicable in robotics and control tasks.
            \end{itemize}
        
        \item \textbf{Improving Sample Efficiency}:
            \begin{itemize}
                \item Uses gradients of the expected return to enhance learning efficiency.
            \end{itemize}
        
        \item \textbf{Exploration and Exploitation}:
            \begin{itemize}
                \item Supports exploration through stochastic policies, simplifying environment exploration.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Theorem}
    \begin{block}{Formula}
        The policy gradient theorem is given by:
        \begin{equation}
            \nabla J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_{\theta} \log \pi_\theta(a|s) Q(s,a) \right]
        \end{equation}
        Where:
        \begin{itemize}
            \item \( J(\theta) \): Expected return
            \item \( Q(s, a) \): Action-value function for expected future returns
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Policy Gradient Methods}
    \begin{enumerate}
        \item \textbf{REINFORCE}: 
            \begin{itemize}
                \item A Monte Carlo method updating policy parameters after each episode based on total returns.
            \end{itemize}
        
        \item \textbf{Actor-Critic}:
            \begin{itemize}
                \item Combines policy gradient (actor) with value function (critic) methods for stable training.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Prioritizes direct policy optimization.
            \item Particularly useful for continuous actions and complex dynamics.
            \item Importance of gradient derivation for effective learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Policy Gradient}
    
    \begin{block}{Introduction to Policy Gradient Theorem}
        The Policy Gradient Theorem provides a framework for optimizing policy-based methods in Reinforcement Learning. It enables agents to directly parameterize and optimize their policies, unlike value-based methods.
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance in Reinforcement Learning}
    
    \begin{itemize}
        \item \textbf{Direct Optimization of Policies:} Enables effective learning in high-dimensional and continuous action spaces.
        \item \textbf{Stochastic Policies:} Useful for exploration, allowing agents to select actions randomly for better reward discovery.
        \item \textbf{Handling Large Action Spaces:} Effective in environments with large or continuous actions, avoiding the need for exhaustive evaluation.
        \item \textbf{On-Policy Learning:} Fosters learning from actions actually taken, enhancing efficiency in dynamic environments.
    \end{itemize}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Policy Gradient Theorem}

    The theorem expresses the gradient of the expected return with respect to the policy parameters:
    
    \begin{equation}
        \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \nabla \log \pi_{\theta}(a | s) A(s, a) \right]
    \end{equation}

    Where:
    \begin{itemize}
        \item \( J(\theta) \) = expected return
        \item \( \tau \) = trajectory (sequence of states and actions)
        \item \( \pi_{\theta}(a | s) \) = policy probability of action \( a \) in state \( s \)
        \item \( A(s, a) \) = advantage function
    \end{itemize}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Policy gradients maximize expected return directly.
            \item They encourage exploration through stochastic policies.
            \item Gradients are used for effective updates of policy parameters.
        \end{itemize}
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Policy Gradient Methods - Policies}
    \begin{block}{Policies}
        A **policy** is a strategy used by an agent to determine the next action based on the current state.
    \end{block}

    \begin{itemize}
        \item **Deterministic Policy**: A function that maps each state to a specific action:
        \[
        \pi(s) = a
        \]
        \item **Stochastic Policy**: A function that provides a probability distribution over actions:
        \[
        \pi(a|s) = P(A = a | S = s)
        \]
    \end{itemize}

    \begin{block}{Example}
        In a game environment, a policy could dictate that when in state 'A', the agent has a 70\% chance to move left and a 30\% chance to move right.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Policy Gradient Methods - Gradients and Likelihood Ratios}
    \begin{block}{Gradients}
        In policy gradient methods, the **gradient** is a vector that represents the direction and rate of change of our objective function concerning the policy parameters.
    \end{block}

    \begin{equation}
    \nabla J(\theta) = E_\pi \left[ \nabla \log \pi_\theta(a|s) Q^\pi(s, a) \right]
    \end{equation}
    where:
    \begin{itemize}
        \item \( J(\theta) \) is the objective,
        \item \( \pi_\theta(a|s) \) is the policy,
        \item \( Q^\pi(s, a) \) is the action-value function.
    \end{itemize}

    \begin{block}{Likelihood Ratios}
        The **likelihood ratio** shows how much more (or less) likely a particular action is under one policy compared to another.
    \end{block}

    \begin{equation}
    \frac{\pi_\theta(a|s)}{\pi_{\theta'}(a|s)}
    \end{equation}
    \begin{itemize}
        \item Helps in understanding how changes in the policy affect expected returns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Policy Gradient Methods - Objective Function}
    \begin{block}{Objective Function}
        The **objective function** in policy gradient methods quantifies the goal of optimizing the policy. 
        The aim is to maximize the expected total reward from following the policy.
    \end{block}

    \begin{equation}
    J(\theta) = E_\pi \left[ \sum_{t=0}^{T} \gamma^t r_t \right]
    \end{equation}
    where:
    \begin{itemize}
        \item \( r_t \) is the reward at time \( t \),
        \item \( \gamma \) is the discount factor (a value between 0 and 1) that prioritizes immediate rewards over distant ones.
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item The objective function represents the cumulative reward.
            \item Optimizing this function guides the agent towards better actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Algorithm - Introduction}
    % Introduction to Policy Gradient Algorithms
    \begin{block}{Overview}
        Policy gradient methods are a class of reinforcement learning algorithms that optimize the policy directly. They focus on improving the policy in the direction that maximizes expected rewards.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Policy ($\pi$)}: Strategy used by an agent to make decisions.
        \item \textbf{Objective Function ($J(\theta)$)}: Represents the expected cumulative reward obtained by following the policy $\pi$ parameterized by $\theta$.
        \item \textbf{Gradient ($\nabla$)}: Change in $J(\theta)$ with respect to the policy parameters $\theta$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Algorithm - Basic Overview}
    % Basic steps of the policy gradient algorithm
    \begin{enumerate}
        \item \textbf{Initialize:} Policy parameters ($\theta$) and settings for learning.
        \item \textbf{Collect Trajectories:} Use the current policy to generate data by interacting with the environment.
        \item \textbf{Calculate Returns:} Compute the return for each state-action pair during the episodes.
        \item \textbf{Compute Policy Gradient:} 
        \begin{equation}
            \nabla J(\theta) = E_{\tau \sim \pi_{\theta}} \left[ \nabla \log(\pi_{\theta}(a_t|s_t)) \cdot R_t \right]
        \end{equation}
        Where $R_t$ is the return at time $t$.
        \item \textbf{Update Policy Parameters:} 
        \begin{equation}
            \theta \leftarrow \theta + \alpha \nabla J(\theta)
        \end{equation}
        Where $\alpha$ is the learning rate.
        \item \textbf{Repeat:} Continue the process until convergence or predefined episodes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Algorithm - Pseudo-Code \& Flowchart}
    
    % Pseudo-code for Policy Gradient Algorithm
    \begin{block}{Pseudo-Code}
        \begin{lstlisting}[language=Python]
Initialize θ  # Policy parameters
For each episode:
    Generate trajectory τ by interacting with the environment
    For each time step t in τ:
        Compute return R_t
        Calculate policy gradient:
        ∇J(θ) += ∇log(π_θ(a_t|s_t)) * R_t
    Update policy parameters:
    θ = θ + α * ∇J(θ)
        \end{lstlisting}
    \end{block}

    % Flowchart Explanation
    \begin{block}{Flowchart Breakdown}
        1. \textbf{Start}: Initialize parameters.\\
        2. \textbf{Interact}: Gather data via policy.\\
        3. \textbf{Reward Calculation}: Compute returns from actions taken.\\
        4. \textbf{Gradient Calculation}: Derive the policy gradient using returns.\\
        5. \textbf{Update Policy}: Modify parameters based on gradient.\\
        6. \textbf{Repeat}: Loop until termination condition is met.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Policy Gradient Methods}
    % Overview of policy gradient methods
    Policy gradient methods are a class of algorithms in reinforcement learning that:
    \begin{itemize}
        \item Directly optimize the policy used by an agent.
        \item Differ from value-based methods that evaluate state-action pairs.
        \item Focus on maximizing the expected return through policy optimization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Popular Types of Policy Gradient Methods}
    Here we outline several popular policy gradient methods:
    \begin{enumerate}
        \item \textbf{REINFORCE}
        \item \textbf{Actor-Critic}
        \item \textbf{Advantage Actor-Critic (A2C)}
        \item \textbf{Proximal Policy Optimization (PPO)}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE}
    \begin{block}{Description}
        The simplest policy gradient method based on Monte Carlo methods. Updates the policy by calculating the gradient using complete episode returns.
    \end{block}
    \begin{block}{Key Formula}
        \[
        \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla \log \pi_\theta(a_t | s_t) \cdot G_t \right]
        \]
    \end{block}
    Where \( G_t \) is the return (cumulative reward) after time \( t \).

    \begin{block}{Example}
        Used in simple environments like CartPole, where the agent learns to balance a pole on a cart.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic}
    \begin{block}{Description}
        Combines value function approximators (critic) with policy optimization (actor) to provide feedback on the policy's performance.
    \end{block}
    \begin{itemize}
        \item \textbf{Actor}: Learns the policy \( \pi \) and suggests actions.
        \item \textbf{Critic}: Evaluates the action taken, estimating the value function \( V(s) \).
    \end{itemize}
    \begin{block}{Key Update Equation}
        \[
        \theta \gets \theta + \alpha \cdot \nabla \log \pi_\theta(a_t | s_t) \cdot (R_t - V(s_t))
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantage Actor-Critic (A2C) and PPO}
    \begin{block}{A2C Description}
        An extension of Actor-Critic that uses an advantage function to reduce variance in policy gradient estimates.
    \end{block}
    \begin{block}{Advantage Function}
        \[
        A(s_t, a_t) = Q(s_t, a_t) - V(s_t)
        \]
    \end{block}
    \begin{block}{PPO Description}
        A modern method balancing exploration and exploitation using a clipped surrogate objective.
    \end{block}
    \begin{block}{PPO Objective Function}
        \[
        L^{CLIP}(\theta) = \mathbb{E}_t \left[\min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Conclusion}
    \begin{itemize}
        \item Policy gradient methods effectively handle high-dimensional and continuous action spaces.
        \item Different methods have trade-offs (e.g., REINFORCE is high variance, PPO is stable).
    \end{itemize}
    \begin{block}{Conclusion}
        Familiarity with various policy gradient methods equips practitioners to select appropriate approaches for specific tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Resources}
    \begin{itemize}
        \item Implementations in TensorFlow or PyTorch for practical insights.
        \item Research papers on the development and application of policy gradient methods for deeper understanding.
    \end{itemize}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Policy Gradient Methods}
    Policy gradient methods are a category of reinforcement learning techniques that optimize a policy directly. Unlike value-based methods, which focus on estimating the value function to derive the policy, policy gradient methods utilize the policy's performance to determine the optimal actions to take in various states.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Advantages of Policy Gradient Methods - Part 1}
    \begin{enumerate}
        \item \textbf{Directly Optimizes the Policy}
        \begin{itemize}
            \item Focuses on maximizing the expected return.
            \item Updates policy parameters $\theta$ based on the gradient of performance:
            \begin{equation}
            \nabla J(\theta) = \mathbb{E}[\nabla \log \pi(a|s; \theta) \cdot R]
            \end{equation}
            where $R$ is the return, and $\pi(a|s; \theta)$ represents the probability of taking action $a$ in state $s$.
        \end{itemize}
        
        \item \textbf{Handles Large and High-Dimensional Action Spaces}
        \begin{itemize}
            \item Suitable for applications involving complex decision-making.
            \item Example: In games like Dota or Chess, policy gradients allow agents to learn optimal strategies by assessing probabilities of actions rather than iterating through all possibilities.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Advantages of Policy Gradient Methods - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Works Well with Continuous Action Spaces}
        \begin{itemize}
            \item Policy gradient methods can handle scenarios requiring continuous actions.
            \item Example Code for sampling actions from a Gaussian distribution:
            \begin{lstlisting}[language=Python]
import numpy as np

# Mean and standard deviation of action distribution
mean = your_policy_net(state)
std_dev = your_policy_net(state + noise)

# Sample from continuous action space
action = np.random.normal(mean, std_dev)
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Useful for Stochastic Policies}
        \begin{itemize}
            \item Allows exploration of different actions leading to better long-term rewards.
            \item Particularly effective in environments with inherent randomness.
        \end{itemize}

        \item \textbf{Improved Sample Efficiency with Baselines}
        \begin{itemize}
            \item Reduces variance and improves stability during training when combined with techniques like baselines.
            \item Applies particularly in Actor-Critic methods.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Directly optimizes policy performance, making them intuitive for various tasks.
        \item Well-suited for high-dimensional and continuous action spaces, broadening applicability.
        \item Enables the use of stochastic policies, fostering exploration in uncertain environments.
        \item Benefits from reduced variance through the integration of baselines.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Policy gradient methods present significant advantages for tackling real-world reinforcement learning problems due to their adaptability to different action spaces and their direct approach to policy optimization. Understanding these strengths equips practitioners to effectively apply these methods across diverse applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations - Overview}
    \begin{block}{Key Points}
        Policy gradient methods are powerful but come with challenges that can hinder performance:
        \begin{itemize}
            \item High Variance: Leads to inconsistent updates and slow convergence.
            \item Sample Inefficiency: Requires many interactions, complicating real-world application.
            \item Stability Issues: Small policy changes can disrupt consistent learning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{High Variance}
    \begin{block}{Concept Explanation}
        Policy gradient methods often experience high variance in their estimates of the policy gradients, primarily due to the stochastic nature of the policies leading to irregular updates.
    \end{block}

    \begin{block}{Example}
        Consider a reinforcement learning agent in a game. If it samples a series of ineffective actions, the gradient update may incorrectly suggest a drastic change in the policy, leading to poor performance.
    \end{block}

    \begin{block}{Key Point}
        High variance can slow down convergence and complicate learning, making variance reduction techniques essential.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Sample Efficiency}
    \begin{block}{Concept Explanation}
        Policy gradient methods typically require a large number of samples to provide reliable updates, resulting in inefficient learning especially in costly data collection environments.
    \end{block}

    \begin{block}{Example}
        In robotic control tasks, collecting samples may require significant real-world interactions, making it hard to gather sufficient diverse experiences quickly.
    \end{block}

    \begin{block}{Key Point}
        Improving sample efficiency is crucial for practical applications. Techniques like experience replay or using off-policy data can help mitigate this limitation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stability Issues}
    \begin{block}{Concept Explanation}
        The learning process in policy gradient methods can be unstable. Small changes in the policy can lead to significant performance fluctuations.
    \end{block}

    \begin{block}{Example}
        An agent might converge to a suboptimal policy that yields poor performance, oscillating between good and bad policies without clear improvement.
    \end{block}

    \begin{block}{Key Point}
        Techniques such as trust region methods (e.g., TRPO) or natural gradients can improve the stability of policy updates and control the learning process.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas/Conceptual Tools}
    \begin{block}{Gradient Estimation}
        \begin{equation}
            \nabla J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \nabla \log \pi_\theta(a_i | s_i) R_i
        \end{equation}
        Where \( N \) is the number of samples, \( a_i \) is the action taken, \( s_i \) is the state, and \( R_i \) is the reward associated with the action.
    \end{block}

    \begin{block}{Variance Reduction Technique}
        Using Generalized Advantage Estimation (GAE),
        \begin{equation}
            A_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
        \end{equation}
        where \( \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the challenges and limitations of policy gradient methods is essential for optimizing their application in reinforcement learning. 
    \begin{itemize}
        \item Addressing issues of high variance, sample efficiency, and stability can significantly enhance the efficacy of these techniques in real-world scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in the Real World - Overview}
    \begin{block}{Introduction to Policy Gradient Methods}
        Policy Gradient Methods are reinforcement learning techniques that directly optimize the policy function. They allow agents to select actions probabilistically based on experiences, making them beneficial in complex action spaces.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Part 1}
    \begin{itemize}
        \item \textbf{Robotics:} 
        \begin{itemize}
            \item \textit{Example:} Training robotic arms for tasks like grasping.
            \item \textit{Case Study:} A robotic hand taught to grasp various shapes utilizing policy gradients to learn gripping strategies.
        \end{itemize}

        \item \textbf{Game Playing:}
        \begin{itemize}
            \item \textit{Example:} AI agents in video games.
            \item \textit{Case Study:} DeepMind's Alphago defeating human champions by learning strategies through self-play using policy gradient methods.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Part 2}
    \begin{itemize}
        \item \textbf{Natural Language Processing (NLP):}
        \begin{itemize}
            \item \textit{Example:} Text generation in chatbots.
            \item \textit{Case Study:} Enhancing conversational quality by optimizing word selection based on user feedback.
        \end{itemize}

        \item \textbf{Finance:}
        \begin{itemize}
            \item \textit{Example:} Algorithmic trading strategies.
            \item \textit{Case Study:} Adaptive trading systems utilizing policy gradients to optimize decisions and improve performance.
        \end{itemize}

        \item \textbf{Healthcare:}
        \begin{itemize}
            \item \textit{Example:} Treatment recommendations in personalized medicine.
            \item \textit{Case Study:} Optimizing treatment plans to maximize recovery rates based on patient outcomes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Direct Optimization:} Focus on optimizing policies directly is advantageous for complex scenarios.
        \item \textbf{Flexibility:} Applicable across diverse fields, from robotics to finance.
        \item \textbf{Example-Driven Learning:} Highlights how agents evolve strategies through interaction.
    \end{itemize}
    \begin{block}{Conclusion}
        Policy Gradient Methods are vital in reinforcement learning with widespread applications that advance AI technology. Recognizing both their benefits and limitations, such as high variance, is crucial for effective integration.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Policy Gradient Methods - Overview}
    \begin{block}{Introduction to Policy Gradient Methods}
        Policy gradient methods are reinforcement learning techniques that optimize the policy directly. 
        Unlike value-based methods, they adjust policy parameters to maximize expected returns.
    \end{block}
    
    \begin{block}{Implementation Goals}
        We will outline a step-by-step implementation using the REINFORCE algorithm to update policy parameters via the gradient of expected rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Environment Setup}
    \begin{enumerate}
        \item \textbf{Environment Setup:} Use OpenAI's gym library.
        \begin{itemize}
            \item Install it using: 
            \begin{lstlisting}[language=bash]
pip install gym
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Import Libraries:}
        \begin{lstlisting}[language=python]
import numpy as np
import gym
import tensorflow as tf
        \end{lstlisting}
        
        \item \textbf{Create the Environment:}
        \begin{lstlisting}[language=python]
env = gym.make('CartPole-v1')
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Policy Model and Training}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Define the Policy Model:}
        \begin{lstlisting}[language=python]
model = tf.keras.Sequential([
    tf.keras.layers.Dense(24, activation='relu', input_shape=(env.observation_space.shape[0],)),
    tf.keras.layers.Dense(env.action_space.n, activation='softmax')
])
        \end{lstlisting}
        
        \item \textbf{Define the Training Function:}
        \begin{lstlisting}[language=python]
def train(episodes, optimizer):
    for episode in range(episodes):
        ...
        # Data collection and policy update logic
        ...
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        \end{lstlisting}
        
        \item \textbf{Discounted Rewards Function:}
        \begin{lstlisting}[language=python]
def discount_rewards(rewards, gamma=0.99):
    discounted = np.zeros_like(rewards)
    ...
    return discounted
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Loss Calculation}
    \begin{enumerate}
        \setcounter{enumi}{6}
        \item \textbf{Loss Calculation Function:}
        \begin{lstlisting}[language=python]
def compute_loss(states, actions, discounted_rewards):
    ...
    return loss
        \end{lstlisting}
    
        \item \textbf{Key Points:}
        \begin{itemize}
            \item \textbf{Exploration vs. Exploitation:} Encourages exploration via stochastic policy.
            \item \textbf{Gradient Descent Approach:} Updates parameters to maximize expected returns.
            \item \textbf{Combining Experience:} Collects experiences before updating the model.
        \end{itemize}
        
        \item \textbf{Wrap-Up:} 
        Experiment with different environments and network architectures to observe effects on learning performance.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Policy Gradient Methods}
    \begin{block}{Introduction}
        As AI evolves, understanding the ethical implications of using Policy Gradient Methods (PGM) becomes vital. These methods optimize policies through gradient ascent, yielding powerful models that raise several ethical concerns.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations - Part 1}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}
        \begin{itemize}
            \item PGM may perpetuate biases in training data.
            \item \textit{Example:} A recommendation system may favor represented users over underrepresented groups.
        \end{itemize}
        
        \item \textbf{Transparency and Interpretability}
        \begin{itemize}
            \item Model decisions can be opaque, challenging for stakeholders.
            \item \textit{Key Point:} Develop interpretability frameworks to clarify decision-making.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Considerations - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Safety and Robustness}
        \begin{itemize}
            \item Models may behave unpredictably in novel scenarios.
            \item \textit{Example:} An autonomous vehicle may make unsafe maneuvers in unusual traffic.
        \end{itemize}
        
        \item \textbf{Environmental Impact}
        \begin{itemize}
            \item Large models require significant computational resources, raising energy consumption concerns.
            \item \textit{Consideration:} Optimize algorithm efficiencies to reduce environmental footprint.
        \end{itemize}
        
        \item \textbf{Manipulation and Misuse}
        \begin{itemize}
            \item PGM can be misused for harmful purposes.
            \item \textit{Key Point:} Ethical guidelines and self-regulation are essential.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Emphasized Points}
    \begin{block}{Conclusion}
        The deployment of PGM requires not just technical excellence but ethical consideration. By addressing bias, transparency, safety, environmental impact, and misuse, we can foster responsible AI practices.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Funding Responsible AI:} Invest in ethical AI frameworks and training.
        \item \textbf{Engagement with Stakeholders:} Dialogue with affected communities is crucial.
        \item \textbf{Iterative Feedback and Improvement:} Implement feedback mechanisms to rectify ethical issues.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways}
    \begin{enumerate}
        \item \textbf{Understanding Policy Gradient Methods}:
        \begin{itemize}
            \item Optimize the policy directly for both discrete and continuous action spaces.
            \item Often outperform value-based methods in complex environments.
        \end{itemize}

        \item \textbf{Key Concepts}:
        \begin{itemize}
            \item \textbf{Policy}: Maps states to actions (stochastic or deterministic).
            \item \textbf{Objective Function}:
            \[
            J(\theta) = \mathbb{E}_{\pi_\theta}[R]
            \]
            \item \textbf{Gradient Ascent}:
            \[
            \theta_{new} = \theta + \alpha \nabla J(\theta)
            \]
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Methods and Implications}
    \begin{enumerate}[resume]
        \item \textbf{Types of Policy Gradient Methods}:
        \begin{itemize}
            \item \textbf{REINFORCE Algorithm}: Estimates the gradient based on complete episodes.
            \item \textbf{Actor-Critic Methods}: Combines the value-based and policy-based approaches to stabilize learning.
        \end{itemize}

        \item \textbf{Advantages}:
        \begin{itemize}
            \item Flexibility in action selection.
            \item Better handling of high-dimensional action spaces.
        \end{itemize}

        \item \textbf{Challenges}:
        \begin{itemize}
            \item High variance in policy gradient estimates.
            \item Sample inefficiency compared to value-based approaches.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Future Research Directions}
    \begin{block}{Implications for Future Research}
        \begin{itemize}
            \item \textbf{Hybrid Models}: Combine gradient policy and value-based methods to improve robustness.
            \item \textbf{Exploration Strategies}: Research intrinsic motivation and curiosity-driven exploration.
            \item \textbf{Real-world Applications}: Focus on ensuring efficiency and ethical considerations.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Policy Gradient Methods are crucial for advancing RL technologies.
            \item Their strengths and challenges present significant research opportunities.
            \item Emphasizing the need for ethical and practical deployment in future studies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
def reinforce(env, policy, num_episodes, learning_rate):
    for episode in range(num_episodes):
        states, actions, rewards = [], [], []
        state = env.reset()
        done = False
        
        while not done:
            action = policy.sample(state)  # Sample action from policy
            next_state, reward, done = env.step(action)
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            state = next_state
            
        # Update policy based on the collected episode's rewards
        policy.update(states, actions, rewards, learning_rate)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A / Interactive Discussion - Introduction}
    \begin{itemize}
        \item Engagement through open discussion and questions related to policy gradient methods.
        \item Overview: Policy Gradient Methods directly optimize the policy in reinforcement learning.
        \begin{itemize}
            \item Aim to maximize expected return by adjusting policy parameters.
            \item Differ from value-based methods by directly parameterizing policies.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A / Interactive Discussion - Key Concepts}
    \begin{itemize}
        \item \textbf{Policy}: A mapping from states to actions; can be deterministic or stochastic.
        \item \textbf{Objective Function}: Maximize expected return (reward):
        \begin{equation}
            J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} r_t \right]
        \end{equation}
        \item \textbf{Policy Gradient Theorem}: Computes gradients of expected return:
        \begin{equation}
            \nabla J(\theta) = E_{\tau \sim \pi_\theta} \left[ \nabla \log \pi_\theta(a_t | s_t) R_t \right]
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A / Interactive Discussion - Example & Questions}
    \begin{itemize}
        \item \textbf{Example}: A robot navigating a maze:
        \begin{itemize}
            \item The policy affects the robot's movement choices.
            \item Adjustments made according to rewards for reaching the maze exit.
        \end{itemize}
        \item \textbf{Interactive Questions}:
        \begin{enumerate}
            \item How do policy gradient methods differ from value-based methods like Q-Learning?
            \item Discuss benefits and challenges of policy gradient methods.
            \item Identify real-world applications for policy gradient methods.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A / Interactive Discussion - Closing Thoughts}
    \begin{itemize}
        \item Encourage sharing thoughts and experiences related to policy gradient methods.
        \item Participation fosters a deeper understanding of reinforcement learning.
        \item Consider how these concepts integrate with other learning objectives.
    \end{itemize}
\end{frame}


\end{document}