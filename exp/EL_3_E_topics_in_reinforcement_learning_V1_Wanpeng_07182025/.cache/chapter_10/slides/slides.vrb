\frametitle{Policy Gradient Algorithm - Pseudo-Code \& Flowchart}

    % Pseudo-code for Policy Gradient Algorithm
    \begin{block}{Pseudo-Code}
        \begin{lstlisting}[language=Python]
Initialize θ  # Policy parameters
For each episode:
    Generate trajectory τ by interacting with the environment
    For each time step t in τ:
        Compute return R_t
        Calculate policy gradient:
        ∇J(θ) += ∇log(π_θ(a_t|s_t)) * R_t
    Update policy parameters:
    θ = θ + α * ∇J(θ)
        \end{lstlisting}
    \end{block}

    % Flowchart Explanation
    \begin{block}{Flowchart Breakdown}
        1. \textbf{Start}: Initialize parameters.\\
        2. \textbf{Interact}: Gather data via policy.\\
        3. \textbf{Reward Calculation}: Compute returns from actions taken.\\
        4. \textbf{Gradient Calculation}: Derive the policy gradient using returns.\\
        5. \textbf{Update Policy}: Modify parameters based on gradient.\\
        6. \textbf{Repeat}: Loop until termination condition is met.
    \end{block}
