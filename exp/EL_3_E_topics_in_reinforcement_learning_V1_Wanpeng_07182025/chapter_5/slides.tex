\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Temporal Difference Learning}
    \begin{block}{Overview}
        Temporal Difference Learning (TD) is a fundamental concept in Reinforcement Learning (RL) that integrates ideas from Monte Carlo methods and dynamic programming. 
    \end{block}
    \begin{itemize}
        \item It estimates the value of states based on current value estimates, avoiding the wait for final outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of TD Learning in RL}
    \begin{itemize}
        \item \textbf{Online Learning:} Suitable for real-time scenarios as it allows learning from incomplete episodes.
        \item \textbf{Efficiency:} Incremental updates often lead to faster convergence in large state spaces.
        \item \textbf{Bootstrapping:} Updates estimates based on other learned estimates, enhancing exploration.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Temporal Difference Learning}
    \begin{enumerate}
        \item \textbf{Value Function Estimation:} Predicts expected future rewards for state-action pairs.
        \item \textbf{TD Target:} The primary formula is:
        \begin{equation}
            V(S_t) \leftarrow V(S_t) + \alpha \left( R_t + \gamma V(S_{t+1}) - V(S_t) \right)
        \end{equation}
        \item \textbf{Example:} In a grid environment, if an agent moves from state \( S_1 \) to \( S_2 \) and receives reward \( R \), the value of \( S_1 \) is updated based on \( S_2 \).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Conclusion}
    \begin{itemize}
        \item \textbf{Game Playing:} Applied successfully in games like Chess and Go through self-play.
        \item \textbf{Robotics:} Assists robots in making immediate decisions in uncertain environments.
    \end{itemize}
    \begin{block}{Conclusion}
        TD Learning is foundational in RL, enhancing agentsâ€™ learning capabilities and paving the way for advanced methods like Q-learning and SARSA.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Definitions - Temporal Difference Learning}
    \begin{block}{Temporal Difference Learning (TD Learning)}
        **Definition:** TD Learning is an essential approach in reinforcement learning that merges concepts from Monte Carlo methods and Dynamic Programming. It allows agents to learn to predict future rewards based on current experiences, updating their knowledge after each step.
    \end{block}

    \begin{itemize}
        \item **Key Concept:**
            \begin{itemize}
                \item Updates state value based on the temporal difference between predicted and actual rewards.
                \item Operates in an off-policy manner.
            \end{itemize}

        \item **Example:**
            \begin{itemize}
                \item Agent predicts reward based on current state and updates using the formula:
                \[
                V(S_t) \leftarrow V(S_t) + \alpha \cdot [R_t + \gamma \cdot V(S_{t+1}) - V(S_t)]
                \]
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Definitions - Monte Carlo Methods}
    \begin{block}{Monte Carlo Methods}
        **Definition:** Monte Carlo Methods are algorithms that utilize repeated random sampling to estimate values or compute numerical results. In reinforcement learning, they evaluate the value of states or actions based on actual returns received over complete episodes.
    \end{block}

    \begin{itemize}
        \item **Key Concept:**
            \begin{itemize}
                \item Require a complete episode before updating value estimates, contrasting with TD Learning's incremental updates.
            \end{itemize}

        \item **Example:**
            \begin{itemize}
                \item Agent records total scores at the end of each episode, updating state values with:
                \[
                V(S) = \frac{\text{Sum of returns from } S}{\text{Number of visits to } S}
                \]
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Definitions - Roles in Reinforcement Learning}
    \begin{itemize}
        \item **TD Learning:**
            \begin{itemize}
                \item Enables online learning from each environment step, beneficial for long or impractical episodes.
            \end{itemize}

        \item **Monte Carlo Methods:**
            \begin{itemize}
                \item Suitably used for fully observable episodes, yielding stable estimates and understanding long-term returns, ideal for batch learning.
            \end{itemize}
    \end{itemize}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Both methods estimate state or state-action values in reinforcement learning.
            \item TD Learning relies on partial information; Monte Carlo methods depend on complete episodes.
            \item Selection of method depends on environmental structure and reward nature.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TD Learning and Monte Carlo Methods - Overview}
    \begin{itemize}
        \item Temporal Difference (TD) Learning and Monte Carlo (MC) methods are fundamental in Reinforcement Learning (RL).
        \item Both aim to learn optimal policies but differ significantly in their approaches and mechanics.
        \item Key areas of differentiation include:
        \begin{itemize}
            \item Mechanism of Learning
            \item Data Dependency
            \item Exploration and Convergence
            \item Applications
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TD Learning and Monte Carlo Methods - Key Differences}
    \begin{block}{1. Mechanism of Learning}
        \begin{itemize}
            \item \textbf{TD Learning:}
                \begin{itemize}
                    \item Updates value estimates with each time step based on predicted and actual rewards.
                    \item Incremental updates using:
                    \begin{equation}
                        V(s_t) \leftarrow V(s_t) + \alpha ( r_{t+1} + \gamma V(s_{t+1}) - V(s_t) )
                    \end{equation}
                    - $V(s_t)$: Value of the current state
                    - $r_{t+1}$: Reward after action
                    - $\gamma$: Discount factor
                    - $\alpha$: Learning rate
                \end{itemize}
            \item \textbf{Monte Carlo Methods:}
                \begin{itemize}
                    \item Updates values after completing episodes.
                    \item Utilizes average return from all visits during an episode.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TD Learning and Monte Carlo Methods - Additional Points}
    \begin{block}{2. Data Dependency}
        \begin{itemize}
            \item \textbf{TD Learning:}
                \begin{itemize}
                    \item Employs bootstrapping: updates based on other value estimates.
                    \item More sample efficient due to learning from each action.
                \end{itemize}
            \item \textbf{Monte Carlo Methods:}
                \begin{itemize}
                    \item Non-bootstrapped: relies solely on actual complete episode returns.
                    \item Needs multiple episodes for stable convergence.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Applications}
        \begin{itemize}
            \item \textbf{TD Learning:}
                \begin{itemize}
                    \item Effective in continuous state problems (e.g., Q-learning, SARSA).
                \end{itemize}
            \item \textbf{Monte Carlo Methods:}
                \begin{itemize}
                    \item Best for clearly defined episodic tasks (e.g., Blackjack).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of TD Learning and Monte Carlo Methods - Conclusion}
    \begin{itemize}
        \item TD Learning is adept at adapting to changing environments while learning incrementally.
        \item Monte Carlo methods yield robust estimates but require entire episodes for updates.
        \item Combining both methods can lead to hybrid approaches that leverage their strengths.
        \item Important to choose the appropriate method based on the specific problem and environment in RL.
    \end{itemize}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Temporal Difference (TD) Learning}
    \begin{block}{Overview}
        Temporal Difference Learning is a core method in reinforcement learning that combines ideas from dynamic programming and Monte Carlo methods. It updates value estimates based not solely on complete episodes but also on partial returns, allowing for efficient and ongoing learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Value Estimates:} 
        These represent the predicted future rewards an agent expects to obtain from different states or state-action pairs.
        \item \textbf{Temporal Differences:} 
        Focuses on differences between estimated rewards at different time steps, specifically the difference between current and updated estimates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{TD Learning Update Mechanism}
    \begin{itemize}
        \item \textbf{Current Value Estimate:} $V(S_t)$ - Predicted value of the current state $S_t$.
        \item \textbf{Reward:} $R_{t+1}$ - Immediate reward after transitioning from $S_t$.
        \item \textbf{Next State Value:} $V(S_{t+1})$ - Predicted value of the next state.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{TD Update Equation}
    TD Learning uses the following formula to update the value estimate:
    \begin{equation}
        V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)
    \end{equation}
    \begin{itemize}
        \item $\alpha =$ Learning rate (controls the impact of new information)
        \item $\gamma =$ Discount factor (importance of future rewards)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    Consider an agent navigating a grid world:
    \begin{itemize}
        \item \textbf{State:} Current position in the grid.
        \item \textbf{Action:} Moving to a neighboring cell.
        \item \textbf{Reward:} Positive for reaching a goal, negative for hitting a wall.
    \end{itemize}
    Upon moving to a new state, the agent:
    \begin{enumerate}
        \item Receives a reward and looks at the estimated value of the new position.
        \item Adjusts its current state value using newly observed rewards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Calculation}
    For instance:
    \begin{itemize}
        \item If $V(S_t) = 5$, $R_{t+1} = 10$, $V(S_{t+1}) = 7$, and $\alpha = 0.1$:
    \end{itemize}
    \begin{equation}
        V(S_t) \leftarrow 5 + 0.1 \left( 10 + 0.9 \times 7 - 5 \right) = 6.13
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Online Learning:} TD learning allows continuous learning as new information is received.
        \item \textbf{Sample Efficiency:} Reduces variance and speeds up learning.
        \item \textbf{Applications:} Widely used in RL contexts like game playing and robotics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of TD Learning - Introduction}
    \begin{block}{Introduction to Temporal Difference Learning}
        Temporal Difference (TD) Learning is a pivotal approach in reinforcement learning. It combines ideas from both Monte Carlo methods and dynamic programming, allowing agents to learn optimal policies directly from their experience.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of TD Learning - Sample Efficiency}
    \begin{block}{Sample Efficiency}
        \begin{itemize}
            \item \textbf{Definition}: Ability to learn effectively from limited samples.
            \item \textbf{Explanation}:
                \begin{itemize}
                    \item TD Learning updates its value estimates based on each step taken in the environment.
                    \item Learning occurs from each individual transition without needing to wait for complete episodes.
                \end{itemize}
            \item \textbf{Illustration}:
                \begin{itemize}
                    \item TD: Updates after every interaction.
                    \item Monte Carlo: Waits until the end of episodes.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of TD Learning - Online Learning}
    \begin{block}{Online Learning Capabilities}
        \begin{itemize}
            \item \textbf{Definition}: Update knowledge in real time with new data.
            \item \textbf{Explanation}:
                \begin{itemize}
                    \item TD Learning updates value estimations incrementally with each new observation.
                    \item Allows timely updates, especially useful in dynamic environments.
                \end{itemize}
            \item \textbf{Example}:
                \begin{itemize}
                    \item A robotic agent navigating a maze can continuously adjust its strategy as it encounters new obstacles.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of TD Learning - Continuous Improvement}
    \begin{block}{Continuous Improvement}
        \begin{itemize}
            \item \textbf{Key Point}: Facilitates ongoing refinement of learning with each experience.
            \item \textbf{Update Rule}:
            \begin{equation}
                V(S_t) \leftarrow V(S_t) + \alpha \cdot \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)
            \end{equation}
            \begin{itemize}
                \item \( V(S_t) \) = value estimate of state \( S_t \)
                \item \( R_{t+1} \) = immediate reward after taking action in state \( S_t \)
                \item \( \gamma \) = discount factor
                \item \( \alpha \) = learning rate
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of TD Learning - Flexibility}
    \begin{block}{Flexibility in Environments}
        \begin{itemize}
            \item \textbf{Key Point}: Works in both deterministic and stochastic environments.
            \item \textbf{Example}:
                \begin{itemize}
                    \item In finance, TD Learning adjusts strategies based on continual evaluation rather than waiting for quarterly changes.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of TD Learning - Summary}
    \begin{block}{Summary}
        TD Learning is highly sample efficient, enabling quick updates and online learning. It facilitates real-time adjustments in dynamic environments. These advantages make TD Learning a powerful approach in reinforcement learning, particularly in robotics, finance, and game playing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Temporal Difference Learning - Overview}
  \begin{block}{What is Temporal Difference (TD) Learning?}
    Temporal Difference Learning is a reinforcement learning approach that combines ideas from Monte Carlo methods and dynamic programming. It enables agents to learn directly from experience without having to wait for the final outcome, which is especially useful in continuously evolving environments.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Temporal Difference Learning - Part 1}
  \begin{enumerate}
    \item \textbf{Game Playing}
      \begin{itemize}
        \item TD Learning was crucial for developing agents that play games like Chess and Go. For example, AlphaGo used a variant of TD Learning combined with deep neural networks to learn complex strategies from millions of games.
        \item \textbf{Key Point:} It allows agents to update their value estimates in real-time based on their moves.
      \end{itemize}

    \item \textbf{Robotics}
      \begin{itemize}
        \item In robotic navigation, TD Learning algorithms are used for path planning and obstacle avoidance. Robots update their knowledge based on sensory feedback while interacting with their environment.
        \item \textbf{Key Point:} This online learning capability allows quick adaptation to new environments without extensive retraining.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Temporal Difference Learning - Part 2}
  \begin{enumerate}
    \setcounter{enumi}{2} % Continue the enumeration
    \item \textbf{Finance}
      \begin{itemize}
        \item TD Learning powers algorithmic trading systems that adapt to changing market conditions. These systems make informed decisions by updating value functions in real-time, balancing risk and return.
        \item \textbf{Key Point:} Dynamic adjustments can enhance financial performance by predicting stock price trends more accurately.
      \end{itemize}

    \item \textbf{Personalized Recommendations}
      \begin{itemize}
        \item Services like Netflix and Spotify utilize TD Learning for improving user recommendations. By analyzing user interactions (e.g., plays and skips), they can refine predictions for what users may want to watch or listen to next.
        \item \textbf{Key Point:} This creates a personalized experience, increasing user engagement.
      \end{itemize}

    \item \textbf{Healthcare}
      \begin{itemize}
        \item In personalized medicine, TD Learning optimizes treatment plans based on patient responses over time. For example, adaptive clinical trials can dynamically adjust strategies using TD methods.
        \item \textbf{Key Point:} This can lead to more effective treatment outcomes by continuously learning from patient reactions.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Takeaways and Conclusion}
  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item TD Learning is a robust framework for real-time learning and decision-making across diverse domains.
      \item Its efficiency in updating value functions facilitates complex problem-solving in dynamic environments.
      \item Continuous interaction with the environment enhances the learning process, underpinning TD Learning's significance in reinforcement learning.
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    As shown, TD Learning is foundational in machine learning and significantly impacts various real-world situations, highlighting its versatility and practical utility in addressing complex problems.
  \end{block}

  \begin{block}{Further Exploration}
    For hands-on implementation, consider using libraries like OpenAI's Gym and TensorFlow to simulate the applications discussed. Practical exercises can enhance your understanding of TD Learning.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{itemize}
        \item Temporal Difference (TD) Learning combines Monte Carlo methods and dynamic programming.
        \item Provides a robust method for reinforcement learning.
        \item Faces several challenges that can hinder performance and convergence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in TD Learning - Convergence Issues}
    \begin{block}{Convergence Issues}
        \begin{itemize}
            \item Finding a balance between exploration and exploitation is essential for convergence.
            \item High learning rates may cause oscillations and prevent convergence.
            \item Low learning rates can result in slow convergence.
        \end{itemize}
    \end{block}
    
    \begin{exampleblock}{Example}
        Consider a robot learning to navigate a maze:
        \begin{itemize}
            \item Aggressive updates may lead to forgetting effective strategies.
            \item Balance is key to effectively finding the exit.
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in TD Learning - Hyperparameter Tuning}
    \begin{block}{Need for Hyperparameter Tuning}
        \begin{itemize}
            \item TD Learning relies on hyperparameters like:
            \begin{itemize}
                \item **Learning Rate** ($\alpha$): Ranges from 0 (no learning) to 1 (full trust in new information).
                \item **Discount Factor** ($\gamma$): Determines future reward importance.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{exampleblock}{Example}
        Setting $\gamma$ too low can lead to ignoring future rewards, resulting in suboptimal decisions.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges in TD Learning - Function Approximation and Exploration}
    \begin{block}{Function Approximation Variability}
        \begin{itemize}
            \item Large state spaces require function approximation methods (e.g., neural networks).
            \item Can introduce instability and biased estimates.
            \item Needs careful architecture design and regularization to prevent overfitting.
        \end{itemize}
    \end{block}

    \begin{block}{Exploration vs. Exploitation Dilemma}
        \begin{itemize}
            \item Sufficient exploration is vital for accurate value estimates.
            \item Insufficient exploration may lead to local optima.
        \end{itemize}
    \end{block}
    
    \begin{exampleblock}{Example}
        An agent picking the highest average reward action may overlook better strategies that haven't been tried yet.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of TD Update Rule}
    \begin{block}{TD Update Rule}
        \begin{equation}
            V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $V(S_t)$ = value estimate of state $S_t$
            \item $R_{t+1}$ = reward received after taking action
            \item $S_{t+1}$ = next state
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Addressing challenges in TD Learning involves thoughtful parameter selection and balancing exploration-exploitation.
        \item Understanding these hurdles is crucial for successfully applying TD Learning in complex environments.
        \item Next, we will summarize key takeaways from this module.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways}
    \begin{enumerate}
        \item \textbf{Definition and Overview}
        \begin{itemize}
            \item Temporal Difference (TD) Learning updates value estimates based on the difference between predicted values and actual rewards.
            \item Monte Carlo Methods rely on complete episodes to update value estimates, using average returns.
        \end{itemize}
        
        \item \textbf{Differences Between TD Learning and Monte Carlo Methods}
        \begin{itemize}
            \item \textbf{Learning Approach}:
            \begin{itemize}
                \item TD Learning updates incrementally after each time step.
                \item Monte Carlo Methods require whole episodes for updates.
            \end{itemize}

            \item \textbf{Convergence and Stability}:
            \begin{itemize}
                \item TD Learning often converges more quickly but may face overestimation bias.
                \item Monte Carlo Methods are more stable but take longer to converge.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Application and Importance}
    
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue item numbering
        \item \textbf{When to Use Each Method}
        \begin{itemize}
            \item Use TD Learning in environments with many states and actions that don't require completed episodes.
            \item Use Monte Carlo Methods in episodic environments for more accurate value estimates.
        \end{itemize}

        \item \textbf{Importance in Reinforcement Learning}
        \begin{itemize}
            \item Both methods are essential for developing reinforcement learning algorithms.
            \item They provide strategies for managing the exploration-exploitation trade-off.
            \item The choice impacts the agent's performance based on the task context.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary Illustration and Final Thoughts}
    
    \begin{itemize}
        \item \textbf{Summary Illustration: Grid World}
        \begin{itemize}
            \item TD Learning: Updates state values dynamically as the agent moves.
            \item Monte Carlo: Updates values only after completing an episode (e.g., reaching a goal).
        \end{itemize}

        \item \textbf{Conclusion}
        \begin{itemize}
            \item Understanding the distinctions between TD Learning and Monte Carlo Methods is vital for building efficient reinforcement learning systems.
            \item The appropriate selection between these methods enhances learning performance in dynamic environments.
        \end{itemize}
    \end{itemize}
\end{frame}


\end{document}