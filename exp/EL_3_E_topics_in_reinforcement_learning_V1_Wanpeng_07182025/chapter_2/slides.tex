\documentclass[aspectratio=169]{beamer}

\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

\title[Week 2: MDPs]{Week 2: Markov Decision Processes (MDPs)}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes (MDPs)}
    \begin{block}{What are MDPs?}
        Markov Decision Processes (MDPs) are a foundational framework used in reinforcement learning (RL) for modeling decision-making scenarios with randomness and decision control.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of MDPs}
    \begin{itemize}
        \item \textbf{States}: Possible situations the agent can be in, containing all necessary information for decision-making.
        \item \textbf{Actions}: Choices available to the agent in each state, affecting future states.
        \item \textbf{Rewards}: Feedback received after actions, indicating the quality of the action relative to achieving goals (positive or negative).
        \item \textbf{Transition Probabilities}: Likelihood of moving from one state to another after taking an action, encapsulating the environment's dynamics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation of MDPs}
    An MDP is formally defined by the tuple \( (S, A, P, R, \gamma) \):
    \begin{itemize}
        \item \( S \): A finite set of states.
        \item \( A \): A finite set of actions.
        \item \( P(s'|s, a) \): Transition probability of reaching state \( s' \) after action \( a \) in state \( s \).
        \item \( R(s, a) \): Reward function assigning a scalar reward for each action in a state.
        \item \( \gamma \): Discount factor, \( 0 \leq \gamma < 1 \), representing future rewards' importance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of MDPs in RL}
    MDPs are pivotal in reinforcement learning because they:
    \begin{itemize}
        \item Provide structured decision-making models for sequential problems.
        \item Support algorithms (e.g., Value Iteration, Policy Iteration) that converge to optimal solutions.
        \item Are applicable across various fields, from robotics to economics, for uncertain decision-making environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Consider a robot navigating a room to reach a charger:
    \begin{itemize}
        \item \textbf{States}: Positions in the room.
        \item \textbf{Actions}: Movements (up, down, left, right).
        \item \textbf{Rewards}: Highest when reaching the charger (positive reward), lower or negative when hitting walls.
        \item \textbf{Transition Probabilities}: Allow for movement uncertainties, representing chances of slipping or mismoving.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item MDPs serve as a backbone for many reinforcement learning algorithms.
        \item Understanding MDPs is essential for designing intelligent agents that learn from their environments.
        \item The interplay between states, actions, rewards, and transition probabilities forms the essence of decision-making in uncertain scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Components - Overview}
    \begin{block}{Key Components of Markov Decision Processes (MDPs)}
        MDPs provide a framework for modeling decision-making in environments with random outcomes. Understanding these components is essential for grasping reinforcement learning algorithms:
    \end{block}
    \begin{itemize}
        \item States (S)
        \item Actions (A)
        \item Rewards (R)
        \item Transition Probabilities (P)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Components - States and Actions}
    \begin{block}{1. States (S)}
        A state represents a specific situation or configuration of the environment at a particular time.
        \begin{itemize}
            \item \textbf{Notation}: Denoted as \( S \).
            \item The state space \( S \) is a collection of all possible states.
            \item \textbf{Example}: In chess, each arrangement of pieces is a different state.
        \end{itemize}
    \end{block}
    \begin{block}{2. Actions (A)}
        An action is a decision made by the agent that may affect the state.
        \begin{itemize}
            \item \textbf{Notation}: Denoted as \( A \).
            \item For every state, there’s a set of actions the agent can take.
            \item \textbf{Example}: In chess, actions include moving a piece or forfeiting.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDP Components - Rewards and Transitions}
    \begin{block}{3. Rewards (R)}
        A reward is a feedback signal received after taking an action in a state, representing the immediate benefit.
        \begin{itemize}
            \item \textbf{Notation}: Denoted as \( R(s, a) \).
            \item Rewards guide the learning process, maximizing cumulative rewards over time.
            \item \textbf{Example}: Capturing a piece in a game might yield +10 points.
        \end{itemize}
    \end{block}
    
    \begin{block}{4. Transition Probabilities (P)}
        Transition probabilities specify the likelihood of moving from one state to another after an action.
        \begin{itemize}
            \item \textbf{Notation}: Denoted as \( P(s' | s, a) \).
            \item Reflects the dynamics of the environment and uncertainty in outcomes.
            \item \textbf{Example}: In a maze, there’s a 70\% chance of moving successfully forward.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States in Markov Decision Processes (MDPs)}
    \begin{block}{Definition of a State}
        A state in an MDP represents a specific situation in the environment at a particular point in time. It encapsulates all the relevant information needed to make a decision.
    \end{block}
    
    \begin{block}{Importance of States}
        States are the foundation of an MDP, determining the current context for decisions. Each state can lead to various actions and different potential outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Characteristics of States}
    \begin{itemize}
        \item \textbf{Discrete vs. Continuous States}
            \begin{itemize}
                \item \textbf{Discrete States}: Clearly distinguished and finite situations (e.g., positions on a chessboard).
                \item \textbf{Continuous States}: Represented on a continuous scale (e.g., height of a robot arm).
            \end{itemize}
        
        \item \textbf{Episodic vs. Continuing Tasks}
            \begin{itemize}
                \item \textbf{Episodic}: The process has a defined endpoint (e.g., game episodes).
                \item \textbf{Continuing}: The process continues indefinitely without a specific end.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Notation}
    \begin{itemize}
        \item \textbf{Examples of States}
            \begin{itemize}
                \item \textbf{Robotics}: Each state could represent the robot's location and orientation (e.g., (x,y) coordinates and angle).
                \item \textbf{Game Playing}: In chess, each state corresponds to a unique configuration of the board and pieces.
                \item \textbf{Navigation}: In a GPS system, states reflect different locations along a route, including current traffic conditions.
            \end{itemize}
        
        \item \textbf{Notation and Representation}
            \begin{itemize}
                \item \textbf{State Space}: Denoted by \( S \), it is the set of all possible states in an MDP.
                \item \textbf{Markov Property}:
                    \[
                    P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_t)
                    \]
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item States are critical as they influence decision-making through the actions that can be taken.
        \item Understanding the structure of states aids in designing effective strategies for control and optimization.
        \item The relationship between states, actions, and rewards forms the core of the MDP framework, guiding an agent's learning algorithm.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    States in MDPs are essential elements that reflect the current situation, impacting possible actions and future outcomes. Understanding these states is vital for comprehending the dynamics of decision-making in uncertain environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions in MDPs - Overview}
    \begin{block}{Understanding Actions in MDPs}
        In Markov Decision Processes (MDPs), an **action** is a decision made by an agent that influences the environment state. Actions are crucial for determining transitions between states.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions in MDPs - Key Concepts}
    \begin{enumerate}
        \item \textbf{Action (A)}:
        \begin{itemize}
            \item A choice made by the agent at any state.
            \item Denoted as \( A \), with various options available.
        \end{itemize}
        \item \textbf{Transition Dynamics}:
        \begin{itemize}
            \item Taking an action leads to a new state with probabilistic outcomes.
            \item Represented as:
            \[
            P(s' | s, a)
            \]
            where \( s \) is the current state, \( a \) is the action, and \( s' \) is the next state.
        \end{itemize}
        \item \textbf{State-Action Pairs}:
        \begin{itemize}
            \item Represent options available from each state - can be visualized in a table or diagram.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Conclusion}
    \begin{block}{Example: Grid World}
        An agent can move in four directions: up, down, left, right.
        \begin{itemize}
            \item \textbf{States}: Positions in the grid.
            \item \textbf{Actions}:
            \begin{itemize}
                \item \( A_1 \): Move up
                \item \( A_2 \): Move down
                \item \( A_3 \): Move left
                \item \( A_4 \): Move right
            \end{itemize}
            \item \textbf{Transition Dynamics}: e.g., from state \( (2,2) \),
            \[
            P((1,2) | (2,2), A_1) = 0.7, \quad P((2,1) | (2,2), A_1) = 0.3
            \]
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding actions in MDPs is vital for modeling decision-making in uncertain environments. The interaction between actions, states, and transitions is key for developing effective reinforcement learning algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Overview}
    Rewards in Markov Decision Processes (MDPs) play a crucial role in guiding the reinforcement learning process. 
    A reward is a scalar signal received after an agent takes an action in a given state, indicating the immediate value of that action and influencing the policy learned by the agent over time.

    \begin{block}{Definition}
        A reward \( R \) quantifies the benefit or cost associated with a specific action taken in a particular state. It can be positive (reward) or negative (penalty).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Significance}
    The significance of rewards in MDPs includes:

    \begin{enumerate}
        \item \textbf{Policy Formation}: Rewards directly influence the agent's policy, mapping states to actions. The objective is to maximize cumulative rewards over time.
        
        \item \textbf{Cumulative Reward and Return}:
        The return \( G_t \) is defined as:
        \begin{equation}
            G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
        \end{equation}
        where:
        \begin{itemize}
            \item \( G_t \) is the return at time \( t \),
            \item \( R_t \) is the reward received at time \( t \),
            \item \( \gamma \) (where \( 0 \leq \gamma < 1 \)) is the discount factor.
        \end{itemize}
        
        \item \textbf{Motivation for Actions}: Rewards motivate agents to explore and exploit their environment, encouraging favorable actions in similar future scenarios.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Example}
    \textbf{Example: Robot in a Grid Environment}

    \begin{itemize}
        \item \textbf{State}: Current position of the robot in a grid.
        \item \textbf{Action}: Move up, down, left, or right.
        \item \textbf{Reward Structure}:
        \begin{itemize}
            \item +10 for reaching a goal state (e.g., finding a treasure).
            \item -1 for hitting a wall or going out of bounds.
        \end{itemize}
    \end{itemize}

    \textbf{Outcome}: The robot learns to prefer actions leading to the goal state, thereby increasing its cumulative rewards over time.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Probabilities - Overview}
    \begin{block}{Definition}
        Transition probabilities in a Markov Decision Process (MDP) represent the likelihood of transitioning from one state to another when a particular action is taken. They quantify the uncertainty associated with the outcomes of actions within the environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{States:} The different configurations in which an agent might find itself, denoted as \( S \).
        \item \textbf{Actions:} Choices available to the agent, denoted as \( A \).
        \item \textbf{Transition Function:} Denoted as \( P(s' | s, a) \):
        \begin{itemize}
            \item \( s \): Current state
            \item \( a \): Action taken
            \item \( s' \): Resulting state
            \item \( P(s' | s, a) \): Probability of transitioning to state \( s' \) when action \( a \) is taken in state \( s \).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Transition Probabilities}
    \begin{block}{Grid World Scenario}
        In a simple grid world, an agent can move Up, Down, Left, or Right. For example:
        \begin{itemize}
            \item \textbf{States (S):} Positions like (1,1), (1,2), (2,1).
            \item \textbf{Actions (A):} Moves (Up, Down, Left, Right).
        \end{itemize}
        If at position (1,1) and the action is Right:
        \begin{itemize}
            \item \( P((1, 2) | (1, 1), \text{Right}) = 0.8 \)
            \item \( P((1, 1) | (1, 1), \text{Right}) = 0.2 \)
        \end{itemize}
        This indicates an 80% chance of moving to (1,2) and a 20% chance of remaining at (1,1).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item \textbf{Deterministic vs. Stochastic:} Transition probabilities can either be deterministic (certain outcome) or stochastic (spread across multiple outcomes).
        \item \textbf{Importance in Learning:} Essential for reinforcement learning as they help predict future states and guide decision-making.
        \item \textbf{Markov Property:} States depend only on the current state and the action taken, not past states or actions.
    \end{itemize}

    \begin{block}{Transition Matrix}
        The transition probabilities can be represented in a matrix \( P \):
        \[
        P = 
        \begin{bmatrix}
        P(s_1 | s_1, a_1) & P(s_2 | s_1, a_1) & \cdots \\
        P(s_1 | s_2, a_1) & P(s_2 | s_2, a_1) & \cdots \\
        \vdots & \vdots & \ddots
        \end{bmatrix}
        \]
        where each entry \( P(s' | s, a) \) signifies the probability of transitioning from state \( s \) to state \( s' \) under action \( a \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Moving Forward}
    Next, we will explore the concept of policies, which dictate how an agent should act in the different states based on transition probabilities. These policies utilize the information gathered from transition probabilities to maximize expected rewards over time.
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Concept of Policies - Introduction}
    \begin{block}{Introduction to Policies}
        In the context of Markov Decision Processes (MDPs), a **policy** is a crucial concept that dictates the behavior of an agent in a given environment. 
        Think of a policy as a strategy that specifies the actions an agent should take when it encounters different states. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Concept of Policies - Understanding Policies}
    \begin{itemize}
        \item \textbf{Definition}: A policy is a mapping from states of the MDP to actions. It can be:
        \begin{itemize}
            \item \textbf{Deterministic Policy}: A specific action is chosen for each state.
            \begin{itemize}
                \item Example: If the state is "Hungry", the action is "Eat".
            \end{itemize}
            \item \textbf{Stochastic Policy}: Actions are chosen based on a probability distribution.
            \begin{itemize}
                \item Example: In the state "Traffic Light: Red", the action could be "Wait" with a probability of 0.9 and "Run" with a probability of 0.1.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Concept of Policies - Formal Notation and Key Points}
    \begin{block}{Formal Notation}
        Let \( \pi \) denote a policy:
        \begin{itemize}
            \item \textbf{Deterministic Policy}:
            \begin{equation}
                \pi: S \rightarrow A
            \end{equation}
            Where \( S \) is the set of states and \( A \) is the set of actions.
            \item \textbf{Stochastic Policy}:
            \begin{equation}
                \pi(a|s) \quad \text{for } a \in A \text{ and } s \in S
            \end{equation}
            This denotes the probability of taking action \( a \) in state \( s \).
        \end{itemize}
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Points to Emphasize}:
        \begin{enumerate}
            \item Policies provide the decision-making framework for agents, guiding them on how to act in varying situations.
            \item The choice of policy significantly impacts the efficiency and effectiveness of an agent's performance.
            \item The quality of a policy can be evaluated using **value functions**, which estimate the expected return (reward) of following that policy from given states.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Introduction}
    \begin{block}{What are Value Functions?}
        Value functions are fundamental in Markov Decision Processes (MDPs) as they evaluate expected returns from states or state-action pairs. They guide decision-making under uncertainty by providing a metric on how advantageous it is to be in a certain state or to take a specific action.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Types}
    
    \textbf{Types of Value Functions:}
    
    \begin{enumerate}
        \item \textbf{State Value Function \(V\)}:
            \begin{itemize}
                \item \textbf{Definition}: The value of a state \(s\), denoted as \(V(s)\), is the expected return starting from state \(s\) and following a policy \(\pi\).
                \item \textbf{Mathematical Expression}:
                \begin{equation}
                    V^\pi(s) = \mathbb{E}_\pi \left[ R_t | S_t = s \right] = \sum_{a \in A} \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]
                \end{equation}
            \end{itemize}
        
        \item \textbf{Action Value Function \(Q\)}:
            \begin{itemize}
                \item \textbf{Definition}: The value of taking action \(a\) in state \(s\), denoted as \(Q(s,a)\), represents the expected return after taking action \(a\) and following policy \(\pi\).
                \item \textbf{Mathematical Expression}:
                \begin{equation}
                    Q^\pi(s, a) = \mathbb{E}_\pi \left[ R_t | S_t = s, A_t = a \right] = \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Functions - Importance and Examples}
    
    \textbf{Importance of Value Functions in MDPs:}
    
    \begin{itemize}
        \item \textbf{Decision Making}: They assess long-term rewards of actions, leading to optimal decisions.
        \item \textbf{Policy Evaluation}: Help in measuring policy effectiveness by estimating expected returns.
        \item \textbf{Learning}: Used in reinforcement learning to refine policies through environment interaction.
    \end{itemize}
    
    \textbf{Example Illustration:}
    
    Consider a grid world scenario:
    \begin{itemize}
        \item agent starts at state \(s\) and can earn a reward of +10 by reaching a goal state.
        \item Action \(a\) (moving towards the goal) has a high transition probability to successfully reach the goal, evaluated by the state value function.
    \end{itemize}
    
    This guides the agent in selecting the action that maximizes returns.
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDPs in Practice - Overview}
    \begin{block}{Definition}
        Markov Decision Processes (MDPs) are mathematical frameworks for modeling decision-making where outcomes are partly random and partly under the control of a decision maker. They consist of:
        \begin{itemize}
            \item \textbf{States (S)}: All possible situations.
            \item \textbf{Actions (A)}: Choices available to the decision maker.
            \item \textbf{Transition Probabilities (P)}: The probability of reaching a next state given the current state and action.
            \item \textbf{Rewards (R)}: Feedback received after transitioning from one state to another.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Dynamic Nature: MDPs effectively handle environments that change over time.
            \item Learning Optimal Policies: MDPs help decision-makers learn the best actions to maximize long-term reward.
            \item Real-World Impact: Understanding MDPs is crucial for applications in various fields including robotics and AI development.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{MDPs in Practice - Case Studies}
    \begin{enumerate}
        \item \textbf{Autonomous Driving}
            \begin{itemize}
                \item \textbf{States}: Positions on the road, nearby vehicles, traffic signals.
                \item \textbf{Actions}: Accelerate, brake, turn, maintain speed.
                \item \textbf{Reward}: Positive for safe driving and quick destination reaching; negative for accidents.
            \end{itemize}
        \item \textbf{Robot Navigation}
            \begin{itemize}
                \item \textbf{States}: Different positions within the building.
                \item \textbf{Actions}: Move North, South, East, West.
                \item \textbf{Reward}: Positive for reaching the target; negative for hitting obstacles.
            \end{itemize}
        \item \textbf{Game Playing (e.g., Chess)}
            \begin{itemize}
                \item \textbf{States}: All possible configurations of pieces on the board.
                \item \textbf{Actions}: Moves allowed by chess rules.
                \item \textbf{Reward}: Positive for winning; negative for losing pieces.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Basic MDP Setup}
    \begin{block}{Basic MDP Structure}
        \begin{itemize}
            \item \textbf{States (S)}: \{S1, S2, S3\}
            \item \textbf{Actions (A)}: \{A1, A2\}
            \item \textbf{Transition Model (P)}:
                \begin{equation}
                    P(S2|S1, A1) = 0.7, \quad P(S3|S1, A1) = 0.3
                \end{equation}
            \item \textbf{Rewards (R)}:
                \begin{equation}
                    R(S1, A1) = 5, \quad R(S2, A2) = 10
                \end{equation}
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        The examples demonstrate the flexibility of MDPs for modeling decision-making in uncertain environments, showcasing their significance in modern AI systems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - MDP Components}
    \begin{block}{Recap of MDP Components}
        Markov Decision Processes (MDPs) are fundamental frameworks used in reinforcement learning. The key components include:
    \end{block}
    \begin{enumerate}
        \item \textbf{States (S)}: Finite set representing all possible situations.
        \item \textbf{Actions (A)}: Finite set of possible actions in each state.
        \item \textbf{Transition Function (T)}: Probability of moving from one state to another given an action, defined as \(T(s, a, s') = P(s' | s, a)\).
        \item \textbf{Reward Function (R)}: Reward after transitioning from one state to another via action, represented as \(R(s, a, s')\).
        \item \textbf{Discount Factor ($\gamma$)}: Value between 0 and 1 that prioritizes immediate rewards over future rewards.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Importance of MDPs}
    \begin{block}{Importance of MDPs in Reinforcement Learning}
        MDPs are crucial for structured decision making in uncertain environments. They enable agents to:
    \end{block}
    \begin{itemize}
        \item \textbf{Optimize Decision-Making:} Compute optimal policies to maximize cumulative rewards.
        \item \textbf{Facilitate Learning:} Algorithms like Q-learning utilize MDPs to learn from interactions.
        \item \textbf{Handle Uncertainty:} Probabilities in T allow agents to adapt to variable environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Future Topics}
    \begin{block}{Connection to Future Topics}
        Understanding MDPs is essential for advancing into complex reinforcement learning topics, including:
    \end{block}
    \begin{itemize}
        \item \textbf{Policy Optimization:} Prepares for learning about policy gradient methods.
        \item \textbf{Exploration vs. Exploitation:} Strategies for balancing exploration and exploitation.
        \item \textbf{Partially Observable MDPs (POMDPs):} Understanding scenarios of incomplete information.
    \end{itemize}
    
    \begin{block}{Key Takeaways}
        MDPs are vital for modeling decisions in uncertain environments, guiding agents towards optimal behavior.
    \end{block}
    
    \begin{equation}
        V^{\pi}(s) = R(s, \pi(s)) + \gamma \sum_{s'} T(s, \pi(s), s') V^{\pi}(s')
    \end{equation}
    This equation shows how expected rewards are calculated over time in MDPs.
\end{frame}


\end{document}