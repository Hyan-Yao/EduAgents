\frametitle{Types of Activation Functions - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{1} % Continue numbering from previous frame
        \item \textbf{ReLU (Rectified Linear Unit)}
            \begin{itemize}
                \item \textbf{Formula:}
                \[
                f(x) = \max(0, x)
                \]
                \item \textbf{Characteristics:}
                    \begin{itemize}
                        \item Output range: [0, âˆž)
                        \item Non-saturating for positive inputs, with constant gradient (1).
                    \end{itemize}
                \item \textbf{Use Cases:}
                    \begin{itemize}
                        \item Common in hidden layers of deep networks.
                        \item Handles large input values well, promoting sparse activations.
                    \end{itemize}
                \item \textbf{Limitations:}
                    \begin{itemize}
                        \item Can suffer from the 'dying ReLU' problem, leading to inactive neurons.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Tanh (Hyperbolic Tangent)}
            \begin{itemize}
                \item \textbf{Formula:}
                \[
                \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
                \]
                \item \textbf{Characteristics:}
                    \begin{itemize}
                        \item Output range: (-1, 1)
                        \item Zero-centered, leading to potentially better performance than sigmoid.
                    \end{itemize}
                \item \textbf{Use Cases:}
                    \begin{itemize}
                        \item Suitable for hidden layers when centered input is needed.
                    \end{itemize}
                \item \textbf{Limitations:}
                    \begin{itemize}
                        \item Can also suffer from vanishing gradient problem.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
