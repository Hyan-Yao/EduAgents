\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 4: Regression Analysis}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Analysis - Overview}
    \begin{block}{Definition}
        Regression Analysis is a powerful statistical method used in data mining that helps to predict continuous outcomes (dependent variables) based on one or more input variables (independent variables). It examines the relationship between these variables to model and understand underlying trends in the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Analysis - Key Concepts}
    \begin{itemize}
        \item \textbf{Dependent and Independent Variables:}
        \begin{itemize}
            \item \textbf{Dependent Variable (Y):} The outcome we want to predict (e.g., sales revenue, temperature).
            \item \textbf{Independent Variable(s) (X):} The input variables that help in making predictions (e.g., advertising budget, time of year).
        \end{itemize}
        
        \item \textbf{Types of Regression:}
        \begin{itemize}
            \item Simple Linear Regression: Involves one independent variable.
            \item Multiple Linear Regression: Involves two or more independent variables.
        \end{itemize}

        \item \textbf{The Regression Equation:}
        \begin{equation}
            Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
        \end{equation}
        Where \(Y\) is the predicted outcome, \(X\) are the independent variables, \(\beta\) are the coefficients, and \(\epsilon\) is the error term.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Regression Analysis - Example}
    \begin{block}{Example Scenario}
        Imagine you are a marketer trying to forecast sales based on advertising spending.
        \begin{itemize}
            \item \textbf{Dependent Variable (Y):} Sales (in dollars).
            \item \textbf{Independent Variable (X):} Advertising Spending (in dollars).
        \end{itemize}
        
        Regression Analysis would provide you with:
        \begin{itemize}
            \item A line of best fit through your data points.
            \item A formula that might look like \(Y = 200 + 5X\), implying for every dollar spent on advertising, sales increase by $5.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Purpose of Regression Analysis}
    Understanding the role of regression analysis in modeling relationships between variables and making predictions based on data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Regression Analysis}
    
    \begin{block}{Definition}
        Regression analysis is a statistical method that allows us to examine the relationship between one dependent variable and one or more independent variables. 
    \end{block}
    
    \begin{itemize}
        \item It helps us understand how the typical value of the dependent variable changes when any one of the independent variables is varied, while others are fixed.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Roles of Regression Analysis}
    
    \begin{enumerate}
        \item \textbf{Modeling Relationships:}
            \begin{itemize}
                \item \textit{Quantifies Relationships:} Regression quantifies the strength of relationships. 
                \item \textit{Example:} Predicting house prices based on square footage.
            \end{itemize}

        \item \textbf{Making Predictions:}
            \begin{itemize}
                \item \textit{Forecasting Trends:} Used to predict future outcomes in various fields.
                \item \textit{Example:} Using past sales data to predict future sales.
            \end{itemize}
        
        \item \textbf{Identifying Relationships:}
            \begin{itemize}
                \item \textit{Determining Influences:} Shows which independent variables significantly influence the dependent variable.
                \item \textit{Key Insight:} Identifying factors such as advertising and pricing helps in decision-making.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula of Simple Linear Regression}
    
    The simple linear regression model can be expressed as:
    
    \begin{equation}
        Y = \beta_0 + \beta_1X + \epsilon
    \end{equation}
    
    Where:
    \begin{itemize}
        \item $Y$ = dependent variable
        \item $X$ = independent variable
        \item $\beta_0$ = y-intercept
        \item $\beta_1$ = slope of the line
        \item $\epsilon$ = error term
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Versatility:} Applicable in fields like economics, biology, engineering, and social sciences.
        \item \textbf{Types of Regression:} Various types including multiple regression, polynomial regression, and logistic regression are tailored for complex relationships.
        \item \textbf{Statistical Significance:} Understanding regression output involves assessing statistical significance, often indicated by p-values.
    \end{itemize}
    
    By recognizing the purpose and applications of regression analysis, foundational understanding is established for exploring more complex statistical methods in subsequent slides.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - Introduction}
    \begin{block}{Introduction to Regression Techniques}
        Regression analysis is a powerful statistical tool used to model and analyze the relationships between variables. 
        Understanding different types of regression is essential to choose the appropriate technique depending on the nature of data and research objectives.
    \end{block}
    
    Here are four fundamental types of regression:
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - Linear Regression}
    \begin{block}{1. Linear Regression}
        \begin{itemize}
            \item \textbf{Definition}: A method to model the linear relationship between a dependent variable (Y) and one independent variable (X).
            \item \textbf{Equation}:
            \begin{equation}
            Y = b_0 + b_1X + \epsilon
            \end{equation}
            where:
            \begin{itemize}
                \item $Y$ = dependent variable
                \item $X$ = independent variable
                \item $b_0$ = y-intercept
                \item $b_1$ = slope of the line
                \item $\epsilon$ = error term
            \end{itemize}
            \item \textbf{Example}: Predicting house prices based on square footage.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Regression - Multiple, Polynomial, and Logistic Regression}
    \begin{block}{2. Multiple Regression}
        \begin{itemize}
            \item \textbf{Definition}: An extension of linear regression that models the relationship between a dependent variable and multiple independent variables.
            \item \textbf{Equation}:
            \begin{equation}
            Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n + \epsilon
            \end{equation}
            \item \textbf{Example}: Estimating a person's salary based on their education level, years of experience, and age.
        \end{itemize}
    \end{block}

    \begin{block}{3. Polynomial Regression}
        \begin{itemize}
            \item \textbf{Definition}: A form of regression that models the relationship between a dependent variable and an independent variable as an nth degree polynomial.
            \item \textbf{Equation}:
            \begin{equation}
            Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n + \epsilon
            \end{equation}
            \item \textbf{Example}: Fitting a quadratic curve to data that shows a curvilinear relationship, like the trajectory of a ball thrown in the air.
        \end{itemize}
    \end{block}

    \begin{block}{4. Logistic Regression}
        \begin{itemize}
            \item \textbf{Definition}: A regression analysis used for prediction of outcome of a categorical dependent variable based on one or more predictor variables.
            \item \textbf{Equation}:
            \begin{equation}
            P(Y=1) = \frac{1}{1 + e^{-(b_0 + b_1X)}}
            \end{equation}
            where:
            \begin{itemize}
                \item $P(Y=1)$ = probability of the dependent variable equaling 1
                \item $e$ = base of the natural logarithm
            \end{itemize}
            \item \textbf{Example}: Predicting whether a student will pass or fail an exam based on study hours.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Next Steps}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Purpose}: Each type of regression serves different purposes, from quantifying relationships (linear and multiple regression) to classifying outcomes (logistic regression).
            \item \textbf{Choice of Method}: The choice of which regression to use depends on the nature of the data, the relationship being examined, and the research question.
            \item \textbf{Assumptions}: Each regression technique comes with its own set of assumptions that need to be satisfied to ensure the validity of the model.
        \end{itemize}
    \end{block}
    
    \textbf{Next Steps}: Prepare for a deep dive into \textbf{Linear Regression}, where we will explore its components, how to interpret the results, and practical application examples.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Overview}
    Linear Regression is a foundational statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (predictors). 
    \begin{itemize}
        \item Assumes a linear relationship.
        \item Graphically represented by a straight line.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - The Equation of a Line}
    The equation for a linear regression model is given by:
    \begin{equation}
    y = mx + b 
    \end{equation}
    Where:
    \begin{itemize}
        \item $y$ = dependent variable (what we are predicting)
        \item $x$ = independent variable (input feature)
        \item $m$ = slope of the line (indicates the change in $y$ for a unit change in $x$)
        \item $b$ = y-intercept (the value of $y$ when $x = 0$)
    \end{itemize}
    
    \begin{block}{Example:}
    Predicting a student's final exam score based on hours of study:
    \begin{equation}
    \text{Score} = 5 \times (\text{Hours Studied}) + 40 
    \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Slope and Intercept}
    Understanding Slope and Intercept:
    \begin{enumerate}
        \item \textbf{Slope ($m$)}:
        \begin{itemize}
            \item Positive Slope: Direct relationship (as one variable increases, so does the other).
            \item Negative Slope: Inverse relationship (as one variable increases, the other decreases).
        \end{itemize}
        
        \item \textbf{Intercept ($b$)}:
        \begin{itemize}
            \item Represents the starting point of the line on the y-axis.
            \item Insight into expected outcome when no predictors impact the dependent variable.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Key Concepts}
    How Linear Regression Models Relationships:
    \begin{itemize}
        \item Fitting the Line: Objective is to find the best-fitting line that minimizes the distance (errors) between actual data points and predicted values, often via least squares.
        \item Residuals: Differences between observed values and predicted values by the regression model.
    \end{itemize}

    \begin{block}{Key Points to Emphasize}
    \begin{itemize}
        \item Assumptions: Linear relationship, homoscedasticity, normally distributed errors.
        \item Applications: Economics, biology, engineering, social sciences for predictive modeling.
        \item Limitations: Sensitive to outliers, poor performance with non-linear relationships.
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression - Formula Recap}
    For a single linear regression model:
    \begin{equation}
    y = m_1 x_1 + m_2 x_2 + \ldots + m_n x_n + b 
    \end{equation}
    Where $m_1, m_2, \ldots, m_n$ are the slopes for multiple independent variables.

    \begin{block}{Conclusion}
    Linear regression serves as a powerful tool for understanding and predicting outcomes based on relationships between variables. Mastery of this concept lays the foundation for more complex regression techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multiple Regression - What is it?}
    Multiple Regression is a statistical technique used to model the relationship between one dependent variable and two or more independent variables. It extends the concept of simple linear regression, which involves only one predictor, allowing for a more comprehensive analysis of complex situations where multiple factors influence an outcome.

    \begin{block}{Formula}
        The multiple regression equation can be expressed as:
        \[
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
        \]
        Where:
        \begin{itemize}
            \item \(Y\) = dependent variable (outcome)
            \item \(X_1, X_2, \ldots, X_n\) = independent variables (predictors)
            \item \(\beta_0\) = y-intercept (constant term)
            \item \(\beta_1, \beta_2, \ldots, \beta_n\) = coefficients for each independent variable
            \item \(\epsilon\) = error term
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multiple Regression - Why Use It?}
    \begin{enumerate}
        \item \textbf{Complex Relationships}: Many real-world scenarios involve multiple factors affecting a single outcome. For example, predicting house prices depends on size, location, age, and amenities.
        
        \item \textbf{Better Predictions}: By including several predictors, models can provide more accurate and personalized forecasts.
        
        \item \textbf{Control of Confounding Variables}: Helps adjust for potential confounders, ensuring clearer insights into the relationship between variables.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multiple Regression - Example}
    Imagine you're a data analyst working for a real estate company. You want to predict the selling price of homes based on the following factors:
    \begin{itemize}
        \item Square footage of the house
        \item Number of bedrooms
        \item Number of bathrooms
        \item Location quality rating (on a scale from 1 to 10)
    \end{itemize}

    Your multiple regression model might look like this:
    \[
    Price = \beta_0 + \beta_1(SquareFootage) + \beta_2(Bedrooms) + \beta_3(Bathrooms) + \beta_4(LocationRating) + \epsilon
    \]
    By fitting this model to historical data, you can generate estimates for \(\beta\) coefficients that reflect the impact of each factor on the home price.

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Interpreting coefficients, assumptions, and evaluating model fit.
            \item Understand how multiple regression helps in examining complex relationships.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Polynomial Regression - Overview}
    \begin{block}{Understanding Polynomial Regression}
        \textbf{Definition:}
        Polynomial regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth degree polynomial. Unlike linear regression, which fits a straight line, polynomial regression can fit curves, making it suitable for modeling non-linear relationships.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Polynomial Regression - Applications}
    \begin{block}{When to Use Polynomial Regression}
        \begin{itemize}
            \item \textbf{Non-linear Relationships:} Appropriate when the scatter plot shows clear curvature.
            \item \textbf{Modeling Trends:} Useful for fitting complex datasets with non-linear trends.
            \item \textbf{Improving Model Fit:} Can provide a better fit compared to linear models, especially with higher degree polynomials.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Polynomial Regression - Example and Formula}
    \begin{block}{Example Scenario}
        Consider a dataset tracking the growth of plants over time under different light conditions, where the relationship between light exposure (X) and growth height (Y) shows a curvilinear pattern.
    \end{block}

    \begin{block}{Polynomial Equation}
        The general form of a polynomial regression equation for degree n is:
        \begin{equation} 
            Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \ldots + b_nX^n 
        \end{equation}
        Where:
        \begin{itemize}
            \item \(Y\) = Dependent variable (response)
            \item \(b_0, b_1, \ldots, b_n\) = Coefficients
            \item \(X\) = Independent variable (predictor)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Polynomial Regression - Practical Implementation}
    \begin{block}{Practical Implementation}
        In Python, you can apply polynomial regression using libraries like \texttt{numpy} and \texttt{scikit-learn}. Here's an example code snippet:
        \begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Sample Data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 8, 18, 32, 50])

# Transforming to Polynomial Features
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Fitting the Polynomial Regression Model
model = LinearRegression()
model.fit(X_poly, y)

# Predicting and Plotting
X_range = np.linspace(1, 5, 100).reshape(-1, 1)
y_pred = model.predict(poly.transform(X_range))

plt.scatter(X, y, color='red')
plt.plot(X_range, y_pred, color='blue')
plt.title('Polynomial Regression Fit')
plt.xlabel('X (Light Exposure)')
plt.ylabel('Y (Plant Growth Height)')
plt.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Polynomial Regression - Summary}
    \begin{block}{Summary}
        Polynomial regression extends linear regression by allowing for non-linear relationships. 
        \begin{itemize}
            \item Useful for data exhibiting curvature.
            \item Watch out for overfitting with higher degree polynomials.
            \item Validate models using techniques like cross-validation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Overview}
    \begin{block}{Definition}
    Logistic Regression is a statistical method used for modeling binary outcomes where the response variable can take on two possible values (0 and 1). 
    \end{block}
    
    \begin{itemize}
        \item Predicting probability of events based on predictor variables.
        \item Useful in various applications (e.g., medical outcomes, customer behavior).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Key Concepts}
    \begin{enumerate}
        \item \textbf{Binary Outcomes:}
        \begin{itemize}
            \item Applied when the outcome variable is binary (Yes/No, Success/Failure).
            \item Examples: Default on a loan, Passing an exam, Disease prediction.
        \end{itemize}
        
        \item \textbf{Logistic Function:}
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
        \begin{itemize}
            \item Transforms linear equation into probability between 0 and 1.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Logistic Regression - Parameter Interpretation}
    \begin{block}{Coefficients (\(\beta\))}
        The coefficients indicate how the log-odds of the outcome change with a unit increase in predictor variables.
    \end{block}
    \begin{itemize}
        \item Positive coefficients imply an increase in probability of the outcome being 1.
        \item Negative coefficients indicate a decrease in probability.
    \end{itemize}
    
    \begin{block}{Example: Predicting Customer Churn}
        \begin{itemize}
            \item Outcome Variable: Churn (1 = yes, 0 = no)
            \item Predictor Variables: Monthly Spending, Number of Support Calls
        \end{itemize}
        Model:
        \begin{itemize}
            \item $\beta_0 = -1.5$ (Intercept)
            \item $\beta_1 = 0.02$ (Monthly Spending)
            \item $\beta_2 = 0.5$ (Support Calls)
        \end{itemize}
        Interpretation:
        \begin{itemize}
            \item Every $1 increase in spending raises the odds of churn.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Regression Analysis}
    
    \begin{block}{Key Assumptions}
        \begin{enumerate}
            \item Linearity
            \item Independence
            \item Homoscedasticity
            \item Normality
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linearity}
    
    \begin{itemize}
        \item \textbf{Definition}: The relationship between independent and dependent variables should be linear.
        \item \textbf{Example}: Increasing advertising budget should lead to a proportional increase in sales.
        \item \textbf{Assessment}: Visualize with scatterplots; check residual plots for linearity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Independence and Homoscedasticity}
    
    \begin{itemize}
        \item \textbf{Independence}
        \begin{itemize}
            \item \textbf{Definition}: Residuals must be independent of each other.
            \item \textbf{Example}: In time-series data, autocorrelation may violate this assumption.
            \item \textbf{Assessment}: Use the Durbin-Watson test to check for independence.
        \end{itemize}
        
        \item \textbf{Homoscedasticity}
        \begin{itemize}
            \item \textbf{Definition}: Variance of residuals should be constant across levels of independent variables.
            \item \textbf{Example}: Error spread in income predictions should remain consistent.
            \item \textbf{Assessment}: Check residual plots for patterns indicating variance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normality and Emphasizing Key Points}
    
    \begin{itemize}
        \item \textbf{Normality}
        \begin{itemize}
            \item \textbf{Definition}: Residuals should be approximately normally distributed.
            \item \textbf{Example}: Non-normal residuals can lead to unreliable hypothesis test results.
            \item \textbf{Assessment}: Use Q-Q plots or the Shapiro-Wilk test.
        \end{itemize}
        
        \item \textbf{Key Points}
        \begin{itemize}
            \item Violating these assumptions can lead to incorrect conclusions.
            \item Validate each assumption for robust regression models.
        \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula and Diagnostic Tools}
    
    \begin{itemize}
        \item \textbf{Durbin-Watson Statistic}:
        \begin{equation}
            DW = \frac{\sum (e_t - e_{t-1})^2}{\sum e_t^2}
        \end{equation}
        Where \( e_t \) is the residual at time \( t \).
        
        \item \textbf{Homoscedasticity Assessment}: Use Breusch-Pagan or White tests.
        
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrap-Up}
    
    Understanding these assumptions is crucial for:
    \begin{itemize}
        \item Developing reliable regression models
        \item Drawing valid inferences from data
    \end{itemize}
    Always validate assumptions before interpreting results!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics}
    \begin{block}{Introduction}
        When building regression models, it's crucial to evaluate their performance. 
        This slide covers four primary metrics used to evaluate regression models:
        \textbf{R-squared, Adjusted R-squared, RMSE, and MAE.}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Part 1}
    \begin{block}{1. R-squared (R²)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the proportion of variance in the dependent variable explained by the independent variable(s).
            \item \textbf{Interpretation}:
                \begin{itemize}
                    \item Values range from 0 (no variance explained) to 1 (perfect prediction).
                \end{itemize}
            \item \textbf{Formula}:
            \begin{equation}
                R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
            \end{equation}
            Where:
            \begin{itemize}
                \item \( SS_{res} \) = sum of squared residuals
                \item \( SS_{tot} \) = total sum of squares
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Part 2}
    \begin{block}{Example of R-squared}
        If a model has an R² of 0.85, it means 85\% of the variance in the response variable is explained by the predictors.
    \end{block}
    
    \begin{block}{2. Adjusted R-squared}
        \begin{itemize}
            \item \textbf{Definition}: Modifies R² to account for the number of predictors in the model.
            \item \textbf{Interpretation}: Helps prevent overfitting; decreases if unnecessary predictors are added.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Adjusted } R^2 = 1 - \left( \frac{1 - R^2}{n - p - 1} \right) \times (n - 1)
            \end{equation}
            Where:
            \begin{itemize}
                \item \( n \) = number of observations
                \item \( p \) = number of predictors
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Part 3}
    \begin{block}{Example of Adjusted R-squared}
        A model with an R² of 0.90 and 5 predictors may have an Adjusted R² of 0.88, indicating some predictors may not be improving the model adequately.
    \end{block}
    
    \begin{block}{3. Root Mean Squared Error (RMSE)}
        \begin{itemize}
            \item \textbf{Definition}: RMSE measures the standard deviation of the residuals (prediction errors).
            \item \textbf{Interpretation}: Lower RMSE values indicate more accurate predictions.
            \item \textbf{Formula}:
            \begin{equation}
                RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Part 4}
    \begin{block}{Example of RMSE}
        If RMSE = 3, the model's predictions are, on average, 3 units away from the actual values.
    \end{block}
    
    \begin{block}{4. Mean Absolute Error (MAE)}
        \begin{itemize}
            \item \textbf{Definition}: MAE measures the average magnitude of errors in predictions.
            \item \textbf{Interpretation}: Provides a straightforward average error measure.
            \item \textbf{Formula}:
            \begin{equation}
                MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Conclusion}
    \begin{block}{Example of MAE}
        If MAE = 2, on average, the model’s predictions are 2 units away from the actual values.
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item R² and Adjusted R² help assess model fit, but don't ensure better predictive power with more variables.
            \item RMSE indicates prediction accuracy and is sensitive to outliers.
            \item MAE is a clear metric for average prediction error.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding these metrics is vital for evaluating regression models, guiding improvements, and ensuring robust predictive performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{References for Further Reading}
    \begin{itemize}
        \item "Introduction to Statistical Learning" – Chapter on Regression
        \item Online courses on regression analysis (e.g., Coursera, edX)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Regression Analysis - Introduction}
    Regression analysis is a statistical tool that helps us understand relationships between variables. It finds applications in various fields, enabling informed decision-making based on data insights. 
    \begin{itemize}
        \item Real-world applications in:
        \begin{itemize}
            \item Business
            \item Healthcare
            \item Social Sciences
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Regression Analysis - Business}
    \begin{enumerate}
        \item \textbf{Sales Forecasting} 
            \begin{itemize}
                \item Predict future sales based on historical data.
                \item Example: Retail store analyzes sales for forecast.
            \end{itemize}

        \item \textbf{Pricing Strategies}
            \begin{itemize}
                \item Determine optimal pricing by analyzing demand.
                \item Example: Airline adjusts ticket prices based on regression analysis.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Regression Analysis - Healthcare \& Social Sciences}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Healthcare Applications}
            \begin{enumerate}
                \item \textbf{Predicting Disease Outbreaks}
                    \begin{itemize}
                        \item Use models to predict disease spread.
                        \item Example: Analyze flu cases with weather variables.
                    \end{itemize}
                
                \item \textbf{Treatment Effectiveness}
                    \begin{itemize}
                        \item Evaluate treatment impact controlling for other factors.
                        \item Example: Assess new drug effect adjusting for demographics.
                    \end{itemize}
            \end{enumerate}

        \item \textbf{Social Sciences Applications}
            \begin{enumerate}
                \item \textbf{Educational Outcomes}
                    \begin{itemize}
                        \item Analyze factors influencing student performance.
                        \item Example: Correlate attendance with grades.
                    \end{itemize}
                
                \item \textbf{Economic Research}
                    \begin{itemize}
                        \item Examine relationships among economic indicators.
                        \item Example: Explore unemployment versus GDP growth.
                    \end{itemize}
            \end{enumerate}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Versatility across diverse fields
            \item Enables data-driven decision making
            \item Real-world examples illustrate practical utility
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        The applications of regression analysis provide insights that inform critical decisions across various fields, emphasizing its relevance in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Formula}
    The simple linear regression formula is given by:
    \begin{equation}
        Y = a + bX + \epsilon
    \end{equation}
    Where:
    \begin{itemize}
        \item $Y$ = dependent variable (e.g., sales)
        \item $a$ = intercept
        \item $b$ = slope (coefficient)
        \item $X$ = independent variable (e.g., advertising spend)
        \item $\epsilon$ = error term
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Software Implementation}
    Regression analysis is commonly performed using software such as:
    \begin{itemize}
        \item R
        \item Python (libraries like scikit-learn)
        \item Excel
    \end{itemize}
\end{frame}


\end{document}