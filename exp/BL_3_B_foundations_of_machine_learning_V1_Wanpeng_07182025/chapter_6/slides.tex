\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}

% Title Page Information
\title[Model Evaluation and Validation]{Chapter 6: Model Evaluation and Validation}
\author[Author Name]{Your Name}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  Email: email@university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Model Evaluation and Validation?}
    \begin{itemize}
        \item Critical processes in machine learning.
        \item Assesses model performance and generalization to unseen data.
        \item Helps identify weaknesses before deployment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Are Model Evaluation and Validation Important?}
    \begin{enumerate}
        \item \textbf{Performance Measurement}
        \begin{itemize}
            \item Quantitative metrics on prediction accuracy.
        \end{itemize}
        
        \item \textbf{Overfitting Detection}
        \begin{itemize}
            \item Identifies models that perform well on training data but poorly on unseen data.
        \end{itemize}
        
        \item \textbf{Model Comparison}
        \begin{itemize}
            \item Evaluates multiple models for optimal selection.
        \end{itemize}
        
        \item \textbf{Trust and Reliability}
        \begin{itemize}
            \item Essential for critical applications (e.g., healthcare, finance).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Validation Sets}: Exclusive data portion for evaluating model performance post-training.
        \item \textbf{Cross-Validation}: Divides data into subsets for multiple training rounds, averaging results for robustness.
        \item \textbf{Performance Metrics}:
        \begin{itemize}
            \item \textit{Accuracy} = $\frac{\text{True Positives} + \text{True Negatives}}{\text{Total Samples}}$
            \item \textit{Precision} = $\frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$
            \item \textit{Recall} = $\frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Illustration}
    \begin{block}{Building a Classification Model}
        \textbf{Scenario}: Distinguishing between cats and dogs.
        
        \begin{itemize}
            \item \textbf{Training Phase}: Learns from labeled images.
            \item \textbf{Validation Phase}: Tests on separate data; e.g., 80 correct out of 100 gives 80\% accuracy.
            \item \textbf{Evaluation Metrics}: Includes precision and recall to assess classification quality.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Takeaways}
    \begin{itemize}
        \item \textbf{Conclusion}: Essential steps in the machine learning lifecycle to ensure robustness and generalization.
        \item \textbf{Takeaways}:
        \begin{itemize}
            \item Validate with a dedicated validation set.
            \item Use appropriate performance metrics.
            \item Employ cross-validation for result reliability.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    In this section, we will outline the learning objectives related to model evaluation techniques and performance metrics. Understanding these concepts is critical for determining how well our machine learning models perform and ensuring their reliability and validity in practical applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Techniques and Metrics}
    \begin{enumerate}
        \item \textbf{Understand the Importance of Model Evaluation}
        \begin{itemize}
            \item Recognize why model evaluation is essential in the machine learning lifecycle.
            \item Discuss how evaluation impacts decision-making in real-world scenarios.
        \end{itemize}
        
        \item \textbf{Identify Key Evaluation Techniques}
        \begin{itemize}
            \item \textbf{Cross-Validation}: Importance of k-fold cross-validation for robust estimation.
            \item \textbf{Train/Test Split}: Benefits and limitations of dataset division.
        \end{itemize}

        \item \textbf{Familiarize with Performance Metrics}
        \begin{itemize}
            \item \textbf{Accuracy}: Overall correctness.
            \item \textbf{Precision and Recall}: Trade-offs in imbalanced datasets.
            \item \textbf{F1 Score}: Harmonic mean of precision and recall.
            \item \textbf{ROC AUC}: Analysis for binary classification models.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Critical Thinking and Teamwork}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Develop Critical Thinking Skills}  
        Encourage the evaluation of model results and determine appropriate metrics for various scenarios.

        \item \textbf{Foster Team Collaboration}  
        Promote teamwork in analyzing performance and encourage diverse viewpoints in discussions.

        \item \textbf{Apply Evaluation Techniques in Practical Scenarios}  
        Hands-on projects to apply learned techniques; analyze and compare model performances.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Metrics}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{The role of evaluation in reducing overfitting}: Importance of validation techniques for generalization.
            \item \textbf{Choosing the right metric}: Differences in metrics suitable for various applications, like healthcare.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics Formulas}
    Key performance metrics include:
    \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
    \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
    \begin{equation}
        F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code for Metrics Calculation}
    Here is an example code snippet in Python to calculate the metrics:
    \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assume y_true and y_pred are the true labels and predicted labels respectively
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Model Evaluation?}
    \begin{block}{Definition of Model Evaluation}
        Model evaluation is the process of systematically assessing the performance of a predictive model to determine its accuracy and reliability. This assessment is crucial to understand how well the model generalizes to unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Relevance to Model Performance Assessment}
    Model evaluation plays a vital role in:
    \begin{enumerate}
        \item \textbf{Understanding Model Effectiveness}: Determines if the model meets intended goals.
        \item \textbf{Comparative Analysis}: Allows comparison of multiple models based on evaluation metrics.
        \item \textbf{Guiding Adjustments and Improvements}: Identifies areas for model iterations and refinements.
        \item \textbf{Avoiding Overfitting}: Ensures that the model generalizes well to unseen data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Considerations in Model Evaluation}
    \begin{itemize}
        \item \textbf{Evaluation Metrics}:
            \begin{itemize}
                \item \textbf{Accuracy}: Proportion of correctly predicted instances.
                \item \textbf{Precision}: Proportion of true positive results in all positive predictions.
                \item \textbf{Recall (Sensitivity)}: Proportion of true positive results in all actual positives.
                \item \textbf{F1 Score}: Harmonic mean of precision and recall.
                \item \textbf{AUC-ROC}: Area under the receiver operating characteristic curve.
            \end{itemize}
        \item \textbf{Data Splits}: Techniques like train-test splits and cross-validation enhance model reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Model Validation Techniques}
    \begin{block}{Introduction}
        Model validation techniques assess how well a model generalizes to unseen data, ensuring it makes accurate predictions on new datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Validation Techniques - Part 1}
    \begin{enumerate}
        \item \textbf{Train-Test Split}
        \begin{itemize}
            \item \textit{Explanation:} Divides the dataset into training (e.g., 80\%) and testing sets (e.g., 20\%).
            \item \textit{Process:}
            \begin{enumerate}
                \item Randomly shuffle the dataset.
                \item Split into training and test sets.
                \item Train the model using the training set.
                \item Evaluate performance using the test set.
            \end{enumerate}
            \item \textit{Example:} For 1,000 samples, use 800 for training and 200 for testing.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Validation Techniques - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{1} % Continue numbering from the previous frame
        \item \textbf{Cross-Validation}
        \begin{itemize}
            \item \textit{Explanation:} Mitigates overfitting and provides a reliable estimate of model performance.
            \item \textit{K-Fold Cross-Validation:}
            \begin{enumerate}
                \item Split the dataset into \( K \) equally sized folds.
                \item For each fold:
                \begin{itemize}
                    \item Train using \( K-1 \) folds.
                    \item Test on the remaining fold.
                \end{itemize}
                \item Average the evaluation metrics across all \( K \) iterations to obtain the final score.
            \end{enumerate}
            \item \textit{Example:} For \( K=5 \), divide into 5 parts. Train 5 times, testing on each part sequentially.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Validation Techniques - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        \item \textbf{Leave-One-Out Cross-Validation (LOOCV)}
        \begin{itemize}
            \item \textit{Explanation:} Each sample is treated as a test set once, with all other samples forming the training set.
            \item \textit{Pros/Cons:}
            \begin{itemize}
                \item \textbf{Pros:} Exhaustive evaluation.
                \item \textbf{Cons:} Computationally expensive for large datasets.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Purpose of Validation:} Ensure model's generalization ability and avoid overfitting.
        \item \textbf{Selection of Technique:} Depends on dataset size, complexity, and specific model used.
        \item \textbf{Performance Metrics:} Common metrics include accuracy, precision, recall, F1-score, and AUC-ROC.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Fold Cross-Validation Score}
    \begin{equation}
        \text{CV Score} = \frac{1}{K} \sum_{i=1}^K \text{Score}_i
    \end{equation}
    \text{Where } \text{Score}_i \text{ is the model's performance metric for fold } i.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Implementing effective model validation techniques is essential for building reliable machine learning models. Understanding different methods allows practitioners to select the appropriate approach for their datasets and improve model robustness.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation Explained - Overview}
    \begin{block}{Understanding Cross-Validation}
        Cross-validation is a statistical method used to estimate the skill of machine learning models. Its primary purpose is to assess how results will generalize to an independent dataset, helping to minimize issues like overfitting.
    \end{block}
    \begin{itemize}
        \item \textbf{Definition:} Evaluation technique for model performance on unseen data.
        \item \textbf{Goal:} Improve generalization to new, unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation Principles}
    \begin{block}{Key Principles}
        \begin{enumerate}
            \item \textbf{Training vs. Validation Sets}:
                \begin{itemize}
                    \item Dataset is split into training and testing datasets.
                    \item Cross-validation further divides training data into multiple parts.
                \end{itemize}
            \item \textbf{Generalization:}
                \begin{itemize}
                    \item Ensures machine learning models perform well in real-world scenarios.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Fold Cross-Validation}
    \begin{block}{K-Fold Description}
        The dataset is divided into 'k' subsets (or folds). The model is trained on 'k-1' subsets and tested on the remaining one, repeated for each fold.
    \end{block}
    \begin{itemize}
        \item \textbf{Example:} 
            \begin{itemize}
                \item For a dataset of 100 instances and k=5:
                \begin{itemize}
                    \item Split into 5 equal parts of 20 instances.
                    \item Train on 4 parts and test on 1, repeat 5 times.
                \end{itemize}
            \end{itemize}
        \item \textbf{Average Accuracy Calculation:}
        \begin{equation}
            \text{Average Accuracy} = \frac{85 + 87 + 82 + 90 + 88}{5} = 86.4\%
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stratified K-Fold Cross-Validation}
    \begin{block}{Stratified K-Fold}
        A variation of k-fold that preserves the proportion of different classes in each fold. Useful for imbalanced datasets to ensure balanced representation.
    \end{block}
    \begin{itemize}
        \item \textbf{Benefits of Cross-Validation:}
            \begin{itemize}
                \item More robust evaluation.
                \item Reduces the risk of overfitting.
            \end{itemize}
        \item \textbf{Choosing 'k':}
            \begin{itemize}
                \item Common choices: 5 or 10 folds.
                \item Smaller 'k': less stable estimate; larger 'k': increased cost but better accuracy.
            \end{itemize}
        \item \textbf{Limitations:}
            \begin{itemize}
                \item Time-consuming for large datasets.
                \item Not suitable for time-series data.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Code Example}
    \begin{block}{Conclusion}
        Cross-validation is a powerful tool in model evaluation. Implementing methods like k-fold cross-validation is crucial for building accurate and reliable models in practice.
    \end{block}
    \begin{block}{Python Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

# Sample dataset features (X) and labels (y)
X, y = ...  # Your dataset here

kf = KFold(n_splits=5)
model = RandomForestClassifier()

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    print(f'Fold Accuracy: {accuracy_score(y_test, predictions)}')
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Metrics Overview}
    \begin{block}{Introduction}
        In model evaluation, performance metrics quantify how well a model makes predictions. Selecting the right metric is crucial depending on the problem at hand, especially in classification tasks.
    \end{block}
    \begin{block}{Key Metrics}
        This presentation will cover four key metrics:
        \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
            \item F1-Score
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy}
    \begin{block}{Definition}
        Accuracy measures the proportion of correctly identified instances out of the total instances.
    \end{block}
    \begin{block}{Formula}
        \begin{equation}
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        where:
        \begin{itemize}
            \item \(TP =\) True Positives
            \item \(TN =\) True Negatives
            \item \(FP =\) False Positives
            \item \(FN =\) False Negatives
        \end{itemize}
    \end{block}
    \begin{block}{When to Use}
        Accuracy is suitable when classes are balanced. In datasets with large imbalance, accuracy may not reflect model performance accurately.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall}
    \begin{block}{Precision}
        \begin{itemize}
            \item \textbf{Definition:} Precision quantifies the number of true positive predictions made relative to total positive predictions.
            \item \textbf{Formula:}
            \begin{equation}
                \text{Precision} = \frac{TP}{TP + FP}
            \end{equation}
            \item \textbf{When to Use:} Use precision when the cost of false positives is high.
        \end{itemize}
    \end{block}

    \begin{block}{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition:} Recall measures the proportion of true positive predictions to actual total positives.
            \item \textbf{Formula:}
            \begin{equation}
                \text{Recall} = \frac{TP}{TP + FN}
            \end{equation}
            \item \textbf{When to Use:} Recall is critical when the cost of false negatives is high.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score and Key Takeaways}
    \begin{block}{F1-Score}
        \begin{itemize}
            \item \textbf{Definition:} The F1-Score is the harmonic mean of precision and recall.
            \item \textbf{Formula:}
            \begin{equation}
                F1 = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{When to Use:} Use the F1-Score when a balance between precision and recall is needed.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Choosing the Right Metric is necessary based on problem context.
            \item Metrics should be viewed collectively; no single metric tells the whole story.
            \item Importance of metrics can vary across fields (e.g., healthcare vs. marketing).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Utilizing multiple performance metrics allows for a more comprehensive evaluation of classification models, leading to better decision-making in model selection and refinement.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Part 1}
    \textbf{What is Accuracy?}
    
    Accuracy is a fundamental performance metric used to evaluate the effectiveness of classification models. It is defined as the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances in the dataset.
    
    \begin{block}{Formula for Accuracy}
    \begin{equation}
    \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    \end{block}
    
    Where:
    \begin{itemize}
        \item \textbf{TP} = True Positives: Correctly predicted positive cases
        \item \textbf{TN} = True Negatives: Correctly predicted negative cases
        \item \textbf{FP} = False Positives: Incorrectly predicted positive cases
        \item \textbf{FN} = False Negatives: Incorrectly predicted negative cases
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Part 2}
    \textbf{When is Accuracy an Appropriate Metric?}

    Accuracy is most appropriate in the following scenarios:

    \begin{enumerate}
        \item \textbf{Balanced Datasets:} When classes in the dataset are balanced (approximately equal instances in each class).
        \item \textbf{Simple Classifications:} For straightforward tasks, such as distinguishing 'spam' from 'not spam.'
        \item \textbf{Preliminary Evaluations:} To quickly assess model performance before exploring more detailed metrics.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Part 3}
    \textbf{Limitations of Accuracy}
    
    Accuracy can be misleading in certain situations:

    \begin{itemize}
        \item \textbf{Imbalanced Datasets:} If one class significantly outnumbers another, accuracy may present a false sense of efficacy. 
        \begin{block}{Example}
            A model predicting 95 out of 100 instances as 'negatives' yields 95\% accuracy without identifying any 'positives.'
        \end{block}
        \item \textbf{Cost of Misclassification:} Significant costs associated with misclassifying one class can lead to poor decisions if accuracy is sole focus.
    \end{itemize}
    
    \textbf{Key Points:}
    - Accuracy is a high-level metric, offering a snapshot of model performance.
    - It should complement precision, recall, and other metrics for comprehensive evaluation.
    
    \textbf{Conclusion:} Use accuracy judiciously alongside other performance metrics to ensure well-rounded assessments! 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Overview}
    \begin{block}{Understanding Precision and Recall}
        Precision and recall are essential metrics for evaluating classification models, especially in scenarios involving imbalanced classes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision - Definition and Formula}
    \begin{block}{Definition of Precision}
        Precision measures the accuracy of the positive predictions made by a model. It is the ratio of true positive (TP) predictions to the total positive predictions (true positives + false positives).
    \end{block}
    
    \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}

    \begin{itemize}
        \item \textbf{True Positives (TP):} Correctly predicted positive instances.
        \item \textbf{False Positives (FP):} Incorrectly predicted positive instances.
    \end{itemize}

    \begin{block}{Example of Precision}
        Consider a binary classification model that identifies spam emails: 
        \begin{itemize}
            \item Out of 100 emails classified as spam, 80 were indeed spam (TP = 80) and 20 were not (FP = 20).
            \begin{equation}
                \text{Precision} = \frac{80}{80 + 20} = \frac{80}{100} = 0.80
            \end{equation}
        \end{itemize}
        This indicates that 80\% of the emails flagged as spam were correctly identified.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recall - Definition and Example}
    \begin{block}{Definition of Recall}
        Recall indicates how well a model captures the actual positive cases. It is defined as the ratio of true positives to the total actual positives (true positives + false negatives).
    \end{block}

    \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}

    \begin{itemize}
        \item \textbf{False Negatives (FN):} Actual positive instances that were incorrectly predicted as negative.
    \end{itemize}

    \begin{block}{Example of Recall}
        Continuing with the spam email example, if there are 100 actual spam emails and our model correctly identifies 80 of them but misses 20 (FN = 20):
        \begin{equation}
            \text{Recall} = \frac{80}{80 + 20} = \frac{80}{100} = 0.80
        \end{equation}
        Thus, the model identifies 80\% of all actual spam emails.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score Interpretation - What is the F1-Score?}
    \begin{itemize}
        \item \textbf{Definition}: The F1-score is a measure of a model's accuracy that considers both precision and recall. 
        \item \textbf{Purpose}: It provides a balance between precision (accuracy of positive predictions) and recall (ability to find all relevant instances), making it a single metric to evaluate classifier performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score Interpretation - Calculating the F1-Score}
    \begin{block}{Formulas}
        \begin{enumerate}
            \item \textbf{Precision}:
            \begin{equation}
                \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
            \end{equation}

            \item \textbf{Recall}:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
            \end{equation}

            \item \textbf{F1-Score}:
            \begin{equation}
                F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example Calculation}
        \begin{itemize}
            \item TP = 40, FP = 10, FN = 5
            \item Calculate Precision: 
            \[
            \text{Precision} = \frac{40}{40 + 10} = 0.8
            \]
            \item Calculate Recall:
            \[
            \text{Recall} = \frac{40}{40 + 5} \approx 0.889
            \]
            \item Calculate F1-Score:
            \[
            F1 \approx 0.842
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score Interpretation - When to Use the F1-Score}
    \begin{itemize}
        \item \textbf{Imbalanced Classes}: Particularly useful in scenarios where one class is significantly more prevalent (e.g., fraud detection).
        \item \textbf{High Stakes Decisions}: Important in contexts like medical diagnosis or legal decisions where false negatives are critical.
        \item \textbf{Comparative Performance}: Suitable when comparing multiple models in binary classification tasks.
    \end{itemize}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Provides a single metric that combines precision and recall.
            \item Insightful for applications with skewed class distributions.
            \item Complements other metrics (e.g., accuracy) for comprehensive evaluation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC and AUC - Introduction}
    \begin{itemize}
        \item **Receiver Operating Characteristic (ROC) Curve**:
            \begin{itemize}
                \item Graphical representation to evaluate the performance of a classification model.
                \item Illustrates the trade-off between sensitivity (True Positive Rate) and specificity (1 - False Positive Rate).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC and AUC - Key Concepts}
    \begin{itemize}
        \item **True Positive Rate (TPR)**:
            \begin{equation}
            TPR = \frac{TP}{TP + FN}
            \end{equation}
        \item **False Positive Rate (FPR)**:
            \begin{equation}
            FPR = \frac{FP}{FP + TN}
            \end{equation}
    \end{itemize}
    \begin{block}{Constructing the ROC Curve}
        \begin{itemize}
            \item Calculate TPR and FPR at different threshold values.
            \item Plot TPR against FPR to form the ROC curve.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC and AUC - Interpretation and Example}
    \begin{itemize}
        \item **Interpreting the ROC Curve**:
            \begin{itemize}
                \item Diagonal line: Represents a random guess at TPR = FPR.
                \item **Area Under the Curve (AUC)**:
                    \begin{itemize}
                        \item AUC ranges from 0 to 1.
                        \item AUC = 0.5: No discrimination capability (random guessing).
                        \item AUC > 0.7: Acceptable discrimination.
                        \item AUC = 1: Perfect model performance.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
    \begin{block}{Example}
        \begin{itemize}
            \item Consider a binary classification model.
            \item Threshold = 0.3:
                \begin{itemize}
                    \item $TP = 80, FP = 20, TN = 50, FN = 5$
                    \item $TPR = 0.94$, $FPR = 0.29 \implies (0.29, 0.94)$ on the ROC curve.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ROC and AUC - Key Points and Summary}
    \begin{itemize}
        \item ROC curves provide insights into model performance across thresholds, especially for imbalanced datasets.
        \item AUC is a robust metric for comparing different models without assumptions about class distribution.
        \item Higher AUC values indicate better model performance, crucial for model selection.
    \end{itemize}
    \begin{equation}
    \text{AUC} = \int_{0}^{1} TPR(FPR) \, dFPR
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    \begin{itemize}
        \item Explore how to choose the right evaluation metrics based on dataset characteristics and business goals.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metrics - Introduction}
    \begin{block}{Introduction to Metrics Selection}
        Selecting the appropriate evaluation metrics is essential for understanding model performance and achieving meaningful results. Different metrics provide different insights into model accuracy, precision, and usefulness based on the specific problem and context.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metrics - Classification Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item Proportion of correctly predicted instances out of total instances.
            \item \textit{Use When}: Classes are balanced (e.g., 50\% positive/50\% negative).
            \item \textit{Example}: In a dataset with 100 instances (60 yes/40 no), if predictions are 55 yes and 30 no, accuracy = \((55 + 30) / 100 = 85\%\).
        \end{itemize}
        \item \textbf{Precision}
        \begin{itemize}
            \item \[ \text{Precision} = \frac{TP}{TP + FP} \]
            \item \textit{Use When}: False positives are costly (e.g., spam detection).
        \end{itemize}
        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \[ \text{Recall} = \frac{TP}{TP + FN} \]
            \item \textit{Use When}: False negatives are costly (e.g., disease detection).
        \end{itemize}
        \item \textbf{F1 Score}
        \begin{itemize}
            \item \[ \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \]
            \item \textit{Use When}: You need a balance between precision and recall.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metrics - Regression Metrics}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Mean Absolute Error (MAE)}
        \begin{itemize}
            \item \[ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| \]
            \item Provides a clear understanding of the magnitude of errors.
        \end{itemize}
        \item \textbf{Mean Squared Error (MSE)}
        \begin{itemize}
            \item \[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]
            \item Emphasizes larger errors more than MAE.
        \end{itemize}
        \item \textbf{R-squared (Coefficient of Determination)}
        \begin{itemize}
            \item \[ R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} \]
            \item Ranges from 0 to 1, where 1 indicates perfect fit.
            \begin{itemize}
                \item \(\text{SS}_{\text{res}} = \sum (y_i - \hat{y}_i)^2\)
                \item \(\text{SS}_{\text{tot}} = \sum (y_i - \overline{y})^2\)
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metrics - Conclusion}
    \begin{block}{Additional Considerations}
        \begin{itemize}
            \item \textbf{Business Context}: Align metric choice with business impact.
            \item \textbf{Class Imbalance}: Use metrics reflecting performance on imbalanced datasets.
            \item \textbf{Interpretability}: Choose understandable metrics to facilitate communication.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Selecting the right evaluation metrics is crucial for informed decisions about model adjustments and predictions. A clear understanding of the problem domain and careful consideration of the metrics will provide invaluable insights into model performance and utility.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Justification - Introduction}
    Model selection is a critical step in the machine learning workflow. 
    \begin{itemize}
        \item It involves choosing the best model from a set of candidates.
        \item The decision is guided by evaluation metrics.
        \item These metrics quantify how well a model performs against expectations or benchmarks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Justification - Evaluation Metrics}
    Importance of evaluation metrics:
    \begin{itemize}
        \item **Accuracy**: Proportion of true results among total cases.
        \item **Precision**: Proportion of true positives in predicted positives.
        \item **Recall (Sensitivity)**: Ability to identify all relevant cases (true positives).
        \item **F1 Score**: Harmonic mean of precision and recall.
        \item **AUC-ROC**: Ability to discriminate between positive and negative classes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Justification - Key Points}
    Key Points to Emphasize:
    \begin{enumerate}
        \item **Context Matters**: Evaluation metrics must align with the specific problem.
        \item **Comparative Analysis**: Same metrics allow side-by-side model comparison.
        \item **Consider Multiple Metrics**: Relying on a single metric can be misleading.
        \item **Baseline Comparison**: Helps assess if a new model adds value.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Justification - Example Scenario}
    \textbf{Scenario:} Predicting whether a patient has a disease based on clinical measurements.
    
    \begin{block}{Model A: Logistic Regression}
        \begin{itemize}
            \item Accuracy: 85\%
            \item Precision: 90\%
            \item Recall: 80\%
            \item F1 Score: 0.84
        \end{itemize}
    \end{block}
    
    \begin{block}{Model B: Random Forest}
        \begin{itemize}
            \item Accuracy: 88\%
            \item Precision: 85\%
            \item Recall: 82\%
            \item F1 Score: 0.83
        \end{itemize}
    \end{block}
    
    **Selection Justification**: If false negatives are more critical, choose Model B despite lower precision.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Justification - Conclusion}
    Justifying model selection through evaluation metrics is crucial:
    \begin{itemize}
        \item Metrics must reflect the problem's needs.
        \item Context is important for informed decision making.
        \item Comprehensive evaluation highlights trade-offs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Selection Justification - Additional Insights}
    Additional Insights:
    \begin{itemize}
        \item Use confusion matrices to visualize model performance.
        \item Continuous monitoring of metrics informs updates or retraining processes.
    \end{itemize}
    
    \textit{Remember: Selecting the right model is about understanding the implications of scores!}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Overfitting and Underfitting - Introduction}
    \begin{block}{Introduction to Overfitting and Underfitting}
        \begin{itemize}
            \item \textbf{Overfitting}: When a model learns both the underlying patterns and noise in training data, resulting in high training accuracy but poor generalization.
            \item \textbf{Underfitting}: Occurs when a model is too simplistic to capture data trends, leading to poor performance on both training and validation datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Overfitting and Underfitting - Evaluation Methods}
    \begin{block}{Evaluation Methods for Identifying Overfitting and Underfitting}
        \begin{enumerate}
            \item \textbf{Visual Inspection: Learning Curves}
            \begin{itemize}
                \item Plot training and validation accuracy/loss over epochs.
                \item \textbf{Key Insights}:
                \begin{itemize}
                    \item Overfitting: Training accuracy increases, while validation accuracy stagnates or decreases.
                    \item Underfitting: Both training and validation accuracy remain low.
                \end{itemize}
            \end{itemize}

            \item \textbf{Cross-Validation}
            \begin{itemize}
                \item Divides the dataset into multiple subsets to train and validate.
                \item \textbf{Benefit}: Provides a clearer picture of model performance.
            \end{itemize}

            \item \textbf{Evaluation Metrics}
            \begin{itemize}
                \item Use metrics like Accuracy, Precision, Recall (for classification) and MAE, MSE (for regression).
                \item Analyze on training and validation datasets to compare performances.
                \item \textbf{Key Point}: Large discrepancies indicate potential overfitting.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Overfitting and Underfitting - Strategies}
    \begin{block}{Strategies to Mitigate Overfitting and Underfitting}
        \begin{itemize}
            \item \textbf{To Combat Overfitting}:
            \begin{itemize}
                \item Increase training data.
                \item Use cross-validation for testing across subsets.
                \item Apply regularization techniques (L1/L2 penalties, dropout).
            \end{itemize}
            \item \textbf{To Combat Underfitting}:
            \begin{itemize}
                \item Increase model complexity (more layers in neural networks).
                \item Perform feature engineering for better data representation.
                \item Ensure sufficient training duration (more epochs).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Applying Evaluation Strategies}
  \begin{itemize}
    \item Model evaluation is crucial for assessing predictive models.
    \item It guides improvements in model tuning and selection.
    \item Various strategies exist for different problems and datasets.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study 1: Predicting House Prices}
  \begin{itemize}
    \item \textbf{Dataset:} The Ames Housing dataset.
    \item \textbf{Evaluation Strategy Used:}
      \begin{enumerate}
        \item Cross-Validation with k-fold (k=5).
        \item Metrics: RMSE and R².
      \end{enumerate}
    \item \textbf{Findings:}
      \begin{itemize}
        \item Training RMSE: \$25,000, Validation RMSE: \$35,000.
        \item R²: 0.85 (good fit, but shows signs of overfitting).
      \end{itemize}
    \item \textbf{Key Takeaway:} Cross-validation identified overfitting.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study 2: Customer Churn Prediction}
  \begin{itemize}
    \item \textbf{Dataset:} Telecommunications company data.
    \item \textbf{Evaluation Strategy Used:}
      \begin{enumerate}
        \item Confusion Matrix & ROC Curve.
        \item Metrics: Precision, Recall, F1 Score.
      \end{enumerate}
    \item \textbf{Findings:}
      \begin{itemize}
        \item Precision: 0.78, Recall: 0.72.
        \item ROC AUC: 0.85 (reliable model performance).
      \end{itemize}
    \item \textbf{Key Takeaway:} Multiple metrics provide a comprehensive view of model performance.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item **Model Evaluation Importance:** Validates model performance in real-world scenarios.
    \item **Adaptability of Strategies:** Different datasets require distinct evaluation techniques.
    \item **Interpreting Results:** Clear understanding of metrics aids stakeholder decision-making.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  \begin{itemize}
    \item Application of evaluation strategies improves model reliability.
    \item Informs strategic direction for future analytics.
    \item Continuous refinement leads to better-performing models.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Code Snippet Example for Model Evaluation (Python)}
  \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)

# Train model
model.fit(X_train, y_train)

# Cross-Validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print("Mean CV Score:", cv_scores.mean())

# Predictions
predictions = model.predict(X_test)

# Evaluation Metrics
print("RMSE:", mean_squared_error(y_test, predictions, squared=False))
print("R² Score:", r2_score(y_test, predictions))
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Points Summary}
    \begin{enumerate}
        \item \textbf{Importance of Model Evaluation}
        \begin{itemize}
            \item Crucial for determining model performance on unseen data.
            \item Helps avoid overfitting and ensures generalizability.
        \end{itemize}

        \item \textbf{Evaluation Metrics}
        \begin{itemize}
            \item Metrics like accuracy, precision, recall, F1-score, and ROC-AUC are essential.
            \item Each metric provides unique insights suited for specific problems.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Points Summary (Cont'd)}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        \item \textbf{Cross-Validation Techniques}
        \begin{itemize}
            \item k-fold cross-validation reduces variance in performance estimates.
            \item Dataset is split into k subsets, trained and validated accordingly.
        \end{itemize}

        \item \textbf{Train/Test Split}
        \begin{itemize}
            \item Essential for model fitting and evaluation.
            \item Typical splits include 80\% training and 20\% testing.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Implications and Continuous Improvement}
    \begin{enumerate}
        \setcounter{enumi}{4} % Continue numbering from the previous frame
        \item \textbf{Real-World Implications}
        \begin{itemize}
            \item Accurate models have significant practical implications, e.g., in healthcare.
        \end{itemize}

        \item \textbf{Continuous Improvement}
        \begin{itemize}
            \item Model evaluation is an ongoing process.
            \item Regular updates based on new data keep models relevant.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Conclusion Statement}
        Effective model evaluation and validation strategies are crucial for developing robust, reliable machine learning models that meet real-world needs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A - Objectives}
    \begin{itemize}
        \item Enhance understanding of model evaluation techniques.
        \item Foster critical thinking about the implications of model performance.
        \item Encourage teamwork by sharing diverse perspectives on evaluation methods.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A - Key Points}
    \begin{enumerate}
        \item \textbf{Understanding Model Evaluation Metrics}
            \begin{itemize}
                \item Importance of metrics like accuracy, precision, recall, and F1-score.
                \item Example: Two models with the same accuracy but different precision and recall. How does this influence your selection?
            \end{itemize}
        \item \textbf{Cross-Validation Techniques}
            \begin{itemize}
                \item Advantages of k-fold cross-validation versus train-test split.
                \item Example: Limited sample dataset affecting cross-validation choice.
            \end{itemize}
        \item \textbf{Bias-Variance Tradeoff}
            \begin{itemize}
                \item Impact of bias and variance on model evaluation.
                \item Example: Identifying overfitting or underfitting through performance metrics.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A - Encouraging Questions}
    \begin{itemize}
        \item What are the most confusing aspects of model evaluation you’ve encountered?
        \item How can we improve our understanding of evaluation metrics?
        \item Are there specific models you’d like to discuss regarding their evaluation metrics?
    \end{itemize}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Metric Selection:} Choosing the right metric depends on context.
            \item \textbf{Iterative Process:} Model evaluation is an ongoing task.
            \item \textbf{Collaboration:} Learning from diverse perspectives enriches understanding.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}