\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Neural Networks Basics]{Chapter 8: Neural Networks Basics}
\subtitle{An Overview of Neural Network Fundamentals}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks}
    \begin{block}{Overview}
        Neural networks are computational models inspired by the human brain's structure and functioning. 
        They consist of interconnected groups of artificial neurons that process information by responding to external inputs. 
        Understanding neural networks is essential as they form a foundational element of machine learning and artificial intelligence.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts: Neurons and Architecture}
    \begin{itemize}
        \item \textbf{Neurons}:
            \begin{itemize}
                \item The basic unit of a neural network, analogous to a biological neuron.
                \item Each neuron receives inputs, processes them, and produces an output.
            \end{itemize}
        \item \textbf{Architecture}:
            \begin{itemize}
                \item \textbf{Input Layer}: Where the network receives data.
                \item \textbf{Hidden Layers}: Layers between input and output that perform complex transformations.
                \item \textbf{Output Layer}: Produces the final output of the network.
                \item \textbf{Connections (Weights)}: The strength of the connection between neurons, adjusted through learning.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Function and Example}
    \begin{itemize}
        \item \textbf{Activation Function}:
            \begin{itemize}
                \item A mathematical function applied to a neuron's output to determine its activation level.
                \item Common functions include sigmoid, ReLU, and tanh.
            \end{itemize}
        \item \textbf{Example: Sigmoid Function}:
            \begin{equation}
                f(x) = \frac{1}{1 + e^{-x}} \quad \text{(Sigmoid)}
            \end{equation}
            Produces outputs between 0 and 1, providing a probability-like output.
    \end{itemize}
    
    \begin{block}{Example of Neural Network in Action}
        \begin{itemize}
            \item \textbf{Input}: Features of an image (e.g., pixels).
            \item \textbf{Hidden Layers}: Extract features (like edges, shapes).
            \item \textbf{Output}: Classification (e.g., "cat" or "dog").
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Process and Applications}
    \begin{itemize}
        \item \textbf{Learning Process}: Neural networks learn through backpropagation, where weights are adjusted based on output errors.
        \item \textbf{Applications}: Used in various domains including:
            \begin{itemize}
                \item Image recognition
                \item Natural language processing
                \item Self-driving cars
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engaging Classroom Activity}
    \begin{block}{Discussion}
        Ask students to brainstorm real-world applications of neural networks. 
        How do they contribute to everyday technology?
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    In this chapter, we focus on providing a foundational understanding of neural networks, emphasizing:
    \begin{itemize}
        \item Their architecture and operation
        \item Practical applications in machine learning
    \end{itemize}
    By the end, you should grasp the basic concepts and engage in discussions regarding their implications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Key Concepts}
    Upon completion of this chapter, you should be able to:
    \begin{enumerate}
        \item \textbf{Define Neural Networks}
            \begin{itemize}
                \item Understand their significance in machine learning.
                \item \textbf{Key Point:} Computational models inspired by the human brain to recognize patterns.
            \end{itemize}
        \item \textbf{Identify Core Components}
            \begin{itemize}
                \item Neurons, layers (input, hidden, output), and activation functions.
            \end{itemize}
        \item \textbf{Understand How Neural Networks Learn}
            \begin{itemize}
                \item Concepts such as forward propagation, loss functions, backpropagation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Applications}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Explore Common Applications}
            \begin{itemize}
                \item Applications in image recognition, natural language processing, and predictive analytics.
                \item \textbf{Case Study:} Convolutional Neural Networks (CNNs) for image classification.
            \end{itemize}
        \item \textbf{Critical Thinking and Team Discussion}
            \begin{itemize}
                \item Discuss ethical considerations and societal impacts.
                \item Encourage teamwork via group discussions on advantages/challenges.
            \end{itemize}
        \item \textbf{Hands-On Experience}
            \begin{itemize}
                \item Implement basic models using Python libraries (TensorFlow, PyTorch).
                \item \textbf{Code Snippet Example:}
                \begin{lstlisting}[language=Python]
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(input_size,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Conclusion}
    By achieving these objectives, you will:
    \begin{itemize}
        \item Gain knowledge about basic neural networks.
        \item Develop critical thinking skills for analyzing real-world applications.
    \end{itemize}
    Prepare to collaborate and apply this knowledge through in-class discussions and hands-on activities.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Neural Network? - Introduction}
    \begin{block}{Definition}
        A \textbf{Neural Network} is a computational model inspired by the biological neural networks in the human brain. It is a key technology in \textbf{machine learning}, enabling computers to learn from data and make decisions based on patterns and predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Neural Network? - Key Components}
    \begin{itemize}
        \item \textbf{Neurons}: Fundamental units that receive inputs, process them via an activation function, and produce outputs.
        \item \textbf{Layers}: 
        \begin{itemize}
            \item Input Layer: Receives the input data.
            \item Hidden Layer(s): Perform computations and learn complex features; can be one or many.
            \item Output Layer: Produces the result or output of the network.
        \end{itemize}
        \item \textbf{Connections}: Neurons are connected through \textbf{weights} adjusted during training to minimize prediction errors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Neural Networks Work - Overview}
    \begin{itemize}
        \item \textbf{Forward Propagation}: Process of sending input data through the network to produce an output.
        \item \textbf{Training}: Adjusting weights using a dataset with known outcomes to minimize errors using an algorithm (e.g., gradient descent).
        \item \textbf{Activation Functions}: Define the output of a neuron for given inputs.
        \begin{itemize}
            \item Sigmoid: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)
            \item ReLU (Rectified Linear Unit): \( f(x) = \max(0, x) \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Neural Networks in Machine Learning}
    \begin{itemize}
        \item Excels in:
        \begin{itemize}
            \item Image and speech recognition
            \item Natural language processing
            \item Predictive analytics
        \end{itemize}
        \item Particularly proficient in identifying patterns in large datasets, often outperforming traditional algorithms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of a Simple Neural Network}
    \begin{itemize}
        \item Consider a neural network for image classification:
        \begin{enumerate}
            \item \textbf{Input}: Pixel values of an image (e.g., handwritten digit).
            \item \textbf{Layers}: 
            \begin{itemize}
                \item Input layer receives pixel data.
                \item Hidden layers learn features like edges or shapes.
                \item Output layer predicts the digit (0–9).
            \end{itemize}
            \item The network learns by updating weights based on classification accuracy during training.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item Neural Networks mimic the brain's structure and learning processes.
        \item Composed of neurons, organized into layers, with weighted connections.
        \item Essential for pattern recognition in complex and large datasets.
        \item Utilizes algorithms to adjust for accuracy over iterations.
    \end{itemize}
    \begin{block}{Conclusion}
        Understanding the foundational elements of neural networks provides insight into their applications and significance in modern machine learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Structure of a Neural Network - Part 1}
    \begin{block}{Understanding Neurons}
        \begin{itemize}
            \item \textbf{Definition}: Neurons are the fundamental building blocks of neural networks, similar to biological neurons.
            \item \textbf{Components}:
                \begin{itemize}
                    \item \textbf{Inputs}: Values fed into the neuron.
                    \item \textbf{Weights}: Reflect the importance of each input.
                    \item \textbf{Bias}: A constant added to the weighted sum for better model fitting.
                \end{itemize}
            \item \textbf{Mathematical Representation}:
                \begin{equation}
                    z = \sum (x_i \cdot w_i) + b
                \end{equation}
                where \( z \) is the weighted sum, \( x_i \) are inputs, \( w_i \) are weights, and \( b \) is the bias.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Structure of a Neural Network - Part 2}
    \begin{block}{Layers in Neural Networks}
        \begin{itemize}
            \item \textbf{Types of Layers}:
                \begin{itemize}
                    \item \textbf{Input Layer}: Receives raw input data.
                    \item \textbf{Hidden Layer(s)}: Intermediate layers performing computations.
                    \item \textbf{Output Layer}: Produces predictions.
                \end{itemize}
            \item \textbf{Example Structure}:
                \begin{itemize}
                    \item Simple Neural Network: 
                    1 Input Layer (3 Neurons) $\rightarrow$ 1 Hidden Layer (2 Neurons) $\rightarrow$ 1 Output Layer (1 Neuron)
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Structure of a Neural Network - Part 3}
    \begin{block}{Connections Between Neurons}
        \begin{itemize}
            \item \textbf{Weights and Connections}: Each neuron is fully connected to the neurons in the next layer.
            \item \textbf{Depth and Breadth}: Neural networks vary in depth (number of layers) and breadth (number of neurons per layer).
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Neurons function via weighted inputs leading to final output.
            \item Layer arrangement matters for performance.
            \item All layers interconnect, facilitating complex processing.
        \end{itemize}
    \end{block}

    \begin{block}{Final Thoughts}
        \begin{itemize}
            \item Understanding the basic structure of neural networks is essential for exploring their functionalities and applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Overview}
    \begin{block}{Overview of Activation Functions}
        Activation functions are a crucial component of neural networks that define how the input signal from one layer is transformed into the output signal for the next layer. 
        These functions introduce non-linearity into the network, enabling it to learn complex patterns.
        Without activation functions, neural networks behave like linear models regardless of the number of layers.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Common Types}
    \begin{enumerate}
        \item \textbf{Sigmoid Function}
            \begin{itemize}
                \item \textbf{Formula:}
                    \begin{equation*}
                    f(x) = \frac{1}{1 + e^{-x}}
                    \end{equation*}
                \item \textbf{Range:} (0, 1)
                \item \textbf{Characteristics:}
                    \begin{itemize}
                        \item Maps output to the range between 0 and 1.
                        \item Commonly used in binary classification tasks.
                    \end{itemize}
                \item \textbf{Example Use Case:} Output layer of binary classification neural networks.
                \item \textbf{Graph:}
                    \begin{itemize}
                        \item S-shaped curve; steepest around the origin, flattens as \(x\) moves away.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Hyperbolic Tangent (tanh) Function}
            \begin{itemize}
                \item \textbf{Formula:}
                    \begin{equation*}
                    f(x) = \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
                    \end{equation*}
                \item \textbf{Range:} (-1, 1)
                \item \textbf{Characteristics:}
                    \begin{itemize}
                        \item Outputs values between -1 and 1, centered at zero.
                        \item Results in faster convergence since it brings outputs closer to zero.
                    \end{itemize}
                \item \textbf{Example Use Case:} Hidden layers in many neural networks.
                \item \textbf{Graph:}
                    \begin{itemize}
                        \item S-shaped curve that crosses the origin; outputs are zero-centered.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - ReLU and Key Points}
    \begin{itemize}
        \item \textbf{Rectified Linear Unit (ReLU)}
            \begin{itemize}
                \item \textbf{Formula:}
                    \begin{equation*}
                    f(x) = \max(0, x)
                    \end{equation*}
                \item \textbf{Range:} [0, $\infty$)
                \item \textbf{Characteristics:}
                    \begin{itemize}
                        \item Output is zero for negative inputs and the input itself for positive inputs.
                        \item Efficiently activates neurons, reduces vanishing gradient issues.
                    \end{itemize}
                \item \textbf{Example Use Case:} Commonly used in hidden layers of deep neural networks.
                \item \textbf{Graph:}
                    \begin{itemize}
                        \item Linear for positive values, flat at zero for negative values.
                    \end{itemize}
            \end{itemize}

        \item \textbf{Key Points}
            \begin{itemize}
                \item Activation functions introduce \textbf{non-linearity} for learning complex patterns.
                \item Choice of activation function can influence network performance and training.
                \item \textbf{Advantages of ReLU:}
                    \begin{itemize}
                        \item Allows for faster training and mitigates vanishing gradient.
                        \item Can lead to \textbf{dying ReLU} issue; variants like Leaky ReLU exist.
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Conclusion}
            \begin{itemize}
                \item Understanding activation functions is essential for effective neural network design.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Overview}
    \begin{block}{Definition}
        Forward propagation is the process through which the input data is passed through the layers of a neural network, generating an output (predictions). Each neuron in the network transforms its input using a weighted sum and an activation function.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Steps}
    \begin{enumerate}
        \item \textbf{Input Layer:}
        \begin{itemize}
            \item Data enters the network through the input layer. Each feature of the dataset corresponds to a neuron in this layer.
        \end{itemize}
        
        \item \textbf{Weighted Sum Calculation:}
        \begin{equation}
            z = \sum (w_i \cdot x_i) + b
        \end{equation}
        Where:
        \begin{itemize}
            \item \( z \) = weighted sum
            \item \( w_i \) = weights
            \item \( x_i \) = inputs
            \item \( b \) = bias term
        \end{itemize}
        
        \item \textbf{Activation Function:}
        The weighted sum \( z \) is passed through an activation function \( f(z) \), common functions include:
        \begin{itemize}
            \item \textbf{Sigmoid:} 
            \begin{equation}
                f(z) = \frac{1}{1 + e^{-z}}
            \end{equation}
            \item \textbf{ReLU (Rectified Linear Unit):} 
            \begin{equation}
                f(z) = \max(0, z)
            \end{equation}
            \item \textbf{Tanh:} 
            \begin{equation}
                f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
            \end{equation}
        \end{itemize}

        \item \textbf{Output Layer:}
        \begin{itemize}
            \item The final layer produces the output of the network, interpreted as probability scores in classification tasks.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Example}
    Let’s assume a simple neural network with:
    \begin{itemize}
        \item Input features: \( x_1 = 0.5, x_2 = 1.5 \)
        \item Weights: \( w_1 = 0.4, w_2 = 0.6 \)
        \item Bias: \( b = 0.2 \)
    \end{itemize}

    \textbf{Step 1: Calculate the weighted sum}
    \begin{equation}
        z = (0.4 \cdot 0.5) + (0.6 \cdot 1.5) + 0.2 = 1.3
    \end{equation}

    \textbf{Step 2: Apply the sigmoid activation function}
    \begin{equation}
        f(z) = \frac{1}{1 + e^{-1.3}} \approx 0.785
    \end{equation}

    \textbf{Output:} The final output for this neuron is approximately **0.785**.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Transformation Process:} Forward propagation transforms input data into outputs through a series of calculations involving weights and activation functions.
        \item \textbf{Role of Activation Functions:} They introduce non-linearity, enabling the model to learn complex patterns.
        \item \textbf{Multiple Layers:} In deep networks, forward propagation occurs across multiple layers, progressively transforming data.
    \end{itemize}

    \textbf{Connection to Loss Functions:} Understanding forward propagation is essential as it precedes loss calculation during training, discussed in the next slide.

    \textbf{Summary:} Forward propagation is fundamental in neural networks, determining how input data is processed to yield the final output, crucial for understanding the full functionality and training of neural networks.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Loss Functions - Introduction}
  \begin{block}{Definition}
    A loss function, also known as a cost function or objective function, is essential in training neural networks. It quantifies the difference between the predicted output and actual target values.
  \end{block}
  
  \begin{block}{Importance of Loss Functions}
    \begin{itemize}
      \item \textbf{Guides Optimization:} Provides feedback to adjust the weights of the neural network.
      \item \textbf{Performance Metric:} Serves as a benchmark for model evaluation.
      \item \textbf{Informs Training Decisions:} Influences the learning process and error minimization.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Loss Functions - Types}
  \begin{enumerate}
    \item \textbf{Mean Squared Error (MSE)}
      \begin{itemize}
        \item \textbf{Usage:} Common for regression tasks.
        \item \textbf{Formula:} 
        \begin{equation}
          MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \end{equation}
        \item \textbf{Explanation:} Averages the squares of the errors, emphasizing larger discrepancies.
      \end{itemize}

    \item \textbf{Binary Cross-Entropy}
      \begin{itemize}
        \item \textbf{Usage:} For binary classification.
        \item \textbf{Formula:}
        \begin{equation}
          L = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
        \end{equation}
        \item \textbf{Explanation:} Measures the performance of models outputting probabilities between 0 and 1.
      \end{itemize}

    \item \textbf{Categorical Cross-Entropy}
      \begin{itemize}
        \item \textbf{Usage:} For multi-class classification.
        \item \textbf{Formula:}
        \begin{equation}
          L = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
        \end{equation}
        \item \textbf{Explanation:} Compares the true and predicted probability distributions.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Loss Functions - Key Points and Conclusion}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item \textbf{Selection:} Choose a loss function aligned with the problem type (regression vs classification).
      \item \textbf{Impact of Loss:} Monitor loss values for model assessment; ideally, loss should decrease during training.
      \item \textbf{Regularization:} Incorporate techniques like L1 or L2 penalties to curb overfitting.
    \end{itemize}
  \end{block}
  
  \begin{block}{Conclusion}
    Loss functions are vital in training neural networks, providing essential feedback for learning. Selecting the right loss function is crucial for optimizing model performance.
  \end{block}
  
  \begin{block}{Code Snippet Example}
  \begin{lstlisting}[language=Python]
import tensorflow as tf

# Define a model and compile it with a loss function
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(input_shape,)),
    tf.keras.layers.Dense(1)  # For regression
])
model.compile(optimizer='adam', loss='mean_squared_error')  # Use 'binary_crossentropy' for binary classification
  \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation - Overview}
    \begin{itemize}
        \item Backpropagation is a supervised learning algorithm for training neural networks.
        \item It optimizes weights by minimizing the loss function.
        \item The method involves error propagation backward through the network.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation - Steps}
    \begin{enumerate}
        \item \textbf{Forward Pass:}
        \begin{itemize}
            \item Input data is passed through the network.
            \item Activations and output are computed.
            \item Loss is calculated using a loss function:
            \begin{equation}
                \text{Loss} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
        \end{itemize}

        \item \textbf{Backward Pass:}
        \begin{itemize}
            \item Compute gradients of the loss function using the chain rule.
            \item Calculate error for each neuron:
            \begin{equation}
                \delta = \frac{\partial \text{Loss}}{\partial \hat{y}} \cdot \text{activation}'(z)
            \end{equation}
        \end{itemize}

        \item \textbf{Weight Update:}
        \begin{itemize}
            \item Adjust weights to minimize loss:
            \begin{equation}
                w = w - \eta \cdot \delta \cdot x
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation - Key Points}
    \begin{itemize}
        \item \textbf{Efficiency:} Reuses computations from the forward pass during the backward pass.
        \item \textbf{Learning Rate ($\eta$):} 
        \begin{itemize}
            \item Critical for convergence.
            \item Too high can diverge; too low can slow training.
        \end{itemize}
        \item \textbf{Activation Functions:} Selection affects performance and stability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation - Example and Code}
    \begin{block}{Example: Simple Neural Network}
        \begin{itemize}
            \item \textbf{Structure:} 2 input neurons, 2 hidden neurons, 1 output neuron.
            \item \textbf{Forward Pass:} Compute hidden and output activations.
            \item \textbf{Backward Pass:} Calculate loss and update weights.
        \end{itemize}
    \end{block}
    
    \begin{block}{Python Code Snippet}
    \begin{lstlisting}[language=Python]
import numpy as np

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

def update_weights(inputs, outputs, weights, learning_rate):
    predictions = relu(np.dot(inputs, weights))
    error = outputs - predictions
    delta = error * relu_derivative(predictions)
    weights += learning_rate * np.dot(inputs.T, delta)
    return weights
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training a Neural Network - Overview}
    \begin{block}{Overview of Neural Network Training}
        Training a neural network adjusts the model's parameters (weights and biases) based on input data to minimize the prediction error. The training process consists of several key steps:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training a Neural Network - Data Preparation}
    \begin{block}{Data Preparation}
        \begin{itemize}
            \item \textbf{Collect Data}: Gather a large dataset relevant to the problem (e.g., images, text, numerical values).
            \item \textbf{Preprocess Data}: Clean and format the data, including:
                \begin{itemize}
                    \item Normalization: Scaling values to a common range (e.g., 0 to 1).
                    \item Encoding: Transforming categorical variables into numerical format (e.g., One-Hot Encoding).
                    \item Splitting: Dividing the dataset into training, validation, and test sets (e.g., 70\% training, 15\% validation, 15\% test).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training a Neural Network - Training Epochs}
    \begin{block}{Training Epochs}
        \begin{itemize}
            \item \textbf{Epoch Definition}: An epoch is one complete pass through the entire training dataset.
            \item \textbf{Multiple Epochs}: Training typically requires multiple epochs to allow learning:
                \begin{itemize}
                    \item Predictions are made on training data.
                    \item Loss is calculated using a loss function (e.g., MSE, Cross-Entropy).
                    \item Backpropagation updates weights based on loss.
                \end{itemize}
            \item \textbf{Convergence}: Monitor loss to ensure it decreases and stabilizes over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training a Neural Network - Performance Evaluation}
    \begin{block}{Performance Evaluation}
        \begin{itemize}
            \item \textbf{Validation Set}: Evaluate the model on the validation set to tune hyperparameters and avoid overfitting.
            \item \textbf{Metrics to Measure Performance}:
                \begin{itemize}
                    \item Accuracy: Proportion of correct predictions.
                    \item Precision, Recall, F1-Score: Useful for imbalanced datasets.
                    \item Confusion Matrix: Visual representation of prediction results.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training a Neural Network - Example Code}
    \begin{block}{Example Code Snippet (Python using Keras)}
        \begin{lstlisting}[language=Python]
# Sample Keras model training code
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

# Create a simple feedforward neural network
model = Sequential()
model.add(Dense(32, activation='relu', input_shape=(input_dimension,)))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Fit the model to the training data
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training a Neural Network - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Importance of Data Quality}: Effective neural network performance relies heavily on high-quality training data.
            \item \textbf{Overfitting vs Underfitting}: Monitor validation loss to ensure good generalization of the model.
            \item \textbf{Hyperparameter Tuning}: Experiment with different hyperparameters to optimize network performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training a Neural Network - Conclusion}
    \begin{block}{Conclusion}
        The training process is fundamental for building effective neural networks. Understanding data preparation, epochs, and evaluation methods leads to successful applications of neural networks in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Overview}
    Neural networks are powerful computational models inspired by the human brain and are widely used to solve complex real-world problems across various domains. Here, we explore some key applications of neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Image Recognition}
    \begin{itemize}
        \item \textbf{Description:} Neural networks, particularly Convolutional Neural Networks (CNNs), excel in recognizing patterns and objects in images.
        \item \textbf{Example:} 
            \begin{itemize}
                \item Facial recognition systems in security
                \item Social media platforms (e.g., Facebook tagging photos)
                \item Self-driving cars identifying pedestrians
            \end{itemize}
        \item \textbf{Key Points:}
            \begin{itemize}
                \item Features Extraction: CNNs automatically learn spatial hierarchies (edges, shapes) from images.
                \item Real-World Impact: Improves user experience, enhances security, and aids in medical diagnostics (e.g., identifying tumors in radiology images).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Natural Language Processing}
    \begin{itemize}
        \item \textbf{Description:} Neural networks help machines understand and generate human language. Recurrent Neural Networks (RNNs) and Transformers are commonly used in NLP.
        \item \textbf{Example:}
            \begin{itemize}
                \item Language translation services (e.g., Google Translate)
                \item Sentiment analysis in social media
                \item Chatbots designed for customer service
            \end{itemize}
        \item \textbf{Key Points:}
            \begin{itemize}
                \item Context Understanding: RNNs use sequential information to understand context in text.
                \item Transformers: Capable of processing and generating human-like text, powering many modern AI applications.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Other Domains}
    \begin{itemize}
        \item \textbf{Speech Recognition:}
            \begin{itemize}
                \item \textbf{Description:} Speech-to-text applications use neural networks to convert spoken language into written text.
                \item \textbf{Example:} Virtual assistants like Siri, Google Assistant.
                \item \textbf{Key Points:}
                    \begin{itemize}
                        \item Acoustic Models: Neural networks model the relationship between sound waves and phonetic units.
                        \item Deep Learning: Allows accurate recognition of diverse accents and dialects.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Healthcare:}
            \begin{itemize}
                \item \textbf{Description:} Neural networks assist in diagnostics, patient monitoring, and personalized medicine.
                \item \textbf{Example:} Predicting disease outbreaks and real-time patient monitoring via wearable devices.
                \item \textbf{Key Points:}
                    \begin{itemize}
                        \item Diagnostic Assistance: Image analysis for X-rays and MRIs.
                        \item Outcome Predictions: Analyzing treatment options using extensive datasets.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Neural Networks - Finance and Summary}
    \begin{itemize}
        \item \textbf{Finance:}
            \begin{itemize}
                \item \textbf{Description:} Neural networks are utilized for predicting stock prices, detecting fraud, and personalized banking experiences.
                \item \textbf{Example:} Algorithmic trading systems analyzing market data.
                \item \textbf{Key Points:}
                    \begin{itemize}
                        \item Pattern Recognition: Identifying trends in large datasets.
                        \item Fraud Detection: Real-time analysis to flag unusual transaction patterns.
                    \end{itemize}
            \end{itemize}
    \end{itemize}
    \begin{block}{Summary}
        Neural networks have transformed various industries through applications in image and speech recognition, finance, and healthcare, improving technology and providing innovative solutions to complex challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formula}
    \begin{equation}
        y = \sigma(Wx + b)
    \end{equation}
    Where:
    \begin{itemize}
        \item \(y\) = output
        \item \(W\) = weights
        \item \(x\) = input features
        \item \(b\) = bias
        \item \(\sigma\) = activation function (e.g., sigmoid, ReLU)
    \end{itemize}
    This equation illustrates the basic computation structure in neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Call to Action}
    As neural networks evolve, understanding their applications allows us to harness their full potential, leading to innovative advancements in our everyday lives!
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Neural Networks - Overview}
  
  \begin{block}{Description}
    This slide discusses common challenges in the deployment and training of neural networks, including:
    \begin{itemize}
      \item Overfitting
      \item Underfitting
      \item Computational resource requirements
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Neural Networks - Part 1: Overfitting}

  \begin{itemize}
    \item \textbf{Definition}: 
      Overfitting occurs when a neural network learns the training data too well, capturing noise and fluctuations rather than the underlying pattern, leading to poor performance on unseen data.
      
    \item \textbf{Example}: 
      Training a model on images of cats and dogs can lead to overfitting if it memorizes each image instead of learning generalizable features.
      
    \item \textbf{Solutions}:
      \begin{itemize}
        \item Regularization: L1/L2 techniques to control model complexity.
        \item Dropout: Randomly deactivating neurons during training.
        \item Early Stopping: Monitoring validation performance to stop training when it degrades.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Neural Networks - Part 2: Underfitting and Resources}

  \begin{itemize}
    \item \textbf{Underfitting}:
      \begin{itemize}
        \item \textbf{Definition}: Occurs when a model is too simple to capture data trends, resulting in poor performance on both training and test datasets.
        \item \textbf{Example}: A linear regression model fitting a complex, non-linear dataset.
        \item \textbf{Solutions}: Increase model complexity or improve feature engineering.
      \end{itemize}
    
    \item \textbf{Computational Resource Requirements}:
      \begin{itemize}
        \item \textbf{Overview}: Training deep learning models requires substantial resources like powerful GPUs and large memory.
        \item \textbf{Challenges}:
          \begin{itemize}
            \item Time: Large models can take hours to weeks to train.
            \item Cost: High-performance cloud services can be expensive.
          \end{itemize}
        \item \textbf{Solutions}:
          \begin{itemize}
            \item Model Optimization: Techniques such as quantization and pruning.
            \item Transfer Learning: Utilizing pre-trained models to reduce resource needs.
          \end{itemize}
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Neural Networks - Conclusion}

  \begin{block}{Key Points}
    \begin{itemize}
      \item Balancing between overfitting and underfitting is crucial for effective neural networks.
      \item Understanding computational requirements aids in proper resource allocation.
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    Recognizing these challenges allows practitioners to adopt strategies to enhance model performance and usability. 
    Through methods like regularization and optimizations, robust neural networks can be created that generalize well to new data.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    \begin{block}{Overview}
        As neural networks become increasingly integrated into critical aspects of society—from healthcare diagnostics to automated decision-making—ethical considerations must be prioritized. 
    \end{block}
    \begin{itemize}
        \item Align deployment and training with societal values.
        \item Promote fairness, accountability, and transparency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Points}
    \begin{enumerate}
        \item \textbf{Bias and Fairness}
            \begin{itemize}
                \item Neural networks trained on biased data can perpetuate existing inequalities.
                \item Example: Hiring algorithms favoring specific demographics.
                \item Impact: Discriminatory practices in vital areas such as employment and law enforcement.
            \end{itemize}
        \item \textbf{Transparency and Explainability}
            \begin{itemize}
                \item Many models operate as "black boxes."
                \item Lack of user understanding leads to distrust.
                \item Importance: Enhancing explainability builds trust in AI systems.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Further Key Points}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Privacy Concerns}
            \begin{itemize}
                \item Models require vast amounts of personal data.
                \item Example: Facial recognition systems risking individual privacy.
                \item Mitigation: Techniques like differential privacy.
            \end{itemize}
        \item \textbf{Accountability and Responsibility}
            \begin{itemize}
                \item Challenges in determining accountability for autonomous decisions.
                \item Example: Responsibility in self-driving car accidents.
                \item Call to Action: Establish clear legal frameworks.
            \end{itemize}
        \item \textbf{Sustainability}
            \begin{itemize}
                \item Training large models can be energy-intensive.
                \item Example: Carbon footprint of model training comparable to flights.
                \item Solutions: Explore energy-efficient architectures.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary of Key Points}
    \begin{enumerate}
        \item \textbf{Definition of Neural Networks}:
        \begin{itemize}
            \item Computational models inspired by the human brain.
            \item Consist of interconnected nodes (neurons) that process data.
        \end{itemize}

        \item \textbf{Architecture}:
        \begin{itemize}
            \item \textbf{Input Layer}: Accepts raw data.
            \item \textbf{Hidden Layers}: Transform data for output.
            \item \textbf{Output Layer}: Provides the final prediction.
        \end{itemize}

        \item \textbf{Activation Functions}:
        \begin{itemize}
            \item \textbf{Sigmoid}: \( f(x) = \frac{1}{1 + e^{-x}} \)
            \item \textbf{ReLU}: \( f(x) = \max(0, x) \)
            \item \textbf{Softmax}: For multi-class classification probabilities.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Training Process and Overfitting}
    \begin{enumerate}[resume]
        \item \textbf{Training Process}:
        \begin{itemize}
            \item \textbf{Forward Propagation}: Data flows through the network.
            \item \textbf{Loss Function}: Measures output accuracy, e.g., MSE or Cross-Entropy.
            \item \textbf{Backpropagation}: Adjusts weights to minimize loss.
        \end{itemize}

        \item \textbf{Overfitting and Regularization}:
        \begin{itemize}
            \item \textbf{Overfitting}: Learning training data too well.
            \item \textbf{Mitigation Techniques}:
            \begin{itemize}
                \item Dropout
                \item L2 Regularization
                \item Early Stopping
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Significance and Key Takeaways}
    \begin{itemize}
        \item \textbf{Significance in Advanced Topics}:
        \begin{itemize}
            \item \textbf{CNNs}: For image processing.
            \item \textbf{RNNs}: Optimal for sequence data.
            \item \textbf{Transfer Learning}: Improving efficiency in new tasks.
        \end{itemize}

        \item \textbf{Key Takeaways}:
        \begin{itemize}
            \item Neural networks are versatile tools for machine learning.
            \item Understanding neural architecture and training is crucial.
            \item Ethical considerations are important in real-world applications.
        \end{itemize}

        \item \textbf{Final Thoughts}:
        \begin{itemize}
            \item Mastery of these basics is essential for advanced applications in AI.
            \item Engage in discussions to deepen understanding.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Overview}
    \begin{block}{Overview}
        This slide opens the floor for discussion on neural networks (NN). 
        Engaging in a Question and Answer session reinforces key concepts and clarifies uncertainties related to NN fundamentals and applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Key Concepts}
    \begin{enumerate}
        \item \textbf{Neural Network Fundamentals}:
        \begin{itemize}
            \item Mimics human brain structure to process data and recognize patterns.
            \item \textbf{Components}: Neurons (nodes), layers (input, hidden, output), and connections (weights).
        \end{itemize}

        \item \textbf{Activation Functions}:
        \begin{itemize}
            \item Functions that determine activation of neurons.
            \item Common types: Sigmoid, ReLU (Rectified Linear Unit), Tanh.
            \item \textbf{Example:} ReLU: \( f(x) = \max(0, x) \)
        \end{itemize}
        
        \item \textbf{Training Process}:
        \begin{itemize}
            \item Adjusting weights through methods like backpropagation.
            \item Key terms: Loss function, optimizer, epochs, batch size.
            \item \textbf{Example:} Mean Squared Error (MSE):
            \begin{equation}
                \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2
            \end{equation}
        \end{itemize}
        
        \item \textbf{Applications of Neural Networks}:
        \begin{itemize}
            \item Image Recognition: CNNs for facial recognition.
            \item Natural Language Processing: RNNs for language translation and sentiment analysis.
            \item Autonomous Systems: Decision-making in self-driving cars.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Discussion Prompts}
    \begin{block}{Discussion Prompts}
        \begin{enumerate}
            \item \textbf{Clarifications}: What part of the NN architecture do you find most confusing? 
            \item \textbf{Applications}: Where could NN transform operations? Share examples.
            \item \textbf{Challenges}: What barriers do you see in the adoption of neural networks?
            \item \textbf{Future Prospects}: How do you envision the role of NNs evolving in technology?
        \end{enumerate}
    \end{block}
    
    \begin{block}{Encouragement for Participation}
        Encourage questions about unclear concepts, real-world examples, or theoretical aspects. 
        Discussing helps deepen understanding and fosters collaborative learning.
    \end{block}
    
    \begin{block}{Closing}
        A Q&A session consolidates knowledge and spurs interest in advanced topics. 
        Let’s engage in an open dialogue to deepen our understanding of neural networks!
    \end{block}
\end{frame}


\end{document}