\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Title Page Information
\title[Machine Learning Algorithms]{Chapter 4: Introduction to Machine Learning Algorithms}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Machine Learning Algorithms - Overview}
    \begin{block}{Overview}
        Machine Learning (ML) is a field of artificial intelligence that focuses on developing systems that can learn from data and make predictions or decisions without being explicitly programmed. This chapter lays the groundwork for understanding various ML algorithms, with a particular emphasis on \textbf{Linear Regression} as a key foundational algorithm.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Definition of Machine Learning}: 
        \begin{itemize}
            \item A subfield of AI enabling systems to learn patterns from data.
            \item Algorithms improve their performance as they are exposed to more data over time.
        \end{itemize}

        \item \textbf{Importance of Algorithms}:
        \begin{itemize}
            \item Core tools in ML that transform input data into actionable insights or predictions.
            \item Each algorithm has its strengths and weaknesses, affecting its applicability to different types of data tasks.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Focus on Linear Regression}
    \begin{block}{Linear Regression}
        Linear Regression is one of the simplest yet most widely used algorithms in machine learning. It is particularly valuable for tasks involving numerical predictions.
    \end{block}
    \begin{itemize}
        \item \textbf{Purpose}: Estimates the relationship between one dependent variable and one or more independent variables.
        \item \textbf{Formula}:
        \begin{equation}
        y = mx + b
        \end{equation}
        where 
        \begin{itemize}
            \item \(y\) = dependent variable,
            \item \(m\) = slope,
            \item \(x\) = independent variable,
            \item \(b\) = y-intercept.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Key Points}
    \begin{block}{Example}
        If predicting house prices based on size in square feet:
        \begin{equation}
        \text{Price} = 150 \cdot (\text{Size in sq ft}) + 20000
        \end{equation}
        This means for every additional square foot, the house price increases by \$150.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize}:
        \begin{itemize}
            \item Foundational Algorithm: A building block for complex algorithms.
            \item Interpretability: Results are simple and easy to interpret.
            \item Assumptions:
            \begin{itemize}
                \item Linear relationship between variables.
                \item Residuals are normally distributed.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engagement Through Discussion}
    \begin{block}{Discussion Activities}
        \begin{itemize}
            \item \textbf{Critical Thinking}: Reflect on scenarios where linear regression might not be appropriate (e.g., non-linear relationships).
            \item \textbf{Teamwork Exercise}: In groups, identify real-world datasets (like housing prices, stock market trends) that could benefit from linear regression analysis.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Definition}
    \begin{block}{Definition}
        Linear regression is a statistical method used in machine learning to model the relationship between a 
        dependent variable (outcome) and one or more independent variables (input features). It assumes that there 
        is a linear relationship between these variables, allowing us to express the output as a linear combination 
        of the inputs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Purpose}
    \begin{block}{Purpose}
        The primary purpose of linear regression is to predict continuous numeric outcomes based on input 
        features. It is widely used in various fields such as:
        \begin{itemize}
            \item Economics
            \item Biology
            \item Engineering
            \item Social Sciences
        \end{itemize}
        Applications include predicting sales, estimating costs, and assessing risks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Key Concepts}
    \begin{itemize}
        \item \textbf{Dependent Variable (Y)}: The outcome variable we want to predict. 
        \begin{itemize}
            \item Example: In predicting house prices, the price is the dependent variable.
        \end{itemize}
        
        \item \textbf{Independent Variables (X)}: The input features or predictors influencing the dependent variable.
        \begin{itemize}
            \item Example: Square footage, number of bedrooms, and location can affect house prices.
        \end{itemize}
        
        \item \textbf{Line of Best Fit}: The algorithm finds the best-fitting line through data points, minimizing the difference 
        between observed and predicted values.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Linear Equation}
    The relationship in linear regression can be expressed as follows:

    \begin{equation}
        Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n + \epsilon
    \end{equation}

    Where:
    \begin{itemize}
        \item \( Y \): Predicted value (dependent variable)
        \item \( b_0 \): Intercept (expected value of \( Y \) when all \( X's \) are 0)
        \item \( b_1, b_2, \ldots, b_n \): Coefficients showing the change in \( Y \) for a one-unit change in \( X \)
        \item \( X_1, X_2, \ldots, X_n \): Independent variables
        \item \( \epsilon \): Error term (difference between predicted and observed values)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Example}
    For instance, to predict the price of a car based on its age and mileage, the model might look like:

    \begin{equation}
        \text{Price} = 20,000 - 1,500 \times \text{Age} - 0.05 \times \text{Mileage}
    \end{equation}

    \begin{itemize}
        \item For every year increase in the car's age, the price decreases by $1,500.
        \item For every mile increase in mileage, the price decreases by $0.05.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression? - Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Linear regression is foundational in machine learning and precedes more complex algorithms.
            \item Understanding linear regression assumptions: linearity, independence, homoscedasticity, 
            and normality of errors.
            \item Its simplicity and interpretability make it popular for initial data analyses.
        \end{itemize}
    \end{block}
    
    Overall, linear regression is crucial for predicting numeric outcomes based on relationships with input features. 
    Its effectiveness and ease of use make it one of the first algorithms learned in machine learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations of Linear Regression - Introduction}
    \begin{block}{Introduction to Linear Regression}
        Linear regression is a statistical method used for predicting a continuous outcome variable based on one or more predictor variables. The goal is to establish a relationship between the input features and the output.
    \end{block}

    \begin{itemize}
        \item Predicts a continuous outcome variable.
        \item Establishes relationships between input features and output.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations of Linear Regression - Fundamental Concepts}
    \begin{block}{Equation of Linear Regression}
        The fundamental equation of a linear regression model with one predictor variable can be formulated as:
        \begin{equation}
            Y = \beta_0 + \beta_1X + \epsilon
        \end{equation}
        Where:
        \begin{itemize}
            \item \( Y \) = Dependent variable (outcome)
            \item \( X \) = Independent variable (predictor)
            \item \( \beta_0 \) = Intercept
            \item \( \beta_1 \) = Slope
            \item \( \epsilon \) = Error term
        \end{itemize}
    \end{block}

    \begin{block}{Multiple Linear Regression}
        When there are multiple predictor variables, the model expands to:
        \begin{equation}
            Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations of Linear Regression - Cost Function and Graphical Representation}
    \begin{block}{Cost Function}
        To estimate the coefficients \( \beta \), we minimize the cost function (Mean Squared Error):
        \begin{equation}
            J(\beta_0, \beta_1) = \frac{1}{m} \sum_{i=1}^m (Y_i - (\beta_0 + \beta_1X_i))^2
        \end{equation}
        Where \( m \) is the number of observations.
    \end{block}

    \begin{block}{Graphical Representation}
        \begin{itemize}
            \item **Scatter Plot**: Depicts the relationship between the independent variable \( X \) and the dependent variable \( Y \). 
            \item Line of best fit provides a visual representation of the linear regression model.
        \end{itemize}
        \includegraphics[width=\textwidth]{path_to_graph_image}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Linear Regression - Overview}
    \begin{itemize}
        \item Step-by-step guide for implementing linear regression using Python's Scikit-learn.
        \item Solidify understanding of linear regression concepts.
        \item Focus on practical application and model evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Linear Regression?}
    \begin{block}{Definition}
        Linear regression is a statistical method used to model the relationship between:
        \begin{itemize}
            \item A dependent (target) variable
            \item One or more independent (predictor) variables
        \end{itemize}
        The goal is to fit a linear equation to the observed data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Scikit-learn?}
    \begin{itemize}
        \item Scikit-learn (sklearn) is a powerful, user-friendly machine learning library in Python.
        \item Provides simple and efficient tools for:
        \begin{itemize}
            \item Data mining
            \item Data analysis
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Import Libraries}
    \begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 2 and 3: Load Dataset and Data Preprocessing}
    \begin{lstlisting}[language=Python]
# Load dataset
data = pd.read_csv('housing_data.csv')
print(data.head())  # View the first few rows

# Define features and target variable
X = data[['size']]  # Feature matrix
y = data['price']  # Target variable
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 4: Split the Dataset}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 5-8: Model Creation, Fitting, and Evaluation}
    \begin{lstlisting}[language=Python]
# Step 5: Create a Linear Regression Model
model = LinearRegression()

# Step 6: Fit the Model
model.fit(X_train, y_train)

# Step 7: Make Predictions
y_pred = model.predict(X_test)

# Step 8: Evaluate the Model
from sklearn.metrics import mean_absolute_error, r2_score

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Absolute Error: {mae}')
print(f'R² Score: {r2}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Data Splitting:} Crucial for evaluating model performance.
        \item \textbf{Model Fit:} Ensures the model learns from training data effectively.
        \item \textbf{Performance Metrics:} Important for validating the model's predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Follow these steps to effectively implement linear regression with Scikit-learn.
        \item Next slide will discuss important data preprocessing techniques before modeling.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preparation for Linear Regression}
    \begin{block}{Overview}
        Before implementing a linear regression model, it is crucial to perform data preparation. 
        Key steps involve cleaning the data and normalizing features.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning}
    Data cleaning involves identifying and correcting errors and inconsistencies.

    \begin{itemize}
        \item \textbf{Handling Missing Values:}
        \begin{itemize}
            \item Dropping rows/columns: Use when the missing values are minimal.
            \item Imputation: Fill missing values with mean, median, or mode.
            \begin{lstlisting}[language=Python]
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
data['feature'] = imputer.fit_transform(data[['feature']])
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Removing Duplicates:}
        \begin{lstlisting}[language=Python]
data.drop_duplicates(inplace=True)
        \end{lstlisting}
        
        \item \textbf{Outlier Detection:}
        Use visualization methods (e.g., box plots) or Z-scores.
        \begin{lstlisting}[language=Python]
from scipy import stats
data = data[(np.abs(stats.zscore(data['feature'])) < 3)]
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalizing Features}
    Normalization ensures that no particular feature dominates due to its scale.

    \begin{itemize}
        \item \textbf{Standardization:}
        \begin{equation}
        z = \frac{(X - \mu)}{\sigma}
        \end{equation}
        where \( X \) is the feature value, \( \mu \) is the mean, and \( \sigma \) is the standard deviation.
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[['feature1', 'feature2']])
        \end{lstlisting}

        \item \textbf{Min-Max Scaling:}
        \begin{equation}
        X' = \frac{(X - X_{min})}{(X_{max} - X_{min})}
        \end{equation}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data[['feature1', 'feature2']])
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Importance of quality data: High-quality data leads to better model accuracy.
            \item Choosing the right method: Different datasets may require different cleaning and normalization methods.
            \item Iterative process: Data preparation is often iterative; revisit steps as new insights emerge.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Thorough data preparation lays the foundation for successful linear regression modeling. 
        By cleaning and normalizing the dataset, we enhance the model's predictive power and reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Techniques - Introduction}
    Model evaluation is crucial in understanding how well your linear regression model predicts outcomes based on input data. Two widely used metrics for this purpose are 
    \textbf{R-squared (R²)} and \textbf{Mean Squared Error (MSE)}. Each provides different insights into the model performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Techniques - R-squared (R²)}
    \begin{block}{Definition}
        R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that is explained by an independent variable or variables in a regression model.
    \end{block}

    \begin{block}{Formula}
        \[
        R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}
        \]
        where:
        \begin{itemize}
            \item \(\text{SS}_{\text{res}}\) = sum of squares of residuals (errors)
            \item \(\text{SS}_{\text{tot}}\) = total sum of squares
        \end{itemize}
    \end{block}

    \begin{block}{Interpretation}
        \begin{itemize}
            \item A value of 0 means the model explains none of the variability of the response data around its mean.
            \item A value of 1 indicates that it explains all the variability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Techniques - Mean Squared Error (MSE)}
    \begin{block}{Definition}
        Mean Squared Error quantifies the average of the squared differences between the predicted and actual values. It measures how close a predicted value is to the eventual outcome.
    \end{block}

    \begin{block}{Formula}
        \[
        \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \]
        where:
        \begin{itemize}
            \item \(n\) = number of predictions
            \item \(y_i\) = actual value
            \item \(\hat{y}_i\) = predicted value
        \end{itemize}
    \end{block}

    \begin{block}{Interpretation}
        \begin{itemize}
            \item Lower MSE values indicate a better fit of the model to the data.
            \item MSE is sensitive to outliers, which can skew the results.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Techniques - Key Points & Tips}
    \begin{block}{Key Points}
        \begin{itemize}
            \item R² provides insight into the explanatory power of the model but does not measure the prediction error.
            \item MSE gives a concrete numerical figure representing the prediction error but does not indicate how much variance is explained.
            \item It is essential to consider both metrics together for a comprehensive evaluation of your linear regression model.
        \end{itemize}
    \end{block}

    \begin{block}{Useful Tips}
        \begin{itemize}
            \item Always visualize your results with scatter plots to enhance understanding.
            \item Consider using adjusted R-squared for models with multiple predictors to account for the number of terms in the model.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Introduction}
    \begin{itemize}
        \item Linear regression models the relationship between a dependent variable and independent variables.
        \item Its simplicity and interpretability make it widely applicable.
        \item Below are some practical applications demonstrating its effectiveness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Real Estate Valuation}
    \begin{block}{Concept}
        Predict property prices based on features such as size, number of bedrooms, and location.
    \end{block}
    \begin{block}{Example}
        \[
        \text{Price} = \beta_0 + \beta_1 \times \text{Square Feet} + \beta_2 \times \text{Bedrooms} + \varepsilon
        \]
        Where:
        \begin{itemize}
            \item \( \text{Price} \) = Market price of the property
            \item \( \beta_0 \) = Intercept
            \item \( \beta_1, \beta_2 \) = Coefficients
            \item \( \varepsilon \) = Error term
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Sales Forecasting}
    \begin{block}{Concept}
        Businesses use linear regression to forecast sales based on advertising spend and other factors.
    \end{block}
    \begin{block}{Example}
        \[
        \text{Sales} = \beta_0 + \beta_1 \times \text{Ad Spend} + \beta_2 \times \text{Seasonality} + \varepsilon
        \]
        This helps in budgeting for marketing and inventory management.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Health and Medical Research}
    \begin{block}{Concept}
        Researchers use linear regression to study relationships between risk factors and health outcomes.
    \end{block}
    \begin{block}{Example}
        \[
        \text{Blood Pressure} = \beta_0 + \beta_1 \times \text{Age} + \beta_2 \times \text{Cholesterol} + \varepsilon
        \]
        Guides public health interventions and policy formulation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Environmental Science}
    \begin{block}{Concept}
        Assess environmental impacts, such as how temperature affects fish populations.
    \end{block}
    \begin{block}{Example}
        \[
        \text{Fish Population} = \beta_0 + \beta_1 \times \text{Temperature} + \varepsilon
        \]
        Informs conservation strategies and resource management.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Key Points}
    \begin{itemize}
        \item \textbf{Simplicity}: A straightforward approach to understanding variable relationships.
        \item \textbf{Interpretability}: Coefficients indicate the nature and strength of relationships.
        \item \textbf{Wide Applicability}: Utilized across diverse fields for decision-making and strategy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Conclusion}
    \begin{itemize}
        \item Linear regression is a versatile data analysis tool.
        \item Facilitates predictions and insights for informed decision-making.
        \item Highlights its importance in data-driven environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications of Linear Regression - Call to Action}
    Reflect on areas in your field where linear regression could be applied. 
    \begin{itemize}
        \item Consider its iterative nature for further analysis and exploratory data tasks!
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Limitations of Linear Regression}
    \begin{block}{Overview of Linear Regression}
        Linear regression is a foundational machine learning algorithm used to model the relationship between a dependent variable and one or more independent variables. While it is widely utilized for making predictions, it has several assumptions and limitations that, if overlooked, can lead to poor model performance and inaccurate predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Assumptions of Linear Regression}
    \begin{enumerate}
        \item \textbf{Linearity:} The relationship should be linear.
        \begin{itemize}
            \item \textit{Example:} Predicting salary based on years of experience assumes a straight-line relationship.
        \end{itemize}
        
        \item \textbf{Independence:} Observations should be independent.
        \begin{itemize}
            \item \textit{Illustration:} In survey data, responses should not influence each other.
        \end{itemize}
        
        \item \textbf{Homoscedasticity:} The variance of errors should remain constant.
        \begin{itemize}
            \item \textit{Key Point:} Systematic error variation can lead to unreliable predictions.
        \end{itemize}
        
        \item \textbf{Normality of Errors:} The residuals should be normally distributed.
        \begin{itemize}
            \item \textit{Example:} Outliers can cause this assumption to fail.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Linear Regression}
    \begin{enumerate}
        \item \textbf{Sensitivity to Outliers:}
        \begin{itemize}
            \item Outliers can skew results; a single high revenue can distort averages.
        \end{itemize}
        
        \item \textbf{Not Suitable for Non-linear Relationships:}
        \begin{itemize}
            \item Fails for relationships that aren't linear; consider polynomial models.
        \end{itemize}
        
        \item \textbf{Multicollinearity:}
        \begin{itemize}
            \item High correlations among predictors distort coefficients.
        \end{itemize}
        
        \item \textbf{Overfitting:}
        \begin{itemize}
            \item Too many predictors can fit training data well but fail on unseen data.
        \end{itemize}

        \item \textbf{Assumption Violations:}
        \begin{itemize}
            \item Real-world data often violate assumptions, making modeling challenging.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{When to Use Alternative Methods}
    \begin{itemize}
        \item \textbf{Non-Linear Relationships:} Use polynomial regression, decision trees, or neural networks.
        \item \textbf{Large Feature Space:} Consider regularization or tree-based models.
        \item \textbf{High Multi-collinearity:} Employ dimensionality reduction techniques like PCA.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    While linear regression is a vital tool in predictive analytics, understanding its limitations is crucial for making informed modeling decisions. Always evaluate the underlying assumptions and consider alternative approaches when the data doesn't meet these criteria.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet for Checking Assumptions}
    \begin{lstlisting}[language=Python]
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Assume 'model' is our fitted linear regression model

# 1. Checking Homoscedasticity
residuals = model.resid
fitted = model.fittedvalues
sns.scatterplot(x=fitted, y=residuals)
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs Fitted Values")
plt.show()

# 2. Checking Normality of Residuals
sm.qqplot(residuals, line='s')
plt.title("QQ Plot of Residuals")
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways}
    
    \begin{enumerate}
        \item \textbf{Understanding Linear Regression}:
        \begin{itemize}
            \item Foundational algorithm for predictive modeling.
            \item Assumes a linear relationship between independent and dependent variables.
            \item Key assumptions: linearity, independence, homoscedasticity, normality of residuals.
        \end{itemize}
        
        \item \textbf{Limitations of Linear Regression}:
        \begin{itemize}
            \item Performs poorly with non-linear data or when assumptions are violated.
            \item Affected by multicollinearity and outliers.
        \end{itemize}
        
        \item \textbf{Application Scenarios}:
        \begin{itemize}
            \item Effective when relationships are approximately linear (e.g., predicting sales).
            \item Not suitable for complex relationships or time-series data without transformations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Emphasized Points}
    
    \begin{itemize}
        \item \textbf{Identification of Data Patterns}:
        \begin{itemize}
            \item Visualize data using scatter plots, residual plots, or correlation matrices.
        \end{itemize}

        \item \textbf{Model Evaluation}:
        \begin{itemize}
            \item Use metrics such as R², Adjusted R², and RMSE for performance evaluation.
        \end{itemize}

        \item \textbf{Next Steps in Learning}:
        \begin{itemize}
            \item Explore more complex machine learning techniques.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Machine Learning}
    
    \begin{enumerate}
        \item \textbf{Exploring Classification Algorithms}:
        \begin{itemize}
            \item Understand the necessity of classification methods for categorical outcomes.
            \item Next Chapter: \textit{Logistic Regression} for binary outcomes.
        \end{itemize}
        
        \item \textbf{Diving into Advanced Algorithms}:
        \begin{itemize}
            \item Learn about Decision Trees & Random Forests for non-linear relationships.
            \item Explore Support Vector Machines (SVM) for high-dimensional data.
        \end{itemize}
        
        \item \textbf{Model Validation Techniques}:
        \begin{itemize}
            \item Discuss cross-validation and grid search for robust models.
        \end{itemize}
        
        \item \textbf{Real-World Applications}:
        \begin{itemize}
            \item Case studies in industries like healthcare, finance, and retail.
        \end{itemize}
    \end{enumerate}
\end{frame}


\end{document}