\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Model Predictive Control]{Week 10: Model Predictive Control}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Predictive Control}
    \begin{block}{Overview}
        Model Predictive Control (MPC) is an advanced control strategy that utilizes optimization techniques to determine the best control actions for dynamic systems. It has established itself as a powerful tool in various engineering disciplines due to its ability to handle multi-variable control problems with constraints.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of MPC}
    \begin{itemize}
        \item \textbf{Prediction and Control Horizons}:
        \begin{itemize}
            \item \textbf{Prediction Horizon}: Future time frame for predicting system behavior (e.g., 5 seconds).
            \item \textbf{Control Horizon}: Set of control actions computed over the prediction horizon.
        \end{itemize}
        
        \item \textbf{Optimization}:
        \begin{itemize}
            \item MPC formulates an optimization problem at each step to minimize a cost function.
            \item The solution yields optimal control inputs, typically applying only the first of the optimized series.
        \end{itemize}
        
        \item \textbf{Feedback Mechanism}:
        \begin{itemize}
            \item Continuous feedback from real-time measurements allows for adjustments.
            \item Ensures model effectiveness against disturbances and changes in dynamics.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of MPC}
    \begin{block}{In Control Systems}
        - \textbf{Robustness}: Can handle uncertainties and non-linearities through continuous updates.
        - \textbf{Constraint Handling}: Explicit incorporation of constraints is crucial for safety and reliability.
    \end{block}
    
    \begin{block}{In Reinforcement Learning}
        - MPC enhances decision-making in dynamic environments, making it a strong ally to RL models.
        - By integrating MPC, agents can better manage exploration vs. exploitation challenges.
    \end{block}
    
    \begin{block}{Example}
        Consider a robot navigating a warehouse:
        \begin{itemize}
            \item \textbf{Prediction}: Predicts future position considering obstacles.
            \item \textbf{Optimization}: Determines best actions to minimize distance to target while avoiding collisions.
            \item \textbf{Feedback}: Adjusts actions based on sensor data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basics of Model Predictive Control - Introduction}
    \begin{itemize}
        \item Model Predictive Control (MPC) is an advanced control strategy.
        \item It is widely used in:
        \begin{itemize}
            \item Process control
            \item Robotics
            \item Automotive systems
        \end{itemize}
        \item MPC predicts future behavior using dynamic models to make optimal control decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basics of Model Predictive Control - Key Principles}
    \begin{enumerate}
        \item \textbf{Prediction Horizon}
        \begin{itemize}
            \item Defines the future time period for predictions, denoted as \(N\).
            \item Example: At \(2\) Hz with a \(5\) second horizon leads to \(10\) steps (assuming \(0.5\) second each).
        \end{itemize}
        
        \item \textbf{Control Law Optimization}
        \begin{itemize}
            \item Minimizes a cost function at each time step.
            \item Cost function \(J\):
            \begin{equation}
                J = \sum_{k=0}^{N-1} \left( \|x(k) - x_{ref}(k)\|^2_Q + \|u(k)\|^2_R \right)
            \end{equation}
            \item Where:
            \begin{itemize}
                \item \(x(k)\) = predicted state
                \item \(x_{ref}(k)\) = reference state
                \item \(u(k)\) = control input
                \item \(Q, R\) = weighting matrices
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basics of Model Predictive Control - Feedback Mechanism}
    \begin{itemize}
        \item MPC updates predictions based on the current state: 
        \begin{itemize}
            \item Measures actual system state
            \item Adjusts predictions
            \item Reoptimizes control actions
        \end{itemize}
        \item This continuous feedback enhances performance and robustness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basics of Model Predictive Control - Example Application}
    \begin{itemize}
        \item \textbf{Example: Autonomous Vehicle Control}
        \begin{itemize}
            \item Models future positions using current speed and acceleration.
            \item Predicts distance to a stop sign over a \(4\) second horizon.
            \item Computes optimal braking force to minimize cost based on:
            \begin{itemize}
                \item Speed limits
                \item Comfort
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basics of Model Predictive Control - Key Points to Remember}
    \begin{itemize}
        \item **Prediction Horizon**: Length of time for predictions.
        \item **Control Law Optimization**: Selecting control inputs to minimize cost.
        \item **Feedback Mechanism**: Continuous adjustment for accuracy and performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation of MPC - Introduction}
    \begin{block}{Overview}
        Model Predictive Control (MPC) optimizes future control actions to minimize a cost function while satisfying system constraints. This requires predicting future behavior over a specified horizon.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation of MPC - Structure}
    \begin{enumerate}
        \item \textbf{Time Horizon and Steps:}     
        Let \( N \) be the prediction horizon, capturing how many time steps the controller will consider.

        \item \textbf{System Dynamics:} The model is expressed as:
        \begin{equation}
        x_{k+1} = Ax_k + Bu_k
        \end{equation}
        \begin{itemize}
            \item \( x_k \): state at time \( k \)
            \item \( u_k \): control input at time \( k \)
            \item \( A, B \): system matrices
        \end{itemize}

        \item \textbf{Objective Function:} The cost function \( J \) is defined as:
        \begin{equation}
        J = \sum_{i=0}^{N-1} \left( \|x_{k+i} - x_{ref}\|_Q^2 + \|u_{k+i}\|_R^2 \right)
        \end{equation}
        where \( Q \) and \( R \) are weighting matrices.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation of MPC - Constraints and Optimization}
    \begin{block}{Constraints}
        \begin{itemize}
            \item \textbf{State Constraints:} 
            \begin{equation}
            x_{min} \leq x_k \leq x_{max}
            \end{equation}
            \item \textbf{Control Constraints:} 
            \begin{equation}
            u_{min} \leq u_k \leq u_{max}
            \end{equation}
        \end{itemize}
    \end{block}
    
    \begin{block}{Optimization Problem}
        At each time step \( k \), the optimization problem is:
        \begin{equation}
        \min_{u_k, u_{k+1}, \ldots, u_{k+N-1}} J \quad \text{subject to:}
        \end{equation}
        \begin{align*}
        x_{k+1} &= Ax_k + Bu_k \quad \text{(system dynamics)} \\
        x_{min} &\leq x_k \leq x_{max} \quad \text{(state constraints)} \\
        u_{min} &\leq u_k \leq u_{max} \quad \text{(control constraints)}
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation Steps of MPC}
    \begin{block}{Introduction}
    Model Predictive Control (MPC) is an advanced control strategy that optimizes control actions by predicting future system behavior over a defined prediction horizon. 
    \end{block}
    \begin{itemize}
        \item Key steps: 
        \begin{enumerate}
            \item Model Prediction
            \item Optimization
            \item Execution of Control Actions
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Model Prediction}
    \begin{block}{Concept}
        At the core of MPC is the prediction of future system behavior based on a mathematical model, which describes how the current state evolves over time given control inputs.
    \end{block}
    \begin{itemize}
        \item \textbf{System Model:} 
            \begin{itemize}
                \item Can be linear or nonlinear.
                \item Must accurately represent the dynamics of the system.
            \end{itemize}
        \item \textbf{State Estimation:} 
            \begin{itemize}
                \item Use real-time data to estimate the current state.
                \item This serves as the basis for future predictions.
            \end{itemize}
        \item \textbf{Example:} 
            \begin{itemize}
                \item For a thermal system, the model could use differential equations relating temperature changes to control inputs like heating or cooling.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Optimization}
    \begin{block}{Concept}
        After making predictions, MPC determines the control inputs by solving an optimization problem that minimizes a cost function.
    \end{block}
    \begin{itemize}
        \item \textbf{Cost Function:} Balances:
            \begin{itemize}
                \item Tracking Error: Deviation from the desired setpoint.
                \item Control Effort: Magnitude of control inputs to prevent aggressive changes.
            \end{itemize}
        \item \textbf{Constraints:} Physical limits on:
            \begin{itemize}
                \item Control inputs
                \item State variables
                \item Operational safety limits 
            \end{itemize}
        \item \textbf{Mathematical Formulation:}
            \begin{equation}
            \begin{aligned}
            & \text{minimize} \quad J = \sum_{k=0}^{N-1} \left( \|y_k - y_{ref}\|^2_Q + \|u_k\|^2_R \right) \\
            & \text{subject to:} \quad x_{k+1} = Ax_k + Bu_k, \\
            & \quad x_{min} \leq x_k \leq x_{max}, \quad u_{min} \leq u_k \leq u_{max}
            \end{aligned}
            \end{equation}
        \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Execution of Control Actions}
    \begin{block}{Concept}
        Optimal control actions are implemented in real-time based on the optimization results.
    \end{block}
    \begin{itemize}
        \item The controller applies only the first control action of the optimal sequence to the system.
        \item The process repeats:
            \begin{itemize}
                \item State is re-estimated
                \item New predictions made
                \item Optimization problem re-solved at the next time step
            \end{itemize}
        \item \textbf{Key Point:} This receding horizon approach continuously adapts to changes or disturbances in the system.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item \textbf{Predict Model Dynamics:} Accurately predict future behavior based on the current state.
        \item \textbf{Optimize Control Inputs:} Solve for the best control actions while considering constraints.
        \item \textbf{Receding Horizon Strategy:} Execute only the first action, then repeat the cycle dynamically.
    \end{itemize}
    \begin{block}{Conclusion}
        By following these steps, MPC can effectively manage complex systems while ensuring optimal performance and adherence to constraints.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison to Traditional Control Methods - Overview}
    \begin{block}{Overview}
        Model Predictive Control (MPC) is a sophisticated approach to control system design, distinguishing itself from classical techniques like Proportional-Integral-Derivative (PID) control. Understanding these paradigms aids in selecting the appropriate method for various control challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison to Traditional Control Methods - Key Concepts}
    \begin{itemize}
        \item \textbf{Model Predictive Control (MPC)}:
        \begin{itemize}
            \item Uses a dynamic model to predict future behavior.
            \item Formulates an optimization problem at each time step.
        \end{itemize}
        
        \item \textbf{PID Control}:
        \begin{itemize}
            \item Feedback control mechanism with Proportional, Integral, and Derivative components.
            \item Adjusts control output based on current and past error.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison to Traditional Control Methods - Comparison Table}
    \begin{table}
        \centering
        \begin{tabular}{@{}lll@{}}
            \toprule
            \textbf{Feature} & \textbf{Model Predictive Control (MPC)} & \textbf{PID Control} \\ \midrule
            \textbf{Complexity} & More complex, requires a model                     & Simpler, does not require a model      \\
            \textbf{Performance} & Optimal for multi-variable systems                 & Effective for single-variable systems    \\
            \textbf{Robustness}   & High, considers constraints and disturbances    & Moderate, relies on tuning parameters     \\
            \textbf{Computation} & Significant computational resources               & Low computational cost                    \\
            \textbf{Adaptability} & Highly adaptive to changes in dynamics          & Less adaptive, requires retuning          \\
            \textbf{Handling Constraints} & Explicitly manages constraints          & Cannot manage constraints inherently       \\ \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison to Traditional Control Methods - Advantages and Disadvantages}
    \begin{block}{Advantages of MPC}
        \begin{itemize}
            \item Multi-variable control: Excels with multiple interacting inputs/outputs.
            \item Explicit constraints management: Directly incorporates constraints.
            \item Predictive nature: Anticipates future events, adjusting actions accordingly.
        \end{itemize}
    \end{block}

    \begin{block}{Disadvantages of MPC}
        \begin{itemize}
            \item Computational intensity: Requires real-time optimization, can be heavy.
            \item Model dependency: Relies on accurate models; poor models yield poor performance.
            \item Implementation complexity: More challenging to set up and tune versus PID.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison to Traditional Control Methods - Use Cases and Conclusion}
    \begin{itemize}
        \item \textbf{Example Use Cases}:
        \begin{itemize}
            \item \textbf{MPC}: Used in autonomous vehicles to manage complex interactions.
            \item \textbf{PID}: Effective in simpler systems like temperature control.
        \end{itemize}
        
        \item \textbf{Conclusion}:
        Choosing between MPC and traditional methods like PID depends on application complexity and performance requirements. While PID is sufficient for simpler tasks, MPC excels in complex systems needing foresight and adherence to constraints.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Model Predictive Control (MPC)}
    \begin{block}{Overview of MPC Applications}
        Model Predictive Control (MPC) is a versatile control strategy with diverse applications across multiple fields, primarily due to its ability to handle constraints and optimize performance in dynamic environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Robotics}
    \begin{itemize}
        \item \textbf{Description}: In robotics, MPC is utilized for path planning and trajectory tracking.
        \item \textbf{Example}: A robotic arm manipulates objects within a constrained space while avoiding obstacles. MPC enables the arm to predict future movements and adjust in real-time.
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Allows for multi-objective optimization (e.g., speed vs. accuracy).
            \item Adapts to dynamic environments through continuous re-evaluation of trajectories.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Automotive Control}
    \begin{itemize}
        \item \textbf{Description}: MPC is applied in modern vehicles for adaptive cruise control, lane-keeping assistance, and electrified vehicle management.
        \item \textbf{Example}: In adaptive cruise control, MPC helps maintain a safe distance from the vehicle in front while optimizing fuel efficiency.
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Handles constraints such as speed limits and safety distances.
            \item Improves passenger comfort by smoothing acceleration and deceleration.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Process Control}
    \begin{itemize}
        \item \textbf{Description}: MPC is extensively used in chemical and manufacturing industries for process optimization and quality control.
        \item \textbf{Example}: In a chemical reactor, MPC can adjust input flows and temperatures to optimize yield while maintaining quality and safety.
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Multi-variable control ensures balance across various process parameters.
            \item Predicts future behavior of the system to minimize disturbances.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Benefits}
    \begin{itemize}
        \item \textbf{Flexibility}: Adapts to various operational conditions and constraints.
        \item \textbf{Efficiency}: Optimizes control actions based on predictions for better resource management.
        \item \textbf{Performance}: Enhances system stability and performance by forecasting future states.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaway}
    Model Predictive Control is a powerful tool for managing complex control tasks across various fields, combining prediction, optimization, and constraint handling to achieve high-performance outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading}
    Refer to scholarly articles and case studies on MPC implementations to explore advanced applications and emerging trends in control systems.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linking MPC and Reinforcement Learning}
    % Introduce how MPC techniques can be integrated with reinforcement learning (RL) frameworks for enhanced decision-making.
    \begin{block}{Introduction}
        Model Predictive Control (MPC) and Reinforcement Learning (RL) are powerful techniques in dynamic decision-making. Their integration enhances performance in complex environments by leveraging the strengths of both.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Model Predictive Control (MPC)}:
            \begin{itemize}
                \item \textbf{Definition}: A strategy that optimizes control actions over a future time horizon by solving an optimization problem at each time step.
                \item \textbf{Process}: At each interval, MPC calculates optimal control inputs by minimizing a cost function based on predicted future states.
            \end{itemize}
        \item \textbf{Reinforcement Learning (RL)}:
            \begin{itemize}
                \item \textbf{Definition}: A learning paradigm where an agent learns actions through trial and error to maximize cumulative rewards.
                \item \textbf{Process}: The agent interacts with the environment, observes states, takes actions, and receives feedback in rewards.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration Strategies}
    \begin{enumerate}
        \item \textbf{Using MPC as a Policy for RL}:
            \begin{itemize}
                \item \textbf{Description}: MPC can structure a policy for an RL agent, especially with an accurate environment model.
                \item \textbf{Example}: In robotic arm control, MPC generates smooth trajectories for the RL agent to follow, reducing the exploration burden.
            \end{itemize}
        
        \item \textbf{Learning the Model for MPC}:
            \begin{itemize}
                \item \textbf{Description}: RL can learn system dynamics to be used in MPC, refining predictions and control actions.
                \item \textbf{Example}: In autonomous driving, an RL agent learns vehicle dynamics, which MPC uses to optimize driving strategies.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Integration}
    \begin{itemize}
        \item \textbf{Improved Sample Efficiency}: Utilizing MPC's model knowledge enhances RL's policy learning speed.
        \item \textbf{Robustness to Model Inaccuracies}: The predictive nature of MPC mitigates model errors in RL, leading to reliable decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simple Example for Illustration}
    \begin{block}{Example: Drone Navigation}
        \begin{itemize}
            \item \textbf{MPC}: Calculates the optimal path by predicting future positions and avoiding obstacles.
            \item \textbf{RL Agent}: Learns from interactions (e.g., rewards for successful navigation and penalties for collisions) to choose control inputs for MPC's cost function.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Linking MPC with RL enhances decision-making capabilities across applications such as robotics, healthcare, and autonomous systems, improving control strategies' adaptability and efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Online vs. Offline MPC}
    Model Predictive Control (MPC) is an optimal control strategy that utilizes a system model to predict future behavior and make decisions efficiently. 
    
    \begin{block}{Key Differences}
        \begin{itemize}
            \item Definition
            \item Data Acquisition
            \item Computational Requirements
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences Between Online and Offline MPC}
    \begin{enumerate}
        \item \textbf{Definition:}
            \begin{itemize}
                \item \textbf{Online MPC:} Real-time decisions based on the current system state; optimization is solved at every time step.
                \item \textbf{Offline MPC:} Control policies are pre-computed before deployment, with no real-time adjustments.
            \end{itemize}
            
        \item \textbf{Data Acquisition:}
            \begin{itemize}
                \item \textbf{Online MPC:} Continuously acquires data, leading to adaptive control strategies.
                \item \textbf{Offline MPC:} Uses historical data for a fixed control policy without adapting in real time.
            \end{itemize}
    
        \item \textbf{Computational Requirements:}
            \begin{itemize}
                \item \textbf{Online MPC:} Significant computational resources needed at each time step, potential for delays.
                \item \textbf{Offline MPC:} Performs heavy computations beforehand for rapid decision-making afterward.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implications in Reinforcement Learning (RL)}
    Key considerations in the context of RL:

    \begin{enumerate}
        \item \textbf{Adaptability:}
            \begin{itemize}
                \item \textbf{Online MPC:} Suitable for uncertain, dynamic environments; aligns with exploration-exploitation strategies in RL.
                \item \textbf{Offline MPC:} Best for stable environments; utilizes pre-trained models without real-time adjustments.
            \end{itemize}

        \item \textbf{Data Efficiency:}
            \begin{itemize}
                \item \textbf{Online MPC:} Benefits from real-time data, leading to faster learning cycles but requires more samples.
                \item \textbf{Offline MPC:} Efficient with existing datasets, allowing learning from extensive historical interactions.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Conclusion}
        Both Online and Offline MPC strategies have unique advantages. Choose based on the environment's stability and available computational resources.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Model Predictive Control (MPC)}
    \begin{block}{Introduction}
        Model Predictive Control (MPC) is a robust control strategy used across various domains. However, its implementation faces several challenges critical to its effectiveness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MPC - Computational Burden}
    \begin{itemize}
        \item \textbf{Real-time Constraints:} Optimization problems must be solved quickly, complicating real-time applications.
        \item \textbf{State and Control Dimensions:} Increasing dimensions of state and control inputs lead to larger optimization problems and longer computation times.
    \end{itemize}
    
    \begin{block}{Example}
        In robotic arm control applications, multiple joints and degrees of freedom create a high-dimensional optimization problem, requiring significant computational resources.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MPC - Model Inaccuracies}
    \begin{itemize}
        \item \textbf{Model Uncertainty:} Accurate model creation can be complex—e.g., chemical processes are affected by variable conditions, complicating predictive accuracy.
        \item \textbf{Disturbances:} External factors can lead to discrepancies between predicted and actual system behaviors, impacting performance.
    \end{itemize}

    \begin{block}{Illustration}
        In a temperature control system, if room insulation changes unexpectedly (e.g., windows opened), the model's predictions become inaccurate, leading to suboptimal control.
    \end{block}

    \begin{block}{Implementation Challenges}
        - \textbf{Tuning Parameters:} Selecting the right parameters for solvers can be non-trivial and impacts stability.
        - \textbf{Initialization:} The initial guess for optimization influences convergence speed.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in MPC - Key Points}
    \begin{itemize}
        \item There is often a trade-off between model accuracy and computational burden.
        \item MPC's predictive ability may be compromised by model inaccuracies and computational delays in fast-changing environments.
    \end{itemize}

    \begin{block}{Conclusion}
        Despite challenges, MPC remains powerful due to its ability to handle constraints and optimize performance. Research into efficient solvers and modeling techniques is crucial for overcoming challenges in real-world applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Fundamentals - Introduction}
    \begin{block}{What is Reinforcement Learning?}
        Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. 
        Unlike supervised learning, the agent learns from the consequences of its actions through interaction with the environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Agent}: The learner or decision-maker that interacts with the environment.
        \item \textbf{Environment}: The world where the agent operates, providing feedback and rewards based on actions.
        \item \textbf{State (s)}: The current situation of the agent, defining the scenario it is dealing with.
        \item \textbf{Action (a)}: Choices made by the agent that affect the environment; can be discrete or continuous.
        \item \textbf{Reward (r)}: A scalar feedback signal received after an action, guiding desired behavior.
        \item \textbf{Policy ($\pi$)}: Strategy used by the agent to decide actions based on the state.
        \item \textbf{Value Function (V)}: Expected long-term reward for a state, assessing its goodness.
        \item \textbf{Q-function (Q)}: Expected long-term reward of taking an action in a state and following the policy thereafter.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Process and Practical Example}
    \begin{block}{Learning Process}
        \begin{itemize}
            \item \textbf{Exploration vs. Exploitation}: Balancing between trying new actions and using known actions to maximize rewards.
            \item \textbf{Temporal Difference Learning}: Evaluates the value of actions over time by adjusting predictions based on differences between predicted and actual rewards.
        \end{itemize}
    \end{block}

    \begin{block}{Example of RL in Practice}
        Imagine a robot navigating a maze:
        \begin{itemize}
            \item \textbf{State (s)}: Robot's current position.
            \item \textbf{Action (a)}: Move up, down, left, or right.
            \item \textbf{Reward (r)}: +10 for reaching exit, -1 for hitting walls, 0 for non-productive moves.
        \end{itemize}
        The robot learns to navigate by trying paths and refining its policy for efficient exits.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Reinforcement Learning in MPC}
    \begin{block}{Integration with Model Predictive Control (MPC)}
        \begin{itemize}
            \item Reinforcement learning complements MPC by allowing agents to learn optimal policies in complex environments.
            \item It helps address model inaccuracies that can undermine traditional MPC strategies.
            \item Learning enables adaptation to environmental changes, enhancing control robustness and flexibility.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding these fundamental concepts of RL sets the stage for exploring how MPC can be modeled in this framework, addressing complex control problems effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Modeling the Environment for RL - Importance of Accurate Models}
    \begin{block}{Understanding the Environment}
        In RL, an agent learns to make decisions by interacting with an environment. An accurately modeled environment allows the agent to predict the consequences of its actions, leading to improved performance and reduced learning time.
        \begin{itemize}
            \item The environment is often represented as a Markov Decision Process (MDP) comprising states, actions, rewards, and transition dynamics.
        \end{itemize}
    \end{block}
    
    \begin{block}{Decision-Making and Planning}
        Accurate models enable better decision-making by providing realistic simulations of outcomes.
        \begin{itemize}
            \item Knowing transition probabilities aids in planning the best sequence of actions.
        \end{itemize}
    \end{block}
    
    \begin{block}{Sample Efficiency}
        In costly data collection environments, having a model allows simulation without physical interaction, enhancing sample efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Modeling the Environment for RL - MPC Overview}
    \begin{block}{Overview of MPC}
        Model Predictive Control (MPC) is an advanced control strategy that uses a model of the system to predict future states.
        \begin{itemize}
            \item It solves an optimization problem at every time step, predicting behavior over a defined horizon.
        \end{itemize}
    \end{block}
    
    \begin{block}{Integration with RL}
        Integrating MPC with RL allows agents to leverage MPC's predictive capabilities. This results in:
        \begin{itemize}
            \item Better exploration as agents can anticipate rewards from potential actions.
            \item Refined policy learning based on accurate predictions of state transitions and rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Modeling the Environment for RL - Examples and Key Points}
    \begin{block}{Examples of MPC in RL}
        \begin{enumerate}
            \item \textbf{Robotic Control:} MPC optimizes the path of a robotic arm while accounting for physical constraints.
            \item \textbf{Autonomous Vehicles:} MPC models a vehicle's environment, predicting responses to steering and acceleration for safer navigation.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Accurate environment modeling is critical for effective learning and decision-making in RL.
            \item MPC optimizes control inputs and improves the accuracy of dynamics modeling.
            \item Integrating MPC into RL enhances exploration and policy learning.
        \end{itemize}
    \end{block}

    \begin{equation}
    \begin{aligned}
    & \text{Minimize} & J = \sum_{t=0}^{T} \left( \| x_t - x_{\text{ref}} \|^2 + \| u_t \|^2 \right) \\
    & \text{subject to} & x_{t+1} = f(x_t, u_t) \quad (dynamics) \\
    & & u_{min} \leq u_t \leq u_{max} \quad (input constraints) \\
    & & x_{min} \leq x_t \leq x_{max} \quad (state constraints)
    \end{aligned}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summarizing Control Objectives - Introduction}
    \begin{block}{Control Objectives in Model Predictive Control (MPC)}
        Model Predictive Control (MPC) serves as a powerful tool in Reinforcement Learning (RL) frameworks. It integrates various control objectives to enhance decision-making processes in dynamic environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summarizing Control Objectives - Key Control Objectives}
    \begin{enumerate}
        \item \textbf{Stability}
            \begin{itemize}
                \item \textit{Explanation}: Return to equilibrium after disturbances, ensuring predictable behavior.
                \item \textit{Example}: Maintaining a steady hover in a drone after wind gusts.
            \end{itemize}
        
        \item \textbf{Tracking}
            \begin{itemize}
                \item \textit{Explanation}: Accurately follow a desired trajectory or setpoint over time.
                \item \textit{Example}: An autonomous vehicle following a predefined path on a road.
            \end{itemize}
        
        \item \textbf{Optimization of Performance Criteria}
            \begin{itemize}
                \item \textit{Explanation}: Optimize performance metrics (e.g., energy consumption, time).
                \item \textit{Example}: Minimize energy usage in resource allocation while maximizing output.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summarizing Control Objectives - Additional Objectives and Conclusion}
    \begin{enumerate}[resume]
        \item \textbf{Safety Constraints}
            \begin{itemize}
                \item \textit{Explanation}: Respect physical limits and safety regulations.
                \item \textit{Example}: A robotic arm should not exceed its range of motion.
            \end{itemize}
        
        \item \textbf{Robustness}
            \begin{itemize}
                \item \textit{Explanation}: Maintain performance under uncertainties in the environment.
                \item \textit{Example}: An HVAC system adjusting to sudden temperature changes.
            \end{itemize}
        
        \item \textbf{Adaptability}
            \begin{itemize}
                \item \textit{Explanation}: Adapt to changing dynamics and requirements over time.
                \item \textit{Example}: A smart home thermostat adjusting settings based on occupancy.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Conclusion}
        Integrating control objectives within an RL framework using MPC techniques allows a structured approach to complex decision-making, enhancing RL agents' ability to navigate dynamic environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conducting a Reinforcement Learning Experiment - Overview}
    \begin{block}{Methodology to Implement RL Experiments Utilizing MPC}
        This methodology outlines the steps to conduct reinforcement learning (RL) experiments utilizing Model Predictive Control (MPC) as a decision-making framework.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 1: Define the Problem}
    \begin{itemize}
        \item \textbf{Objective:} Clearly articulate the task or environment to be optimized.
        \item Consider aspects such as stability, performance, and robustness.
        \item \textbf{Example:} Optimize the control of a robotic arm to reach a target position while minimizing movement time and energy consumption.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 2: Model the System Dynamics}
    \begin{itemize}
        \item Use system identification techniques to derive or estimate the dynamics of the environment.
        \item \textbf{Tools:} Simulink, Python libraries (e.g., Numpy, SciPy).
        \item \textbf{Model Example:}
        \begin{equation}
            x_{t+1} = Ax_t + Bu_t
        \end{equation}
        where \( x_t \) is the state, \( u_t \) is the control input, \( A \) is the state transition matrix, and \( B \) is the control matrix.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 3: Implement Model Predictive Control (MPC)}
    \begin{itemize}
        \item Design the MPC algorithm using the modeled dynamics. 
        \item \textbf{Key Equations:}
        \begin{equation}
            \text{Minimize: } J = \sum_{k=0}^{N-1} (x_{t+k|t} - x_{\text{ref}})^T Q (x_{t+k|t} - x_{\text{ref}}) + (u_{t+k|t})^R (u_{t+k|t})
        \end{equation}
        where \( J \) is the cost function, \( Q \) and \( R \) are weight matrices.
        \item \textbf{Example:} Use quadratic cost functions to penalize deviations from the desired path.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 4: Integrate Reinforcement Learning (RL)}
    \begin{itemize}
        \item Employ RL algorithms to fine-tune the MPC parameters.
        \item \textbf{Choose an RL Algorithm:} Q-learning, DDPG, or PPO.
        \item \textbf{Objective in RL Framework:}
        \begin{equation}
            \text{Maximize: } \sum_{t=0}^{T} \gamma^t r_t
        \end{equation}
        where \( r_t \) is the reward at time step \( t \) and \( \gamma \) is the discount factor.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 5: Experiment Setup}
    \begin{itemize}
        \item \textbf{Environment:} Define state, action, and reward spaces.
        \item \textbf{Example Environment:}
        \begin{itemize}
            \item \textbf{State:} Position and velocity of the robotic arm.
            \item \textbf{Action:} Adjustments to joint angles.
            \item \textbf{Reward:} Positive reward for reaching the target and negative for excessive energy usage.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 6: Conduct Training and Evaluation}
    \begin{itemize}
        \item Train the RL agent with the MPC policy, iteratively updating control strategies based on the agent's learning and system feedback.
        \item \textbf{Training Loop:}
        \begin{enumerate}
            \item Initialize the environment.
            \item Update system state.
            \item Select action via MPC.
            \item Execute action, obtain new state, and compute reward.
            \item Update RL model based on collected data.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 7: Analyze Results}
    \begin{itemize}
        \item Monitor performance metrics such as convergence speed, control accuracy, and computational efficiency.
        \item Find areas for improvement and retrain if necessary.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Interplay of MPC and RL:} MPC provides a structured framework for decision-making while RL optimizes control strategies.
        \item \textbf{Flexibility:} The methodology can be adapted for various applications including robotics, autonomous vehicles, and resource management.
        \item \textbf{Iterative Nature:} Both modeling and learning processes benefit from continual iterations based on system performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=Python]
import numpy as np

def mpc_control(state, model, horizon, cost_matrices):
    # Implement your MPC optimization here
    # Optimize control inputs based on state and model dynamics
    # Returns optimal control actions
    return optimal_action

# Usage
state = np.array([0, 0])  # Example initial state
action = mpc_control(state, system_model, prediction_horizon, [Q, R])
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating MPC in RL Scenarios - Introduction}
    \begin{block}{Overview}
        Model Predictive Control (MPC) is a powerful strategy used in conjunction with Reinforcement Learning (RL) to navigate dynamic and uncertain environments. Evaluating the effectiveness of MPC is crucial to ensure alignment with RL goals.
    \end{block}
    \begin{block}{Key Objective}
        This slide discusses key metrics for assessing the performance of MPC techniques within RL scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating MPC in RL Scenarios - Key Evaluation Metrics}
    \begin{enumerate}
        \item \textbf{Cumulative Reward}
            \begin{itemize}
                \item \textbf{Definition:} Total reward received over a number of time steps.
                \item \textbf{Importance:} Reflects long-term performance of the MPC controller.
                \item \textbf{Example:} In a grid-world, reaching the goal yields +1, while each step costs -1.
            \end{itemize}
        \item \textbf{Stability and Robustness}
            \begin{itemize}
                \item \textbf{Definition:} The MPC's ability to maintain performance under varied conditions.
                \item \textbf{Importance:} Detects sensitivity to environmental changes.
                \item \textbf{Example:} Assessing performance with noise to observe instability.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating MPC in RL Scenarios - Continued Metrics}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Execution Time/Computational Efficiency}
            \begin{itemize}
                \item \textbf{Definition:} Time to solve the MPC optimization problem at each control step.
                \item \textbf{Importance:} High times can impede real-time applications.
                \item \textbf{Tip:} Measure average computing time at each step.
            \end{itemize}
        \item \textbf{Trajectory Tracking Error}
            \begin{itemize}
                \item \textbf{Definition:} Difference between desired and actual paths.
                \item \textbf{Importance:} Evaluates closeness to target trajectory.
                \item \textbf{Formula:} 
                \begin{equation}
                \text{Tracking Error} = ||\text{Desired Path} - \text{Actual Path}||_2
                \end{equation}
                \item \textbf{Example:} Autonomous vehicle's deviation from the planned route due to actuator limits.
            \end{itemize}
        \item \textbf{Sample Efficiency}
            \begin{itemize}
                \item \textbf{Definition:} Measures effectiveness of the algorithm's learning from limited interactions.
                \item \textbf{Importance:} Minimizes samples for learning a good policy in RL.
                \item \textbf{Example:} Comparing performance with random vs. systematic exploration strategies.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating MPC in RL Scenarios - Conclusion}
    \begin{block}{Conclusion}
        Evaluating MPC in RL requires a multi-faceted approach considering various performance metrics. This analysis provides insights into the effectiveness of MPC in enhancing RL decision-making.
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Cumulative rewards inform long-term effectiveness.
            \item Stability ensures reliable performance under uncertainty.
            \item Computational efficiency is vital for real-time applications.
            \item Trajectory errors indicate precision of control.
            \item Sample efficiency relates to the learning capability of the algorithm.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies: MPC in Action}
    \begin{block}{Introduction to Model Predictive Control (MPC)}
        Model Predictive Control (MPC) is an advanced control strategy that leverages optimization techniques to predict and regulate the behavior of dynamic systems. It is highly beneficial in Reinforcement Learning (RL) scenarios, facilitating better decision-making based on anticipated outcomes in complex environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 1: Autonomous Vehicle Navigation}
    \begin{block}{Overview}
        \begin{itemize}
            \item \textbf{Objective}: Optimize routing for autonomous vehicles in urban settings.
            \item \textbf{MPC Application}: Integrated with RL for real-time decisions using sensor data and traffic information.
        \end{itemize}
    \end{block}
    \begin{block}{Outcomes}
        \begin{itemize}
            \item Up to 30\% reduction in travel time.
            \item Improved fuel efficiency through better path planning.
            \item Enhanced safety via predictive obstacle avoidance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 2: Energy Management in Smart Grids}
    \begin{block}{Overview}
        \begin{itemize}
            \item \textbf{Objective}: Efficiently manage energy distribution in smart grids utilizing renewable sources.
            \item \textbf{MPC Application}: Predictions of energy demand and optimization of generation schedules in conjunction with RL.
        \end{itemize}
    \end{block}
    \begin{block}{Outcomes}
        \begin{itemize}
            \item Reduction in operational costs by 15\%.
            \item Enhanced grid stability through improved load balancing.
            \item Increased utilization of renewable sources.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study 3: Industrial Process Control}
    \begin{block}{Overview}
        \begin{itemize}
            \item \textbf{Objective}: Optimize production lines to enhance throughput and minimize waste.
            \item \textbf{MPC Application}: Adjust control signals in real-time feedback, paired with RL for continuous improvement.
        \end{itemize}
    \end{block}
    \begin{block}{Outcomes}
        \begin{itemize}
            \item Increased production efficiency by 20\%.
            \item Decreased material waste and enhanced product quality.
            \item Self-optimizing control that adapts to changing production requirements.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Insights and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Synergy of MPC and RL enhances decision-making in uncertain environments.
            \item Adaptability: MPC predicts future states while RL learns from interactions.
            \item Practical applications span various industries like autonomous driving, energy management, and industrial control.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        These case studies highlight the effectiveness of MPC in real-world RL scenarios, showcasing its adaptability and efficiency in creating robust systems that can anticipate and react to changing conditions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Research Trends in MPC and RL}
    \begin{itemize}
        \item Model Predictive Control (MPC) operates using a dynamic model to forecast and optimize control decisions.
        \item Reinforcement Learning (RL) involves agents learning from rewards to enhance decision-making in an environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Research Trends}
    \begin{enumerate}
        \item \textbf{Integration of MPC and RL}
            \begin{itemize}
                \item Synergizing model-based MPC with trial-and-error RL improves performance in uncertain environments.
                \item \textit{Example}: Using RL to dynamically tune MPC parameters based on real-time metrics.
            \end{itemize}
        
        \item \textbf{Data-Driven Approaches}
            \begin{itemize}
                \item Advanced methods like Gaussian processes and neural networks allow for real-time model updates.
                \item Enhances MPC's robustness against model uncertainty.
            \end{itemize}
        
        \item \textbf{Hierarchical Control Structures}
            \begin{itemize}
                \item High-level decision-making (MPC) combined with low-level control (RL).
                \item \textit{Illustration}: A robot navigates: MPC plans the path, RL adapts to obstacles.
            \end{itemize}
        
        \item \textbf{Safe and Robust RL}
            \begin{itemize}
                \item Safety constraints incorporated into RL frameworks to ensure learned policies are safe.
                \item Focus on performance bounds under uncertainty.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Insight}
    \begin{block}{Optimization Problem}
        The integration of MPC and RL can be described by the following optimization problem:
        \begin{equation}
            \min_{\mathbf{u}} \sum_{t=0}^{N-1} L(x_t, u_t) + \Phi(x_N)
        \end{equation}
        subject to:
        \begin{equation}
            x_{t+1} = f(x_t, u_t), \quad u_t \in \mathcal{U}
        \end{equation}
    \end{block}
    \begin{itemize}
        \item \(x_t\) = State at time \(t\)
        \item \(u_t\) = Control action
        \item \(L\) = Running cost function
        \item \(\Phi\) = Terminal cost function
        \item \(\mathcal{U}\) = Set of control constraints
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Research Directions}
    \begin{itemize}
        \item \textbf{End-to-End Learning}: Train RL agents to optimize control objectives while adhering to constraints.
        \item \textbf{Transfer Learning}: Adapt learned policies to new environments with slightly different dynamics.
        \item \textbf{Multi-Agent Systems}: Investigate MPC and RL in multi-agent settings for cooperation and competition strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Integration of MPC and RL offers adaptive, efficient, and robust control solutions.
        \item Key research areas include data-driven methods, safety, and hierarchical architectures.
        \item \textbf{Key Takeaway}: Convergence of MPC and RL provides a powerful toolkit for complex control problems, ensuring safety and adaptability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Model Predictive Control (MPC) for Reinforcement Learning (RL) Applications}
    \begin{block}{Overview}
        MPC's integration with reinforcement learning enhances decision-making across various applications, raising ethical concerns regarding transparency and accountability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terms}
    \begin{itemize}
        \item \textbf{Model Predictive Control (MPC)}: An advanced control strategy optimizing actions based on future predictions.
        \item \textbf{Reinforcement Learning (RL)}: A paradigm where agents learn to make decisions to maximize cumulative reward.
        \item \textbf{Transparency}: Understanding the decision-making process of algorithms.
        \item \textbf{Accountability}: The responsibility of developers and users for system actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications}
    \begin{enumerate}
        \item \textbf{Decision-Making Transparency}
            \begin{itemize}
                \item Importance: Users need to understand decisions, especially in critical applications.
                \item Example: An autonomous vehicle's swerving to avoid an obstacle necessitates clarification of the decision process.
                \item Consequence: Lack of transparency may lead to mistrust and reduce acceptance.
            \end{itemize}
            
        \item \textbf{Accountability}
            \begin{itemize}
                \item Responsibility: Complexity in establishing liability during failures (e.g., car accidents).
                \item Example: Determining accountability in algorithmic decisions made by drones.
                \item Implication: Clear protocols for accountability are crucial to reinforce trust.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Considerations for Ethical Implementation}
    \begin{itemize}
        \item \textbf{Documentation}: Maintain detailed records of algorithms and decisions made.
        \item \textbf{Audit Trails}: Mechanisms for tracking and analyzing MPC and RL decisions.
        \item \textbf{Inclusivity}: Engage diverse stakeholders to ensure equitable outcomes.
        \item \textbf{Regulatory Compliance}: Stay informed about ethical guidelines for AI technologies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Transparency and accountability are vital for ethical MPC deployment in RL systems.
        \item Stakeholder engagement improves ethical practices in design and monitoring.
        \item Ongoing discussions on ethics shape standards for future automation and AI applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Collaborative Work in MPC and RL}
    \begin{block}{Importance of Collaboration}
        Collaboration in Model Predictive Control (MPC) and Reinforcement Learning (RL) can lead to:
        \begin{enumerate}
            \item Leveraging Diverse Expertise
            \item Integrating Perspectives
            \item Addressing Complex Problems
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Collaboration - Details}
    \begin{enumerate}
        \item \textbf{Leveraging Diverse Expertise}:
            \begin{itemize}
                \item Combining fields like robotics, operations research, and AI enriches control strategies.
                \item Example: Engineers and computer scientists collaboratively design adaptive systems.
            \end{itemize}
        \item \textbf{Integrating Perspectives}:
            \begin{itemize}
                \item Different viewpoints promote innovative ideas.
                \item Example: Insights from human behavior enhance RL models in robotics.
            \end{itemize}
        \item \textbf{Addressing Complex Problems}:
            \begin{itemize}
                \item Integration of MPC and RL tackles real-world applications (e.g., autonomous driving).
                \item Example: Data scientists and urban planners optimize traffic management systems.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Interdisciplinary Efforts}
    \begin{itemize}
        \item \textbf{Healthcare Applications:}
            \begin{itemize}
                \item Teams of medical professionals and AI researchers optimize chronic disease treatment protocols.
            \end{itemize}
        \item \textbf{Robotics:}
            \begin{itemize}
                \item Mechanical engineers and AI specialists develop robotic arms using MPC for stability and RL for learning.
            \end{itemize}
        \item \textbf{Smart Grid Management:}
            \begin{itemize}
                \item Collaboration between energy policy experts and control theorists enhances energy distribution using MPC and RL.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Interdisciplinary collaboration enhances the robustness and application of MPC and RL.
        \item Combining expertise leads to innovative solutions to real-world problems.
        \item Real-world applications illustrate the potential of integrating MPC and RL methodologies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-on Workshop Objectives}
    In this workshop, we will cover the following objectives related to Model Predictive Control (MPC):
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objective 1: Understand Model Predictive Control (MPC) Principles}
    \begin{itemize}
        \item \textbf{Goal}: Gain a foundational understanding of MPC concepts and techniques.
        \item \textbf{Activities}:
        \begin{itemize}
            \item Review theoretical underpinnings of MPC, including receding horizon control and optimization.
            \item Discuss the role of prediction models and constraints in controlling dynamic systems.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objective 2: Implement a Basic MPC Algorithm}
    \begin{itemize}
        \item \textbf{Goal}: Develop practical skills in coding a simple MPC algorithm.
        \item \textbf{Activities}:
        \begin{itemize}
            \item Write a code snippet to implement a basic MPC controller using Python.
            \item Utilize libraries such as NumPy and SciPy for optimization tasks.
        \end{itemize}
    \end{itemize}
    \begin{lstlisting}[language=Python]
import numpy as np
from scipy.optimize import minimize

# Define the prediction horizon
horizon = 10

# Define the cost function
def cost_function(control_inputs):
    # Calculate cost based on control inputs, states, and references
    return np.sum(control_inputs**2)  # Simple quadratic cost

# Initial guess for control inputs
init_guess = np.zeros(horizon)

# Optimize control inputs to minimize cost
result = minimize(cost_function, init_guess)
optimized_controls = result.x
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objective 3: Simulate MPC in a Controlled Environment}
    \begin{itemize}
        \item \textbf{Goal}: Apply the implemented MPC algorithm in a simulated environment.
        \item \textbf{Activities}:
        \begin{itemize}
            \item Set up a simulation of a dynamic system (e.g., a simple pendulum or cart-pole).
            \item Use the MPC controller to manage the system’s behavior in real-time during simulation.
        \end{itemize}
    \end{itemize}
    \begin{block}{Illustration}
        A flowchart depicting the steps from system state measurement to control action application.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objective 4: Analyze Performance Metrics}
    \begin{itemize}
        \item \textbf{Goal}: Evaluate the performance of the implemented MPC strategy.
        \item \textbf{Activities}:
        \begin{itemize}
            \item Measure and analyze the system's response (tracking error, stability).
            \item Discuss how changing parameters affect performance (e.g., prediction horizon, weights in the cost function).
        \end{itemize}
        \item \textbf{Key Metrics}:
        \begin{itemize}
            \item Settling time
            \item Overshoot
            \item Control input smoothness
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Interdisciplinary Application}: Recognize the importance of MPC in various fields such as robotics, aerospace, and economics, reinforcing collaboration as discussed in the previous slide.
        \item \textbf{Real-World Relevance}: Understand how practical skills in MPC can apply to current research and industry problems, preparing students for collaborative projects in future sessions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Conclusion}
    By the end of this workshop, students should be able to code, simulate, and evaluate a basic MPC system, setting a solid foundation for advanced applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Review of Learning Objectives}
    % Reiterate the learning objectives for the week to ensure students understand what they need to take away from the lesson.
    By the end of this week, students should be able to:
    \begin{itemize}
        \item Comprehend the principles and workings of Model Predictive Control (MPC).
        \item Recognize and apply MPC in various real-world contexts.
        \item Set up and analyze the optimization problem integral to MPC.
        \item Manage constraints effectively within an MPC framework.
        \item Evaluate the performance of MPC through practical simulations and comparisons.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives for Week 10: Model Predictive Control (MPC)}
    % Understanding Model Predictive Control Concepts
    \begin{enumerate}
        \item \textbf{Understanding Model Predictive Control Concepts}
        \begin{itemize}
            \item Through fundamental principles of MPC, a control strategy using an optimization approach.
            \item MPC utilizes a model of the system to predict future behavior and optimize control inputs.
        \end{itemize}
        
        \item \textbf{Application of MPC in Real-World Scenarios}
        \begin{itemize}
            \item Example: Autonomous vehicles using MPC for real-time speed and steering decisions.
            \item Important to understand MPC's implementation across industries like automotive and robotics.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulating the Optimization Problem in MPC}
    % Learning how to set up the optimization problem central to MPC
    \begin{itemize}
        \item \textbf{Formulating the Optimization Problem}
        \begin{itemize}
            \item Set up the cost function:
            \begin{equation}
                J = \sum_{k=0}^{N} \left( x_k^T Q x_k + u_k^T R u_k \right)  
            \end{equation}
            where:
            \begin{itemize}
                \item \( J \): cost function
                \item \( x_k \): state vector
                \item \( u_k \): control input
                \item \( Q, R \): weighting matrices
            \end{itemize}
            \item Importance of cost function design for performance and stability.
        \end{itemize}
        
        \item \textbf{Implementing Constraints in MPC}
        \begin{itemize}
            \item Understand incorporating constraints on states and inputs to ensure safe operation (e.g., max speed, actuator limits).
        \end{itemize}

        \item \textbf{Performance Assessment of MPC}
        \begin{itemize}
            \item Analyze performance through simulations, comparing MPC against other strategies such as PID control.
            \item Evaluate tracking error and stability for a deep understanding of MPC advantages.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simulation in MPC Environments}
    % Engage in simulated exercises related to MPC concepts
    \begin{itemize}
        \item \textbf{Simulation in MPC Environments}
        \begin{itemize}
            \item Engage in simulated exercises applying MPC concepts to control a dynamic system (e.g., robotic arm, temperature control system).
            \item Practical simulations reinforce theoretical concepts and enhance problem-solving skills.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A - Overview of Model Predictive Control (MPC)}
    \begin{block}{What is MPC?}
        \begin{itemize}
            \item Model Predictive Control (MPC) is an advanced control strategy.
            \item Utilizes a dynamic model to predict future states and optimize control actions at each time step.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Features}
        \begin{itemize}
            \item \textbf{Optimization}: Solves an optimization problem at each interval.
            \item \textbf{Prediction}: Predicts future behavior using a mathematical model.
            \item \textbf{Constraints Handling}: Incorporates constraints directly into the control problem.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A - Key Concepts Recap}
    \begin{itemize}
        \item \textbf{Cost Function}:
        \begin{equation}
            J = \sum_{t=0}^{N} (x_t^T Q x_t + u_t^T R u_t)
        \end{equation}
        \begin{itemize}
            \item \(J\): Total cost to be minimized
            \item \(x_t\): State at time \(t\)
            \item \(u_t\): Control input at time \(t\)
            \item \(Q, R\): Weighting matrices for state errors and control efforts.
        \end{itemize}

        \item \textbf{Prediction Horizon}: Denoted as \(N\), the future time period for predictions.
        
        \item \textbf{Control Horizon}: The period for computing control actions, typically shorter than the prediction horizon.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion and Q\&A - Example Discussion Questions}
    \begin{enumerate}
        \item \textbf{MPC Optimization Process}: 
        How does the optimization algorithm ensure real-time performance?
        % (Including algorithms like Sequential Quadratic Programming or Gradient Descent)
        
        \item \textbf{Applications of MPC}: 
        What are some effective real-world applications of MPC?
        % (Process control, autonomous navigation, energy management)
        
        \item \textbf{Handling Constraints}: 
        How does MPC manage constraints on states and inputs?
        % (Through constrained optimization techniques)
    \end{enumerate}

    \begin{block}{Benefits of MPC}
        \begin{itemize}
            \item \textbf{Flexibility}: Adapts to various system dynamics and complex systems.
            \item \textbf{Performance}: Provides better performance than traditional control strategies.
            \item \textbf{Robustness}: Effective for systems with changing dynamics or poorly understood parameters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resources and Further Reading - Part 1}
    \textbf{Model Predictive Control (MPC) \& Reinforcement Learning (RL)}

    \begin{block}{Recommended Textbooks}
        \begin{enumerate}
            \item \textbf{"Model Predictive Control: Theory and Design"} by James B. Rawlings and David Q. Mayne 
            \begin{itemize}
                \item Comprehensive introduction to MPC, covering theory and implementation.
                \item Great for understanding mathematical foundations and practical algorithms.
            \end{itemize}
            
            \item \textbf{"Reinforcement Learning: An Introduction"} by Richard S. Sutton and Andrew G. Barto 
            \begin{itemize}
                \item Fundamental text covering core concepts, algorithms, and applications in RL.
                \item Insights on integrating RL with MPC.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resources and Further Reading - Part 2}
    
    \begin{block}{Important Articles}
        \begin{enumerate}
            \item \textbf{"Model Predictive Control: A Survey"} by E. F. Camacho and C. Bordons (2004)
            \begin{itemize}
                \item Review of various MPC applications, advantages, challenges, and implementations.
            \end{itemize}

            \item \textbf{"Reinforcement Learning for Control: A Survey"} by Milan O. Schor et al. (2021)
            \begin{itemize}
                \item Discusses the intersection of RL and control systems, enhancing control strategies.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Resources and Further Reading - Part 3}
    
    \begin{block}{Online Resources}
        \begin{itemize}
            \item \textbf{MPC Toolbox for MATLAB}: An extensive toolbox for implementing and testing various MPC strategies. 
            \textit{[Visit:} \texttt{https://www.mathworks.com/products/mpc.html}\textit{]} 

            \item \textbf{OpenAI Gym}: A platform for developing and comparing reinforcement learning algorithms via simulated environments. 
            \textit{[Visit:} \texttt{https://gym.openai.com/}\textit{]} 
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Interconnection of MPC and RL can yield improved adaptive control strategies.
            \item Real-world applications include autonomous vehicles, robotics, and process control.
            \item Importance of simulation—utilize platforms like OpenAI Gym for hands-on experience.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Model Predictive Control in Reinforcement Learning}
    \begin{block}{Key Takeaways}
        \begin{enumerate}
            \item \textbf{Understanding MPC:} 
                \begin{itemize}
                    \item Advanced control strategy that uses a model to predict future behavior and optimize control actions. 
                    \item Evaluates a cost function continuously considering constraints and dynamics.
                \end{itemize}
            \item \textbf{Applications of MPC:} 
                \begin{itemize}
                    \item Used in robotics, automotive, finance, and process control.
                    \item Example: Self-driving cars use MPC for safe and efficient path planning.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Integration of MPC and RL}
    \begin{block}{MPC and Reinforcement Learning (RL)}
        \begin{itemize}
            \item RL agents learn optimal policies through environment interactions.
            \item Combining MPC with RL enhances learning efficiency:
                \begin{itemize}
                    \item MPC provides structured planning and guaranteed performance.
                    \item RL contributes adaptability through experience learning.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Benefits of MPC in RL}
        \begin{itemize}
            \item Balances exploration and exploitation with better long-term performance.
            \item Explicitly manages constraints, such as safety limits.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Further Exploration}
    \begin{block}{Encouragement for Further Exploration}
        \begin{itemize}
            \item Delve deeper into the merging of MPC and RL:
                \begin{itemize}
                    \item Explore how MPC improves sample efficiency in RL algorithms.
                    \item Investigate applications utilizing both MPC and RL for real-world challenges.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Key Formulas to Remember}
        \begin{equation}
            J = \sum_{t=0}^{N} \left( x_t^T Q x_t + u_t^T R u_t \right)
        \end{equation}
        where:
        \begin{itemize}
            \item \( x_t \) = state vector
            \item \( u_t \) = control input
            \item \( Q, R \) = weight matrices.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}