\frametitle{The Bellman Equation}
    \begin{block}{State Value Function}
        The value of a state is the maximum expected return obtainable from that state:
        \begin{equation}
            V(s) = \max_{a \in A} \left( R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V(s') \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $V(s)$: Value of state $s$
            \item $R(s, a)$: Immediate reward for taking action $a$ in state $s$
            \item $\gamma$: Discount factor (0 â‰¤ $\gamma$ < 1)
            \item $P(s'|s, a)$: Transition probability to state $s'$ from state $s$ taking action $a$
        \end{itemize}
    \end{block}

    \begin{block}{Action Value Function}
        The value of taking action $a$ in state $s$:
        \begin{equation}
            Q(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) \max_{a' \in A} Q(s', a')
        \end{equation}
    \end{block}
