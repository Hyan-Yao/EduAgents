\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 7: Policy Gradient Methods]{Week 7: Policy Gradient Methods}
\subtitle{An Overview of Key Concepts and Applications}
\author[J. Doe]{Jane Doe, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Policy Gradient Methods}
    \begin{block}{Overview}
        Policy Gradient Methods are algorithms in Reinforcement Learning (RL) that optimize the policy directly.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Policy Gradient Methods?}
    \begin{itemize}
        \item Focus on optimizing the policy directly, unlike value-based methods.
        \item Adjust policy based on the gradients of expected rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Policy}: Defines the agent's behavior; can be deterministic or stochastic.
        \begin{itemize}
            \item Deterministic Policy: Maps states to actions (\( a = \pi(s) \)).
            \item Stochastic Policy: Probability distribution over actions (\( \pi(a|s) \)).
        \end{itemize}
        
        \item \textbf{Objective}: Maximize expected return:
        \begin{equation}
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
        \end{equation}
        
        \item \textbf{Gradient Ascent}: Updates policy parameters using:
        \begin{equation}
            \theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)
        \end{equation}
        where \( \alpha \) is the learning rate.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example}
    Consider a robot navigating a maze:
    \begin{itemize}
        \item The robot uses a policy \( \pi(a|s) \) to decide movements (left, right, up, down).
        \item It collects rewards (e.g., +10 for reaching the goal) and updates its policy parameters based on the gradient of the total rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Direct Policy Optimization}: Optimizes the policy directly.
        \item \textbf{Flexibility}: Effective in high-dimensional action spaces.
        \item \textbf{Sample Efficiency}: Useful in continuous action domains where value-based methods struggle.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Policy gradient methods focus on optimizing the policy directly. They:
    \begin{itemize}
        \item Utilize gradient ascent techniques for policy parameter adjustments.
        \item Provide flexibility and effectiveness in diverse environments.
        \item Maintain an intuitive approach to learning by directly optimizing decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reinforcement Learning Basics - Overview}
    \begin{block}{Understanding Key Concepts in Reinforcement Learning}
        \begin{itemize}
            \item \textbf{Agent}: An autonomous entity that makes decisions based on observations from the environment.
            \item \textbf{Environment}: Represents everything the agent interacts with and provides feedback.
            \item \textbf{State}: A distinct situation of the agent containing relevant information.
            \item \textbf{Action}: A decision made by the agent that affects the environment.
            \item \textbf{Reward}: A scalar feedback signal given to the agent after taking an action.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Detail}
    \begin{enumerate}
        \item \textbf{Agent}
            \begin{itemize}
                \item Definition: An *agent* is an autonomous entity that makes decisions based on observations from the environment.
                \item Example: In a video game, the player or character controlled by the player acts as the agent, making choices that impact gameplay.
            \end{itemize}
        
        \item \textbf{Environment}
            \begin{itemize}
                \item Definition: The *environment* represents everything an agent interacts with and provides feedback based on the actions taken.
                \item Example: In the same video game, the level, obstacles, and other characters serve as the environment for the agent.
            \end{itemize}
        
        \item \textbf{States}
            \begin{itemize}
                \item Definition: A *state* is a distinct situation in which the agent can find itself, encapsulating relevant information from the environment.
                \item Example: In autonomous driving, states might include the position of the car, speed, and traffic light color.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions and Rewards}
    \begin{enumerate}
        \setcounter{enumi}{3} % To continue the numbering from the previous frame
        \item \textbf{Actions}
            \begin{itemize}
                \item Definition: An *action* is a decision made by the agent affecting the environment and leading to a new state.
                \item Example: In an autonomous vehicle, actions can include accelerating, braking, and turning.
            \end{itemize}
        
        \item \textbf{Rewards}
            \begin{itemize}
                \item Definition: A *reward* is a scalar feedback signal received after an action is taken, with the agent aiming to maximize cumulative rewards.
                \item Example: In a board game, landing on certain spaces earns points as rewards.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Interaction Loop}: RL revolves around a loop where the agent observes a state, takes action, receives reward, and transitions to a new state.
            \item \textbf{Objective}: The goal is to learn a policy that maximizes the sum of rewards over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Policy Gradient Methods?}
    \begin{block}{Definition}
        Policy Gradient Methods are a class of algorithms in Reinforcement Learning that optimize the policy directly by estimating the gradient of expected rewards with respect to policy parameters.
    \end{block}
    \begin{block}{Significance}
        \begin{itemize}
            \item Direct Optimization of the policy function.
            \item Stochastic Policies for exploration.
            \item Effectiveness in Continuous Action Spaces.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Policy Gradient Methods}
    \begin{enumerate}
        \item \textbf{Policy } (\(\pi\)): A mapping from states to actions (\(\pi: S \rightarrow A\)).
        \item \textbf{Objective Function}: The goal is to maximize the expected return:
        \begin{equation}
            J(\theta) = E\left[\sum_{t=0}^{T} \gamma^t R_t\right]
        \end{equation}
        where \(R_t\) is the reward at time \(t\) and \(\gamma\) is a discount factor.
        \item \textbf{Gradient Ascent}: The update rule for policy parameters:
        \begin{equation}
            \theta \leftarrow \theta + \alpha \nabla J(\theta)
        \end{equation}
        where \(\alpha\) is the learning rate.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Conclusion}
    \begin{block}{Example}
        Consider a robot navigating a maze:
        \begin{itemize}
            \item The policy \(\pi\) outputs probabilities for moving left, right, up, or down.
            \item Policy gradient iteratively updates \(\pi\) based on rewards obtained from actions, steering the robot towards optimal paths.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Policy Gradient Methods are crucial for advancing Reinforcement Learning strategies, providing robust representations that enhance learning efficacy in diverse environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Direct Policy Optimization}
    \begin{block}{Overview}
        Direct Policy Optimization improves the agent's policy directly without estimating value functions, contrasting with traditional reinforcement learning methods.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Direct Policy Optimization?}
    \begin{itemize}
        \item Improves the agent's policy directly.
        \item Avoids estimating or relying on value functions.
        \item Contrasts with traditional methods that guide learning through value approximation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Direct Policy Optimization?}
    \begin{itemize}
        \item \textbf{Improved Exploration}:
            \begin{itemize}
                \item Prevents local optima and improves action space exploration.
            \end{itemize}
        \item \textbf{Stochastic Policies}:
            \begin{itemize}
                \item Represents stochastic behavior, sampling actions based on probabilities.
                \item Results in more natural behavior in uncertain environments.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Policy $\pi(a|s)$}:
            \begin{itemize}
                \item Probability distribution over actions given a state.
                \item Samples actions instead of selecting them deterministically.
            \end{itemize}
        \item \textbf{Objective Function}:
            \begin{equation}
                J(\theta) = E_{\tau \sim \pi(\theta)} \left[ R(\tau) \right]
            \end{equation}
            where \( \tau \) is a trajectory and \( R(\tau) \) is the total return.
        \item \textbf{Gradient Ascent}:
            \begin{equation}
                \nabla J(\theta) = E_{\tau \sim \pi(\theta)} \left[ \nabla \log \pi(a|s; \theta) R(\tau) \right]
            \end{equation}
            Represents the direction to adjust parameters to increase expected return.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Grid World}
    \begin{block}{Consider a simple grid world:}
        An agent navigates a 5x5 grid to reach a target while avoiding obstacles. 

        \begin{itemize}
            \item The policy outputs probabilities for moving in each direction based on the current state.
            \item If the agent has a high probability of moving towards an obstacle, adjustments are made to reduce that probability, steering the policy towards more favorable actions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Direct Policy Optimization acts independently of value functions.
        \item Stochastic policies enhance exploration and adaptability.
        \item The primary goal is to maximize expected return using techniques like gradient ascent.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    In direct policy optimization, we prioritize enhancing the policy via probabilistic actions without relying on value function approximations. This approach promotes greater flexibility and potential success in more complex environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Policy Gradient Methods - Introduction}
    \begin{block}{Policy Gradient Methods}
        Policy gradient methods are a class of reinforcement learning algorithms that optimize a policy directly, rather than estimating value functions. These methods are particularly effective in environments with high-dimensional action spaces.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Policy Gradient Methods - Action Probabilities}
    \begin{enumerate}
        \item \textbf{Action Probabilities}:
            \begin{itemize}
                \item \textbf{Definition}: The probability of taking an action given a state, denoted as \( \pi_\theta(a|s) \), where \( \theta \) represents the policy parameters.
                \item \textbf{Example}: In chess, probabilities can indicate the likelihood of various moves for a specific board state.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Policy Gradient Methods - Rewards and Return Calculation}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Rewards}:
            \begin{itemize}
                \item \textbf{Definition}: Instant feedback from the environment after an action, represented as \( r_t \). Rewards can be positive or negative.
                \item \textbf{Example}: In a maze, moving closer to the exit yields a positive reward; hitting a wall incurs a negative reward.
            \end{itemize}
        
        \item \textbf{Return Calculation}:
            \begin{itemize}
                \item \textbf{Definition}: The return \( G_t \) at time \( t \) represents the total expected reward from time \( t \) onward, defined as:
                \begin{equation}
                    G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots = \sum_{k=0}^{\infty} \gamma^k r_{t+k}
                \end{equation}
                \item \textbf{Discount Factor \( \gamma \)}: A value between 0 and 1 that prioritizes immediate rewards over future ones.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summarization}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Directly optimizing policies allows for greater flexibility in continuous action spaces.
            \item Probability distributions enable exploration of various actions rather than always choosing the highest reward option.
            \item Balance between exploration and exploitation is essential for effective application of policy gradients.
        \end{itemize}
    \end{block}
    
    \begin{block}{Summary Formula}
        The expected return for a policy can be expressed as:
        \begin{equation}
            J(\theta) = \mathbb{E}_{\pi_\theta} [G_t]
        \end{equation}
        This indicates our goal to maximize the expected return \( J(\theta) \) by adjusting the parameters \( \theta \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        Policy gradient methods are crucial for developing adaptive agents in complex environments. A solid understanding of action probabilities, rewards, and return calculations is foundational to effective policy learning.
    \end{block}

    \begin{block}{Next Steps}
        Prepare to delve into \textbf{Generalized Advantage Estimation (GAE)}, a technique designed to reduce variance in policy gradient estimates and enhance training efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Generalized Advantage Estimation (GAE)}
    \begin{block}{Overview}
        Generalized Advantage Estimation (GAE) is a technique used in policy gradient methods to balance bias and variance in reinforcement learning, enhancing learning efficiency and stability of policy updates.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantage Function}
    \begin{block}{Definition}
        The advantage function quantifies how much better taking a specific action is compared to the average action from a given state:
        \begin{equation}
        A_t = Q_t - V(s_t)
        \end{equation}
    \end{block}
    \begin{itemize}
        \item \(A_t\) = Advantage at time step \(t\)
        \item \(Q_t\) = Action-value function
        \item \(V(s_t)\) = State-value function
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Estimation}
    \begin{itemize}
        \item High Variance: Directly estimating \(A_t\) leads to instability due to high variance.
        \item Bias: Bootstrapped values can introduce bias and not capture true values fully.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Generalized Advantage Estimation (GAE)}
    \begin{block}{GAE's Solution}
        By introducing a parameter \(\lambda\), GAE trades-off bias and variance.
    \end{block}
    \begin{block}{GAE Formula}
        The GAE is defined recursively:
        \begin{equation}
        \hat{A_t} = \delta_t + (\gamma \lambda) \delta_{t+1} + (\gamma \lambda^2) \delta_{t+2} + \dots
        \end{equation}
    \end{block}
    \begin{itemize}
        \item \(\delta_t\) = Temporal Difference (TD) error at timestep \(t\)
        \item \(\gamma\) = Discount factor
        \item \(\lambda\) = Smoothing parameter (range [0,1])
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Temporal Difference Error}
    The TD error is calculated as:
    \begin{equation}
    \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Benefits of GAE}
    \begin{itemize}
        \item Reduced Variance: Smooths fluctuations leading to stable gradients.
        \item Controlled Bias: The \(\lambda\) parameter allows for tuning bias.
        \item Improved Sample Efficiency: Faster convergence and improved policy updates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{GAE Effect - Illustration}
    \begin{block}{Example}
        Imagine estimating the benefits of a new action in a game with high variance; GAE helps maintain a smoother trajectory of learning, revealing the true value of actions through averages instead of immediate fluctuations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{itemize}
        \item GAE balances bias and variance in policy gradients effectively.
        \item The \(\lambda\) parameter offers flexibility to practitioners.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE Algorithm - Overview}
    \begin{block}{Overview}
        The REINFORCE algorithm is a fundamental technique used in policy gradient methods for reinforcement learning. It optimizes the policy directly by estimating gradients based on actions taken and rewards received.
    \end{block}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Policy:} A mapping from states to actions, represented as a probability distribution, denoted as $\pi_\theta(a|s)$.
            \item \textbf{Rewards:} Scalar feedback after taking an action, aiming to maximize expected returns.
            \item \textbf{Return:} Total discounted rewards at time step $t$: 
            \[
            G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
            \]
            where $0 \leq \gamma < 1$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE Algorithm - Update Process}
    \begin{block}{Step-by-Step Update Process}
        \begin{enumerate}
            \item \textbf{Generate Episode:} Collect states, actions, and rewards until the terminal state is reached.
            \item \textbf{Calculate Return:} Compute the return $G_t$ for each time step $t$.
            \item \textbf{Policy Gradient Estimation:} Update policy parameters $\theta$ using:
            \[
            \nabla J(\theta) = \mathop{\mathbb{E}}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla \log \pi_\theta(a_t | s_t) G_t \right]
            \]
            \item \textbf{Policy Update:} Update parameters using learning rate $\alpha$:
            \[
            \theta_{new} = \theta_{old} + \alpha \nabla J(\theta)
            \]
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{REINFORCE Algorithm - Example Walkthrough}
    \begin{block}{Example Walkthrough}
        Consider a grid world where an agent moves to a goal:
        \begin{itemize}
            \item \textbf{States:} $S_1, S_2, S_3$
            \item \textbf{Actions:} {Up, Down, Left, Right}
            \item \textbf{Episode:} Agent moves from $S_1$ to $S_2$, receiving rewards: $r_0 = -1, r_1 = 1$.
        \end{itemize}
    \end{block}

    \begin{block}{Compute Returns}
        \begin{itemize}
            \item At $t = 0$: $G_0 = -1 + \gamma \cdot 1 = -1 + \gamma$
            \item At $t = 1$: $G_1 = 1$
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Uses \textbf{Monte Carlo methods} for estimating policy gradients.
            \item Can have high variance; techniques like Generalized Advantage Estimation (GAE) help.
            \item Simple to implement but may require many episodes to converge.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continuous vs. Discrete Actions}
    \begin{block}{Understanding Action Spaces}
        \begin{itemize}
            \item \textbf{Action Space}: The set of all possible actions an agent can take.
            \item \textbf{Discrete Actions}: Finite set of actions (e.g., move left, right, up, down).
            \item \textbf{Continuous Actions}: Infinite set of actions, often real-valued vectors (e.g., steering angle).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods Overview}
    \begin{block}{Policy Gradient Methods}
        \begin{itemize}
            \item Techniques that optimize the policy directly by using gradients.
            \item Updates are based on actions taken and rewards received.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discrete Action Policy Gradient Methods}
    \begin{block}{Example and Mechanism}
        \begin{itemize}
            \item \textbf{Example}: REINFORCE algorithm.
            \item \textbf{Mechanism}:
            \begin{itemize}
                \item Treats action probabilities as outputs from a softmax function.
                \item Updates policy weights based on likelihood of selected actions:
                \begin{equation}
                \theta_{new} = \theta_{old} + \alpha \cdot \nabla J(\theta)
                \end{equation}
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continuous Action Policy Gradient Methods}
    \begin{block}{Examples and Mechanism}
        \begin{itemize}
            \item \textbf{Examples}: DDPG, PPO.
            \item \textbf{Mechanism}:
            \begin{itemize}
                \item Neural networks represent the policy, outputting continuous action values.
                \item Actions sampled from Gaussian distribution:
                \begin{equation}
                a_t \sim \mathcal{N}(\mu(s_t|\theta), \sigma^2)
                \end{equation}
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences}
    \begin{itemize}
        \item \textbf{Complexity}:
        \begin{itemize}
            \item Discrete methods utilize simple probability distributions.
            \item Continuous methods require advanced optimization techniques.
        \end{itemize}
        \item \textbf{Stability}:
        \begin{itemize}
            \item Discrete methods find stable policies easily but may struggle with exploration.
            \item Continuous methods need careful parameter tuning for stability.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Points}
    \begin{itemize}
        \item The choice between methods depends on the nature of the problem and complexity of action space.
        \item Continuous actions require more complex representation and optimization.
        \item Understanding the distinctions is crucial for effective policy gradient method implementation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Policy Gradient Methods - Introduction}
    \begin{block}{Introduction to Policy Gradient Methods}
        Policy Gradient Methods are a type of reinforcement learning algorithm that directly optimize the policy function by adjusting the agent's actions based on the gradient of expected rewards. Unlike value-based methods, which learn a value function, policy gradient approaches learn the policy itself.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Policy Gradient Methods - Key Advantages}
    \begin{itemize}
        \item \textbf{Convergence Properties}
        \begin{itemize}
            \item Designed to seek a local optimum of the expected return.
            \item Convergence is ensured with proper exploration strategies and learning rates.
            \item \textit{Example}: A simple grid world where an agent learns to reach a goal using a stochastic policy.
        \end{itemize}

        \item \textbf{Performance in High-Dimensional Spaces}
        \begin{itemize}
            \item Effective in high-dimensional action spaces where discrete methods may struggle.
            \item \textit{Illustration}: In robotics, complex tasks that involve multiple joints effectively managed by policy gradients.
        \end{itemize}

        \item \textbf{Flexibility in Policy Representation}
        \begin{itemize}
            \item Easily use function approximators (e.g., neural networks) for policy representation.
        \end{itemize}
        
        \item \textbf{Unbiased Gradient Estimates}
        \begin{itemize}
            \item Provides unbiased estimates of expected reward using Monte Carlo methods for policy evaluation.
        \end{itemize}

        \item \textbf{Simplicity of Implementation}
        \begin{itemize}
            \item The principle of using gradients to adjust policy parameters is straightforward.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Policy Gradient Methods - Conclusion}
    \begin{block}{Conclusion}
        Policy gradient methods provide robust tools for dealing with complex environments characterized by high-dimensional action spaces and intricate dynamics. They offer clear advantages in convergence behavior, flexibility, and implementation simplicity, making them a valuable technique in modern reinforcement learning.
    \end{block}

    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Effective for continuous and high-dimensional action spaces.
            \item Promote convergence towards local optima.
            \item Simplified implementation through direct optimization.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Policy Gradient Methods - Reference Formula}
    \begin{block}{Policy Gradient Theorem}
        \begin{equation}
            \nabla J(\theta) = \mathbb{E}_\pi \left[ \nabla \log \pi_\theta(a|s) Q^\pi(s, a) \right]
        \end{equation}
        This formula illustrates how the policy gradient is computed, showing the expected reward for action \(a\) given state \(s\) and parameterized by \(\theta\).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with Policy Gradient Methods}
    \begin{block}{Understanding Policy Gradient Methods}
        Policy Gradient methods are reinforcement learning algorithms that optimize the policy directly. While effective for high-dimensional action spaces, they present notable challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Challenges}
    \begin{enumerate}
        \item \textbf{High Variance}
            \begin{itemize}
                \item \textbf{Explanation:} High variance due to sampling for gradient estimates leads to instability in learning.
                \item \textbf{Illustration:} Averaging scores from a few basketball games can give a misleading average.
                \item \textbf{Example:} Risky actions yielding large rewards might skew the agent’s learning direction.
            \end{itemize}
        \item \textbf{Sample Inefficiency}
            \begin{itemize}
                \item \textbf{Explanation:} Requires many episodes for reliable policy updates; many actions yield little useful information.
                \item \textbf{Illustration:} Learning a language by practicing one sentence a day is inefficient.
                \item \textbf{Example:} An agent can require thousands of episodes to refine its policy, which is impractical in costly simulations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Address Challenges}
    \begin{block}{Formulas and Techniques}
        \begin{itemize}
            \item \textbf{Variance Reduction Techniques:}
                \begin{itemize}
                    \item Using baselines to mitigate high variance:
                        \[
                        \text{Advantage} = Q(s, a) - V(s)
                        \]
                    This focuses on the value of action \( a \) compared to the expected value \( V(s) \).
                \end{itemize}
            \item \textbf{Use of REINFORCE Algorithm:}
                \begin{lstlisting}[language=Python]
                # Pseudo-code for REINFORCE with baseline:
                for each episode:
                    Compute reward-to-go
                    for each time step:
                        Update policy using:
                        ∇J(θ) ≈ E[∇ log π(a|s; θ) * (R - baseline)]
                \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Introduction}
    \begin{block}{Introduction to Actor-Critic Methods}
        Actor-Critic methods blend the strengths of both value-based and policy-based reinforcement learning methods. 
        This hybrid approach helps mitigate limitations faced by each method in isolation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Key Concepts}
    \begin{enumerate}
        \item \textbf{Actor}:
            \begin{itemize}
                \item Responsible for selecting actions based on the current state.
                \item Can utilize either deterministic or stochastic policies.
            \end{itemize}
        \item \textbf{Critic}:
            \begin{itemize}
                \item Evaluates the actions taken by the actor by estimating value functions.
                \item Typically works with state-value function \( V(s) \) or advantage function \( A(s, a) \).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - How They Work}
    \begin{block}{Collaboration}
        The actor generates actions based on the policy, while the critic provides feedback on the performance of those actions. This feedback helps adjust the policy gradually, leading to an efficient and stable learning process.
    \end{block}

    \begin{block}{Advantages Over Standalone Methods}
        \begin{itemize}
            \item \textbf{Reduced Variance}: Critic's estimates allow for lower variance in policy updates compared to traditional methods.
            \item \textbf{Learning Efficiency}: The critic’s feedback provides a stable learning signal, enhancing sample efficiency.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: A Simple Game Scenario}
    Imagine an agent learning to play a simple grid-based game:
    \begin{itemize}
        \item \textbf{Actor}: Chooses actions like "move up," "move down" based on the grid state.
        \item \textbf{Critic}: Evaluates chosen actions and provides feedback using a scoring system based on rewards (positive or negative).
    \end{itemize}
    In case the actor moves into a wall (resulting in a negative reward), the critic adjusts the action probabilities to guide the actor towards better strategies.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Key Points and Formulas}
    \begin{itemize}
        \item \textbf{Combining Strengths}: Actors optimize policies directly while critics focus on value estimation.
        \item \textbf{Learning Process}: Continuous improvement through actor updates based on critic feedback.
        \item \textbf{Flexibility}: Adaptable to various environments and tasks.
    \end{itemize}

    \begin{block}{Policy Update Rule}
        The policy is updated based on advantage estimates:
        \begin{equation}
            \Delta \theta \propto \nabla \log \pi_\theta(a|s) A(s, a)
        \end{equation}
        where \( A(s, a) = Q(s, a) - V(s) \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Actor-Critic methods represent a robust hybrid approach in reinforcement learning, merging the benefits of both policy and value-based learning. This allows for more efficient learning algorithms with reduced variability in action evaluation.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Theorem - Overview}
    
    \begin{block}{Understanding the Policy Gradient Theorem}
        The Policy Gradient Theorem is fundamental in reinforcement learning, allowing for direct optimization of policy-based methods based on environmental feedback.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Policy}: A function $\pi(a|s;\theta)$ that maps states $s$ to actions $a$, parameterized by $\theta$.
        \item \textbf{Objective}: Maximize expected return:
        \[
        J(\theta) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]
        \]
        \item \textbf{Gradient Ascent}: Compute gradient of objective with respect to parameters $\theta$.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Theorem - Derivation}
    
    \begin{block}{Derivation Using the Reinforce Algorithm}
        1. **Gradient of the Objective**:
        \[
        \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \nabla \log \pi(a_t | s_t; \theta) \right]
        \]
        
        2. **Key Interpretation**: Improve policy parameters $\theta$ towards actions $a_t$ with higher returns $R(\tau)$.
        
        3. **Variance Reduction**: Introduce a baseline $b(s)$:
        \[
        \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi} \left[ (R(\tau) - b(s)) \nabla \log \pi(a_t | s_t; \theta) \right]
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Theorem - Practical Application}

    \begin{block}{Practical Steps}
        \begin{enumerate}
            \item \textbf{Collect trajectories}: Simulate the environment to generate trajectories.
            \item \textbf{Compute returns}: Calculate total return $R(\tau)$ for each trajectory.
            \item \textbf{Update policy}: Adjust parameters $\theta$ using computed gradient $\nabla J(\theta)$.
        \end{enumerate}
    \end{block}

    \begin{block}{Example Scenario}
        An agent plays a simple game, observing the state and choosing actions based on the current policy, updating its policy to favor actions that yield higher rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Theorem - Summary and Key Points}
    
    \begin{itemize}
        \item The Policy Gradient Theorem is essential for enhancing policy-based reinforcement learning strategies.
        \item Understanding the gradient's role in optimizing policy parameters is crucial for effective implementations.
        \item Incorporating baselines can lead to more robust training, affecting convergence and performance.
    \end{itemize}
    
    \begin{block}{Conclusion}
        The Policy Gradient Theorem allows for the direct optimization of policies in reinforcement learning, enabling powerful techniques including modern actor-critic methods.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementation of Policy Gradient Methods}
    Implementation of Policy Gradient Methods involves several practical considerations that significantly influence performance.
\end{frame}

\begin{frame}
    \frametitle{Introduction to Policy Gradient Methods}
    \begin{itemize}
        \item Policy Gradient Methods optimize policies directly in reinforcement learning.
        \item Requires consideration of:
        \begin{itemize}
            \item Choice of Policy Representation
            \item Gradient Estimation Techniques
            \item Experience Collection
            \item Training Stability and Convergence
            \item Computational Considerations
            \item Hyperparameter Tuning
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Considerations: Policy Representation}
    \begin{itemize}
        \item Policies represented with function approximators:
        \begin{itemize}
            \item \textbf{Neural Networks}: For high-dimensional spaces. 
            \item \textbf{Linear Models}: Quick convergence in simpler scenarios.
        \end{itemize}
        \item \textbf{Example:} In a cart-pole task, a neural network learns the mapping between state and action.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Considerations: Gradient Estimation Techniques}
    \begin{itemize}
        \item Policy gradient theorem can introduce high variance.
        \begin{itemize}
            \item \textbf{REINFORCE Algorithm}: Uses complete returns; straightforward but high variance.
            \item \textbf{Baseline Reductions}:
            \begin{equation}
            \nabla J(\theta) = \mathbb{E} \left[ \nabla \log \pi_\theta(a|s) \left( R - b(s) \right) \right]
            \end{equation}
            where \( b(s) \) is a baseline function.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Considerations: Experience Collection}
    \begin{itemize}
        \item Efficient experience collection is crucial.
        \begin{itemize}
            \item \textbf{On-policy vs. Off-policy}:
            \begin{itemize}
                \item \textbf{On-policy}: Learns from data generated by the current policy.
                \item \textbf{Off-policy}: Learns from datasets by other policies.
            \end{itemize}
            \item \textbf{Example:} Data from each episode in on-policy vs. replay buffer in off-policy.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Considerations: Training Stability and Convergence}
    \begin{itemize}
        \item Policy gradient methods can be unstable.
        \begin{itemize}
            \item Use \textbf{Actor-Critic architectures} for stability and reduced variance.
            \item \textbf{Entropy Regularization} promotes exploration:
            \begin{equation}
            J'(\theta) = J(\theta) - \beta H(\pi_\theta)
            \end{equation}
            where \( H(\pi_\theta) \) is the policy entropy.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Considerations: Computational Considerations}
    \begin{itemize}
        \item Significant computational overhead in training:
        \begin{itemize}
            \item \textbf{Batch Size}: Affects noise in gradient estimation.
            \item \textbf{Parallelization}: Enhances sampling efficiency.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Considerations: Hyperparameter Tuning}
    \begin{itemize}
        \item Important parameters include:
        \begin{itemize}
            \item Learning Rate
            \item Discount Factor
            \item Number of Episodes
        \end{itemize}
        \item Incorrect tuning can lead to poor convergence or divergence.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item Effective implementation requires careful consideration of:
        \begin{itemize}
            \item Policy representation
            \item Gradient estimation techniques
            \item Computational resources
        \end{itemize}
        \item Understanding these factors ensures robust and efficient reinforcement learning training.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Code Snippet Illustrations}
    \begin{block}{Loss Function with Entropy Regularization}
    \begin{equation}
    L(\theta) = -\mathbb{E} [\log \pi_\theta(a|s) \cdot G] - \beta H(\pi_\theta)
    \end{equation}
    \end{block}
    \begin{block}{Basic Python Pseudocode}
    \begin{lstlisting}
    for episode in range(num_episodes):
        state = env.reset()
        while not done:
            action = policy(state)
            next_state, reward, done = env.step(action)
            store_transition(state, action, reward)
            state = next_state
        update_policy()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Overview}
    Hyperparameters are crucial settings that influence the performance of policy gradient methods. Proper tuning can significantly enhance learning efficiency and effectiveness. The following key hyperparameters are often adjusted:
    
    \begin{enumerate}
        \item Learning Rate ($\alpha$)
        \item Discount Factor ($\gamma$)
        \item Batch Size
        \item Entropy Coefficient ($\beta$)
        \item Number of Epochs
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Key Hyperparameters}
    \begin{block}{Learning Rate ($\alpha$)}
        \begin{itemize}
            \item \textbf{Description}: Rate of parameter updates.
            \item \textbf{Impact}:
            \begin{itemize}
                \item High $\alpha$: Instability and potential divergence.
                \item Low $\alpha$: Stability with slower convergence.
            \end{itemize}
            \item \textbf{Example}: $\alpha = 0.01$ (simpler tasks) vs $\alpha = 0.001$ (complex tasks).
        \end{itemize}
    \end{block}
    
    \begin{block}{Discount Factor ($\gamma$)}
        \begin{itemize}
            \item \textbf{Description}: Importance of future rewards.
            \item \textbf{Impact}: Close to 1 encourages long-term rewards; close to 0 favors immediate rewards.
            \item \textbf{Example}: In survival scenarios, $\gamma = 0.99$ promotes future state consideration.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Additional Hyperparameters}
    \begin{block}{Batch Size}
        \begin{itemize}
            \item \textbf{Description}: Number of samples per update step.
            \item \textbf{Impact}:
            \begin{itemize}
                \item Large: Better gradient estimates and stability.
                \item Small: More frequent updates.
            \end{itemize}
            \item \textbf{Example}: Batch size of 64 vs 1024 for learning speed/stability.
        \end{itemize}
    \end{block}
    
    \begin{block}{Entropy Coefficient ($\beta$)}
        \begin{itemize}
            \item \textbf{Description}: Controls exploration.
            \item \textbf{Impact}: Higher $\beta$ leads to more exploration, avoiding premature convergence.
            \item \textbf{Example}: $\beta = 0.01$ is more exploratory than $\beta = 0.001$.
        \end{itemize}
    \end{block}
    
    \begin{block}{Number of Epochs}
        \begin{itemize}
            \item \textbf{Description}: Iterations over the dataset.
            \item \textbf{Impact}: Too few = underfitting, too many = overfitting.
            \item \textbf{Example}: 10 epochs for stable dynamics vs adaptive tuning for dynamic environments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Conclusion}
    \begin{itemize}
        \item \textbf{Importance of Tuning}: Fine-tuning can differentiate between poor and effective agents.
        \item \textbf{Interdependency}: Changes in one hyperparameter can impact others; adjust accordingly.
        \item \textbf{Trial and Error}: Best hyperparameters often found through experimental tuning.
    \end{itemize}
    
    \vspace{0.5cm}
    
    Hyperparameter tuning is vital for effective application of policy gradient methods, promoting better agent learning and exploration. Remember to document tuning experiments for future reference.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Policy Gradient Methods}
    \begin{block}{Introduction to Policy Gradient Methods}
        Policy Gradient Methods optimize the policy directly rather than learning value functions.\newline
        Their ability to handle high-dimensional action spaces makes them suitable for complex tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}
    \begin{enumerate}
        \item \textbf{Robotics}
            \begin{itemize}
                \item \textit{Task: Robot Manipulation}
                \item \textit{Example}: Training robotic arms for tasks like picking and assembling.
                \item \textit{How it Works}: Maps sensor inputs to motor commands.
                
                \item \textit{Task: Autonomous Navigation}
                \item \textit{Example}: Self-driving cars navigating urban environments.
                \item \textit{How it Works}: Predictions based on real-time sensor data.
            \end{itemize}
            
        \item \textbf{Game Playing}
            \begin{itemize}
                \item \textit{Task: Video Game AI}
                \item \textit{Example}: OpenAI’s Dota 2, Google DeepMind’s AlphaGo.
                \item \textit{How it Works}: Learning from game outcomes to adjust strategies.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continued Applications and Key Points}
    \begin{enumerate}
        \setcounter{enumi}{2} % Start enumeration from 3
        \item \textbf{Finance}
            \begin{itemize}
                \item \textit{Task: Portfolio Management}
                \item \textit{Example}: Optimizing investment strategies.
                \item \textit{How it Works}: Creating adaptive strategies for asset allocation.
            \end{itemize}

        \item \textbf{Healthcare}
            \begin{itemize}
                \item \textit{Task: Personalized Treatment Plans}
                \item \textit{Example}: Designing optimal pathways for chronic diseases.
                \item \textit{How it Works}: Using sequential decision-making for interventions.
            \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points}
        - Direct policy optimization handles complexity effectively.\newline
        - Continuous learning is effective in dynamic environments.\newline
        - Scalability across various fields from robotics to finance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and References}
    \begin{block}{Conclusion}
        Policy Gradient Methods represent a significant advancement in reinforcement learning.\newline
        They are effective across diverse fields, highlighting their versatility in solving complex tasks.
    \end{block}
    
    \begin{block}{References}
        1. Sutton, R. S., \& Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.\newline
        2. OpenAI. (2020). ``Dota 2 AI'' � A comprehensive overview of AI advancements in gaming.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis - Overview}
    In this section, we will examine the distinct characteristics of Policy Gradient Methods (PGMs) in relation to other popular reinforcement learning (RL) approaches such as Value-Based Methods and Actor-Critic Methods. This comparative analysis will help highlight the strengths and weaknesses of PGMs and provide insights into their practical applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Reinforcement Learning Approaches}
    \begin{enumerate}
        \item \textbf{Value-Based Methods}
            \begin{itemize}
                \item \textbf{Description}: Optimize a value function to derive the best action (e.g., Q-learning, DQN).
                \item \textbf{Strengths}: Efficient in well-defined environments; faster convergence in discrete spaces.
                \item \textbf{Weaknesses}: Struggles with continuous actions; requires extensive exploration leading to convergence issues.
                \item \textbf{Example}: In Chess, Q-learning evaluates board positions based on expected future rewards.
            \end{itemize}
            
        \item \textbf{Policy-Based Methods (PGMs)}
            \begin{itemize}
                \item \textbf{Description}: Directly parameterize and optimize policy function, i.e., probability of actions given a state.
                \item \textbf{Strengths}: Handles high-dimensional and continuous spaces; learns stochastic policies.
                \item \textbf{Weaknesses}: High variance in updates; requires careful tuning of learning rates and exploration strategies.
                \item \textbf{Example}: In a robotic arm task, a PGM learns the probability distribution over actions to adjust grip based on conditions.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Reinforcement Learning Approaches (Continued)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Actor-Critic Methods}
            \begin{itemize}
                \item \textbf{Description}: Combine benefits of value-based and policy-based approaches; 'actor' optimizes policy while 'critic' estimates the value function.
                \item \textbf{Strengths}: Stability from value function guidance; utilizes both techniques for effective exploration.
                \item \textbf{Weaknesses}: Complex implementation with two networks; may still face bias-variance tradeoff.
                \item \textbf{Example}: In game-playing scenarios, the Actor-Critic model promotes actions leading to winning strategies while evaluating these based on expected outcomes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Efficiency}: Value-based methods excel in discrete spaces; PGMs are strong in continuous and complex environments.
        \item \textbf{Exploration vs. Exploitation}: Policy gradients encourage exploration of the action space, crucial in uncertain environments.
        \item \textbf{Variance}: PGMs may suffer from high variance; strategies like baselines or Trust Region Policy Optimization (TRPO) can help stabilize training.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Estimate}
    \begin{equation}
        \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla \log \pi_\theta(a_t | s_t) \cdot R_t \right]
    \end{equation}
    Where:
    \begin{itemize}
        \item \( J(\theta) \): Objective function for policy optimization.
        \item \( \pi_\theta \): Policy parameterized by \( \theta \).
        \item \( R_t \): Reward obtained after taking action \( a_t \) in state \( s_t \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications - Introduction}
    \begin{block}{Overview}
        Policy Gradient Methods (PGMs) are powerful tools in reinforcement learning, enabling agents to learn optimal behaviors through direct optimization of policy.
    \end{block}
    However, the growing application of PGMs raises important ethical considerations that warrant discussion.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications - Key Considerations}
    \begin{enumerate}
        \item \textbf{Bias in Learning}
            \begin{itemize}
                \item \textbf{Explanation:} PGMs can inadvertently perpetuate or amplify existing biases in training data.
                \item \textbf{Example:} An AI trained in hiring may favor certain demographics due to implicit biases in historical data.
                \item \textbf{Key Point:} Continuous monitoring and improvement of training data is crucial.
            \end{itemize}

        \item \textbf{Transparency and Interpretability}
            \begin{itemize}
                \item \textbf{Explanation:} The decision-making processes of PGMs can often be opaque, complicating accountability.
                \item \textbf{Example:} An autonomous vehicle's navigation decisions may be unclear, complicating understanding of incidents.
                \item \textbf{Key Point:} Developing models that prioritize interpretability can improve trust and acceptance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        \item \textbf{Responsibility and Accountability}
            \begin{itemize}
                \item \textbf{Explanation:} As AI systems become more autonomous, determining liability for failures becomes complex.
                \item \textbf{Example:} If a PGM-based AI misdiagnoses a condition in healthcare, who is accountable?
                \item \textbf{Key Point:} Establishing clear guidelines and regulations for accountability is essential.
            \end{itemize}

        \item \textbf{Impact on Employment}
            \begin{itemize}
                \item \textbf{Explanation:} The deployment of PGMs can lead to job displacement.
                \item \textbf{Example:} Automation can replace customer service roles with chatbots.
                \item \textbf{Key Point:} Consider retraining and reskilling initiatives to support affected workers.
            \end{itemize}

        \item \textbf{Safety and Security Concerns}
            \begin{itemize}
                \item \textbf{Explanation:} PGMs can be exploited, leading to harmful consequences.
                \item \textbf{Example:} An AI for content moderation might suppress freedom of expression.
                \item \textbf{Key Point:} Establishing ethical boundaries and security measures is vital to prevent misuse.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications - Conclusion}
    Understanding the ethical implications of policy gradient methods is essential for fostering responsible AI development. Addressing these concerns ensures that advancements in AI benefit society while minimizing harm.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Research Trends and Developments}
    Overview of current trends and future directions in the field of policy gradient methods.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Current Trends in Policy Gradient Methods}
    
    \begin{block}{1. Increasing Popularity of Deep Reinforcement Learning (DRL)}
        \begin{itemize}
            \item \textbf{Concept:} Policy gradient methods are gaining traction within DRL, allowing agents to learn optimal policies directly.
            \item \textbf{Example:} Algorithms like Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO) showcase how policy gradients have revolutionized complex decision-making tasks in environments such as robotics and video games.
        \end{itemize}
    \end{block}

    \begin{block}{2. Exploration Strategies}
        \begin{itemize}
            \item \textbf{Concept:} Effective exploration remains a key challenge; current research focuses on improving strategies to enhance learning.
            \item \textbf{Example:} Techniques such as curiosity-driven exploration and adding noise to action distributions allow agents to gather more diverse experiences, thus improving performance.
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Sample Efficiency Improvements}
        \begin{itemize}
            \item \textbf{Concept:} Traditional policy gradient methods can be sample inefficient, recent studies aim to enhance this.
            \item \textbf{Example:} Using replay buffers and off-policy learning, as in the Soft Actor-Critic (SAC) algorithm, significantly boosts learning by reusing past experiences more effectively.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Policy Gradient Methods}
    
    \begin{block}{1. Integration with Other Learning Paradigms}
        \begin{itemize}
            \item \textbf{Concept:} Growing interest in integrating policy gradients with techniques from supervised and unsupervised learning.
            \item \textbf{Expected Results:} This hybrid approach could lead to more robust methods benefiting from both real-time learning and pre-trained models, especially in environments with limited data.
        \end{itemize}
    \end{block}

    \begin{block}{2. Addressing Ethical Considerations}
        \begin{itemize}
            \item \textbf{Concept:} Ethical implications of AI necessitate frameworks in future research to address these concerns.
            \item \textbf{Example:} Developing transparent models and ensuring fairness to prevent biased outcomes in decision-making processes.
        \end{itemize}
    \end{block}
    
    \begin{block}{3. Advancements in Architecture}
        \begin{itemize}
            \item \textbf{Concept:} Exploration of novel neural network architectures to enhance scalability and computational efficiency.
            \item \textbf{Illustration:} Utilizing recurrent neural networks (RNNs) and attention mechanisms can help agents retain long-term dependencies in sequential decision-making tasks.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item Policy gradient methods are at the forefront of modern AI and robotics, driving significant research interest.
        \item Continuous innovation is essential to overcome existing challenges like sample efficiency and ethical deployments.
        \item Future research will likely center around multi-disciplinary approaches that harness the strengths of various learning paradigms for enhanced performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Call to Action and Concluding Thought}
    
    \begin{block}{Further Reading}
        Stay up-to-date with current journals and conferences focused on reinforcement learning to explore the latest advancements and applications of policy gradient methods.
    \end{block}

    \begin{block}{Discussion Prompts for Next Session}
        What potential ethical dilemmas could arise from the application of advanced policy gradient methods in real-world scenarios? How might we mitigate these issues?
    \end{block}

    \begin{block}{Concluding Thought}
        The trajectory of policy gradient methods is pivotal in shaping the landscape of artificial intelligence, and ongoing exploration will be crucial for responsible and impactful implementation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Troubleshooting Common Issues in Policy Gradient Methods}
    \begin{block}{Introduction}
        Policy Gradient methods are powerful tools for reinforcement learning, but they can present unique challenges during implementation. Understanding these common issues can significantly improve your results.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues and Troubleshooting Tips - Part 1}
    \begin{enumerate}
        \item \textbf{High Variance in Returns}
          \begin{itemize}
            \item \textbf{Issue:} Policy gradient estimates can exhibit high variance, influencing the stability of learning.
            \item \textbf{Solution:}
              \begin{itemize}
                  \item \textbf{Baseline Subtraction:} Subtract a baseline (e.g., average reward) from the returns to reduce variance without bias.
                  \item \textbf{Generalized Advantage Estimation (GAE):} Blends n-step returns for an informative advantage estimate.
              \end{itemize}
            \item \textbf{Example:} If rewards in an episode are [1, 2, 3], using an average reward baseline of 2 gives modified returns of [-1, 0, 1].
          \end{itemize}
          
        \item \textbf{Slow Convergence}
          \begin{itemize}
            \item \textbf{Issue:} Policy gradient algorithms may converge slowly, requiring extensive training.
            \item \textbf{Solution:}
              \begin{itemize}
                  \item \textbf{Learning Rate Tuning:} Adjust learning rate; too high overshoots optimal policies, too low leads to slow updates.
                  \item \textbf{Adaptive Learning Rates:} Use optimizers like Adam to adjust learning rates dynamically.
              \end{itemize}
          \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues and Troubleshooting Tips - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue enumeration from previous frame
        \item \textbf{Exploration vs. Exploitation Dilemma}
          \begin{itemize}
            \item \textbf{Issue:} The algorithm may converge to suboptimal policies if it explores too little or too much.
            \item \textbf{Solution:}
              \begin{itemize}
                  \item \textbf{Entropy Regularization:} Add an entropy term to encourage exploration and maintain policy diversity.
                  \item \textbf{Exponential Decay:} Gradually reduce exploration rates as training progresses.
              \end{itemize}
            \item \textbf{Illustration:} Policy evolves from a diverse random policy (high entropy) to focusing on the best policies (low entropy).
          \end{itemize}

        \item \textbf{Policy Degradation}
          \begin{itemize}
            \item \textbf{Issue:} The learned policy may begin performing worse over time due to poor updates or suboptimal actions.
            \item \textbf{Solution:}
              \begin{itemize}
                  \item \textbf{Policy Clipping:} In methods like PPO, apply clipping to keep updates within a trust region.
                  \item \textbf{Multiple Epochs:} Update the policy with more batches to stabilize learning.
              \end{itemize}
          \end{itemize}

        \item \textbf{Resource Intensive Training}
          \begin{itemize}
            \item \textbf{Issue:} Training may require significant computational resources (CPU/GPU/Memory).
            \item \textbf{Solution:}
              \begin{itemize}
                  \item \textbf{Batch Size Adjustment:} Experiment with smaller batch sizes to reduce memory usage and stabilize training.
                  \item \textbf{Distributed Training:} Leverage parallel processing to speed up training and experience collection.
              \end{itemize}
          \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Conclusion}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Understanding and addressing common challenges leads to more efficient training and better-performing policies.
            \item Utilize adaptive methods and regularization techniques to stabilize learning and enhance exploration.
            \item Monitoring the training process and adjusting hyperparameters can prevent common pitfalls.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        By anticipating these issues, you can improve the robustness of your policy gradient implementations. The journey toward a successful reinforcement learning model involves continuous troubleshooting and adaptation!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future of Policy Gradient Methods}
    Speculation on the future developments and improvements in policy gradient approaches.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advancements in Policy Gradient Methods - Part 1}
    As we explore the future advancements in Policy Gradient Methods, it’s important to speculate on potential developments that could lead to more efficient, robust, and flexible reinforcement learning algorithms.
    
    \begin{enumerate}
        \item \textbf{Integration with Other Learning Paradigms}
        \begin{itemize}
            \item Hybrid Approaches: Combining Policy Gradients with value-based methods to enhance stability and performance.
            \item Meta-Learning: Allowing systems to adapt policies across various tasks for faster learning.
        \end{itemize}
        
        \item \textbf{Sample Efficiency Improvements}
        \begin{itemize}
            \item Advanced Sample Selection: Techniques like prioritized experience replay to improve policy update efficiency.
            \item Utilizing Simulated Environments: High-fidelity simulators enhance sample efficiency.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advancements in Policy Gradient Methods - Part 2}
    Continuing with the potential advancements in Policy Gradient Methods:
    
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from previous frame
        \item \textbf{Exploration Strategies}
        \begin{itemize}
            \item Curiosity-Driven Exploration: Using intrinsic motivation for effective state exploration.
            \item Adaptive Exploration Rates: Dynamically adjusting strategies based on learning progress.
        \end{itemize}
        
        \item \textbf{Policy Optimization Techniques}
        \begin{itemize}
            \item Second-Order Methods: Investigating Natural Policy Gradient for better convergence rates.
            \item Adaptive Learning Rates: Tailoring learning rates to policy parameters for efficient training.
        \end{itemize}
        
        \item \textbf{Robustness to Noise and Uncertainty}
        \begin{itemize}
            \item Stochastic Policy Approaches: Developing policies that factor in state uncertainty.
            \item Distributional Approaches: Modeling the full distribution of rewards for better outcome handling.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advancements in Policy Gradient Methods - Part 3}
    Final thoughts on the future of Policy Gradient Methods:
    
    \begin{enumerate}
        \setcounter{enumi}{5} % Continue numbering from previous frame
        \item \textbf{Scalability Across Diverse Domains}
        \begin{itemize}
            \item Generalization Across Tasks: Enhancements for transferable skills across different environments.
            \item Multi-agent Systems: Addressing challenges for multiple agents in collaborative settings.
        \end{itemize}
    \end{enumerate}
    
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Resilience and Adaptability: Future advancements will focus on agents' ability to adapt.
        \item Enhanced Cooperation Among Agents: Improving multi-agent systems will open new avenues.
        \item Broadened Application Areas: Techniques may find applications in robotics, finance, and healthcare.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Policy Gradient Methods - Part 1}
    \begin{block}{Key Concepts Recap}
        \begin{enumerate}
            \item \textbf{Policy Gradient Methods Overview}
                \begin{itemize}
                    \item Optimize the policies directly.
                    \item Contrast with value-based methods that estimate value functions first.
                    \item \textbf{Policy}: A strategy that the agent follows to determine actions based on the current state.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Policy Gradient Methods - Part 2}
    \begin{block}{Types of Policy Gradient Approaches}
        \begin{enumerate}
            \setcounter{enumi}{1}
            \item \textbf{Basic Policy Gradient}
            \item \textbf{REINFORCE Algorithm}
            \item \textbf{Actor-Critic Methods}
                \begin{itemize}
                    \item Combines policy gradients (actor) with value function approximation (critic).
                \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Advantages and Challenges}
        \begin{itemize}
            \item \textbf{Advantages:}
                \begin{itemize}
                    \item Can learn stochastic policies.
                    \item Handles complex reward structures and large action spaces.
                \end{itemize}
            \item \textbf{Challenges:}
                \begin{itemize}
                    \item High variance in policy updates can lead to instability.
                    \item Requires careful tuning of hyperparameters.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Policy Gradient Methods - Part 3}
    \begin{block}{Key Formulas}
        \begin{equation}
            \nabla J(\theta) = \mathbb{E} \left[ \nabla \log \pi_\theta(a|s) Q^\pi(s, a) \right]
        \end{equation}
        where:
        \[
        \begin{array}{ll}
            \theta & \text{is the policy parameters} \\
            \pi_\theta(a|s) & \text{is the probability of taking action } a \text{ in state } s \\
            Q^\pi(s, a) & \text{is the action-value function}
        \end{array}
        \]
    \end{block}

    \begin{block}{Real-World Applications}
        \begin{itemize}
            \item Robotics: Efficient navigation and manipulation.
            \item Game AI: Intelligent agents (e.g., AlphaGo).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Questions and Discussion}
    This slide opens the floor to questions and discussions about policy gradient methods.
\end{frame}

\begin{frame}
    \frametitle{Introduction to Policy Gradient Methods}
    \begin{block}{Overview}
        As we conclude our exploration, we summarize key concepts of policy gradient methods that play a foundational role in reinforcement learning (RL).
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts of Policy Gradient Methods}
    \begin{enumerate}
        \item \textbf{Definition:} Algorithms that optimize the policy directly by following the gradient of expected rewards with respect to policy parameters.
        \item \textbf{Comparison:} Unlike value-function methods, policy gradients handle high-dimensional action spaces and stochastic policies effectively.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Benefits and Challenges}
    \begin{block}{Benefits}
        \begin{itemize}
            \item Effective in optimizing complex policies over continuous action spaces.
            \item Maintain probability distributions for actions, permitting exploration.
        \end{itemize}
    \end{block}

    \begin{block}{Challenges}
        \begin{itemize}
            \item High variance in gradient estimates—techniques like baseline subtraction are often necessary for stabilization.
            \item Sample inefficiency; require more data to learn effectively compared to state-value methods.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Algorithms in Policy Gradients}
    \begin{itemize}
        \item \textbf{REINFORCE:} Uses Monte Carlo returns for gradient estimation.
        \item \textbf{Actor-Critic:} Combines policy gradient (actor) and value function (critic) to reduce variance.
        \item \textbf{Proximal Policy Optimization (PPO):} A popular algorithm emphasizing stable learning during policy updates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet: REINFORCE Algorithm}
    \begin{lstlisting}[language=Python]
import numpy as np

def reinforce(policy, episodes, learning_rate):
    for episode in range(episodes):
        rewards = []
        states = []
        actions = []

        # Generate episode
        state = env.reset()
        done = False
        while not done:
            action = np.random.choice(range(len(policy)), p=policy[state])
            next_state, reward, done = env.step(action)
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            state = next_state

        # Compute return (discounted rewards)
        G = sum(rewards)

        # Update policy
        for t in range(len(actions)):
            policy[states[t]][actions[t]] += learning_rate * (G - baseline) 
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Discussion Questions}
    \begin{itemize}
        \item What are the real-world applications of policy gradient methods?
        \item Can you think of scenarios where policy gradient might outperform value-based methods?
        \item What strategies could be implemented to reduce the variance in policy gradient estimates?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Closing Remarks}
    \begin{block}{Engagement}
        This is your opportunity to clarify concepts, share insights, or express reservations regarding policy gradient methods. 
        Let's shape our understanding and practical applications together.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Overview}
    \begin{block}{Understanding Policy Gradient Methods}
        Policy Gradient Methods are crucial algorithms in Reinforcement Learning (RL), enabling agents to optimize policies directly based on expected rewards. Exploring a range of resources can deepen understanding of their mathematical foundations and practical implementations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Recommended Textbooks}
    \begin{enumerate}
        \item \textbf{"Reinforcement Learning: An Introduction" by Sutton and Barto}
        \begin{itemize}
            \item Covers foundational concepts in RL, including policy gradient methods.
            \item Provides solid theoretical insights on policy optimization.
        \end{itemize}
        
        \item \textbf{"Deep Reinforcement Learning Hands-On" by Maxim Lapan}
        \begin{itemize}
            \item A practical guide offering coding examples and projects on RL algorithms, including policy gradients.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Papers and Online Resources}
    \begin{block}{Key Papers}
        \begin{enumerate}
            \item \textit{"Policy Gradient Methods for Reinforcement Learning with Function Approximation"} by Sutton et al. (2000)
            \item \textit{"Trust Region Policy Optimization"} by Schulman et al. (2015)
            \item \textit{"Proximal Policy Optimization Algorithms"} by Schulman et al. (2017)
        \end{enumerate}
    \end{block}

    \begin{block}{Online Courses and Tutorials}
        \begin{itemize}
            \item Coursera: \textit{"Deep Learning Specialization by Andrew Ng"}
            \item OpenAI Spinning Up in Deep RL
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Repositories and Key Points}
    \begin{block}{Code Repositories}
        \begin{itemize}
            \item OpenAI Baselines: High-quality implementations of RL algorithms.
            \item Stable Baselines3: Reliable implementations in PyTorch for policy gradient methods.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Focus on optimizing policies, not value functions.
            \item Understand the policy gradient theorem for intuition.
            \item Practical coding enhances retention of complex topics.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Formula}
    The policy gradient theorem can be expressed as:
    \begin{equation}
    \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla \log \pi_\theta(a | s) Q^\pi(s, a) \right]
    \end{equation}
    Where:
    \begin{itemize}
        \item $J(\theta)$ is the expected return.
        \item $\tau$ represents trajectories generated by the policy $\pi$.
        \item $Q^\pi(s, a)$ is the action-value function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Engaging with these resources will deepen your understanding of policy gradient methods and their applications in RL. 
    \begin{itemize}
        \item Start with foundational texts, progress to key papers, and implement code examples.
    \end{itemize}
\end{frame}


\end{document}