\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 3: Dynamic Programming]{Week 3: Dynamic Programming}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}
    \frametitle{Introduction to Dynamic Programming}
    \begin{block}{What is Dynamic Programming?}
        Dynamic Programming (DP) is a powerful algorithmic technique used for solving complex problems by breaking them down into simpler subproblems, especially when they exhibit overlapping subproblems and optimal substructure properties.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Dynamic Programming: Key Concepts}
    \begin{itemize}
        \item \textbf{Overlapping Subproblems:} Problems can be divided into smaller, repetitive problems solved individually and reused.
        
        \item \textbf{Optimal Substructure:} An optimal solution can be constructed from optimal solutions of its subproblems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming: Memoization}
    \begin{block}{Memoization}
        Memoization is a top-down approach where solutions to subproblems are cached after computation, preventing redundant calculations:
    \end{block}
    \begin{lstlisting}[language=Python]
def fibonacci(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)
    return memo[n]
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming: Tabulation}
    \begin{block}{Tabulation}
        Tabulation is a bottom-up approach where all subproblems are solved and their results stored in a table, often being more space-efficient:
    \end{block}
    \begin{lstlisting}[language=Python]
def fibonacci(n):
    if n <= 1:
        return n
    fib = [0] * (n + 1)
    fib[0], fib[1] = 0, 1
    for i in range(2, n + 1):
        fib[i] = fib[i-1] + fib[i-2]
    return fib[n]
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Relevance to Reinforcement Learning}
    \begin{itemize}
        \item DP is foundational for estimating value functions and policies in Markov Decision Processes (MDPs).
        
        \item Two classic DP algorithms in RL:
            \begin{enumerate}
                \item \textbf{Value Iteration:} Iteratively updates the value of each state until convergence.
                \item \textbf{Policy Iteration:} Alternates between policy evaluation and policy improvement.
            \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item DP is essential in RL for efficiently solving problems with many states and actions.
        \item It minimizes computations, crucial for real-time decision-making applications (e.g., robotics, game playing).
        \item Understanding DP is a foundation for tackling advanced RL techniques and learning optimal policies systematically.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Overview}
    \begin{block}{Overview of Dynamic Programming}
        Understanding the concept of Dynamic Programming (DP) as a method for efficiently solving complex problems by breaking them down into simpler subproblems.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Key Areas of Focus}
    \begin{enumerate}
        \item \textbf{Principles of Dynamic Programming}
            \begin{itemize}
                \item Grasp the core principles:
                    \begin{itemize}
                        \item \textbf{Optimal Substructure}:
                        A problem exhibits optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems.
                        
                        \item \textbf{Overlapping Subproblems}:
                        A problem is said to have overlapping subproblems if the same subproblems are solved multiple times.
                    \end{itemize}
            \end{itemize}

        \item \textbf{Types of Dynamic Programming}
            \begin{itemize}
                \item Differentiate between:
                    \begin{itemize}
                        \item \textbf{Top-Down (Memoization)}: Solving the problem recursively and storing results of subproblems (caching).
                        \item \textbf{Bottom-Up (Tabulation)}: Solving the problem by solving all possible subproblems first, typically using iteration.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Applications and Approaches}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Applications of Dynamic Programming}
            \begin{itemize}
                \item Explore real-world problems such as:
                    \begin{itemize}
                        \item \textbf{Knapsack Problem}: Choosing items with given weights and values to maximize value without exceeding the weight capacity.
                        \item \textbf{Longest Common Subsequence}: Finding the longest subsequence present in two sequences where the elements are not required to be contiguous.
                    \end{itemize}
            \end{itemize}

        \item \textbf{Recursive vs Iterative Approaches}
            \begin{itemize}
                \item Understand the trade-offs:
                    \begin{itemize}
                        \item Time Complexity: Analyze the efficiency of both methods in terms of time and space.
                        \item Code Snippet Example (Fibonacci):
                            \begin{lstlisting}[language=Python]
# Memoization (Top-Down)
def fib_memo(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)
    return memo[n]

# Tabulation (Bottom-Up)
def fib_tab(n):
    if n <= 1:
        return n
    fib = [0] * (n + 1)
    fib[1] = 1
    for i in range(2, n + 1):
        fib[i] = fib[i - 1] + fib[i - 2]
    return fib[n]
                            \end{lstlisting}
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Recognize that dynamic programming can significantly reduce the time complexity of naive recursive algorithms.
            \item Importance of identifying subproblems and overlapping solutions.
            \item Familiarity with coding dynamic programming solutions will enhance problem-solving skills in algorithms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is Dynamic Programming? - Definition}
  \begin{block}{Definition}
    Dynamic Programming (DP) is a powerful algorithmic technique used to solve optimization problems by breaking them down into simpler subproblems. 
    It is particularly effective for problems that can be divided into overlapping subproblems, where the solution to a larger problem can be constructed efficiently from solutions to smaller subproblems.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is Dynamic Programming? - Role in Optimization}
  \begin{block}{Role in Optimization Problems}
    Dynamic programming plays a critical role in optimization by:
    \begin{enumerate}
      \item \textbf{Reducing Complexity}: Instead of recalculating the results of subproblems multiple times, DP stores these results, allowing for faster computation.
      \item \textbf{Optimal Solution}: DP ensures that the solution to the overall problem is built using optimal solutions to its subproblems. It guarantees that local optimums lead to a global optimum.
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is Dynamic Programming? - Key Characteristics}
  \begin{block}{Key Characteristics}
    \begin{itemize}
      \item \textbf{Overlapping Subproblems}: 
      Many problems can be broken into subproblems that recur multiple times. 
      For example, the Fibonacci sequence can be expressed as:
      \begin{equation}
        F(n) = F(n-1) + F(n-2)
      \end{equation}
      Here, $F(n-1)$ and $F(n-2)$ are calculated multiple times unless stored.
      
      \item \textbf{Optimal Substructure}: 
      A problem exhibits optimal substructure if an optimal solution can be constructed efficiently from optimal solutions to its subproblems.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is Dynamic Programming? - Examples}
  \begin{block}{Common Examples}
    \begin{itemize}
      \item \textbf{Fibonacci Sequence}: 
      \begin{itemize}
        \item Without DP: O($2^n$) complexity using a naive recursive approach.
        \item With DP (Memoization): O($n$) complexity by storing computed values.
      \end{itemize}
      \item \textbf{Knapsack Problem}: 
      Given a set of items, each with a weight and value, determine the most valuable combination that fits within a given weight limit.
      \item \textbf{Shortest Path Problems}: 
      Such as finding the shortest path in a weighted graph using the Bellman-Ford or Floyd-Warshall algorithms.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is Dynamic Programming? - Code Snippet}
  \begin{block}{Illustration: Fibonacci Sequence}
    Consider a simple recursive solution to compute Fibonacci numbers:
    \begin{lstlisting}[language=Python]
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
    \end{lstlisting}
    
    Optimized Using DP: 
    \begin{lstlisting}[language=Python]
def fibonacci(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)
    return memo[n]
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is Dynamic Programming? - Key Points}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item DP is essential for efficiently solving complex optimization problems.
      \item Recognizing overlapping subproblems is crucial in identifying when to apply DP.
      \item By leveraging the properties of optimal substructure, problems are solvable more efficiently compared to straightforward recursive methods.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Characteristics of Dynamic Programming - Introduction}
    \begin{itemize}
        \item Dynamic Programming (DP) is a technique for solving optimization problems.
        \item It breaks problems into simpler subproblems to find optimal solutions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Characteristics of Dynamic Programming - Key Concepts}
    \begin{enumerate}
        \item \textbf{Overlapping Subproblems}
            \begin{itemize}
                \item Definition: Problems with smaller, reusable components that recur.
                \item DP saves results in a table to avoid redundant calculations.
            \end{itemize}
        \item \textbf{Optimal Substructure}
            \begin{itemize}
                \item Definition: Optimal solutions of subproblems can construct the optimal solution of the overall problem.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overlapping Subproblems - Example}
    \begin{block}{Fibonacci Sequence}
        \begin{equation}
            F(n) = F(n-1) + F(n-2)
        \end{equation}
        Computing Fibonacci(5) involves recursive calls to Fibonacci(4) and Fibonacci(3), leading to redundant calculations without memoization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimal Substructure - Example}
    \begin{block}{0/1 Knapsack Problem}
        \begin{equation}
            \text{Maximize Value} = V(i) + \text{MaxValue}(W - W(i), i-1)
        \end{equation}
        The optimal solution for a larger knapsack can be derived from the optimal solutions of smaller configurations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Storage Efficiency:} Reduces time complexity from exponential to polynomial.
        \item \textbf{Applicability:} Common in computer science, economics, and bioinformatics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding overlapping subproblems and optimal substructure is crucial for applying dynamic programming to complex problems, offering significant improvements over naive methods.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet - Fibonacci with Memoization}
    \begin{lstlisting}[language=Python]
def fib(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fib(n-1, memo) + fib(n-2, memo)
    return memo[n]

# Example Usage
print(fib(10)) # Output: 55
    \end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Recursive vs Iterative Approach}
  \begin{block}{Overview}
    Dynamic programming (DP) can be implemented using two primary techniques: \textbf{recursion} and \textbf{iteration}. This slide compares these two approaches, highlighting their characteristics, advantages, and disadvantages.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Recursive Approach}
  \begin{block}{Explanation}
    The \textbf{recursive approach} involves breaking a problem down into subproblems by defining the solution in terms of smaller instances of itself. It utilizes the call stack to keep track of computations, often leading to elegantly concise code.
  \end{block}
    
  \begin{block}{Example: Fibonacci Sequence}
    \begin{lstlisting}[language=Python]
def fibonacci_recursive(n):
    if n <= 1:
        return n
    return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)
    \end{lstlisting}
    \textbf{Drawback}: This naive recursive function recalculates the same values multiple times, leading to exponential time complexity \( O(2^n) \).
  \end{block}

  \begin{itemize}
    \item \textbf{Pros}:
      \begin{itemize}
        \item Simple and easy to implement.
        \item Straightforward representation of the problem.
      \end{itemize}
    \item \textbf{Cons}:
      \begin{itemize}
        \item Inefficient due to repeated calculations.
        \item Deep recursion can lead to a stack overflow.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Iterative Approach}
  \begin{block}{Explanation}
    The \textbf{iterative approach} involves using loops (e.g., for or while) to compute the result progressively. It emphasizes state management without recursion.
  \end{block}

  \begin{block}{Example: Fibonacci Sequence (Iterative)}
    \begin{lstlisting}[language=Python]
def fibonacci_iterative(n):
    if n <= 1:
        return n
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b
    \end{lstlisting}
    \textbf{Advantage}: This method has a linear time complexity \( O(n) \) and uses constant space \( O(1) \).
  \end{block}

  \begin{itemize}
    \item \textbf{Pros}:
      \begin{itemize}
        \item More efficient in terms of time and space.
        \item Avoids the risk of stack overflow.
      \end{itemize}
    \item \textbf{Cons}:
      \begin{itemize}
        \item Code can be less intuitive for problems that are naturally recursive.
        \item Requires manual management of state variables.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Comparison Summary}
  \begin{tabular}{|l|l|l|}
    \hline
    Aspect & Recursive Approach & Iterative Approach \\
    \hline
    \textbf{Clarity} & High & Moderate \\
    \hline
    \textbf{Efficiency} & Generally less efficient & More efficient \\
    \hline
    \textbf{Memory Usage} & High (due to call stack) & Low (constant space) \\
    \hline
    \textbf{Implementation} & Elegant but may lead to stack overflow & Typically longer but avoids overflow \\
    \hline
  \end{tabular}
  
  \begin{block}{Conclusion}
    Choosing between recursive and iterative approaches depends on the problem context and constraints. While recursion can provide conceptual clarity, iteration typically offers better performance and memory efficiency. Familiarity with both methods will enhance your problem-solving flexibility in dynamic programming.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space Representation - Overview}
    \begin{block}{Understanding State Space Representation}
        \textbf{Definition:}  
        In dynamic programming, a state represents a specific configuration of the problem at a given point in time. The state space is the complete set of possible states that can be reached from the initial state, encapsulating all potential outcomes of the problem.
    \end{block}
    \begin{block}{Significance}
        \begin{itemize}
            \item \textbf{Compactness:} Simplifies complex problems by breaking them down into manageable parts.
            \item \textbf{Guidance for Solution Construction:} Helps in developing recursive formulas or relations to compute solutions.
            \item \textbf{Avoid Redundant Calculations:} Memorizing computed states minimizes repeated work (overlapping subproblems).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space Representation - Approaches}
    \begin{block}{Representing States}
        States can be represented in various forms, depending on the nature of the problem:
        \begin{itemize}
            \item \textbf{Integer Indices:} Useful for problems described with numbers.
            \begin{itemize}
                \item \textit{Example:} For Fibonacci, the state can be represented as $F(n)$ where $n$ is an index.
            \end{itemize}
            \item \textbf{Tuples:} For problems with multiple dimensions.
            \begin{itemize}
                \item \textit{Example:} In a grid pathfinding problem, a state might be defined as $(x, y)$ where $x$ and $y$ are grid coordinates.
            \end{itemize}
            \item \textbf{Bitmasks:} Used in problems dealing with subsets, tracking inclusion/exclusion of elements efficiently.
            \begin{itemize}
                \item \textit{Example:} For items $\{A, B, C\}$, a bitmask $011$ indicates inclusion of $B$ and $C$.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space Representation - Example}
    \begin{block}{Example: Knapsack Problem}
        \textbf{Problem Overview:}
        Given a set of items, each with a weight and a value, maximize the value of items placed into a knapsack without exceeding the weight capacity.
        
        \textbf{State Representation:}  
        A state can be defined as $dp[i][w]$, where:
        \begin{itemize}
            \item $i$ represents the number of items considered.
            \item $w$ represents the current weight capacity of the knapsack.
        \end{itemize}
        
        \textbf{Dynamic Relation:}
        \begin{itemize}
            \item If the $i^{th}$ item is included:  
            \begin{equation}
                dp[i][w] = dp[i-1][w - weight[i]] + value[i]
            \end{equation}
            \item If the item is excluded:  
            \begin{equation}
                dp[i][w] = dp[i-1][w]
            \end{equation}
        \end{itemize}
        
        \textbf{Base Case:}  
        $dp[0][w] = 0$ for all $w$ (0 items means 0 value).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space Representation - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Every dynamic programming solution involves defining a state space.
            \item Choosing a clear and concise representation of states is critical for efficiency.
            \item Understanding the relationship between different states through recursion or iteration is essential for building effective solutions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{State Space Representation - Conclusion}
    \begin{block}{Conclusion}
        Effective state space representation is foundational in dynamic programming. It provides the framework to simplify problems into subproblems, guiding the computation for optimal solutions through systematic exploration of states. Understanding how to represent and manipulate these states is key to leveraging dynamic programming techniques successfully.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Bellman Equations - Introduction}
    \begin{block}{What are Bellman Equations?}
        Bellman equations are fundamental recursive relationships that form the backbone of dynamic programming. They express the relationship between the value of a particular state and the values of subsequent states, accounting for the possible actions taken from that state.
    \end{block}
    \begin{block}{Why are Bellman Equations Important?}
        \begin{itemize}
            \item \textbf{Optimal Decision Making:} They allow us to compute the optimal policy by determining the best action to take in each state based on future rewards.
            \item \textbf{Dynamic Programming Framework:} They serve as the foundational framework for solving complex problems involving sequential decision making, such as Markov Decision Processes (MDPs).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Bellman Equation}
    \begin{block}{State Value Function}
        The value of a state is the maximum expected return obtainable from that state:
        \begin{equation}
            V(s) = \max_{a \in A} \left( R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V(s') \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $V(s)$: Value of state $s$
            \item $R(s, a)$: Immediate reward for taking action $a$ in state $s$
            \item $\gamma$: Discount factor (0 ≤ $\gamma$ < 1)
            \item $P(s'|s, a)$: Transition probability to state $s'$ from state $s$ taking action $a$
        \end{itemize}
    \end{block}
    
    \begin{block}{Action Value Function}
        The value of taking action $a$ in state $s$:
        \begin{equation}
            Q(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) \max_{a' \in A} Q(s', a')
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Bellman Equations}
    \begin{block}{Consider a simple MDP:}
        States: $S = \{s_1, s_2, s_3\}$
        
        Actions: $A = \{up, down, left, right\}$
        
        Calculate the rewards and transitions. For example:
        \begin{itemize}
            \item If in $s_1$ and move $down$ to $s_2$, then:
            \begin{equation}
                R(s_1, \text{down}) = 10
            \end{equation}
            and the transition probabilities $P(s_2|s_1,\text{down}) = 1$.
        \end{itemize}
    \end{block}
    
    \begin{block}{Using the Bellman Equations}
        Use the Bellman equations to compute $V(s_1)$ based on the possibilities from $s_1$.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Recursive Nature:} Bellman equations decompose decision-making over time into simpler subproblems.
            \item \textbf{Policy Evaluation:} They are crucial for evaluating the quality of different policies and ensuring optimality.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding Bellman equations is essential for solving dynamic programming problems effectively, as they allow us to build an organized approach to finding optimal solutions in a structured manner.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Method - Introduction}
    \begin{block}{What is Value Iteration?}
        Value Iteration is a fundamental algorithm in reinforcement learning and dynamic programming used to find the optimal policy for a Markov Decision Process (MDP). 
        It is based on the principle of dynamic programming, which decomposes complex problems into simpler subproblems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Markov Decision Process (MDP)}: 
        \begin{itemize}
            \item Defined by states, actions, transition probabilities, and rewards.
            \item Models decision-making situations with stochastic outcomes.
        \end{itemize}
        \item \textbf{Value Function}: 
        \begin{itemize}
            \item Represents the maximum expected return (cumulative reward) from a given state following the optimal policy.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Algorithm}
    \begin{enumerate}
        \item \textbf{Initialize}: Start with an arbitrary value function \( V(s) \) for all states \( s \).
        \[
        V(s) = 0 \quad \forall s \in S
        \]

        \item \textbf{Update Values}: Repeatedly update using the Bellman equation until convergence:
        \[
        V(s) \leftarrow \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') \right)
        \]
        \begin{itemize}
            \item \( R(s, a) \): Immediate reward from state \( s \) under action \( a \)
            \item \( \gamma \): Discount factor (0 ≤ \( \gamma < 1 \))
            \item \( P(s'|s, a) \): Transition probability to state \( s' \) after action \( a \)
        \end{itemize}

        \item \textbf{Convergence}: Continue until \( |V_{\text{new}}(s) - V_{\text{old}}(s)| < \epsilon \).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Value Iteration}
    Consider a simple MDP with 3 states (A, B, C) and two actions (1, 2):
    
    \begin{block}{Rewards:}
        \begin{itemize}
            \item \( R(A, 1) = 5 \), \( R(A, 2) = 0 \)
            \item \( R(B, 1) = 0 \), \( R(B, 2) = 10 \)
            \item \( R(C, 1) = 0 \), \( R(C, 2) = 15 \)
        \end{itemize}
    \end{block}
    
    \begin{block}{Transition Probabilities:}
        \begin{itemize}
            \item From A: 
              \begin{itemize}
                \item Action 1: to B with probability 1
                \item Action 2: to C with probability 1
              \end{itemize}
            \item From B: 
              \begin{itemize}
                \item Action 1: to A with probability 1
                \item Action 2: to C with probability 0.5, remain at B with probability 0.5
              \end{itemize}
            \item From C: 
              \begin{itemize}
                \item Action 1: to B with probability 1
                \item Action 2: to C with probability 1
              \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Convergence}: Value iteration converges to the optimal value function due to its mathematical properties.
        \item \textbf{Computational Complexity}: The algorithm can be computationally intensive, especially with large state spaces.
        \item \textbf{Discount Factor (\( \gamma \))}: Balances immediate vs. future rewards.
    \end{itemize}

    \textbf{Conclusion:} Value Iteration is a powerful technique for solving MDPs, providing a methodical approach to optimizing decision-making in uncertain environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration Method}
    \begin{block}{Overview of Policy Iteration}
        The Policy Iteration Method is a dynamic programming approach used to solve Markov Decision Processes (MDPs). It aims to find the optimal policy that maximizes expected rewards in given states.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps in Policy Iteration}
    \begin{enumerate}
        \item \textbf{Initialize Policy:} Begin with an arbitrary policy ($\pi$).
        
        \item \textbf{Policy Evaluation:}
        Evaluate the current policy by calculating the value function $V(s)$ for all states:
        \begin{equation}
            V(s) = \sum_{s'} P(s'|s, \pi(s)) \left[ R(s, \pi(s), s') + \gamma V(s') \right]
        \end{equation}
        where:
        \begin{itemize}
            \item $P(s'|s, \pi(s))$ is the transition probability.
            \item $R(s, \pi(s), s')$ is the immediate reward.
            \item $\gamma$ is the discount factor.
        \end{itemize}
        
        \item \textbf{Policy Improvement:} Update the policy by maximizing the expected value:
        \begin{equation}
            \pi'(s) = \arg\max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V(s') \right]
        \end{equation}

        \item \textbf{Check for Convergence:} If $\pi' = \pi$, then $\pi$ is optimal; otherwise, return to step 2.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Differences from Value Iteration}
    \begin{itemize}
        \item \textbf{Policy Iteration:}
        \begin{itemize}
            \item Alternates between policy evaluation and improvement.
            \item Generally converges quickly to an optimal policy.
            \item Requires complete policy evaluation each iteration.
        \end{itemize}
        
        \item \textbf{Value Iteration:}
        \begin{itemize}
            \item A single-step process updating value functions.
            \item Focuses on immediate rewards and long-term value.
            \item Can be more computationally intensive.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Key Points}
        Policy Iteration effectively finds an optimal policy and is suited for small state-action spaces, comparing favorably with iterative methods like value iteration.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Dynamic Programming in Reinforcement Learning - Introduction}
  
  \begin{block}{What is Dynamic Programming?}
    Dynamic Programming (DP) is a methodology to solve complex problems by breaking them into simpler subproblems. 
  \end{block}
  
  \begin{block}{Why Use DP in Reinforcement Learning?}
    In the context of **Reinforcement Learning (RL)**, DP aids in making optimal decisions in stochastic environments.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts of DP in RL}
  
  \begin{enumerate}
    \item \textbf{States and Actions:}
      \begin{itemize}
        \item \textbf{State (s)}: Representation of the environment at a given time.
        \item \textbf{Action (a)}: Choice made by the agent that affects the state.
      \end{itemize}
  
    \item \textbf{Policy (π):} A strategy used by the agent to determine actions in states.
    
    \item \textbf{Value Function (V):} Expected return from a state under a particular policy.
    
    \item \textbf{State-Action Value Function (Q):} Expected return of taking an action in a state and following a policy thereafter.
    
    \item \textbf{Bellman Equations:} 
      \begin{itemize}
        \item \textbf{Value Function:}  
        \[
        V(s) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V(s') \right]
        \]

        \item \textbf{Q-Value Function:}  
        \[
        Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s')
        \]
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of DP in RL}
  
  \begin{enumerate}
    \item \textbf{Policy Evaluation:} Iteratively assesses the value of a policy until convergence.
    
    \item \textbf{Policy Improvement:} Updates the policy by acting greedily based on the value function.
    
    \item \textbf{Policy Iteration:} 
      Combines evaluation and improvement in a loop to find an optimal policy, distinguishing it from value iteration.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Gridworld}
  
  Consider a simple Gridworld where:
  
  \begin{itemize}
    \item **States (s)**: Each cell in a 5x5 grid.
    \item **Actions (a)**: Move up, down, left, or right.
    \item **Rewards (R)**: Positive for reaching terminal state, negative for traps.
  \end{itemize}
  
  Using DP, compute optimal policy and value function via Bellman equation iteratively until convergence.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Conclusion}
  
  \begin{itemize}
    \item Dynamic Programming systematically solves RL problems via the principle of optimality.
    \item Bellman equations are central to policy evaluation and improvement.
    \item DP can become computationally expensive in large-scale problems, necessitating approximations.
  \end{itemize}
  
  \begin{block}{Conclusion}
    DP techniques in RL are fundamental for evaluating and improving policies, ensuring optimal decision-making in complex environments.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Dynamic Programming - Introduction}
    \begin{block}{What is Dynamic Programming?}
        Dynamic Programming (DP) is a powerful algorithmic technique used for:
        \begin{itemize}
            \item Breaking complex problems into simpler subproblems.
            \item Solving optimization problems to find the best solution based on specific criteria.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Dynamic Programming - Examples}
    
    \textbf{Real-World Applications of Dynamic Programming}
    
    \begin{enumerate}
        \item \textbf{Optimal Resource Allocation}
        \begin{itemize}
            \item \textbf{Concept}: Allocating limited resources to maximize output or profits.
            \item \textbf{Example}: Companies optimize resource allocation across projects for high ROI.
        \end{itemize}

        \item \textbf{Knapsack Problem}
        \begin{itemize}
            \item \textbf{Concept}: Maximize the value in a knapsack of fixed capacity.
            \item \textbf{Example}: Budget allocations in marketing campaigns.
            \item \textbf{Key Formula}:
            \begin{equation}
            V[i, w] = \max(V[i-1, w], V[i-1, w - w_i] + v_i)
            \end{equation}
        \end{itemize}
        
        \item \textbf{Shortest Path in Graphs}
        \begin{itemize}
            \item \textbf{Concept}: Finding the shortest path from a source node to others.
            \item \textbf{Example}: Algorithms used in GPS for route calculations.
        \end{itemize}
        
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Dynamic Programming - Continued}

    \begin{enumerate}[resume]
        \item \textbf{String Matching and Comparison}
        \begin{itemize}
            \item \textbf{Concept}: Finding edit distances or longest common subsequences.
            \item \textbf{Example}: DNA sequencing for research and treatment.
            \item \textbf{Key Formula for Edit Distance}:
            \begin{equation}
            D(i, j) = 
            \begin{cases} 
            i & \text{if } j=0 \\ 
            j & \text{if } i=0 \\ 
            D(i-1, j-1) & \text{if } x[i] = y[j] \\ 
            1 + \min(D(i-1,j), D(i,j-1), D(i-1,j-1)) & \text{if } x[i] \neq y[j] 
            \end{cases}
            \end{equation}
        \end{itemize}

        \item \textbf{Dynamic Programming in Game Theory}
        \begin{itemize}
            \item \textbf{Concept}: Analyzing optimization problems in games.
            \item \textbf{Example}: Minimax algorithm in chess or tic-tac-toe.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Conclusion}
        Dynamic Programming is prevalent in various fields, enhancing problem-solving efficiency and system optimization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item DP involves breaking down problems into simpler subproblems and building optimal solutions.
        \item Applicable across domains: resource allocation, pathfinding, string matching, and game analysis.
        \item Mastery of DP techniques enhances capabilities in tackling complex optimization issues.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Introduction}
    \begin{block}{Dynamic Programming Challenges}
        Dynamic programming (DP) is a powerful optimization technique used in various computational problems, but its application is not without challenges. Understanding these challenges can help you effectively apply DP methods to solve complex problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Problem Decomposition}
    \begin{itemize}
        \item \textbf{Challenge:} Identifying the right way to break down a problem into subproblems can be difficult. 
        \item \textbf{Example:} In the Fibonacci sequence, incorrect formulation of the recursive relation F(n) = F(n-1) + F(n-2) could lead to incorrect outputs.
        \item \textbf{Key Point:} Ensure a clear understanding of how the problem can be divided into smaller overlapping subproblems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Overlapping Subproblems}
    \begin{itemize}
        \item \textbf{Challenge:} Recognizing overlapping subproblems in complex problems is not always straightforward, potentially leading to significant computational inefficiencies.
        \item \textbf{Example:} The Traveling Salesman Problem (TSP) may require recalculating the same subproblem results multiple times.
        \item \textbf{Key Point:} Efficiently identify and store intermediate results to avoid redundant calculations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Space Complexity}
    \begin{itemize}
        \item \textbf{Challenge:} Storing intermediate results requires substantial memory, leading to space complexity issues for large state spaces.
        \item \textbf{Example:} The traditional DP solution for the Knapsack problem can require O(nW) space.
        \item \textbf{Key Point:} Explore in-place techniques or space-optimized methods to reduce memory usage (e.g., using 1D arrays).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - State Representation}
    \begin{itemize}
        \item \textbf{Challenge:} Poor or unclear state representation complicates the DP transition and result calculations.
        \item \textbf{Example:} In the longest common subsequence problem, improperly defining states can lead to overlooked solutions.
        \item \textbf{Key Point:} Choose state variables that capture all necessary information while being as minimal as possible.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Transition Relations}
    \begin{itemize}
        \item \textbf{Challenge:} Formulating the correct transition relations can be intellectually demanding. Errors can yield incorrect final answers.
        \item \textbf{Example:} In the Edit Distance problem, incorrect operations can lead to inaccurate results.
        \item \textbf{Key Point:} Thoroughly validate transition equations and test with small input examples to ensure correctness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Complexity Analysis}
    \begin{itemize}
        \item \textbf{Challenge:} Understanding time and space complexity for DP solutions is crucial for efficiency, especially for large input sizes.
        \item \textbf{Example:} Some problems may require such high complexity that they become impractical.
        \item \textbf{Key Point:} Always analyze complexity before implementation to confirm feasibility.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Dynamic Programming - Conclusion}
    \begin{block}{Conclusion}
        Dynamic programming is an invaluable tool in solving optimization problems, but being aware of potential challenges can enhance your problem-solving strategies. 
        \newline \textbf{Next Steps:} Explore Approximate Dynamic Programming in the following slide to learn techniques that help mitigate some challenges associated with standard DP methods.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Approximate Dynamic Programming}
    \begin{block}{What is Approximate Dynamic Programming?}
        Approximate Dynamic Programming (ADP) refers to techniques used to solve complex decision-making problems that are too large or unwieldy for classical dynamic programming (DP) methods.
    \end{block}
    \begin{block}{Why is ADP Necessary?}
        \begin{enumerate}
            \item Scalability
            \item Curse of Dimensionality
            \item Real-World Applications
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is ADP Necessary?}
    \begin{itemize}
        \item \textbf{Scalability:} Traditional DP approaches compute values for every possible state and action, which becomes infeasible as the state space grows.
        \item \textbf{Curse of Dimensionality:} Increased dimensions lead to an exponential growth of states, making classical methods impractical.
        \item \textbf{Real-World Applications:} Many applications such as robotics, finance, and traffic management require quick decision-making in high-dimensional spaces.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Approximate Dynamic Programming}
    \begin{itemize}
        \item \textbf{Function Approximation:} Uses approximators (e.g., linear models, neural networks) to estimate state or state-action values.
        \item \textbf{Policy Evaluation:} Evaluates how good a specific policy is without requiring full value function computation.
        \item \textbf{Policy Improvement:} Iterates between evaluation and improvement to derive a near-optimal policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Approximate Dynamic Programming}
    \begin{block}{Robot Navigation}
        \begin{itemize}
            \item \textbf{Traditional DP:} Requires evaluating the value of being in every grid cell, which is computationally expensive.
            \item \textbf{ADP Approach:} Uses function approximation to estimate the expected utility of paths and focuses exploration on promising areas.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item ADP is essential for large-scale problems where traditional DP falls short.
        \item It enhances scalability and efficiency in solving complex decision-making tasks.
        \item Understanding ADP methods is crucial for practitioners in dynamic decision-making fields.
    \end{itemize}
    \begin{block}{Conclusion}
        Approximate Dynamic Programming offers a practical framework by sacrificing some accuracy for computational efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Function Approximation}
    \begin{equation}
        V(s) \approx \theta^T \phi(s)
    \end{equation}
    \begin{itemize}
        \item \( V(s) \) is the approximated value of state \( s \).
        \item \( \theta \) represents the parameters of the approximating function.
        \item \( \phi(s) \) is the feature representation of state \( s \).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Comparing Classical and Approximate Methods}
  
  \begin{block}{1. Introduction to Dynamic Programming}
    Dynamic Programming (DP) helps solve complex problems by breaking them down into simpler subproblems. It encompasses two key approaches: Classical and Approximate Dynamic Programming.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{2. Classical Dynamic Programming}
  
  \begin{block}{Definition}  
    Classical DP relies on finding the exact solution of a problem through systematic enumeration of possible outcomes.
  \end{block}

  \begin{itemize}
    \item \textbf{Characteristics:}
    \begin{itemize}
      \item Deterministic: Produces exact solutions.
      \item Exact Methods: Guarantees to find the optimal solution via exhaustive search and recursion.
      \item State Representation: Typically uses a fixed state space represented with arrays or tables.
    \end{itemize}
    
    \item \textbf{Example:}
    \begin{itemize}
      \item Fibonacci Sequence Computation:
      \begin{equation*}
      F(n) = F(n-1) + F(n-2), \quad \text{with base cases } F(0) = 0, F(1) = 1
      \end{equation*}
    \end{itemize}
    
    \item \textbf{Key Point:} 
    Classical methods are effective when state spaces are manageable and the problem landscape is well-defined.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3. Approximate Dynamic Programming (ADP)}
  
  \begin{block}{Definition}  
    ADP is designed for large and complex state spaces where classical methods become impractical. It finds satisfactory solutions by approximating value functions or policies.
  \end{block}

  \begin{itemize}
    \item \textbf{Characteristics:}
    \begin{itemize}
      \item Stochastic: Handles uncertainty and variability in real-world scenarios.
      \item Computational Efficiency: Focuses on speed and resource usage rather than exact solutions.
      \item Generalization Techniques: Uses techniques like function approximation (e.g., neural networks) to generalize learnings across states.
    \end{itemize}
    
    \item \textbf{Example:}
    \begin{itemize}
      \item Reinforcement Learning in Robotics: A robot learns to navigate a maze using Q-learning, which approximates the action-value function $Q(s, a)$ instead of calculating it for every state-action pair.
    \end{itemize}
    
    \item \textbf{Key Point:}
    Approximate methods excel in large-scale problems and adapt to changing environments, making them suitable for real-world applications.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{4. Comparison Summary}

  \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Feature} & \textbf{Classical Dynamic Programming} & \textbf{Approximate Dynamic Programming} \\
    \hline
    Solution Type & Exact & Approximate \\
    \hline
    Problem Size & Smaller; manageable state space & Larger; extensive state space \\
    \hline
    Speed & Slower, due to exhaustive search & Faster, leveraging approximation \\
    \hline
    Learning Adaptability & Low; static once computed & High; adaptable to changes \\
    \hline
    Complexity Handling & Direct implementation & Requires models and heuristics \\
    \hline
  \end{tabular}
\end{frame}

\begin{frame}[fragile]
  \frametitle{5. Conclusion}
  
  Both classical and approximate methods have their places in the domain of dynamic programming. Selecting the appropriate method depends on problem size, resource constraints, and required solution accuracy. Understanding these differences is crucial for effectively applying dynamic programming techniques across various disciplines.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{itemize}
        \item Dynamic Programming (DP) is vital for solving optimal decision-making problems in Reinforcement Learning (RL).
        \item Multi-agent scenarios introduce complexities due to:
        \begin{itemize}
            \item Agent interactions
            \item Shared environments
            \item Independent decision-making processes
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Multi-Agent Systems:} Groups of agents that interact within an environment, each often having individual goals that affect one another.
        \item \textbf{Multi-Agent Reinforcement Learning (MARL):} Each agent learns from its experiences while considering the actions of others, requiring coordination and competition.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming Adaptation}
    \begin{itemize}
        \item Techniques like Value Iteration and Policy Iteration are extended to multi-agent scenarios through:
        \begin{itemize}
            \item \textbf{Joint State and Action Spaces:} Combining states and actions of multiple agents (e.g. from Agent A and B).
            \item \textbf{Multi-Agent Bellman Equations:}
            \begin{equation}
                Q^{\pi}(s, a_1, a_2) = R(s, a_1, a_2) + \gamma \sum_{s'} P(s'|s, a_1, a_2) V^{\pi}(s')
            \end{equation}
            \end{itemize}
    \end{itemize}
    \begin{block}{Where:}
        \begin{itemize}
            \item \( Q^{\pi} \) = action-value function for states \( s \), actions \( a_1 \), \( a_2 \)
            \item \( R(s, a_1, a_2) \) = expected reward
            \item \( P(s'|s, a_1, a_2) \) = transition probabilities
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Cooperative vs. Competitive Agents}
    \begin{itemize}
        \item \textbf{Cooperative Scenario:} Agents collaborate to achieve a common goal (e.g., transporting an object).
        \item \textbf{Competitive Scenario:} Agents compete against each other (e.g., in games), optimizing their strategies by considering others' actions.
    \end{itemize}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Interactions between agents are crucial for multi-agent dynamic programming.
            \item Proper formulation of state, action spaces, and reward structures is essential.
            \item Complexity of joint state-action spaces can grow exponentially with the number of agents.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications and Summary}
    \begin{itemize}
        \item \textbf{Applications:}
        \begin{itemize}
            \item Robotics (e.g., swarm robotics)
            \item Multi-player games
            \item Economic models
        \end{itemize}
        \item \textbf{Summary:} Multi-Agent Dynamic Programming extends dynamic programming algorithms to environments with multiple agents, requiring unique considerations for decision-making, reward distribution, and interaction dynamics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study: Application in Robotics}
  \begin{block}{Introduction to Dynamic Programming in Robotics}
    Dynamic Programming (DP) is a powerful optimization technique used to solve complex problems by breaking them down into simpler subproblems. It is particularly useful in robotics, where decision-making and path planning are crucial for efficient navigation and task execution.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts of Dynamic Programming}
  \begin{enumerate}
    \item \textbf{Optimal Substructure}: Problems can be broken into smaller, manageable subproblems that can be solved independently. The optimal solution can be constructed from optimal solutions of its subproblems.
    \item \textbf{Overlapping Subproblems}: Many subproblems recur multiple times, which allows storing their solutions for future reference (memoization) to avoid redundant calculations.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study: Robot Path Planning}
  
  \begin{block}{Example Scenario}
    A robot in a 5x5 grid must move from the top-left corner (0,0) to the bottom-right corner (4,4) while avoiding certain cells that contain obstacles.
  \end{block}

  \textbf{Steps in the Dynamic Programming Approach:}
  \begin{enumerate}
    \item \textbf{Define the State}:
      Let \( dp[i][j] \) represent the minimum number of moves required to reach the cell (i, j) from the start (0, 0).
    
    \item \textbf{Recurrence Relation}:
      \begin{equation}
      dp[i][j] = \min(dp[i-1][j], dp[i][j-1]) + 1
      \end{equation}
      Check for obstacles:
      \begin{equation}
      dp[i][j] = \infty \quad \text{if cell (i, j) is an obstacle.}
      \end{equation}
    
    \item \textbf{Base Case}:
      \( dp[0][0] = 0 \): It takes 0 moves to be at the starting point.  
      For any cell that is an obstacle, set \( dp[i][j] = \infty \).
    
    \item \textbf{Iterate through the grid}: For each cell (i, j), apply the recurrence relation to fill out the `dp` table.
    
    \item \textbf{Retrieve the Solution}: The solution will be in \( dp[4][4] \). If it's still \( \infty \), the robot cannot reach the goal.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Efficiency and Applications}
  
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item \textbf{Efficiency}: Dynamic programming reduces time complexity from exponential in naive recursive solutions to polynomial time, making it feasible for real-time robotic applications.
      \item \textbf{Real-World Applications}: DP is not just limited to grid-based navigation; it's used in tasks like resource allocation in robotic teams and planning in dynamic environments.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example Code Snippet}
  
  \begin{lstlisting}[language=Python]
def robot_path(grid):
    n, m = len(grid), len(grid[0])
    dp = [[float('inf')] * m for _ in range(n)]
    dp[0][0] = 0 if grid[0][0] == 0 else float('inf')
    
    for i in range(n):
        for j in range(m):
            if grid[i][j] == 1:  # 1 denotes an obstacle
                dp[i][j] = float('inf')
            else:
                if i > 0:
                    dp[i][j] = min(dp[i][j], dp[i-1][j] + 1)
                if j > 0:
                    dp[i][j] = min(dp[i][j], dp[i][j-1] + 1)

    return dp[n-1][m-1]
  
# Example grid, 0=free space, 1=obstacle
grid = [
    [0, 0, 0, 0, 1],
    [0, 1, 0, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 1, 0, 0, 0],
    [0, 0, 0, 0, 0]
]
print(robot_path(grid))  # Outputs the minimum moves to reach the goal
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  Dynamic programming is a foundational approach in robotics for tasks such as path planning, enabling robots to make intelligent decisions in complex environments. Understanding these principles equips students with the tools necessary to tackle a range of optimization problems in robotics and beyond.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Application in Healthcare}
    \begin{block}{Introduction to Dynamic Programming in Healthcare}
        Dynamic programming (DP) is a powerful optimization technique used to solve complex problems by breaking them into simpler subproblems.
        In healthcare, DP enhances decision-making processes and the allocation of resources, improving patient outcomes and operational efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Application: Treatment Optimization in Chronic Diseases}
    \begin{block}{Case Study Overview}
        The case study focuses on optimizing treatment plans for patients with chronic diseases, such as diabetes.
        The goal is to determine the optimal schedule for medication administration to minimize complications and improve quality of life.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dynamic Programming Process}
    \begin{enumerate}
        \item \textbf{Define the Problem}: Develop individualized treatment plans based on unique health metrics.
        \item \textbf{Establish State Variables}: 
            \[
            S(n, d) \text{ denotes the optimal expected outcome for the first } n \text{ days at dosage } d.
            \]
        \item \textbf{Decision Variables}: Decisions involve changing medication dosages daily.
        \item \textbf{Recurrence Relation}:
            \[
            S(n, d) = \max \left( S(n-1, d), R(n, d) + S(n-1, d-1), R(n, d+1) + S(n-1, d+1) \right)
            \]
        \item \textbf{Base Case}:
            \[
            S(0, d) = \text{Initial Health Status}
            \]
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Consider a patient with diabetes requiring medication adjustments. By applying the DP approach:
    \begin{itemize}
        \item Day 1: Start with a baseline dosage.
        \item Monitor blood glucose levels.
        \item Apply the recurrence to decide the best dosage for the next day.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Efficiency}: Reduces computational complexity from exponential to polynomial time, feasible for real-time decision-making.
        \item \textbf{Personalization}: Customizes treatment plans according to individual patient needs, improving adherence and outcomes.
        \item \textbf{Scalability}: Established DP models can accommodate large datasets from electronic health records.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    This case study illustrates the impact of dynamic programming on healthcare optimization by enhancing decision-making processes related to treatment efficacy.
    As healthcare systems embrace data-driven approaches, the potential for dynamic programming applications continues to grow.
\end{frame}

\begin{frame}[fragile]
    \frametitle{References}
    \begin{itemize}
        \item Bellman, R. (1957). \textit{Dynamic Programming}.
        \item George, K., \& Mukherjee, R. (2015). \textit{Dynamic Programming in Medicine: Applications and Understanding}.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Ethical Considerations in Dynamic Programming}
  
  \begin{block}{Overview}
    Dynamic Programming (DP) is a powerful algorithmic approach used to solve complex problems. While it offers benefits in optimization, understanding its ethical implications is crucial, especially in sensitive sectors like healthcare, finance, and AI.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Ethical Implications}
  
  \begin{enumerate}
    \item \textbf{Data Privacy and Security}
    \begin{itemize}
      \item DP relies on large datasets, which can compromise individual privacy.
      \item \textbf{Example}: In healthcare, patient data used for optimization must be securely handled to protect personal health information (PHI).
    \end{itemize}
    
    \item \textbf{Bias and Fairness}
    \begin{itemize}
      \item Algorithms can reflect historical biases, leading to systemic discrimination.
      \item \textbf{Example}: A DP model in credit scoring might favor specific demographic groups, perpetuating inequality.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{More Ethical Implications}
  
  \begin{enumerate}
    \setcounter{enumi}{2} % Start numbering from 3
    \item \textbf{Transparency and Accountability}
    \begin{itemize}
      \item The complexity of DP solutions can hinder understanding and accountability.
      \item \textbf{Example}: In healthcare, the rationale behind a treatment denial must be clear to stakeholders.
    \end{itemize}

    \item \textbf{Impact on Employment}
    \begin{itemize}
      \item DP may reduce job opportunities in certain sectors while increasing demand in tech fields.
      \item \textbf{Example}: Optimizing logistics routes could decrease the need for human decision-making, raising job displacement concerns.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Considerations for Ethical Implementation}
  
  \begin{itemize}
    \item \textbf{Conduct Impact Assessments}: Analyze how DP applications impact stakeholders.
    \item \textbf{Ensure Data Integrity}: Implement measures for data confidentiality and quality.
    \item \textbf{Promote Algorithmic Transparency}: Adopt standards for clear decision-making processes.
    \item \textbf{Foster Inclusive Data Practices}: Include diverse data to reduce biases and ensure fairness.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  
  While dynamic programming provides substantial problem-solving advantages, careful consideration of its ethical implications is imperative. Addressing these concerns can promote responsible and equitable use of DP across various applications.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Remember}
  
  \begin{itemize}
    \item Ethical applications of DP are crucial across sectors.
    \item Addressing data privacy, bias, transparency, and employment impacts can foster trust and accountability.
    \item Collaboration with stakeholders is essential for creating ethical guidelines for DP implementations.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Dynamic Programming within Reinforcement Learning}
    \begin{block}{Overview}
        Dynamic Programming (DP) has evolved as a cornerstone technique in reinforcement learning (RL). Emerging trends in DP promise to enhance the efficiency and applicability of RL across various domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends and Developments - Part 1}
    \begin{enumerate}
        \item \textbf{Integration with Deep Learning}
            \begin{itemize}
                \item \textbf{Concept}: Combining DP with deep learning to manage high-dimensional state spaces.
                \item \textbf{Example}: Leveraging Deep Q-Networks (DQN) for approximating Q-values.
            \end{itemize}
        
        \item \textbf{Scalability and Parallelization}
            \begin{itemize}
                \item \textbf{Concept}: Enhancements in DP algorithms focusing on scalability via parallel processing.
                \item \textbf{Example}: Using Asynchronous Actor-Critic Architecture (A3C) with multiple agents.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends and Developments - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering
        \item \textbf{Hierarchical Reinforcement Learning (HRL)}
            \begin{itemize}
                \item \textbf{Concept}: Breaking down tasks into subtasks, leveraging DP for policy learning.
                \item \textbf{Example}: Using options for decision-making in robotics.
            \end{itemize}
        
        \item \textbf{Improved Policy Evaluation Techniques}
            \begin{itemize}
                \item \textbf{Concept}: Advancements in policy evaluation without a complete model.
                \item \textbf{Example}: Using Monte Carlo methods and Temporal Difference learning.
                \item \textbf{Formula}: Value function update: 
                \begin{equation}
                    V(s) \gets V(s) + \alpha (r + \gamma V(s') - V(s))
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Trends and Developments - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{4} % Continue numbering
        \item \textbf{Adaptive Dynamic Programming}
            \begin{itemize}
                \item \textbf{Concept}: Changing DP strategies based on environmental dynamics.
                \item \textbf{Example}: Algorithms switching between model-based and model-free strategies.
            \end{itemize}
        
        \item \textbf{Ethical and Responsible AI Approaches}
            \begin{itemize}
                \item \textbf{Concept}: Integrating ethics into AI and RL practices.
                \item \textbf{Example}: Ensuring policies derived from DP do not reinforce bias.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Discussion Prompt}
    \begin{block}{Conclusion}
        The future of dynamic programming in reinforcement learning holds great promise. By embracing these trends, researchers can unlock new potentials across various applications.
    \end{block}
    
    \begin{block}{Discussion Prompt}
        \textit{How might these future trends impact the design and implementation of reinforcement learning systems in your field of interest?}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Group Discussion: Challenges and Benefits of Dynamic Programming}
    \begin{block}{Introduction to Dynamic Programming}
        Dynamic Programming (DP) is an algorithmic technique used to solve complex problems by breaking them down into simpler subproblems. It is particularly useful for optimization problems and significantly reduces computational time compared to naive approaches.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Benefits of Dynamic Programming}
    \begin{enumerate}
        \item \textbf{Efficiency:}
            \begin{itemize}
                \item DP allows solutions in polynomial time by avoiding repeated calculations.
                \item Example: Fibonacci sequence calculation in O(n) vs O(2^n) with naive recursion.
            \end{itemize}

        \item \textbf{Optimality:}
            \begin{itemize}
                \item Guarantees finding the optimal solution in problems like the Knapsack problem or Shortest Path algorithms.
                \item Example: Bellman-Ford algorithm guarantees shortest paths in weighted graphs.
            \end{itemize}

        \item \textbf{Versatility:}
            \begin{itemize}
                \item Applicable in various domains—operations research, economics, bioinformatics.
                \item Example: Used in sequence alignment and decoding in genetic algorithms.
            \end{itemize}

        \item \textbf{Structured Approach:}
            \begin{itemize}
                \item Encourages systematic problem-solving by identifying recursive relationships.
                \item Example: Edit distance modeled through state transitions in a matrix.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges of Dynamic Programming}
    \begin{enumerate}
        \item \textbf{Space Complexity:}
            \begin{itemize}
                \item DP can require significant space for storing intermediate results.
                \item Example: Classic Fibonacci solution consumes O(n) space with memoization.
            \end{itemize}

        \item \textbf{Difficulty in Problem Formulation:}
            \begin{itemize}
                \item Insight required to identify overlapping subproblems and optimal structures.
                \item Example: Transitioning from recursion to DP can require deep understanding.
            \end{itemize}

        \item \textbf{Implementation Complexity:}
            \begin{itemize}
                \item Coding can be complex and lead to bugs; careful implementation is critical.
                \item Example: Mismanagement of indices in DP tables can yield incorrect solutions.
            \end{itemize}

        \item \textbf{Not Always Effective:}
            \begin{itemize}
                \item Not all problems are suitable; effectiveness requires optimal substructure and overlapping.
                \item Example: Purely linear problems do not benefit from DP.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions}
    \begin{itemize}
        \item What real-world problems can you think of that would benefit from dynamic programming?
        \item Can you discuss a scenario where the implementation of DP became overly complicated? What challenges did you face?
        \item How do you decide whether a problem is suitable for a dynamic programming approach?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion on Dynamic Programming}
    
    \begin{block}{Overview of Dynamic Programming (DP)}
        Dynamic programming is a powerful algorithmic technique used to solve optimization problems by breaking them down into simpler subproblems. It stores the solutions to these subproblems to avoid redundant calculations, particularly in scenarios involving overlapping subproblems and optimal substructure.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts Covered}
    
    \begin{enumerate}
        \item \textbf{Optimal Substructure}
        \begin{itemize}
            \item An optimal solution can be constructed from optimal solutions of its subproblems.
            \item \textbf{Example}: Shortest path in a graph.
        \end{itemize}

        \item \textbf{Overlapping Subproblems}
        \begin{itemize}
            \item Subproblems that are reused multiple times.
            \item \textbf{Example}: Calculating Fibonacci numbers recursively.
        \end{itemize}

        \item \textbf{Memoization vs. Tabulation}
        \begin{itemize}
            \item \textbf{Memoization}: Top-down approach storing results from recursive calls.
            \item \textbf{Tabulation}: Bottom-up approach solving smaller subproblems first.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Next Steps}
    
    \begin{enumerate}
        \item Dynamic programming is essential for efficiently solving complex problems by utilizing previously computed values.
        \item Understanding the properties of optimal substructure and overlapping subproblems is critical.
        \item Familiarity with both memoization and tabulation strategies enhances problem-solving capabilities.
    \end{enumerate}
    
    \begin{block}{Next Steps}
        Prepare your questions for our upcoming Q\&A session. Consider how dynamic programming can be applied in different contexts and share any challenges you faced while learning these concepts!
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Q\&A Session}
  \begin{block}{Objective}
    This session is designed to clarify students' doubts and solidify understanding of dynamic programming concepts. Engaging in Q\&A fosters a deeper comprehension of the material covered in this week's lessons.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts to Reflect On}
  \begin{itemize}
    \item \textbf{Dynamic Programming Definition}:
        DP is a method for solving complex problems by breaking them down into simpler subproblems, particularly useful when subproblems overlap.

    \item \textbf{Key Techniques}:
        \begin{itemize}
          \item \textbf{Memoization}: Storing results of expensive function calls to reuse results for the same inputs.
          \item \textbf{Tabulation}: Building a table in a bottom-up manner, solving smaller problems to address larger ones iteratively.
        \end{itemize}
        
    \item \textbf{Common DP Problems}:
        \begin{itemize}
          \item Fibonacci Sequence
          \item Knapsack Problem
          \item Longest Common Subsequence
        \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Fibonacci Sequence}
  \begin{block}{Traditional Recursive Solution}
  \begin{equation}
  \text{fib}(n) = \text{fib}(n-1) + \text{fib}(n-2)
  \end{equation}
  \end{block}

  \begin{block}{DP Approach}
  \begin{lstlisting}[language=Python]
  def fib(n):
      dp = [0] * (n + 1)
      dp[1] = 1
      for i in range(2, n + 1):
          dp[i] = dp[i - 1] + dp[i - 2]
      return dp[n]
  \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Encouraged Types of Questions}
  \begin{itemize}
    \item Clarifications on dynamic programming concepts (e.g., "What is the difference between memoization and tabulation?")
    \item Specific problems and their DP solutions (e.g., "How would you approach solving the knapsack problem using dynamic programming?")
    \item Real-life applications of dynamic programming (e.g., "In what scenarios is dynamic programming particularly beneficial?")
    \item Understanding time and space complexity related to DP approaches (e.g., "How can we evaluate the efficiency of our DP solution?")
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Wrap-Up & Next Steps}
  \begin{block}{Wrap-Up}
    Encourage participation by inviting students to ask questions they have pondered during the week's discussions. Asking questions is essential for learning, especially in dynamic programming, where concepts can interlink and deepen in complexity.
  \end{block}
  
  \begin{block}{Next Steps}
    Prepare to transition into next week’s topics, exploring advanced dynamic programming scenarios and building upon the foundational knowledge gained this week.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Week Preview - Overview of Upcoming Topics}
  In our next class, we will delve deeper into \textbf{Dynamic Programming (DP)}.
  The main areas of focus will include:
  \begin{enumerate}
    \item Advanced DP Techniques
    \item Common DP Problems
    \item Real-World Applications
    \item Optimization Techniques
    \item Hands-On Coding Session
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Week Preview - Advanced DP Techniques}
  \textbf{Memoization vs. Tabulation}:
  \begin{itemize}
    \item \textbf{Memoization}: Caching results of expensive function calls and reusing them.
    \item \textbf{Tabulation}: Building a table in a bottom-up manner, starting with smaller subproblems.
  \end{itemize}
  \textbf{Example:} Comparing both methods using the Fibonacci sequence calculations.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Week Preview - Common DP Problems}
  \textbf{1. Knapsack Problem}
  \begin{itemize}
    \item Illustrates optimization in resource allocation.
    \item \textbf{Example:} Maximize the value in a knapsack of fixed capacity with given weights and values.
  \end{itemize}

  \textbf{2. Longest Common Subsequence (LCS)}
  \begin{itemize}
    \item Finds the longest subsequence in both sequences without altering their order.
    \item \textbf{Illustration:} Strings like "ABCBDAB" and "BDCAB" to demonstrate DP efficiency.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Week Preview - Real-World Applications}
  Discuss applications of DP in various domains:
  \begin{itemize}
    \item \textbf{Data Science:} Pattern matching and analytics.
    \item \textbf{Operations Research:} Resource allocation.
    \item \textbf{Computer Science:} Algorithms like Edit Distance and Pathfinding in graphs.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Week Preview - Optimization Techniques and Coding Session}
  \textbf{Space Optimization:}
  \begin{itemize}
    \item Reduce space complexity of DP solutions by storing only necessary data.
    \item \textbf{Key Point:} Many DP problems can be solved in linear or constant space.
  \end{itemize}

  \textbf{Hands-On Coding Session:}
  \begin{itemize}
    \item Students will implement a DP algorithm from scratch using Python or Java.
    \item \textbf{Code Snippet Example:}
    \begin{lstlisting}[language=Python]
def fibonacci(n):
    fib = [0, 1]
    for i in range(2, n + 1):
        fib.append(fib[i - 1] + fib[i - 2])
    return fib[n]
    \end{lstlisting}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Week Preview - Summary and Key Questions}
  \textbf{Summary:} Prepare for an interactive class where we uncover the potential of dynamic programming in problem-solving and algorithm design!

  \textbf{Key Questions to Consider:}
  \begin{itemize}
    \item How can we distinguish between problems suitable for DP and those that are not?
    \item What strategies can we adopt to solve complex DP problems efficiently?
  \end{itemize}
  
  By the end of next week’s class, you will have a solid framework for tackling various DP problems and applying these concepts to real-world scenarios.
\end{frame}


\end{document}