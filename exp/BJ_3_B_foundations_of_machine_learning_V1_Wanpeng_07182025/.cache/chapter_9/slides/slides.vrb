\frametitle{Rectified Linear Unit (ReLU)}
    \begin{itemize}
        \item \textbf{Formula}:
        \begin{equation}
        f(x) = \max(0, x)
        \end{equation}
        \item \textbf{Range}: [0, âˆž)
        \item \textbf{Characteristics}:
        \begin{itemize}
            \item Computationally efficient; requires simple thresholding.
            \item Introduces sparsity; during training, some neurons may not activate.
            \item Can suffer from the "dying ReLU" problem where neurons output zeros.
        \end{itemize}
        \item \textbf{Use Case}: Popular in convolutional neural networks and deep learning architectures.
        \item \textbf{Example}: Activation in deep layers of a CNN for image classification tasks.
    \end{itemize}
