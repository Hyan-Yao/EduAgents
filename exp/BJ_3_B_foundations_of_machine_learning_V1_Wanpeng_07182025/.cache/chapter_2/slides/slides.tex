\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 2: Preparing Data for ML]{Week 2: Preparing Data for Machine Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \title{Introduction to Data Preparation}
    \maketitle
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Data Preparation in Machine Learning}
    Data preparation is a critical step in the machine learning workflow, directly influencing the performance and accuracy of predictive models. 
    \begin{itemize}
        \item Inadequate preparation can lead to flawed insights and poor model predictions.
        \item Effective data preparation is key to informed decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Preparation}
    \begin{enumerate}
        \item \textbf{Quality of the Model:}
        \begin{itemize}
            \item Reliable data builds trustworthiness in models.
            \item Issues like inconsistencies and missing values can skew results.
        \end{itemize}
        
        \item \textbf{Improved Performance:}
        \begin{itemize}
            \item Clean, well-prepared data leads to better learning and generalization.
            \item Example: Models on normalized datasets typically outperform those on raw data.
        \end{itemize}
        
        \item \textbf{Data Bias Mitigation:}
        \begin{itemize}
            \item Correct preparation helps identify and reduce biases.
            \item Including diverse demographic groups can prevent biased outcomes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps in Data Preparation}
    \begin{enumerate}
        \item \textbf{Data Cleaning:}
        \begin{itemize}
            \item Identifying and correcting errors or inconsistencies. 
            \item Actions: Removing duplicates, handling missing values, correcting erroneous entries.
        \end{itemize}
        
        \item \textbf{Data Transformation:}
        \begin{itemize}
            \item Normalization & Standardization: Scaling values to a common range (e.g., Min-Max scaling).
            \item Encoding Categorical Variables: Using techniques like One-Hot Encoding.
        \end{itemize}
        
        \item \textbf{Feature Selection and Engineering:}
        \begin{itemize}
            \item Selecting relevant features that enhance prediction power.
            \item Engineering new features (e.g., interaction terms).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact of Data Preparation on Model Performance}
    \begin{block}{Case Study Example}
        A company aiming to predict customer churn experienced a drop to 60\% accuracy using raw transaction data. 
        After thorough data preparation including cleaning and feature engineering, their accuracy improved to 85\%.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Data preparation is crucial for optimal results in machine learning.
        \item Investing time in data preparation pays dividends during model evaluation.
        \item A well-prepared dataset allows models to learn effectively from diverse sources.
    \end{itemize}
    
    \textbf{Additional Resources:}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load dataset
data = pd.read_csv('data.csv')

# Remove duplicates
data = data.drop_duplicates()

# Fill missing values
data['column_name'].fillna(data['column_name'].mean(), inplace=True)

# One-hot encoding of categorical features
data = pd.get_dummies(data, columns=['categorical_column'])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection - Overview}
    \begin{block}{Importance of Data Collection}
        Data collection is the foundation of machine learning. The quality, diversity, and representativeness of the data collected directly influence the performance of machine learning models.
    \end{block}
    
    \begin{itemize}
        \item Introduction to methods for data collection
        \item Sources and types of data
        \item Best practices for ensuring comprehensive datasets
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection - Methods}
    \begin{enumerate}
        \item \textbf{Surveys and Questionnaires}
            \begin{itemize}
                \item Description: Collecting data directly from individuals through structured interviews or surveys.
                \item Example: Online polls for customer feedback on a product.
            \end{itemize}
        
        \item \textbf{Web Scraping}
            \begin{itemize}
                \item Description: Using automated scripts to extract data from websites.
                \item Example: Fetching product prices from e-commerce sites using Python libraries like Beautiful Soup.
            \end{itemize}

        \item \textbf{API (Application Programming Interface)}
            \begin{itemize}
                \item Description: Accessing data provided by services through their APIs.
                \item Example: Pulling real-time weather data using a weather service API.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection - More Methods and Best Practices}
    \begin{enumerate}\setcounter{enumi}{3}
        \item \textbf{Existing Datasets}
            \begin{itemize}
                \item Description: Utilizing publicly available datasets from repositories.
                \item Example: Using the UCI Machine Learning Repository for various ML datasets.
            \end{itemize}

        \item \textbf{Sensors and Devices}
            \begin{itemize}
                \item Description: Data collection through IoT devices and sensors.
                \item Example: Wearable fitness trackers logging health metrics.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Best Practices for Diverse Datasets}
        \begin{itemize}
            \item Diversity: Represents various demographics to reduce bias.
            \item Random Sampling: Select a subset of data from a larger population.
            \item Stratified Sampling: Divide the dataset into homogeneous subgroups before sampling.
            \item Continuous Data Collection: Regular updates to datasets.
            \item Data Documentation: Maintain thorough records of data sources.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning - Overview}
    % Overview of data cleaning processes in machine learning
    Data cleaning is a critical preprocess in the data preparation phase for machine learning, aimed at enhancing the quality of the data by correcting inaccuracies and ensuring consistency. This process significantly impacts the performance of machine learning models. The focus here is on three primary aspects:
    \begin{itemize}
        \item Handling Missing Values
        \item Dealing with Duplicates
        \item Addressing Outliers
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning - Handling Missing Values}
    % Discusses strategies for handling missing values
    Missing values can distort analysis and predictive modeling. Strategies to manage missing values include:
    \begin{enumerate}
        \item \textbf{Removal}: Delete rows or columns with missing data.
              \begin{itemize}
                  \item Example: Consider removing features with 10\% or more missing values.
              \end{itemize}
        \item \textbf{Imputation}: Fill in missing values using statistical methods.
              \begin{itemize}
                  \item Mean/Median Imputation: 
                  \begin{equation}
                  x_{imputed} =  \begin{cases}
                  x & \text{if } x \text{ is not missing} \\
                  \text{mean}(X) & \text{if } x \text{ is missing}
                  \end{cases}
                  \end{equation}
              \end{itemize}
        \item \textbf{Predictive Model}: Use a machine learning model to predict and fill missing values based on available data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning - Dealing with Duplicates and Outliers}
    % Addresses handling duplicates and outliers
    \textbf{Dealing with Duplicates}
    \begin{itemize}
        \item \textbf{Identification}: Use techniques such as:
        \begin{lstlisting}
        duplicates = df[df.duplicated()]
        \end{lstlisting}
        \item \textbf{Removal}: After identifying duplicates:
        \begin{lstlisting}
        df.drop_duplicates(inplace=True)
        \end{lstlisting}
    \end{itemize}

    \textbf{Addressing Outliers}
    \begin{itemize}
        \item \textbf{Detection}:
            \begin{itemize}
                \item Visualization: Boxplots or scatter plots.
                \item Statistical Tests: Z-score or IQR method.
            \end{itemize}
        \item \textbf{Treatment}:
            \begin{itemize}
                \item Removal of outliers.
                \item Transformation to replace outliers with a defined boundary.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning - Key Points and Conclusion}
    % Summary and conclusion of data cleaning importance
    \textbf{Key Points:}
    \begin{itemize}
        \item Quality of data directly impacts model accuracy.
        \item Addressing missing values, duplicates, and outliers is essential for effective data preprocessing.
        \item Techniques used should align with the dataset and the problem at hand.
    \end{itemize}

    \textbf{Conclusion:}
    Data cleaning is an essential step in preparing datasets for machine learning applications, ensuring that models are built on accurate and reliable data. Mastering these techniques enhances your capability to develop robust machine learning solutions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Introduction}
    Data preprocessing is a crucial step in the machine learning pipeline. It involves transforming raw data into a clean dataset that enhances the performance of machine learning algorithms. 
    In this presentation, we will explore three major preprocessing techniques:
    \begin{itemize}
        \item Normalization
        \item Standardization
        \item Encoding of categorical variables
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Normalization}
    \textbf{Normalization} refers to the process of scaling numerical data into a specific range, typically [0, 1]. 
    This technique is particularly useful when features have different scales, as they can affect model performance. 

    \textbf{Example:}
    \begin{itemize}
        \item Dataset features: Age (0-100) and Salary (30,000-120,000).
        \item Without normalization, Salary may dominate model performance due to scale.
    \end{itemize}

    \textbf{Formula:}
    \begin{equation}
        X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Standardization}
    \textbf{Standardization} involves centering the data around the mean and scaling to unit variance, resulting in a mean of 0 and standard deviation of 1. 
    This technique is helpful for algorithms that assume a normal distribution.

    \textbf{Example:}
    \begin{itemize}
        \item Standardizing exam scores allows interpretation relative to peers, regardless of absolute score values.
    \end{itemize}

    \textbf{Formula:}
    \begin{equation}
        X_{stand} = \frac{X - \mu}{\sigma}
    \end{equation}
    Where:
    \begin{itemize}
        \item \( \mu \) = Mean of the feature
        \item \( \sigma \) = Standard deviation of the feature
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Preprocessing Techniques - Encoding Categorical Variables}
    Machine learning models require numerical input, necessitating the conversion of categorical variables into numerical form. Two common techniques are:
    
    \begin{itemize}
        \item \textbf{Label Encoding:} Assigns a unique integer to each category (e.g., Red = 0, Green = 1, Blue = 2).
        \item \textbf{One-Hot Encoding:} Creates a binary column for each category.
    \end{itemize}

    \textbf{Illustration:}
    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            Red & Green & Blue \\
            \hline
            1   & 0     & 0    \\
            0   & 1     & 0    \\
            0   & 0     & 1    \\
            \hline
        \end{tabular}
    \end{center}

    \textbf{Key Points:}
    \begin{itemize}
        \item Choose the encoding method based on the number of categories and model capabilities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering - Overview}
    \begin{block}{Understanding Feature Engineering}
        Feature engineering is the process of using domain knowledge to extract features (variables, attributes) from raw data to improve machine learning model performance. 
    \end{block}
    \begin{itemize}
        \item Critical step in data preprocessing.
        \item Often determines a model's success.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering - Importance}
    \begin{enumerate}
        \item \textbf{Enhances Model Performance}
        \item \textbf{Reduces Overfitting}
        \item \textbf{Improves Interpretability}
    \end{enumerate}
    \begin{itemize}
        \item Selecting or transforming features can lead to significant improvements.
        \item Focus on relevant features simplifies the model.
        \item Meaningful features make results easier to explain.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Selection vs. Feature Extraction}
    \begin{block}{Feature Selection}
        \begin{itemize}
            \item Identifying and removing irrelevant features.
            \item Techniques include:
            \begin{itemize}
                \item Filter Methods
                \item Wrapper Methods
                \item Embedded Methods
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Feature Extraction}
        \begin{itemize}
            \item Creating new features from existing ones.
            \item Techniques include:
            \begin{itemize}
                \item Principal Component Analysis (PCA)
                \item Domain-Specific Transformations
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Feature Engineering}
    \begin{itemize}
        \item \textbf{Date-Time Features:}
            \begin{itemize}
                \item Year, Month, Day of the week, Hour
            \end{itemize}
        \item \textbf{Text Data:}
            \begin{itemize}
                \item Count of keywords, Sentiment scores, TF-IDF vectors
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Iterative Process: Requires many iterations.
        \item Domain Knowledge: Leads to better selections.
        \item Cross-Validation: Validate features across datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example - Feature Selection}
    \begin{lstlisting}[language=Python, basicstyle=\small]
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest, f_classif

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Select top 2 features based on ANOVA F-statistic
selector = SelectKBest(score_func=f_classif, k=2)
X_new = selector.fit_transform(X, y)

print("Original features shape:", X.shape)
print("Reduced features shape:", X_new.shape)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Effective feature engineering is crucial for building robust models. By selecting the right features or creating new ones, we can enhance predictive accuracy and gain valuable insights.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Imbalanced Data - Introduction}
    \begin{block}{Introduction to Imbalanced Data}
        Imbalanced data occurs when the number of observations in different classes of a classification problem is unequal. 
        For example, in a medical diagnosis dataset, you might have 95\% healthy patients and only 5\% patients with a rare disease. 
        Such imbalance can lead to biased models that favor the majority class, causing poor predictive performance for the minority class.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Imbalanced Data - Challenges}
    \begin{block}{Challenges of Imbalanced Data}
        \begin{itemize}
            \item \textbf{Model Bias:} Models tend to predict the majority class more often than the minority class.
            \item \textbf{High Misclassification Costs:} In many applications (e.g., fraud detection, disease diagnosis), 
            misclassifying the minority class can have severe consequences.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Imbalanced Data - Techniques}
    \begin{block}{Techniques for Handling Imbalanced Data}
        \begin{enumerate}
            \item \textbf{Resampling Methods}
            \begin{itemize}
                \item \textbf{Oversampling:} Increase the number of instances in the minority class.
                    \begin{itemize}
                        \item Example: Randomly duplicating samples from the minority class.
                        \item Tools: SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic examples.
                    \end{itemize}
                \item \textbf{Undersampling:} Reduce the number of instances in the majority class.
                    \begin{itemize}
                        \item Example: Randomly removing samples from the majority class.
                        \item Caution: May lead to loss of potentially valuable data.
                    \end{itemize}
            \end{itemize}
            \item \textbf{Synthetic Data Generation}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Imbalanced Data - Synthetic Data Generation}
    \begin{block}{SMOTE Explanation}
        SMOTE generates synthetic samples by calculating the distances between minority class instances and their nearest neighbors.
        \begin{equation}
            x_{new} = x + \lambda \cdot (x_{nn} - x)
        \end{equation}
        where $\lambda$ is a random number between 0 and 1.
    \end{block}
    \begin{itemize}
        \item \textbf{Advantages:} Helps to create a more nuanced representation of the minority class.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Imbalanced Data - Algorithm-Level Approaches}
    \begin{block}{Algorithm-Level Approaches}
        \begin{itemize}
            \item Use algorithms that are less sensitive to class imbalance, such as decision trees and ensemble methods.
            \item \textbf{Class Weighting:} Adjust the importance of different classes during model training.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Addressing data imbalance is crucial for model performance.
            \item Resampling methods require careful consideration to avoid overfitting.
            \item Evaluate model performance using metrics like F1-score or ROC-AUC instead of relying solely on accuracy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Imbalanced Data - Conclusion}
    Properly handling imbalanced datasets is crucial for building effective machine learning models. 
    Utilizing resampling methods and synthetic data generation can significantly enhance the performance and reliability of model predictions, particularly for minority classes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Splitting for Validation}
    \begin{block}{Best Practices for Splitting Data}
        When preparing datasets for machine learning, splitting data into three subsets is essential:
        \begin{itemize}
            \item \textbf{Training Set}: Used to train the model.
            \item \textbf{Validation Set}: Used to tune hyperparameters.
            \item \textbf{Test Set}: Used for final evaluation of model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Split Data?}
    \begin{itemize}
        \item \textbf{Avoid Overfitting:} Ensures model generalizes well by training on one set and validating on another.
        \item \textbf{Hyperparameter Tuning:} Allows fine-tuning of model parameters for improved performance.
        \item \textbf{Final Model Evaluation:} Test set provides a measure of real-world performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Data Splitting}
    \begin{itemize}
        \item \textbf{Common Split Ratios:}
        \begin{itemize}
            \item 70/15/15: 70\% training, 15\% validation, 15\% test.
            \item 80/10/10: 80\% training, 10\% validation, 10\% test (ideal for larger datasets).
        \end{itemize}
        \item \textbf{Stratified Sampling:} Reflects overall class distribution, crucial for imbalanced datasets.
        \item \textbf{Randomization:} Shuffle data to minimize bias before splits.
        \item \textbf{K-Fold Cross-Validation:} Divides data into 'k' folds for robust model evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Data Splitting}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

# Assuming you have features `X` and target `y`
X_train, X_temp, y_train, y_temp = train_test_split(X, y, 
    test_size=0.3, random_state=42)  # 70% Training

# Now split temp set into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, 
    test_size=0.5, random_state=42)  # 15% Validation, 15% Test
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Define objectives and expected performance metrics prior to splitting.
        \item Ensure data integrity to prevent information leakage.
        \item Document the splitting process and random seeds for reproducibility.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Effective data splitting is crucial for building robust machine learning models. By following best practices such as appropriate ratios, stratification, and randomization, you enhance your model's readiness for real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Handling}
    % Introduction to ethical considerations in data handling
    Data handling in machine learning involves critical ethical issues that impact model quality and fairness.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Ethical Considerations}
    % Overview of ethical considerations in data handling
    \begin{itemize}
        \item Data handling goes beyond collection and processing.
        \item It requires awareness of ethical implications.
        \item Ethical considerations impact model fairness and effectiveness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Issues}
    % Listing key ethical issues: Bias, Privacy, Data Governance
    \begin{block}{Bias in Data}
        \begin{itemize}
            \item \textbf{Definition:} Systematic favoritism in data or algorithms.
            \item \textbf{Example:} Hiring algorithms reflecting gender or racial biases.
            \item \textbf{Importance:} Biased models can perpetuate discrimination.
        \end{itemize}
    \end{block}

    \begin{block}{Privacy Concerns}
        \begin{itemize}
            \item \textbf{Definition:} Safeguarding personal information during data usage.
            \item \textbf{Example:} Using health records without consent.
            \item \textbf{Regulations:} GDPR and CCPA aim to protect users' rights.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Issues (cont.)}
    % Continuing with Data Governance
    \begin{block}{Data Governance}
        \begin{itemize}
            \item \textbf{Definition:} Management of data’s availability, usability, integrity, and security.
            \item \textbf{Key Aspects:}
            \begin{itemize}
                \item Protocols for data collection and usage.
                \item Ensuring data quality and legal compliance.
                \item Promoting transparency in data usage.
            \end{itemize}
            \item \textbf{Importance:} Fosters trust with users and stakeholders.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Emphasizing Key Points}
    % Highlighting key takeaways regarding ethical data handling
    \begin{itemize}
        \item Ethical data handling is crucial for:
        \begin{enumerate}
            \item Avoiding harmful consequences from biased algorithms.
            \item Protecting individuals' rights and freedoms.
            \item Ensuring compliance with legal and ethical standards.
        \end{enumerate}
        \item Stakeholders (data scientists, organizations) must implement ethical practices.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    % Summarizing the importance of ethical considerations in AI systems
    Recognizing and addressing ethical issues is essential for creating responsible AI systems.
    \begin{itemize}
        \item Ethical data handling leads to models that perform well and maintain integrity.
        \item Focus on these dimensions promotes compliant, trustworthy, and socially responsible AI practices.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Summary - Key Points Recap}
    \begin{enumerate}
        \item \textbf{Importance of Data Quality}:
        \begin{itemize}
            \item High-quality data is fundamental for effective model performance.
            \item Example: Incomplete data can lead to missed trends and unreliable predictions.
        \end{itemize}
        
        \item \textbf{Data Cleaning}:
        \begin{itemize}
            \item Remove duplicates, fix inconsistencies, and handle missing values.
            \item Techniques: Imputation (mean, median, mode), Outlier Detection (IQR rule, Z-scores).
        \end{itemize}
        
        \item \textbf{Feature Engineering}:
        \begin{itemize}
            \item Modify or create new features to improve model effectiveness.
            \item Example: Transforming date variables for time-based trend analysis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Summary - Continued}
    \begin{enumerate}
        \setcounter{enumi}{3}
        
        \item \textbf{Data Normalization and Scaling}:
        \begin{itemize}
            \item Standardization aids in model convergence and performance.
            \item Common Methods: Min-Max Scaling (0 to 1), Z-score Standardization (mean-centered).
        \end{itemize}
        
        \item \textbf{Data Splitting}:
        \begin{itemize}
            \item Split datasets into training and testing sets for accurate evaluation.
            \item Common Ratios: 70\% training - 30\% testing; 80\% training - 20\% testing.
        \end{itemize}

        \item \textbf{Addressing Ethical Considerations}:
        \begin{itemize}
            \item Understand ethical implications in data use (bias, privacy).
            \item Handle data responsibly to prevent bias propagation.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Summary - Key Takeaway}
    \begin{block}{Overall Influence on Model Success}
        Data preparation processes directly affect the efficacy and reliability of machine learning models. Neglecting these steps may lead to biased predictions or underperformance.
    \end{block}
    
    \vspace{0.5cm}
    
    \begin{block}{Key Takeaway}
        Investing time in effective data preparation is crucial—it's the foundation for robust machine learning models that yield trustworthy insights.
    \end{block}
    
    \begin{lstlisting}[language=Python, caption={Code Snippet for Data Splitting in Python}]
from sklearn.model_selection import train_test_split

# Assume X is your features matrix and y is your target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    \end{lstlisting}
\end{frame}


\end{document}