\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Supervised Learning]{Week 3: Supervised Learning - Linear Regression}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Linear Regression - Overview}
    \begin{block}{What is Linear Regression?}
        Linear Regression is a fundamental supervised learning technique used to model the relationship between a dependent variable and one or more independent variables. The goal is to find the best-fitting straight line that minimizes the difference between observed and predicted values.
    \end{block}
    
    \begin{block}{Importance in Machine Learning}
        \begin{itemize}
            \item \textbf{Predictive Modeling}: Used for making predictions and forecasting outcomes.
            \item \textbf{Interpretability}: Coefficients indicate the effect of each variable, valuable in fields like economics.
            \item \textbf{Foundation for Other Techniques}: Serves as a basis for advanced models like polynomial regression.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Linear Regression}
    \begin{itemize}
        \item \textbf{Dependent Variable (Y)}: The outcome aimed to predict (e.g., house prices).
        \item \textbf{Independent Variables (X)}: The input features used to make predictions (e.g., size of the house).
        \item \textbf{Regression Line}: Represents the relationship mathematically as:
        \begin{equation}
            Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
        \end{equation}
        where $\beta_0$ is the intercept, $\beta_i$ are coefficients, and $\epsilon$ is the error term.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Linear Regression}
    \begin{block}{Example Equation}
        For a simple linear regression with one independent variable:
        \begin{equation}
            Price = 50,000 + 200 \times Size
        \end{equation}
        Here, 50,000 is the intercept, and 200 indicates that for every additional square foot, the price increases by $200.
    \end{block}
    
    \begin{block}{Conclusion}
        Linear regression is a cornerstone of statistical modeling and machine learning. It provides a straightforward approach for predicting outcomes based on the influence of independent variables.
    \end{block}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Models linear relationships between variables.
            \item Predictive and interpretable.
            \item Familiarity with equations aids in understanding advanced techniques.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Concepts in Linear Regression - Part 1}
    \frametitle{Key Concepts in Linear Regression}
    
    \begin{block}{1. Dependent and Independent Variables}
        \begin{itemize}
            \item \textbf{Independent Variable (Predictor/Feature)}: The input variable you control or manipulate.
                \begin{itemize}
                    \item \textbf{Example}: In predicting house prices, independent variables can include square footage, number of bedrooms, and age of the house.
                \end{itemize}
            
            \item \textbf{Dependent Variable (Response/Target)}: The output variable you aim to predict or explain. It depends on the values of the independent variables.
                \begin{itemize}
                    \item \textbf{Example}: In the house price example, the dependent variable is the price of the house.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Concepts in Linear Regression - Part 2}
    \frametitle{The Concept of the Regression Line}
    
    \begin{block}{Regression Line}
        The regression line represents the relationship between the independent and dependent variables. It is a straight line that best fits the data points plotted in a scatter plot.
        
        \begin{equation}
            y = mx + b
        \end{equation}
        
        Where:
        \begin{itemize}
            \item \(y\) = predicted value (dependent variable)
            \item \(m\) = slope of the line (determines how much \(y\) changes for a one-unit change in \(x\))
            \item \(x\) = value of the independent variable
            \item \(b\) = y-intercept (value of \(y\) when \(x = 0\))
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Concepts in Linear Regression - Part 3}
    \frametitle{Parameters of the Model}

    \begin{block}{Model Parameters}
        \begin{itemize}
            \item \textbf{Slope (\(m\))}: Indicates the steepness of the line and the relationship between \(x\) and \(y\).
                \begin{itemize}
                    \item \textbf{Example}: If \(m = 2\), it means for every unit increase in \(x\), \(y\) increases by 2 units.
                \end{itemize}
            
            \item \textbf{Intercept (\(b\))}: The point where the regression line crosses the y-axis.
                \begin{itemize}
                    \item \textbf{Example}: If \(b = 5\), when \(x = 0\), the predicted value of \(y\) will be 5.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Linear regression helps understand relationships between variables.
            \item The regression line predicts outcomes based on input features.
            \item Adjusting parameters \(m\) and \(b\) allows for better data fitting.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula Recap}
        Remember the key formula:
        \begin{equation}
            y = mx + b
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Foundations of Linear Regression}
    \begin{block}{Introduction}
        Linear regression models the relationship between a dependent variable \(Y\) and independent variables \(X\).
        The goal is to find the linear equation that best fits the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Formulation}
    The linear regression model is expressed as:
    
    \begin{equation}
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n + \epsilon
    \end{equation}
    
    \begin{itemize}
        \item \(Y\): Dependent variable
        \item \(\beta_0\): Intercept
        \item \(\beta_1, \beta_2, ..., \beta_n\): Coefficients for independent variables
        \item \(X_1, X_2, ..., X_n\): Independent variables
        \item \(\epsilon\): Error term
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cost Function and Gradient Descent}
    To measure the model performance, we use the **Cost Function**. The Mean Squared Error (MSE) is defined as:
    
    \begin{equation}
    MSE = \frac{1}{m} \sum_{i=1}^{m} (Y_i - \hat{Y}_i)^2
    \end{equation}
    
    \begin{itemize}
        \item \(m\): Number of observations
        \item \(Y_i\): Actual value
        \item \(\hat{Y}_i\): Predicted value
    \end{itemize}

    **Gradient Descent** updates parameters:
    
    \begin{equation}
    \beta_j := \beta_j - \alpha \frac{\partial}{\partial \beta_j} MSE
    \end{equation}
    
    \begin{itemize}
        \item \(\alpha\): Learning rate
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Least Squares Error Method}
    The **Least Squares Method** minimizes the sum of squared residuals:

    \begin{equation}
    \hat{\beta} = (X^TX)^{-1}X^TY
    \end{equation}

    Where:
    \begin{itemize}
        \item \(X\): Matrix of input features (including a column of ones for intercept)
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Assumes a linear relationship between variables.
            \item Cost function influences the regression outcome.
            \item Gradient descent and least squares are key estimation tools.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the mathematical foundations of linear regression, including the cost function, gradient descent, and least squares error method, is crucial for effective model implementation. These concepts are foundational in statistical learning and predictive analytics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simple vs Multiple Linear Regression}
    Linear regression is a statistical method to model relationships between a dependent variable (target) and independent variables (predictors).   
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simple Linear Regression (SLR)}
    \begin{block}{Definition}
        SLR involves one independent variable to predict a dependent variable.
        \begin{equation}
            Y = \beta_0 + \beta_1 X + \epsilon
        \end{equation}
    \end{block}
    \begin{itemize}
        \item \( Y \): dependent variable
        \item \( X \): independent variable
        \item \( \beta_0 \): y-intercept
        \item \( \beta_1 \): slope coefficient
        \item \( \epsilon \): error term (residual)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Key Points of SLR}
    \begin{block}{Example}
        Predicting house prices based solely on square footage.
    \end{block}
    \begin{itemize}
        \item Simple and interpretable.
        \item Suitable for linear relationships with one predictor.
        \item Limited complexity - can't capture interactions among multiple variables.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multiple Linear Regression (MLR)}
    \begin{block}{Definition}
        MLR uses two or more independent variables to predict a dependent variable.
        \begin{equation}
            Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
        \end{equation}
    \end{block}
    \begin{itemize}
        \item \( X_1, X_2, \ldots, X_n \): independent variables
        \item \( \beta_1, \beta_2, \ldots, \beta_n \): coefficients of independent variables
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example and Key Points of MLR}
    \begin{block}{Example}
        Predicting house prices based on square footage, number of bedrooms, and location.
    \end{block}
    \begin{itemize}
        \item Handles complex relationships with multiple predictors.
        \item Captures interactions and multicollinearity among variables.
        \item Risk of overfitting with too many variables without justification.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison Table}
    \begin{tabular}{|c|c|c|}
        \hline
        Feature & Simple Linear Regression & Multiple Linear Regression \\
        \hline
        Number of Predictors & One & Two or more \\
        \hline
        Complexity & Simpler model & More complex model \\
        \hline
        Interpretability & Easier to interpret & Results can be complex \\
        \hline
        Use Cases & Appropriate for single factor & Suitable for multifactor analysis \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the distinction between SLR and MLR is crucial for selecting the appropriate modeling technique. 
    \begin{itemize}
        \item SLR is best for simple relationships.
        \item MLR is necessary for capturing the complexity of real-world data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Overview}
    \begin{itemize}
        \item Linear regression is a statistical technique for predicting a continuous outcome based on one or more predictor variables.
        \item For valid results, key assumptions must be satisfied:
            \begin{enumerate}
                \item Linearity
                \item Independence
                \item Homoscedasticity
                \item Normality of errors
            \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Part 1}
    \begin{block}{1. Linearity}
        \begin{itemize}
            \item \textbf{Definition:} The relationship between independent variable(s) and the dependent variable must be linear.
            \item \textbf{Example:} Predicting a student's exam score based on study hours should show a consistent increase in score for each additional hour studied.
            \item \textbf{Visual Representation:} A scatter plot with a straight line of best fit indicates linearity.
        \end{itemize}
    \end{block}

    \begin{block}{2. Independence}
        \begin{itemize}
            \item \textbf{Definition:} Residuals (errors) should be independent of each other.
            \item \textbf{Example:} Patterns in residuals may suggest that an underlying factor influences multiple observations (e.g., repeated measurements on the same subject).
            \item \textbf{Key Point:} Independence is crucial for valid statistical inference.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assumptions of Linear Regression - Part 2}
    \begin{block}{3. Homoscedasticity}
        \begin{itemize}
            \item \textbf{Definition:} The variance of residuals should remain constant across levels of independent variable(s).
            \item \textbf{Example:} When analyzing income variation based on education years, residuals should not fan out or contract as income increases.
            \item \textbf{Visual Representation:} A residual plot should show a random scatter around zero, indicating constant variance.
        \end{itemize}
    \end{block}

    \begin{block}{4. Normality of Errors}
        \begin{itemize}
            \item \textbf{Definition:} Residuals should be approximately normally distributed.
            \item \textbf{Example:} This is often assessed with a histogram or Q-Q plot of residuals. A bell-shaped curve indicates that the assumption is met.
            \item \textbf{Key Point:} Most errors should cluster around zero, suggesting symmetric prediction errors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formula Reference}
    \begin{block}{Simple Linear Regression Model}
        For a simple linear regression model:
        \begin{equation}
            Y = \beta_0 + \beta_1 X + \epsilon
        \end{equation}
        Where:
        \begin{itemize}
            \item $Y$ = dependent variable
            \item $X$ = independent variable
            \item $\beta_0$ = y-intercept
            \item $\beta_1$ = slope of the line
            \item $\epsilon$ = error term
        \end{itemize}
    \end{block}
    
    \begin{block}{Final Thoughts}
        Checking these assumptions is crucial for the validity and reliability of a linear regression model. Violations may lead to biased estimates and incorrect conclusions.
    \end{block}

    \begin{block}{Next Steps}
        In the upcoming slide, we will explore the practical implementation of linear regression using Python's Scikit-learn library.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Implementation of Linear Regression}
    \begin{block}{Overview}
        Linear Regression is a fundamental supervised learning algorithm used for predicting a continuous target variable based on one or more predictor variables. In this section, we will implement a simple linear regression model using Python's Scikit-learn library.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Guide - Data Preparation}
    \begin{enumerate}
        \item \textbf{Import Libraries}
        \begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
        \end{lstlisting}

        \item \textbf{Load the Dataset}
        \begin{lstlisting}[language=Python]
data = pd.read_csv('data.csv')
        \end{lstlisting}
        
        \item \textbf{Select Features and Target Variable}
        \begin{lstlisting}[language=Python]
X = data[['feature1', 'feature2']]  # Replace with actual feature names
y = data['target']  # Replace with the actual target variable name
        \end{lstlisting}

        \item \textbf{Split the Dataset}
        \begin{lstlisting}[language=Python]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Guide - Model Fitting and Predictions}
    \begin{enumerate}
        \setcounter{enumi}{4} % continue enumeration
        \item \textbf{Model Fitting}
        \begin{lstlisting}[language=Python]
model = LinearRegression()
model.fit(X_train, y_train)
        \end{lstlisting}
        
        \item \textbf{Understanding Model Coefficients}
        \begin{lstlisting}[language=Python]
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)
        \end{lstlisting}

        \item \textbf{Making Predictions}
        \begin{lstlisting}[language=Python]
predictions = model.predict(X_test)
        \end{lstlisting}

        \item \textbf{Visualizing Predictions (Optional)}
        \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

plt.scatter(y_test, predictions)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.show()
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Importance of Data Preparation: Properly preparing the data is crucial for a successful model.
            \item Model Fitting: Understanding the output coefficients helps interpret the impact of each feature.
            \item Validation: Predictions should always be validated using a separate test set to avoid overfitting.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Implementing linear regression in Python is straightforward with Scikit-learn. By following the above steps, you can create and evaluate a linear regression model, setting the foundation for more complex predictive modeling tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Model Performance - Introduction}
    In this slide, we will explore how to assess the performance of a linear regression model using key metrics:
    \begin{itemize}
        \item \textbf{R-squared}
        \item \textbf{Mean Absolute Error (MAE)}
        \item \textbf{Mean Squared Error (MSE)}
    \end{itemize}
    These metrics will help us understand how well our model predicts outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metric - R-squared (R²)}
    \begin{block}{Definition}
        R-squared measures the proportion of variance in the dependent variable explained by the independent variables in the model.
    \end{block}
    \begin{itemize}
        \item \textbf{Range}: From 0 to 1
        \begin{itemize}
            \item 0: Model explains none of the variability
            \item 1: Model explains all the variability
        \end{itemize}
        \item \textbf{Interpretation}:
        \begin{itemize}
            \item Higher R² indicates a better fit.
            \item Example: If $R^2 = 0.85$, 85\% of variance is explained.
        \end{itemize}
    \end{itemize}
    \begin{block}{Formula}
        \begin{equation}
            R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}
        \end{equation}
        Where:
        \begin{itemize}
            \item $\text{SS}_{\text{res}} = \sum (y_i - \hat{y}_i)^2$ (Residual sum)
            \item $\text{SS}_{\text{tot}} = \sum (y_i - \bar{y})^2$ (Total sum)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics - MAE and MSE}
    \begin{block}{Mean Absolute Error (MAE)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the average magnitude of errors, without considering direction.
            \item \textbf{Interpretation}:
            \begin{itemize}
                \item Lower MAE indicates better performance.
                \item Example: An MAE of 5 means predictions deviate by an average of 5 units.
            \end{itemize}
        \end{itemize}
        \begin{block}{Formula}
            \begin{equation}
                MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
            \end{equation}
        \end{block}
    \end{block}
    
    \begin{block}{Mean Squared Error (MSE)}
        \begin{itemize}
            \item \textbf{Definition}: Average of the squares of the errors.
            \item \textbf{Interpretation}:
            \begin{itemize}
                \item Hyper-sensitive to large errors.
                \item Example: A MSE of 25 indicates average squared error is 25.
            \end{itemize}
        \end{itemize}
        \begin{block}{Formula}
            \begin{equation}
                MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
        \end{block}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Example Code}
    \begin{block}{Summary of Key Points}
        \begin{itemize}
            \item R-squared indicates explanatory power of the model.
            \item MAE provides a straightforward measure of prediction accuracy.
            \item MSE is sensitive to large errors, helpful for identifying outliers.
        \end{itemize}
    \end{block}
    
    \begin{block}{Considerations}
        \begin{itemize}
            \item Always use multiple metrics for evaluation.
            \item Utilize Python's Scikit-learn functions: \texttt{r2\_score}, \texttt{mean\_absolute\_error}, and \texttt{mean\_squared\_error}.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Code Snippet (Python)}
        \begin{lstlisting}[language=Python]
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Example data
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]

# Calculating metrics
r_squared = r2_score(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)

print(f"R²: {r_squared}, MAE: {mae}, MSE: {mse}")
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Model Results - Overview}
    \begin{block}{Definition}
        This slide provides a guide for interpreting the coefficients of a linear regression model and understanding their significance in predicting the dependent variable.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Coefficients in Linear Regression}
    \begin{itemize}
        \item Linear regression predicts a dependent variable based on independent variables.
        \item Coefficients describe the relationship between each independent variable and the dependent variable.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Coefficient Interpretation}
    \begin{block}{Regression Equation}
        \begin{equation}
        \text{y} = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
        \end{equation}
    \end{block}
    \begin{itemize}
        \item \( y \): Predicted value of dependent variable
        \item \( b_0 \): Y-intercept (constant term)
        \item \( b_i \): Coefficients for independent variables \( x_i \)
    \end{itemize}
    \begin{block}{Interpretation}
        Each coefficient \( b_i \) represents the change in \( y \) for a one-unit increase in \( x_i \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Coefficients}
    \begin{itemize}
        \item Coefficients accompany p-values to indicate statistical significance.
        \item Small p-value (\(< 0.05\)): strong evidence against the null hypothesis.
        \item Large p-value: suggests no significant impact of the independent variable on the dependent variable.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Analysis}
    \begin{block}{Scenario}
        Predicting house prices based on square footage:
        \begin{itemize}
            \item Intercept (\( b_0 \)): 50,000
            \item Coefficient for square footage (\( b_1 \)): 200
        \end{itemize}
    \end{block}
    \begin{block}{Interpretation}
        \begin{itemize}
            \item Price starts at \$50,000 for zero square footage.
            \item Price increases by \$200 for each additional square foot.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Practical Considerations}
    \begin{itemize}
        \item Magnitude of coefficients: Larger values indicate stronger influence.
        \item Direction of influence: Positive means direct; negative means inverse.
        \item Use statistical software (e.g., R, Python) for analysis.
        \item Assess overall model fit (R-squared, MAE, MSE) for evaluation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding coefficients in a linear regression model enables practitioners to make informed predictions and actionable business decisions from data analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues in Linear Regression}
    \begin{block}{Overview}
        Discuss common pitfalls such as multicollinearity, overfitting, and how to diagnose and resolve these issues.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues in Linear Regression - Part 1: Multicollinearity}
    \begin{itemize}
        \item \textbf{Definition:} Multicollinearity occurs when two or more independent variables in a regression model are highly correlated.
        \item \textbf{Implications:}
            \begin{itemize}
                \item Inflated standard errors of coefficients, making them less significant.
                \item Difficulty in determining the individual effect of predictors.
            \end{itemize}
        \item \textbf{Diagnosis:}
            \begin{itemize}
                \item \textbf{Variance Inflation Factor (VIF):} A common method to assess multicollinearity.
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{VIF}_{i} = \frac{1}{1 - R^2_i}
                \end{equation}
                \item Where \(R^2_i\) is the R-squared value obtained by regressing the \(i^{th}\) independent variable on all other independent variables.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues in Linear Regression - Part 2: Overfitting}
    \begin{itemize}
        \item \textbf{Definition:} Overfitting occurs when a model is too complex, capturing noise along with the underlying relationship.
        \item \textbf{Implications:}
            \begin{itemize}
                \item High accuracy on training data but significantly lower accuracy on validation or test data.
                \item A model that is too tailored to the training data lacks predictive power.
            \end{itemize}
        \item \textbf{Diagnosis:}
            \begin{itemize}
                \item Compare performance metrics (e.g., R-squared, Mean Squared Error) between training and validation datasets.
                \item Learning curves illustrate training vs. validation performance as more data is used.
            \end{itemize}
        \item \textbf{Resolution:}
            \begin{itemize}
                \item \textbf{Regularization:} Techniques like Lasso (L1) and Ridge (L2) can help manage complexity.
                \item \textbf{Simplifying the Model:} Reduce the number of predictors or use simpler models (e.g., linear instead of polynomial regression).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Multicollinearity:} Look for correlations between predictors and be prepared to eliminate or combine them.
        \item \textbf{Overfitting:} Watch for discrepancies between training and validation performance; use regularization to promote a simpler model.
    \end{itemize}
    \begin{block}{Important Note}
        Validating model assumptions and checking model performance metrics is essential for building robust linear regression models!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Linear Regression - Introduction}
    \begin{block}{Introduction to Linear Regression}
        Linear regression is a widely used statistical method for modeling the relationship between a dependent variable and one or more independent variables. 
        It helps us understand how the value of the dependent variable changes as we change the independent variables.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Linear Regression - Real-World Applications}
    \begin{enumerate}
        \item \textbf{Economics}
            \begin{itemize}
                \item \textit{Example:} Predicting Economic Growth
                \item Economists model the relationship between GDP growth and factors such as interest rates, inflation, and unemployment.
                \item \textbf{Key Point:} Analysts can make informed predictions about future economic conditions based on historical data.
            \end{itemize}
        
        \item \textbf{Healthcare}
            \begin{itemize}
                \item \textit{Example:} Relationship Between Exercise and Health Outcomes
                \item Assessing how levels of physical activity impact health metrics (e.g., blood pressure, cholesterol).
                \item \textbf{Key Point:} This can guide interventions aimed at improving public health.
            \end{itemize}
        
        \item \textbf{Social Sciences}
            \begin{itemize}
                \item \textit{Example:} Analyzing the Impact of Education on Income
                \item Researchers evaluate how years of education influence income.
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{Income} = \beta_0 + \beta_1 \cdot (\text{Years of Education}) + \epsilon
                \end{equation}
                \item \textbf{Key Point:} Provides insights into social mobility and educational policies.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Linear Regression - Additional Applications and Conclusion}
    \begin{itemize}
        \item \textbf{Marketing}
            \begin{itemize}
                \item \textit{Example:} Estimating Sales Based on Advertising Spend
                \item Companies quantify how changes in ad expenditures affect sales.
                \item \textbf{Key Point:} Helps businesses allocate marketing budgets effectively.
            \end{itemize}
    \end{itemize}

    \begin{block}{Conclusion}
        Linear regression serves as a powerful tool across various disciplines, enhancing decision-making and strategy development. 
        Understanding its applications can yield significant insights.
    \end{block}

    \begin{block}{Final Thoughts}
        \textbf{Key Takeaway:} The versatility of linear regression is crucial in data analysis, applicable from economics to healthcare and beyond.
    \end{block}
\end{frame}


\end{document}