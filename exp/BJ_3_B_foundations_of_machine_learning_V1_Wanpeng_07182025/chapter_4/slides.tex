\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

% Set Theme Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}

% Title Page Information
\title[Supervised Learning]{Week 4: Supervised Learning - Classification Algorithms}
\author[Your Name]{Your Name}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Overview}
    \begin{block}{Definition}
        Supervised learning is a type of machine learning where a model is trained using a labeled dataset consisting of input-output pairs. 
    \end{block}
    \begin{block}{Relevance}
        Understanding supervised learning is essential for applications in various domains such as finance, healthcare, and marketing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Process}
    \begin{enumerate}
        \item \textbf{Data Collection}: Gather labeled data that includes features and their associated labels.
        \item \textbf{Model Selection}: Choose an appropriate algorithm (e.g., decision trees, SVMs, neural networks).
        \item \textbf{Model Training}: Train the model with labeled data to learn the mapping between inputs and outputs.
        \item \textbf{Model Evaluation}: Test the model on unseen data to evaluate performance and generalization.
        \item \textbf{Prediction}: Use the trained model to predict output labels for new, unlabeled data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Applications & Examples}
    \begin{block}{Types of Problems Addressed}
        \begin{itemize}
            \item \textbf{Classification}: Assign labels based on predefined categories (e.g., spam detection).
            \item \textbf{Regression}: Predict continuous output values (e.g., house prices).
        \end{itemize}
    \end{block}
    \begin{block}{Example: Email Classification}
        Consider classifying emails as "spam" or "not spam":
        \begin{itemize}
            \item \textbf{Input Features}: Email length, number of links, specific keywords.
            \item \textbf{Output Labels}: "Spam" or "Not Spam".
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Key Insights}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Supervised learning relies on labeled data.
            \item Models are trained to generalize from training data to unseen data.
            \item Both classification and regression problems can be addressed using supervised learning techniques.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning - Basic Formula}
    \begin{block}{Logistic Regression Formula}
        The classification boundary can be represented as:
        \begin{equation}
            P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
        \end{equation}
        \begin{itemize}
            \item $P(Y=1 | X)$: Probability of class membership.
            \item $\beta_0, \beta_1, ..., \beta_n$: Coefficients of the model.
            \item $X_1, X_2, ..., X_n$: Input features.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Classification Algorithms Overview - Part 1}
    \begin{block}{Introduction to Classification Algorithms}
        Classification algorithms are a subset of supervised learning methods used in machine learning. They categorize data into predefined classes or labels based on input features. 
    \end{block}
    \begin{block}{Role in Supervised Learning}
        In supervised learning, models are trained on labeled datasets (input-output pairs) to learn from data and make predictions on unseen instances. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Classification Algorithms Overview - Part 2}
    \begin{block}{Applications}
        Classification is crucial for various applications, including:
        \begin{itemize}
            \item \textbf{Email Filtering}: Classifying emails as spam or not spam.
            \item \textbf{Medical Diagnosis}: Predicting diseases based on patient data.
            \item \textbf{Sentiment Analysis}: Determining the sentiment of product reviews.
        \end{itemize}
    \end{block}
    \begin{block}{Common Classification Algorithms}
        \begin{itemize}
            \item \textbf{Logistic Regression}
            \item \textbf{Decision Trees}
            \item \textbf{Support Vector Machines (SVM)}
            \item \textbf{k-Nearest Neighbors (k-NN)}
            \item \textbf{Neural Networks}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Classification Algorithms Overview - Part 3}
    \begin{block}{Example: Email Classification}
        \begin{itemize}
            \item \textbf{Features}: Words in the email (e.g., “Congratulations,” “free”).
            \item \textbf{Training Data}: Emails labeled as 'Spam' or 'Not Spam'.
            \item \textbf{Prediction}: New emails classified using learned rules.
        \end{itemize}
    \end{block}
    \begin{block}{Logistic Regression Formula}
        For binary classification using logistic regression, the relationship is given by:
        \begin{equation}
        P(Y = 1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Classification Algorithms Overview - Conclusion}
    \begin{block}{Conclusion}
        Classification algorithms are essential in supervised learning for predictive modeling with labeled data. Understanding their mechanisms and applications is vital for effective machine learning.
    \end{block}
    \begin{block}{Next Up}
        In the upcoming slide, we will explore \textbf{Decision Trees} in detail, including their structure and classification process.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Definition}
    \begin{block}{Definition}
        A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It represents decisions and their possible consequences as a tree-like model.
    \end{block}
    \begin{itemize}
        \item Intuitive and easy to understand.
        \item Consists of nodes, branches, and leaves.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Structure}
    \begin{enumerate}
        \item \textbf{Root Node:} 
            \begin{itemize}
                \item The topmost node representing the entire dataset.
                \item Splits into branches based on feature values.
            \end{itemize}
        \item \textbf{Splitting:}
            \begin{itemize}
                \item Dividing a node into sub-nodes based on decision rules.
                \item Each split distinguishes different classes or values.
            \end{itemize}
        \item \textbf{Internal Nodes:}
            \begin{itemize}
                \item Represent a feature used to make decisions.
                \item Branches out based on the test outcome of the feature.
            \end{itemize}
        \item \textbf{Branches:}
            \begin{itemize}
                \item Links between nodes representing test outcomes.
            \end{itemize}
        \item \textbf{Leaf Nodes:}
            \begin{itemize}
                \item End points indicating predicted outputs (class labels or values).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Key Points and Example}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Interpretability:} Easy to interpret and visualize decisions.
            \item \textbf{Feature Importance:} Insights into the importance of features in decision-making.
            \item \textbf{Non-Parametric:} No assumptions on data distribution, versatile across datasets.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Consider a decision tree for classifying whether a person will play tennis based on weather conditions:
        \begin{verbatim}
                        [Weather]
                          /    \
                      Sunny     Rainy
                      / \         \
                   Humid Windy    Overcast
                    /      |       \
                No Play   Yes     Yes
        \end{verbatim}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Formulas and Metrics}
    \begin{block}{Formulas}
        \textbf{Gini Impurity}:
        \begin{equation}
            Gini = 1 - \sum (p_i^2)
        \end{equation}
        Where \( p_i \) is the probability of class \( i \) in the dataset.

        \textbf{Entropy}:
        \begin{equation}
            Entropy = -\sum (p_i \log_2(p_i))
        \end{equation}

        \textbf{Information Gain}:
        \begin{equation}
            IG = Entropy(parent) - \left(\frac{|\text{children}|}{|\text{parent}|} \times Entropy(children)\right)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Decision Trees? - Conclusion}
    Decision Trees are powerful tools in supervised learning for classification due to their:
    \begin{itemize}
        \item Intuitive structure.
        \item Ability to handle both numerical and categorical data.
    \end{itemize}
    The next slide will discuss how decision trees determine the best splits for decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Overview}
    \begin{block}{Overview}
        Decision trees are a powerful and popular method for classification tasks in supervised learning. They model decisions and their consequences in a tree-like graph with:
    \end{block}
    \begin{itemize}
        \item \textbf{Nodes}: Where decisions are made
        \item \textbf{Branches}: Representing the outcome of decisions
        \item \textbf{Leaves}: End decisions or classifications
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Working Principle}
    \begin{enumerate}
        \item \textbf{Root Node}: The process starts at the root node, encompassing the entire dataset.
        \item \textbf{Splitting}: Data is divided into subsets based on specific criteria aimed at achieving pure child nodes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Decision Trees Work - Splitting Criteria}
    \begin{block}{Splitting Criteria}
        \begin{itemize}
            \item \textbf{Gini Impurity}:
            \begin{equation}
                Gini(D) = 1 - \sum_{i=1}^{C} p_i^2
            \end{equation}
            where \( p_i \) is the proportion of instances classified to class \( i \).
            
            \item \textbf{Entropy}:
            \begin{equation}
                Entropy(D) = -\sum_{i=1}^{C} p_i \log_2(p_i)
            \end{equation}
            
            \item \textbf{Information Gain}:
            \begin{equation}
                Information\, Gain = Entropy(parent) - \sum \left( \frac{|D_{child}|}{|D|} \times Entropy(D_{child}) \right)
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Introduction}
    \begin{block}{Introduction to Decision Trees}
        \begin{itemize}
            \item Decision trees are a popular tool in supervised learning used for classification and regression tasks.
            \item They use a tree-like model to represent decisions and their possible consequences, leading to a final outcome or prediction.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Key Benefits}
    \begin{block}{Key Advantages of Decision Trees}
        \begin{enumerate}
            \item \textbf{Interpretability and Visualization:}
                \begin{itemize}
                    \item Intuitive and easy to understand.
                    \item Visualization aids comprehension, even for non-experts.
                \end{itemize}
            \item \textbf{No Need for Feature Scaling:}
                \begin{itemize}
                    \item Directly handle both numerical and categorical data.
                \end{itemize}
            \item \textbf{Handling Non-linear Relationships:}
                \begin{itemize}
                    \item Capable of capturing non-linear relationships between features and outcomes.
                \end{itemize}
            \item \textbf{Robustness to Outliers:}
                \begin{itemize}
                    \item Less sensitive to outliers than many models.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - More Benefits}
    \begin{block}{Additional Advantages}
        \begin{enumerate}
            \setcounter{enumi}{4} % To continue numbering
            \item \textbf{Feature Importance:}
                \begin{itemize}
                    \item Insights into which features are most important for predictions.
                \end{itemize}
            \item \textbf{Versatility:}
                \begin{itemize}
                    \item Applicable for both classification and regression tasks.
                \end{itemize}
            \item \textbf{No Assumptions of Data Distribution:}
                \begin{itemize}
                    \item Flexible and does not assume any specific data distribution.
                \end{itemize}
            \item \textbf{Fast Training and Prediction:}
                \begin{itemize}
                    \item Efficient for large datasets after tree construction.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees - Conclusion}
    \begin{block}{Conclusion}
        Decision trees blend interpretability, robustness, and flexibility, making them invaluable in the classification toolkit and foundational for techniques like Random Forests and Gradient Boosting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code - Decision Tree with Scikit-learn}
    \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample data (features and labels)
X, y = load_data()  # Replace with actual data loading

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the decision tree classifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Make predictions and evaluate model
y_pred = clf.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees}
    \begin{block}{Overview}
        While decision trees are powerful tools in classification tasks, they come with several limitations and challenges that can impact their performance and applicability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Part 1}
    \begin{enumerate}
        \item \textbf{Overfitting}
        \begin{itemize}
            \item Explanation: Prone to overfitting with complex data, creating overly complex models that do not generalize well.
            \item Example: A decision tree perfectly classifying apples and oranges based on every tiny variation, failing on unseen data.
        \end{itemize}

        \item \textbf{Instability}
        \begin{itemize}
            \item Explanation: Small changes in data can lead to entirely different tree structures.
            \item Example: Adding more data for fruit classification can result in a completely different decision tree.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Decision Trees - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Biased Trees}
        \begin{itemize}
            \item Explanation: Can be biased toward majority classes, leading to under-representation of minority classes.
            \item Illustration: In a dataset with 90\% apples and 10\% oranges, the tree might misclassify many oranges.
        \end{itemize}

        \item \textbf{Difficulty Handling Continuous Variables}
        \begin{itemize}
            \item Explanation: Decision trees must discretize continuous variables, potentially losing information.
            \item Example: For height, limited splits (e.g., $<60$, $>60$) may discard valuable information.
        \end{itemize}

        \item \textbf{Non-robustness}
        \begin{itemize}
            \item Explanation: Poor performance with noisy data or overlaps between classes.
            \item Example: Misclassification of bruised apples due to solely focusing on appearance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Techniques}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Overfitting: Need for pruning or setting constraints.
            \item Instability: Importance of ensemble methods like Random Forest.
            \item Bias: Techniques like stratified sampling to address class imbalance.
        \end{itemize}
    \end{block}

    \begin{block}{Suggested Techniques}
        \begin{itemize}
            \item Pruning: Trimming the tree post-creation to avoid overfitting.
            \item Ensemble Methods: Using Random Forest or Boosted Trees for improved accuracy.
            \item Cross-Validation: Implementing k-fold for robust model evaluation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding the limitations of decision trees is essential to making informed choices about when to use them and how to improve their performance through various techniques. Next, we will explore alternative algorithms, like K-Nearest Neighbors (KNN), which address some of these challenges while maintaining their own unique characteristics.
\end{frame}

\begin{frame}
    \frametitle{What is K-Nearest Neighbors (KNN)?}
    K-Nearest Neighbors (KNN) is a supervised learning algorithm commonly used for classification tasks. It classifies data based on the principle that similar data points are usually close to each other in feature space.
\end{frame}

\begin{frame}
    \frametitle{Definition and Classification Mechanism}
    \begin{block}{Definition}
        K-Nearest Neighbors (KNN) is a simple yet powerful supervised learning algorithm for classification.
    \end{block}
    
    \begin{block}{Classification Mechanism}
        KNN classifies a data point based on the majority class among its 'k' closest neighbors using a distance metric (e.g., Euclidean distance):
        \begin{equation}
            \text{Euclidean Distance} = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
        \end{equation}
        where \(x_i\) and \(y_i\) are coordinates in n-dimensional space.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Step-by-Step Process}
    \begin{enumerate}
        \item Choose 'k': Determine the number of neighbors 'k' to consider.
        \item Calculate Distance: For each data point to be classified, calculate its distance to all training data points.
        \item Identify Neighbors: Sort distances and select the top 'k' nearest neighbors.
        \item Vote for Class: Classify the data point into the class with the most votes from the neighbors.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Example: Classifying a New Fruit}
    \begin{itemize}
        \item Scenario: Classifying an unknown berry based on features like color and size.
        \item Procedure: 
        \begin{itemize}
            \item Feature space includes classes: 'Berry' and 'Cherry'.
            \item Calculate distances to labeled fruits (e.g., among 3 berries and 2 cherries).
            \item Classify as 'Berry' if it appears more frequently among the 'k' neighbors.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item No Training Phase: KNN is a lazy learning algorithm without a distinct training phase.
        \item Scalability: KNN can be computationally intensive with large datasets.
        \item Choosing 'k': A small 'k' can lead to noise sensitivity, whereas a large 'k' can smooth the decision boundary.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: KNN in Python}
    \begin{lstlisting}[language=Python]
from sklearn.neighbors import KNeighborsClassifier

# Sample dataset
X = [[1, 2], [2, 3], [3, 3], [6, 5], [7, 7]]
y = ['Berry', 'Berry', 'Berry', 'Cherry', 'Cherry']

# Create KNN classifier with k=3
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X, y)

# Predict class for a new data point
new_data_point = [[5, 5]]
prediction = knn.predict(new_data_point)
print("Predicted class:", prediction)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    KNN is an intuitive and effective algorithm for classification tasks. Understanding its mechanism and the impact of parameters is crucial for effective application.
\end{frame}

\begin{frame}[fragile]
    \frametitle{How KNN Works - Overview}
    \begin{block}{Overview of KNN Mechanism}
        The K-Nearest Neighbors (KNN) algorithm is a simple yet powerful classification method that operates on the principle of proximity. It classifies a data point based on how its neighbors are classified. Here’s how KNN works step-by-step:
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How KNN Works - Steps in KNN Classification}
    \begin{enumerate}
        \item \textbf{Choose the number of neighbors (K)}  
        Decide the number of closest neighbors to consider for making a classification decision. Common values for K are odd numbers (e.g., K = 1, 3, 5).
        
        \item \textbf{Calculate Distance Metrics}  
        Measure the distance between the new data point and all points in the training dataset. Common distance metrics include:
        \begin{itemize}
            \item \textbf{Euclidean Distance:}  
            \begin{equation}
                d(p_1, p_2) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
            \end{equation}
            \item \textbf{Manhattan Distance:}
            \begin{equation}
                d(p_1, p_2) = |x_2 - x_1| + |y_2 - y_1|
            \end{equation}
            \item \textbf{Minkowski Distance:}  
            Generalized version defined by a parameter \( p \):
            \begin{equation}
                d(p_1, p_2) = (|x_2 - x_1|^p + |y_2 - y_1|^p)^{1/p}
            \end{equation}
        \end{itemize}

        \item \textbf{Find K Nearest Neighbors}  
        Identify the K training examples that are closest to the new data point based on the chosen distance metric.
        
        \item \textbf{Voting System}  
        Each neighbor votes for its class (label). The class with the majority vote among the K nearest neighbors is assigned to the new data point.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How KNN Works - Example}
    \begin{block}{Example to Illustrate KNN}
        Consider a scenario with the following existing labels:

        \begin{center}
        \begin{tabular}{|c|c|}
        \hline
        \textbf{Point} & \textbf{Class} \\
        \hline
        (1, 2) & A \\
        (2, 2) & A \\
        (3, 3) & B \\
        (6, 5) & B \\
        \hline
        \end{tabular}
        \end{center}
        
        If \textbf{K=3}, for the new point (2, 3):
        \begin{itemize}
            \item Calculate distances:  
            \begin{itemize}
                \item Distance to (1, 2): \( d \approx 1.41 \)
                \item Distance to (2, 2): \( d = 1.0 \)
                \item Distance to (3, 3): \( d = 1.0 \)
                \item Distance to (6, 5): \( d \approx 4.47 \)
            \end{itemize}
            \item Nearest neighbors are (1, 2), (2, 2), and (3, 3) with classes A, A, and B.
            \item \textbf{Result:} Majority vote returns class A.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How KNN Works - Key Points}
    \begin{itemize}
        \item \textbf{No Assumptions:} KNN does not assume anything about the underlying data distribution, making it a non-parametric method.
        \item \textbf{Distance Metric Choice:} The choice of distance metric significantly impacts the algorithm's performance, particularly in high-dimensional datasets.
        \item \textbf{Scalability:} KNN can be computationally intensive as it must compute distances to all training instances, especially with large datasets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How KNN Works - Practical Implementation}
    \begin{block}{Python Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.neighbors import KNeighborsClassifier

# Sample data
X = [[1, 2], [2, 2], [3, 3], [6, 5]]
y = ['A', 'A', 'B', 'B']

# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)

# Fit the model
knn.fit(X, y)

# Predict on a new data point
new_point = [[2, 3]]
prediction = knn.predict(new_point)
print(f'The predicted class for {new_point} is: {prediction[0]}')
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to K-Nearest Neighbors (KNN)}
    K-Nearest Neighbors (KNN) is a widely-used classification algorithm in machine learning that relies on proximity—how close data points are to one another in feature space. 
    \begin{itemize}
        \item Simple and intuitive algorithm.
        \item Classifies based on the majority class of nearest neighbors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Advantages of KNN - Part 1}
    \begin{enumerate}
        \item \textbf{Simplicity and Intuition:}
            \begin{itemize}
                \item Easy to understand and implement.
                \item Example: Classify an animal based on the closest 3 animals.
            \end{itemize}

        \item \textbf{No Assumptions About Data Distribution:} 
            \begin{itemize}
                \item Flexible, does not assume an underlying data distribution (e.g., normal distribution).
            \end{itemize}

        \item \textbf{Effective in High-Dimensional Spaces:} 
            \begin{itemize}
                \item Suitable for complex datasets such as image recognition or text classification.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Advantages of KNN - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3}  % Continue numbering
        \item \textbf{Adaptable to Different Distance Metrics:} 
            \begin{itemize}
                \item Customizable based on the problem domain (e.g., Euclidean, Manhattan).
                \begin{block}{Code Snippet}
                    \begin{lstlisting}[language=Python]
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')
                    \end{lstlisting}
                \end{block}
            \end{itemize}
        
        \item \textbf{No Training Phase:} 
            \begin{itemize}
                \item Instance-based learning algorithm; stores the entire dataset for on-the-fly classification.
                \item Ideal for quick classification of incoming data.
            \end{itemize}
        
        \item \textbf{Versatile with Multi-Class Problems:}
            \begin{itemize}
                \item Naturally handles multi-class classification problems.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    KNN's simplicity, flexibility, and effectiveness make it a compelling choice for various classification problems.
    \begin{itemize}
        \item Understanding these advantages helps in choosing appropriate algorithms for data science tasks.
    \end{itemize}
    
    \textbf{Visual Elements:}
    \begin{itemize}
        \item Consider adding an illustration describing KNN classification.
        \item Remember the formula for distance: 
        \begin{equation}
            d = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
        \end{equation}
        where \(d\) is the distance between points \(x\) and \(y\).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Overview}
    \begin{itemize}
        \item K-Nearest Neighbors (KNN) is a simple yet powerful supervised learning algorithm commonly used for classification tasks.
        \item While it has several advantages, KNN exhibits significant limitations that can impact its performance and effectiveness in certain scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Key Limitations}
    \begin{enumerate}
        \item Computational Complexity
        \item Memory Requirement
        \item Sensitivity to Noise
        \item Choice of 'k'
        \item Curse of Dimensionality
        \item Imbalanced Classes
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Computational Complexity}
    \begin{itemize}
        \item \textbf{High Time Complexity:} KNN's prediction phase involves calculating the distance to every training sample, resulting in a time complexity of $O(n)$, where $n$ is the number of training samples.
        \item \textbf{Example:} In large datasets, predicting the class of a new instance can become impractical due to the extensive distance calculations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Memory Requirement}
    \begin{itemize}
        \item \textbf{Storage of Training Data:} KNN requires storing the entire training dataset in memory, which can be prohibitive with large datasets, making it less scalable.
        \item \textbf{Illustration:} Imagine a dataset with millions of instances; KNN must retain all this data, leading to high memory consumption.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Sensitivity to Noise}
    \begin{itemize}
        \item \textbf{Impact of Irrelevant Features:} KNN relies heavily on feature similarity, making it sensitive to noise and irrelevant features.
        \item \textbf{Example:} If a few noisy data points are situated close to a test instance, KNN might incorrectly classify based on those neighbors.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Choice of 'k'}
    \begin{itemize}
        \item \textbf{Effect of $k$ Value:} The choice of $k$ (number of neighbors) is crucial.
        \item \textbf{Illustration:} 
        \begin{itemize}
            \item $k=1$: Highly sensitive to noise.
            \item $k=20$: May miss local patterns in the data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Curse of Dimensionality}
    \begin{itemize}
        \item \textbf{Distance Metric Effectiveness:} As the number of features increases, data points tend to become equidistant from each other, making distance-based methods like KNN less effective.
        \item \textbf{Example:} In a high-dimensional space (e.g., a dataset with hundreds of features), the concept of 'closeness' becomes blurred, making classification challenging.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Imbalanced Classes}
    \begin{itemize}
        \item \textbf{Class Distribution Impact:} KNN can struggle with imbalanced datasets where certain classes are overrepresented.
        \item \textbf{Illustration:} In a dataset with 90\% class A and 10\% class B, KNN might classify the majority class more often, failing to recognize instances from class B.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of K-Nearest Neighbors (KNN) - Conclusion}
    \begin{itemize}
        \item While K-Nearest Neighbors is a straightforward and effective classification algorithm for many problems, understanding its limitations is essential for its successful application.
        \item \textbf{Key Takeaway:} Always consider KNN's computational demands, sensitivity to noise, the curse of dimensionality, and the choice of parameters when utilizing this algorithm for classification tasks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics for Classification}
    \begin{block}{Overview}
        When building classification models, it’s crucial to evaluate their performance accurately. The effectiveness of a model can be summarized using various metrics, each highlighting different aspects of its predictive abilities.
    \end{block}
    \begin{itemize}
        \item Accuracy
        \item Precision
        \item Recall (Sensitivity)
        \item F1 Score
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Accuracy}
    \begin{block}{Definition}
        Accuracy measures the proportion of correctly predicted instances out of the total instances evaluated.
    \end{block}
    \begin{equation}
        \text{Accuracy} = \frac{\text{True Positives (TP)} + \text{True Negatives (TN)}}{\text{Total Instances}}
    \end{equation}
    \begin{block}{Example}
        If out of 100 instances, 90 are correctly classified (80 true positives and 10 true negatives), then:
        \begin{equation}
            \text{Accuracy} = \frac{80 + 10}{100} = 0.90 \text{ or } 90\%
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Precision, 3. Recall, & 4. F1 Score}
    \begin{block}{Precision}
        \begin{itemize}
            \item \textbf{Definition}: Indicates how many of the instances predicted as positive are actually positive.
            \item \textbf{Formula}:
                \[
                \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
                \]
            \item \textbf{Example}: If a model predicts 40 instances as positive and only 30 are true positives, then:
                \[
                \text{Precision} = \frac{30}{30 + 10} = 0.75 \text{ or } 75\%
                \]
        \end{itemize}
    \end{block}
    
    \begin{block}{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: Measures the proportion of actual positive instances that were correctly identified by the model.
            \item \textbf{Formula}:
                \[
                \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
                \]
            \item \textbf{Example}: If there are 50 actual positive instances, and the model correctly identifies 30 of them, then:
                \[
                \text{Recall} = \frac{30}{30 + 20} = 0.60 \text{ or } 60\%
                \]
        \end{itemize}
    \end{block}
    
    \begin{block}{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of precision and recall, providing a balance between the two metrics.
            \item \textbf{Formula}:
                \[
                \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
                \]
            \item \textbf{Example}: If precision is 0.75 and recall is 0.60, then:
                \[
                \text{F1 Score} \approx 0.67 \text{ or } 67\%
                \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Trade-offs}: High accuracy does not always mean a good model, especially in imbalanced datasets. Precision and recall are important to consider.
        \item \textbf{Context Matters}: The choice of metric may depend on the application; for instance, in medical diagnoses, recall might be more crucial to avoid missing positive cases.
        \item \textbf{Comprehensive Evaluation}: Use multiple metrics to create a holistic view of model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Decision Trees and KNN - Introduction}
    
    \begin{block}{Decision Trees}
        \begin{itemize}
            \item Graphical representation of decisions and consequences.
            \item Segments data into branches for decision inference based on input features.
        \end{itemize}
    \end{block}

    \begin{block}{K-Nearest Neighbors (KNN)}
        \begin{itemize}
            \item Instance-based learning algorithm.
            \item Classifies data points based on the 'K' closest training examples.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Decision Trees}

    \begin{block}{Applications}
        \begin{enumerate}
            \item \textbf{Healthcare:}
                \begin{itemize}
                    \item Predicts patient outcomes and diagnoses diseases based on symptoms and history.
                \end{itemize}
            \item \textbf{Finance:}
                \begin{itemize}
                    \item Used for credit scoring, assessing the likelihood of loan defaults based on income and credit history.
                \end{itemize}
            \item \textbf{Marketing:}
                \begin{itemize}
                    \item Categorizes customers based on purchasing behavior.
                \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Point}
        Decision Trees provide a clear visualization, making them easy to interpret across various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - KNN}
    
    \begin{block}{Applications}
        \begin{enumerate}
            \item \textbf{Recommendation Systems:}
                \begin{itemize}
                    \item Used by platforms like Netflix and Amazon for personalized content suggestions.
                \end{itemize}
            \item \textbf{Retail:}
                \begin{itemize}
                    \item Identifies sales patterns to predict future product demand.
                \end{itemize}
            \item \textbf{Security:}
                \begin{itemize}
                    \item Employed for intrusion detection by classifying network traffic.
                \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Point}
        KNN's simplicity and effectiveness make it ideal for real-time classification tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Applications}

    \begin{itemize}
        \item \textbf{Decision Trees:} 
        \begin{itemize}
            \item Visual and interpretable, used in healthcare, finance, and marketing.
        \end{itemize}
        
        \item \textbf{KNN:}
        \begin{itemize}
            \item Distance-based, effective for recommendations and classifications in e-commerce and security.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Overview}
    \begin{block}{Key Points Recap: Decision Trees \& K-Nearest Neighbors (KNN)}
        \begin{enumerate}
            \item Decision Trees: A flowchart-like structure for classification.
            \item K-Nearest Neighbors (KNN): A non-parametric, instance-based learning algorithm.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Decision Trees}
    \begin{itemize}
        \item \textbf{Definition:} A decision tree is a flowchart-like structure where:
        \begin{itemize}
            \item Each internal node represents a feature (attribute).
            \item Each branch denotes a decision rule.
            \item Each leaf node signifies an outcome.
        \end{itemize}
        
        \item \textbf{Working Principle:} 
        \begin{itemize}
            \item Series of binary splits based on feature values are used to classify data.
            \item Handles both categorical and numerical data effectively.
        \end{itemize}

        \item \textbf{Example:} Loan approval decision tree considering:
        \begin{itemize}
            \item Features: "Credit Score", "Income Level", "Debt-to-Income Ratio".
            \item Outcomes: "Approved", "Rejected".
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - KNN and Importance}
    \begin{itemize}
        \item \textbf{K-Nearest Neighbors (KNN):}
        \begin{itemize}
            \item \textbf{Definition:} Non-parametric, instance-based learning algorithm that classifies data points based on 'k' closest training examples.
            \item \textbf{Working Principle:}
            \begin{itemize}
                \item Calculates distance to existing points and assigns the most common label among the 'k' nearest neighbors.
                \item Utilizes distance metrics like Euclidean or Manhattan distance.
            \end{itemize}
            \item \textbf{Example:} For an image classification task, a new image might be classified as:
            \begin{itemize}
                \item 'Cat' if it is closest to three 'Cat' images and two 'Dog' images (with $k=5$).
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Importance in Supervised Learning:}
        \begin{itemize}
            \item \textbf{Interpretability:} Decision trees are intuitive and facilitate model transparency.
            \item \textbf{Versatility:} Applicable in diverse fields like finance, healthcare, and marketing.
            \item \textbf{Foundation for Advanced Models:} Decision trees underpin models like Random Forests and Gradient Boosting.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Questions and Discussion}
    \begin{block}{Overview}
        This slide presents an opportunity for students to engage in an interactive discussion regarding classification algorithms, a key aspect of supervised learning. The goal is to clarify any doubts and deepen understanding through questions and collaborative learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts to Reflect Upon}
    \begin{enumerate}
        \item \textbf{What is Supervised Learning?}
        \begin{itemize}
            \item Involves training a model on a labeled dataset.
            \item Example: Identifying fruit types based on features (color, size).
        \end{itemize}
        
        \item \textbf{Classification Algorithms:}
        \begin{itemize}
            \item \textbf{Decision Trees:} 
            \begin{itemize}
                \item Splits data based on feature values.
                \item Example: Classifying emails as spam.
            \end{itemize}
            
            \item \textbf{k-Nearest Neighbors (k-NN):} 
            \begin{itemize}
                \item Classifies based on neighbors.
                \item Example: Classifying mushrooms as edible or poisonous.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Evaluation Metrics:}
        \begin{itemize}
            \item Accuracy, precision, recall, and F1-score.
            \item Formula: Accuracy = $\frac{TP + TN}{TP + TN + FP + FN}$
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Points}
    \begin{itemize}
        \item \textbf{Challenges in Classification:}
        \begin{itemize}
            \item Class imbalance and overfitting.
            \item Impacts on model performance.
        \end{itemize}
        
        \item \textbf{Real-World Applications:}
        \begin{itemize}
            \item Medical diagnosis, sentiment analysis, etc.
        \end{itemize}
        
        \item \textbf{Choice of Algorithm:}
        \begin{itemize}
            \item Factors: runtime efficiency, complexity, interpretability.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Engagement Strategy}
        \begin{itemize}
            \item Start with guiding questions.
            \item Open the floor for student questions.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}