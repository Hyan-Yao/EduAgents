\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 3: Introduction to Apache Hadoop}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Hadoop}
    \begin{block}{Overview of the Significance of Hadoop in Data Processing}
        Apache Hadoop is an open-source framework designed to process and store large datasets across clusters of computers using simple programming models. 
        It is particularly suited for big data applications, enabling organizations to analyze vast volumes of data efficiently.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is Hadoop Significant?}
    \begin{enumerate}
        \item \textbf{Scalability}:
        \begin{itemize}
            \item Scales from a single server to thousands of machines.
            \item Example: A small organization can start with a single-node setup and expand as data grows.
        \end{itemize}

        \item \textbf{Cost-efficiency}:
        \begin{itemize}
            \item Runs on commodity hardware, reducing costs.
            \item Example: Organizations can use existing servers or inexpensive hardware to build clusters.
        \end{itemize}

        \item \textbf{Fault Tolerance}:
        \begin{itemize}
            \item Automatic data replication across nodes.
            \item Example: If one node fails, the system continues to operate using data from other nodes.
        \end{itemize}

        \item \textbf{Data Variety}:
        \begin{itemize}
            \item Processes structured, semi-structured, and unstructured data.
            \item Example: Integrates insights from social media, sensors, and transaction data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Hadoop}
    \begin{block}{Hadoop Distributed File System (HDFS)}
        A distributed file system that stores data across multiple nodes, ensuring fault tolerance and high throughput access to data.
    \end{block}

    \begin{block}{MapReduce}
        A programming model for processing large data sets with a distributed algorithm on a cluster. 
        \begin{itemize}
            \item \textbf{Map}: Processes input data into key-value pairs.
            \item \textbf{Reduce}: Aggregates output from the Map tasks.
        \end{itemize}
    \end{block}

    \begin{block}{YARN (Yet Another Resource Negotiator)}
        Manages and schedules resources across the cluster, enabling multiple data processing engines to function efficiently.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways}
    \begin{block}{Conclusion}
        Apache Hadoop revolutionizes data processing by making it scalable, cost-effective, fault-tolerant, and capable of handling diverse data types. 
        It plays a crucial role in the era of big data, enabling organizations to harness the value of their information effectively.
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Understand the fundamental significance of Hadoop in handling large-scale data.
            \item Recognize the critical features that make Hadoop a preferred choice for big data processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Apache Hadoop?}
    \begin{block}{Definition of Apache Hadoop}
        Apache Hadoop is an open-source framework designed to facilitate the distributed storage and processing of large datasets across clusters of computers. It addresses the challenges posed by big data by providing scalability, fault tolerance, and high-throughput access to application data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Purpose of Apache Hadoop}
    \begin{itemize}
        \item \textbf{Distributed Computing}: Hadoop enables the processing of vast amounts of data by distributing tasks across multiple machines in a cluster, reducing the time required for data analysis.
        \item \textbf{Data Storage}: Using a distributed file system, Hadoop manages large datasets across multiple servers for efficient access.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Apache Hadoop}
    \begin{enumerate}
        \item \textbf{Hadoop Common}: Common utilities and libraries that support other Hadoop modules.
        \item \textbf{HDFS (Hadoop Distributed File System)}: The storage layer that allows data to be stored across multiple machines, providing redundancy and reliability.
            \begin{itemize}
                \item \textit{Example}: If one node fails, the data can still be accessed because it is replicated on other nodes.
            \end{itemize}
        \item \textbf{MapReduce}: A programming model and processing engine that allows complex data processing tasks to be executed in parallel.
            \begin{itemize}
                \item \textit{Example}: Analyzing log files by mapping the data (key-value pairs) and reducing results to summarize counts or findings.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Scalability}: Users can add more nodes as data grows, allowing horizontal scaling.
        \item \textbf{Fault Tolerance}: Data and tasks are replicated, ensuring reliability during hardware failures.
        \item \textbf{Cost-Effective}: Utilizes commodity hardware for clusters, making Hadoop an economical solution for big data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    Imagine a university that wants to analyze student performance across thousands of courses and millions of grades. Instead of using a single server, Apache Hadoop allows data to be spread out over several machines. Each machine processes a portion of the data independently, then combines the results, enabling quicker insights and reducing the load on any one server.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Word Count Example}
    Hereâ€™s a simple example of a MapReduce job in Python that counts the number of occurrences of each word in a text file:
    \begin{lstlisting}[language=Python]
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")

text_file = sc.textFile("hdfs:///path/to/textfile.txt")
counts = text_file.flatMap(lambda line: line.split(" ")) \
                 .map(lambda word: (word, 1)) \
                 .reduceByKey(lambda a, b: a + b)

counts.saveAsTextFile("hdfs:///path/to/output")
    \end{lstlisting}
    This snippet reads a text file from HDFS, counts occurrences of each word, and saves the result to HDFS.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Apache Hadoop is a powerful tool for organizations seeking to harness the capabilities of big data. Its robust architecture and capabilities allow for efficient data storage and processing, making it an industry-leading option for handling large datasets.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Architecture - Overview}
    \begin{block}{Overview}
        Apache Hadoop is designed for the distributed storage and processing of large datasets across clusters of computers.
        It provides high availability and fault tolerance.
    \end{block}
    \begin{block}{Key Subsystems}
        The architecture consists of two major subsystems:
        \begin{itemize}
            \item Hadoop Distributed File System (HDFS)
            \item MapReduce
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Architecture - Key Components}
    \begin{enumerate}
        \item \textbf{Hadoop Distributed File System (HDFS)}
        \begin{itemize}
            \item \textbf{Purpose:} Storage layer for very large files across multiple machines.
            \item \textbf{Structure:}
            \begin{itemize}
                \item Block Storage: Default block size is 128 MB.
                \item Data Redundancy: Each block is replicated 3 times.
            \end{itemize}
            \item \textbf{Architecture:}
            \begin{itemize}
                \item NameNode: Manages metadata and regulates file access.
                \item DataNodes: Store actual data blocks and report status to NameNode.
            \end{itemize}
        \end{itemize}

        \item \textbf{MapReduce}
        \begin{itemize}
            \item \textbf{Purpose:} Processing layer for parallel processing of large datasets.
            \item \textbf{Job Execution:}
            \begin{itemize}
                \item Map Phase: Input data divided into smaller sub-problems.
                \item Shuffle \& Sort Phase: Intermediate outputs sorted for the reduce phase.
                \item Reduce Phase: Merges processed data to produce final output.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Architecture - Key Points}
    \begin{itemize}
        \item \textbf{Scalability:} Hadoop clusters can easily scale by adding more nodes.
        \item \textbf{Fault Tolerance:} Data replication and redundancy ensure data is not lost.
        \item \textbf{Cost-Effective:} Built on commodity hardware to reduce processing costs.
    \end{itemize}
    \begin{block}{Summary Diagram}
    \centering
    \includegraphics[width=0.8\linewidth]{hadoop_diagram.png}
    % Replace with actual diagram later
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Components of Hadoop - Overview}
    \begin{block}{Introduction to YARN}
        YARN (Yet Another Resource Negotiator) enhances Hadoop's resource management.
        It allows multiple data processing engines to utilize data on a single platform, improving overall efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Functions of YARN}
    \begin{enumerate}
        \item \textbf{Resource Management:} Dynamically allocates resources based on application needs, enhancing scalability and efficiency.
        \item \textbf{Job Scheduling:} Supports various policies for efficient queue management and job prioritization.
        \item \textbf{Monitoring:} Monitors resource consumption and job performance for better allocation and troubleshooting.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Components of YARN}
    \begin{itemize}
        \item \textbf{ResourceManager (RM):} Master daemon managing cluster resources and allocation.
        \item \textbf{NodeManager (NM):} Slave daemon on each node, managing resources and reporting status to RM.
        \item \textbf{ApplicationMaster (AM):} Application-specific master negotiating resources and coordinating with NMs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How YARN Contributes to Hadoop's Functionality}
    \begin{itemize}
        \item \textbf{Multi-tenancy:} Supports various data processing frameworks sharing the same resources.
        \item \textbf{Increased Scalability:} Seamlessly scales resources as demand increases.
        \item \textbf{Improved Resource Utilization:} Optimizes resource usage, enhancing performance while lowering costs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario}
    Imagine a data processing cluster with diverse loads:
    \begin{itemize}
        \item Real-time stream processing and batch processing running simultaneously.
        \item YARN dynamically allocates resources based on real-time requirements, preventing resource monopolization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{YARN Architecture Diagram}
    \begin{center}
        \texttt{
        [ResourceManager] \\
                |                       | \\
             ------                ------ \\
             |    |                 |    | \\
        [NodeManager]       [NodeManager] \\
        [NodeManager]       [NodeManager]
        }
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    YARN:
    \begin{itemize}
        \item Central to resource management in Hadoop.
        \item Enables multiple applications to run simultaneously.
        \item Separates compute from storage for efficient data processing.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Hadoop Ecosystem}
    \begin{block}{Overview}
        The Hadoop Ecosystem consists of various components and tools that enhance data processing and analytics capabilities. At its core are:
        \begin{itemize}
            \item HDFS (Hadoop Distributed File System)
            \item YARN (Yet Another Resource Negotiator)
        \end{itemize}
        Several other tools integrate seamlessly with Hadoop to optimize its functionality for big data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of the Hadoop Ecosystem}
    \begin{enumerate}
        \item \textbf{HDFS}
            \begin{itemize}
                \item Primary storage for large data sets.
                \item Data is stored in blocks across multiple nodes for fault tolerance.
            \end{itemize}
        \item \textbf{YARN}
            \begin{itemize}
                \item Resource management layer for applications in a Hadoop cluster.
                \item Enables multiple processing engines on a single platform.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Ecosystem - Key Tools A}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Apache Hive}
            \begin{itemize}
                \item Data warehousing software with HiveQL language.
                \item Example:
                \begin{lstlisting}[language=SQL]
SELECT product_id, SUM(sales_amount)
FROM sales
GROUP BY product_id;
                \end{lstlisting}
            \end{itemize}
        \item \textbf{Apache Pig}
            \begin{itemize}
                \item High-level platform using Pig Latin for processing data.
                \item Example:
                \begin{lstlisting}[language=Pig]
data = LOAD 'sales_data' USING PigStorage(',') AS (product_id:int, sales_amount:float);
filtered_data = FILTER data BY sales_amount > 100;
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hadoop Ecosystem - Key Tools B}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Apache HBase}
            \begin{itemize}
                \item Distributed NoSQL database on top of HDFS.
                \item Real-time read/write access with a flexible data model.
            \end{itemize}
        \item \textbf{Apache Spark}
            \begin{itemize}
                \item Engine for large-scale data processing with high-level APIs.
                \item Focuses on in-memory processing for improved speed.
            \end{itemize}
        \item \textbf{Apache Flume}
            \begin{itemize}
                \item Service for collecting and moving log data to HDFS/HBase.
            \end{itemize}
        \item \textbf{Apache ZooKeeper}
            \begin{itemize}
                \item Service for maintaining configuration and distributed synchronization.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Integration and Scalability:} Tools designed to leverage Hadoop's scalability for efficient data processing.
        \item \textbf{Data Processing Workflow:} Understanding component interaction is crucial for effective workflows.
        \item \textbf{Skill Versatility:} Users can select tools based on needs, e.g., SQL-like (Hive) for analytics or NoSQL (HBase) for fast lookups.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Prerequisites}
    % Overview of the necessary system requirements and prerequisites for installing Hadoop.
    Before diving into the installation of Apache Hadoop, it is crucial to ensure that your system meets specific prerequisites.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Hadoop Installation Prerequisites}
    \begin{block}{Key Highlights}
        \begin{itemize}
            \item Facilitates smoother installation
            \item Ensures optimal performance of Hadoop
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Hardware Requirements}
    \begin{itemize}
        \item \textbf{General Requirements:}
            \begin{itemize}
                \item \textbf{CPU:} Multi-core processor (4 cores recommended)
                \item \textbf{RAM:} Minimum 8 GB; 16 GB or more preferable
                \item \textbf{Disk Space:} At least 100 GB free per node
            \end{itemize}
        \item \textbf{Example Configuration for a Single Node:}
            \begin{itemize}
                \item \textbf{Processor:} Intel Core i7
                \item \textbf{RAM:} 16 GB
                \item \textbf{Storage:} 500 GB HDD/SSD
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Operating System}
    \begin{itemize}
        \item \textbf{Supported OS:}
            \begin{itemize}
                \item Primarily Unix/Linux systems (e.g., Ubuntu, CentOS)
                \item Windows supported with different configurations
            \end{itemize}
        \item \textbf{Recommended Version:}
            \begin{itemize}
                \item Use an up-to-date version (e.g., Ubuntu 20.04 or later)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Java Development Kit (JDK)}
    \begin{itemize}
        \item \textbf{Requirement:} 
            \begin{itemize}
                \item JDK 8 or later is essential for executing Java programs.
            \end{itemize}
        \item \textbf{Installation Example:}
            \begin{lstlisting}
sudo apt-get install openjdk-11-jdk
            \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. SSH Client}
    \begin{itemize}
        \item \textbf{Purpose:} 
            \begin{itemize}
                \item Required for managing and communicating between nodes.
            \end{itemize}
        \item \textbf{Installation:}
            \begin{lstlisting}
sudo apt-get install openssh-server
            \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{5. Network Configuration}
    \begin{itemize}
        \item \textbf{IP Configuration:}
            \begin{itemize}
                \item Each node must have a unique IP address.
                \item Use a static IP to avoid changes upon reboot.
            \end{itemize}
        \item \textbf{Example Configuration:}
            \begin{itemize}
                \item Ensure proper hostname and routing configuration.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Critical to meet prerequisites for single-node and multi-node setups.
        \item Always verify installed software versions and system configurations.
        \item Insufficient hardware or software may lead to performance issues.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By ensuring that you meet these prerequisites, you will lay a solid foundation for a successful Hadoop installation. 
    In the next slide, we will discuss the step-by-step process of setting up Hadoop to harness its powerful data processing capabilities.
\end{frame}

\begin{frame}
    \frametitle{Setting Up Hadoop}
    \begin{block}{Overview}
        This guide provides a step-by-step process for installing and configuring Apache Hadoop in local or cloud environments. 
        Key steps include downloading Hadoop, installing Java, setting environment variables, and starting services.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 1 - Download Hadoop}
    \begin{itemize}
        \item Visit the official Apache Hadoop website: 
        \texttt{https://hadoop.apache.org/releases.html}
        \item Choose the appropriate version:
        \begin{itemize}
            \item Download the binary distribution for Windows, macOS, or Linux.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps 2-4 - Install Java and Extract Hadoop}
    \begin{enumerate}
        \item \textbf{Install Java}
        \begin{itemize}
            \item Hadoop requires JDK version 8 or higher.
            \item Verify installation:
            \begin{lstlisting}
            java -version
            \end{lstlisting}
            \item Set \texttt{JAVA\_HOME} in your environment:
            \begin{lstlisting}
            export JAVA_HOME=/path/to/your/jdk
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Extract Hadoop Files}
        \begin{itemize}
            \item Unzip the downloaded Hadoop archive:
            \begin{lstlisting}
            tar -xzf hadoop-x.y.z.tar.gz
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Configure Hadoop Environment Variables}
        \begin{itemize}
            \item Edit your \texttt{.bashrc} or \texttt{.bash\_profile}:
            \begin{lstlisting}
            export HADOOP_HOME=/path/to/hadoop
            export PATH=$PATH:$HADOOP_HOME/bin
            export PATH=$PATH:$HADOOP_HOME/sbin
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps 5-8 - Configuration and Services}
    \begin{enumerate}
        \item \textbf{Edit Configuration Files}
        \begin{itemize}
            \item Example for \texttt{core-site.xml}:
            \begin{lstlisting}
            <configuration>
              <property>
                <name>fs.defaultFS</name>
                <value>hdfs://localhost:9000</value>
              </property>
            </configuration>
            \end{lstlisting}
            \item Similar configurations for \texttt{hdfs-site.xml}, \texttt{mapred-site.xml}, and \texttt{yarn-site.xml}.
        \end{itemize}
        
        \item \textbf{Format HDFS}
        \begin{itemize}
            \item Before starting Hadoop, format the distributed file system:
             \begin{lstlisting}
            hdfs namenode -format
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Start Hadoop Services}
        \begin{itemize}
            \item Use the following commands:
            \begin{lstlisting}
            start-dfs.sh
            start-yarn.sh
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Verify Installation}
        \begin{itemize}
            \item Access the web interface:
            \begin{itemize}
                \item NameNode: \texttt{http://localhost:9870}
                \item ResourceManager: \texttt{http://localhost:8088}
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Testing Hadoop Installation - Introduction}
    \begin{block}{Overview}
        Once you've set up Apache Hadoop, it's crucial to ensure that the installation is successful and functional. 
        This presentation guides you through the steps to verify your Hadoop installation by running sample jobs and checking system performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Testing Hadoop Installation - Step 1: Verify Installation}
    To confirm that Hadoop has been installed correctly, use the following command in your terminal or command prompt:

    \begin{lstlisting}
    hadoop version
    \end{lstlisting}

    \textbf{Expected Output:} 
    This command should display the version of Hadoop installed, along with other configuration details. If you see this output, it means Hadoop is functioning correctly.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Testing Hadoop Installation - Step 2: Start Hadoop Services}
    Before running any sample jobs, all necessary Hadoop services must be running. Use the following commands:

    \begin{lstlisting}
    start-dfs.sh
    start-yarn.sh
    \end{lstlisting}

    \textbf{Key Points:}
    \begin{itemize}
        \item \texttt{start-dfs.sh} starts the Distributed File System (HDFS).
        \item \texttt{start-yarn.sh} starts the Yet Another Resource Negotiator (YARN) services.
    \end{itemize}

    You can check the running services using:

    \begin{lstlisting}
    jps
    \end{lstlisting}

    \textbf{Expected Output:} 
    This command should display processes such as NameNode, DataNode, ResourceManager, and NodeManager. If these services are listed, your Hadoop cluster is ready.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Testing Hadoop Installation - Step 3: Running Sample Jobs}
    Hadoop provides sample jobs that can be executed to verify correct functionality. One common task is counting words in a text file.

    \textbf{Example: Word Count Program}
    \begin{enumerate}
        \item \textbf{Generate test input data:}
        Create a sample text file:
        \begin{lstlisting}
        echo "Hello Hadoop" > input.txt
        \end{lstlisting}

        \item \textbf{Put the input file into HDFS:}
        \begin{lstlisting}
        hadoop fs -put input.txt /input
        \end{lstlisting}

        \item \textbf{Run the Word Count job:}
        \begin{lstlisting}
        hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples*.jar wordcount /input/input.txt /output
        \end{lstlisting}

        \item \textbf{Retrieve the output:}
        \begin{lstlisting}
        hadoop fs -cat /output/part-r-00000
        \end{lstlisting}
    \end{enumerate}

    \textbf{Expected Output:}
    This should return the word counts:
    \begin{lstlisting}
    Hello 1
    Hadoop 1
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Testing Hadoop Installation - Step 4: Check HDFS Health}
    To ensure that your HDFS is functioning properly, check its health status with the following command:

    \begin{lstlisting}
    hdfs dfsadmin -report
    \end{lstlisting}

    \textbf{Key Points:}
    \begin{itemize}
        \item This command provides information about the cluster, including status, number of live nodes, and total capacity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Testing Hadoop Installation - Conclusion}
    By following these steps, you can confirm that your Hadoop installation is successful. Running sample jobs not only checks installation but also familiarizes you with Hadoop's user commands. 

    \textbf{Next Topic:} Stay tuned for the "Common Issues and Troubleshooting" slide, where we will cover frequent challenges and their solutions.
\end{frame}

\begin{frame}
    \frametitle{Testing Hadoop Installation - Key Points to Remember}
    \begin{itemize}
        \item Run \texttt{hadoop version} to verify installation.
        \item Start HDFS and YARN services before running jobs.
        \item Use sample jobs like Word Count to ensure proper functionality.
        \item Regularly check HDFS health for a well-maintained system.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues and Troubleshooting - Overview}
    \begin{block}{Overview}
        Setting up Apache Hadoop can come with its challenges. 
        Understanding common issues and their solutions is crucial 
        to ensure a smooth deployment and effective operation of 
        your Hadoop ecosystem. This guide outlines prevalent 
        problems, their causes, and troubleshooting steps.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues}
    \begin{enumerate}
        \item \textbf{Java Version Compatibility}
            \begin{itemize}
                \item \textbf{Description:} Hadoop requires Java to function properly. 
                Using an incompatible version can lead to startup failures.
                \item \textbf{Solution:} Ensure that you have the correct Java version installed.
                \begin{lstlisting}
java -version
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Configuration Errors}
            \begin{itemize}
                \item \textbf{Description:} Misconfigurations in core-site.xml, 
                hdfs-site.xml, or mapred-site.xml can cause failures in communication.
                \item \textbf{Solution:} Double-check configuration files for typos and correct properties.
                \begin{lstlisting}
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
</property>
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Resource Allocation Issues}
            \begin{itemize}
                \item \textbf{Description:} Insufficient memory or CPU resources 
                can lead to task failures.
                \item \textbf{Solution:} Monitor your resource allocation using management tools.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Issues (Continued)}
    \begin{enumerate}
        \setcounter{enumi}{3} % To continue numbering from the previous frame
        \item \textbf{Firewall and Network Problems}
            \begin{itemize}
                \item \textbf{Description:} Firewall settings may block required ports.
                \item \textbf{Solution:} Ensure necessary ports are open.
                \begin{lstlisting}
sudo iptables -A INPUT -p tcp --dport 50070 -j ACCEPT
                \end{lstlisting}
            \end{itemize}

        \item \textbf{HDFS Issues}
            \begin{itemize}
                \item \textbf{Description:} Problems like "DataNode not found" or "under-replicated blocks".
                \item \textbf{Solution:} Check DataNode logs and health of the filesystem.
                \begin{lstlisting}
hdfs fsck /
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Key Points to Remember}
            \begin{itemize}
                \item Verify Java compatibility with Hadoop versions.
                \item Double-check configurations for errors.
                \item Monitor resource utilization effectively.
                \item Ensure seamless communication between nodes.
                \item Inspect logs for insights into issues.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Addressing common issues in Hadoop installation and setup is essential 
    for smooth operations. Familiarize yourself with these pitfalls and 
    their remedies to ensure a robust Hadoop environment. By understanding 
    these troubleshooting steps, you will empower yourself to manage 
    and optimize your Hadoop cluster effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps - Key Takeaways}
    \begin{enumerate}
        \item \textbf{What is Apache Hadoop?}
        \begin{itemize}
            \item Open-source framework for distributed storage and processing of large datasets.
            \item Enables efficient and cost-effective handling of vast amounts of data.
        \end{itemize}
        
        \item \textbf{Core Components of Hadoop:}
        \begin{itemize}
            \item \textbf{HDFS}: Distributed file storage that splits large files into blocks for fault tolerance.
            \item \textbf{MapReduce}: Programming model for parallel processing with Map and Reduce tasks.
            \item \textbf{YARN}: Resource manager that schedules resources across the cluster.
        \end{itemize}
        
        \item \textbf{Benefits of Using Hadoop:}
        \begin{itemize}
            \item Scalability from a single server to thousands.
            \item Cost-effectiveness using commodity hardware.
            \item Flexibility for different data types: structured, semi-structured, and unstructured.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps - Upcoming Topics}
    In the upcoming weeks, we will cover the following topics:
    
    \begin{enumerate}
        \item \textbf{Hadoop Architecture:}
        \begin{itemize}
            \item In-depth understanding of NameNode, DataNode, and secondary NameNode roles.
        \end{itemize}
        
        \item \textbf{Data Ingestion Techniques:}
        \begin{itemize}
            \item Tools for moving data into HDFS, such as Apache Flume and Apache Sqoop.
        \end{itemize}
        
        \item \textbf{Processing Data with MapReduce:}
        \begin{itemize}
            \item Hands-on opportunities to write and execute MapReduce jobs.
        \end{itemize}
        
        \item \textbf{Hadoop Ecosystem:}
        \begin{itemize}
            \item Exploring tools like Apache Hive, Apache Pig, and Apache HBase.
        \end{itemize}
        
        \item \textbf{Best Practices and Optimization:}
        \begin{itemize}
            \item Strategies for optimizing cluster performance and data storage techniques.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps - Summary and Closing Note}
    \begin{block}{Summary}
        Understanding the core components of Apache Hadoop forms a strong foundation for big data processing. 
        Engage in hands-on exercises and group discussions to enhance your learning.
    \end{block}

    \begin{block}{Closing Note}
        Prepare for a deep dive into Hadoop! Review installation procedures and troubleshooting techniques discussed. 
        Feel free to reach out with questions as we embark on this exciting journey into big data!
    \end{block}
\end{frame}


\end{document}