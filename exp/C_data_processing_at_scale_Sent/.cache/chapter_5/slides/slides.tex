\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 5: Data Processing with Spark}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Processing with Spark}
    \begin{block}{Overview of Data Processing}
        Data processing transforms raw data into meaningful information critical for various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data Processing}
    \begin{itemize}
        \item \textbf{Extract Insights:} Uncover hidden trends and patterns.
        \item \textbf{Make Informed Decisions:} Base business strategies on data-driven evidence.
        \item \textbf{Enhance Efficiency:} Automate repetitive tasks and streamline operations.
    \end{itemize}
    \begin{block}{Example}
        A retail company analyzes sales data to identify peak buying seasons, allowing for better inventory management.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Role of Spark in Data Processing}
    \begin{itemize}
        \item \textbf{Speed:} In-memory data processing is faster than traditional disk-based methods.
        \item \textbf{Ease of Use:} APIs in Java, Scala, Python, and R allow easy application development.
        \item \textbf{Versatility:} Supports SQL queries, streaming data, machine learning, and graph processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Spark}
    \begin{itemize}
        \item \textbf{Fault Tolerance:} Automatic recovery from failures ensuring data integrity.
        \item \textbf{Scalability:} Efficiently scales from a single server to thousands of machines.
        \item \textbf{Unified Engine:} Handles various data types, from batch to streaming.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Flow in Spark}
    \begin{block}{Illustration}
        \begin{center}
            Data Sources $\rightarrow$ Spark Processing Engine $\rightarrow$ Data Storage \\
            (like HDFS, S3)
        \end{center}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Data processing is essential for extracting actionable intelligence from raw data.
        \item Apache Spark efficiently manages large datasets while providing flexibility and speed.
        \item Understanding Spark’s capabilities lays the groundwork for mastering data processing techniques.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    As we dive deeper into Spark, we will explore its key components, starting with Resilient Distributed Datasets (RDDs). RDDs are foundational to Spark’s operation, enabling robust data manipulation across distributed systems.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding RDDs - Part 1}

    \textbf{What are Resilient Distributed Datasets (RDDs)?}
    \begin{itemize}
        \item \textbf{Definition}: 
        RDDs are fundamental data structures in Apache Spark, enabling distributed data processing. They are a collection of objects that can be processed in parallel across a cluster.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding RDDs - Part 2}

    \textbf{Importance of RDDs}
    \begin{itemize}
        \item \textbf{Fault Tolerance}: 
        RDDs automatically recover lost data due to node failures using lineage information, which tracks the operations used to create them.
        
        \item \textbf{Efficiency}: 
        RDDs optimize data processing by allowing in-memory computation, reducing the need for disk I/O.
        
        \item \textbf{Immutable}:
        Once created, RDDs cannot be modified. Any transformation creates a new RDD, ensuring data integrity during processing.
        
        \item \textbf{Lazy Evaluation}:
        RDD operations are not executed immediately. Instead, Spark builds a logical plan and executes it once an action is called, optimizing tasks for performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding RDDs - Part 3}

    \textbf{How RDDs Work in Spark}
    
    \textbf{Creation}
    \begin{itemize}
        \item \textbf{From Existing Datasets}:
        \begin{lstlisting}[language=Python]
rdd = spark.textFile("hdfs:///path/to/file.txt")
        \end{lstlisting}
        
        \item \textbf{From Parallelized Collections}:
        \begin{lstlisting}[language=Python]
data = [1, 2, 3, 4, 5]
rdd = spark.sparkContext.parallelize(data)
        \end{lstlisting}
    \end{itemize}

    \textbf{Transformations and Actions}
    \begin{itemize}
        \item \textbf{Transformations}: Produce a new RDD; examples include \texttt{map()}, \texttt{filter()}, \texttt{flatMap()}.
        \begin{lstlisting}[language=Python]
squares_rdd = rdd.map(lambda x: x ** 2)
        \end{lstlisting}

        \item \textbf{Actions}: Trigger computation of the RDD; examples include \texttt{count()}, \texttt{collect()}, \texttt{reduce()}.
        \begin{lstlisting}[language=Python]
result = squares_rdd.collect()  # Returns all elements of squares_rdd as a list
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames Overview}
    \begin{block}{Introduction to DataFrames}
        DataFrames in Apache Spark are distributed collections of data organized into named columns.
        They are similar to tables in a relational database or DataFrames in Pandas.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames vs. RDDs}
    \begin{itemize}
        \item \textbf{RDDs (Resilient Distributed Datasets):}
        \begin{itemize}
            \item Fundamental data structure in Spark for distributed data processing.
            \item Unstructured and no enforced schema leading to more boilerplate code.
            \item Low-level operations make data processing more complex.
        \end{itemize}
        
        \item \textbf{DataFrames:}
        \begin{itemize}
            \item Higher-level API that abstracts RDD complexities.
            \item SQL-like interface simplifies data manipulation and analysis.
            \item Automatically optimized execution via the Catalyst optimizer enhances performance.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of DataFrames}
    \begin{enumerate}
        \item \textbf{Schema Support:}
        \begin{itemize}
            \item Provides metadata for better data structure comprehension.
        \end{itemize}
        
        \item \textbf{Optimized Execution Plans:}
        \begin{itemize}
            \item Catalyst optimizer helps plan query execution, enhancing performance.
        \end{itemize}
        
        \item \textbf{Interoperability:}
        \begin{itemize}
            \item Easy conversion to/from RDDs, allowing use of both APIs.
        \end{itemize}
        
        \item \textbf{Integration with SQL:}
        \begin{itemize}
            \item Execute SQL queries directly on DataFrames using the \texttt{spark.sql()} function.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Creating a DataFrame}
    \begin{block}{Python Code Snippet}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()

# Load data into a DataFrame
df = spark.read.json("path/to/employees.json")

# Show the DataFrame
df.show()
        \end{lstlisting}
    \end{block}
    \begin{itemize}
        \item This script initializes a Spark session, reads a JSON file, and displays its contents in a structured format.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item DataFrames simplify interaction with structured data in Spark.
        \item They offer advantages over RDDs regarding ease of use and performance optimization.
        \item Mastering DataFrame creation and manipulation is crucial for efficient data processing in Spark.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Working with Datasets - Overview}
    \begin{itemize}
        \item \textbf{Definition:} A Dataset is a distributed collection of data that is strongly typed, providing type-safety at compile time.
        \item \textbf{Characteristics:} 
        \begin{itemize}
            \item Supports both functional and relational operations.
            \item Can be constructed from RDDs and optimized using Catalyst query optimization.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Working with Datasets - Advantages}
    \begin{enumerate}
        \item \textbf{Type Safety:}
        \begin{itemize}
            \item Compile-time type checking reduces errors.
            \item Example:
            \begin{lstlisting}[language=Scala]
case class Employee(id: Int, name: String)
val ds = spark.createDataset(Seq(Employee(1, "John"), Employee(2, "Jane")))
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Optimized Performance:}
            \begin{itemize}
                \item Leveraging Catalyst Optimizer for better execution plans.
                \item Utilizes off-heap storage for faster processing.
            \end{itemize}
            
        \item \textbf{Interoperability:}
            \begin{itemize}
                \item Easy conversion between Datasets and DataFrames.
                \item Allows complex aggregation and queries while preserving type safety.
            \end{itemize}

        \item \textbf{Rich API:}
            \begin{itemize}
                \item Provides both typed transformations and SQL-like operations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Working with Datasets - When to Use}
    \begin{itemize}
        \item \textbf{Use Datasets when:}
        \begin{itemize}
            \item Compile-time type safety is required.
            \item There's a need for combining functional and relational programming paradigms.
            \item Dealing with complex data types demanding type-specific operations.
        \end{itemize}
        
        \item \textbf{Use DataFrames when:}
        \begin{itemize}
            \item Working with unstructured or semi-structured data without the need for type safety.
            \item Focusing primarily on SQL-like queries and transformations.
        \end{itemize}
        
        \item \textbf{Use RDDs when:}
        \begin{itemize}
            \item Handling unstructured data that doesn't fit well into a schema.
            \item Requiring low-level transformations and actions.
            \item Needing fine-grained control over data processing.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Spark SQL - What is Spark SQL?}
    Spark SQL is a module within Apache Spark designed to work with structured data. It offers:
    \begin{itemize}
        \item A programming interface for working with DataFrames.
        \item The ability to execute SQL queries alongside data processing tasks in Spark.
        \item Integration of relational data processing with Spark's functional programming capabilities.
    \end{itemize}
    This makes Spark SQL a powerful tool for data analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Spark SQL - Purpose}
    The purpose of Spark SQL includes:
    \begin{itemize}
        \item \textbf{SQL Support}: Provides a familiar SQL interface for querying structured and semi-structured data.
        \item \textbf{Unified Data Processing}: Combines SQL queries, DataFrames, and datasets to leverage both SQL and Spark’s capabilities.
        \item \textbf{Optimized Query Execution}: Utilizes the Catalyst optimizer for efficient SQL query execution.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Spark SQL - Key Features}
    Key features of Spark SQL include:
    \begin{itemize}
        \item \textbf{DataFrame API}: Optimized distributed collections of data organized into named columns.
        \item \textbf{Seamless Integration}: Execute SQL queries alongside DataFrame operations efficiently.
        \item \textbf{Support for Various Data Sources}: Can read data from sources like Hive, Avro, Parquet, ORC, JSON, and JDBC.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Spark SQL - Example}
    Here is an example of using Spark SQL:
    \begin{lstlisting}[language=Python]
# Start a Spark Session
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Spark SQL Example") \
    .getOrCreate()

# Create a DataFrame
data = [('Alice', 1), ('Bob', 2), ('Cathy', 3)]
columns = ["Name", "Id"]
df = spark.createDataFrame(data, columns)

# Register the DataFrame as a temporary view
df.createOrReplaceTempView("people")

# Perform SQL query
result = spark.sql("SELECT Name FROM people WHERE Id > 1")
result.show()
    \end{lstlisting}

    \textbf{Output:}
    \begin{verbatim}
+-----+
| Name|
+-----+
|  Bob|
|Cathy|
+-----+
    \end{verbatim}
    This example creates a DataFrame, registers it as a temporary view, and executes a SQL query to filter records.
\end{frame}

\begin{frame}
    \frametitle{Key Differences between RDDs, DataFrames, and Datasets}
    % Overview of Spark's Data Structures
    Apache Spark provides three main abstractions for working with structured and semi-structured data: 
    \begin{itemize}
        \item \textbf{Resilient Distributed Datasets (RDDs)}
        \item \textbf{DataFrames}
        \item \textbf{Datasets}
    \end{itemize}
    Each abstraction has its own strengths, weaknesses, and use cases.
\end{frame}

\begin{frame}[fragile]
    \frametitle{RDDs}
    \begin{block}{Definition}
        An RDD is a fundamental data structure in Spark, representing a distributed collection of objects that can be processed in parallel.
    \end{block}

    \begin{itemize}
        \item \textbf{Usability:}
        \begin{itemize}
            \item Low-level abstraction, full control over data manipulation.
            \item More boilerplate code required compared to DataFrames and Datasets.
        \end{itemize}
        \item \textbf{Performance:}
        \begin{itemize}
            \item Not optimized; execution plans are not utilized.
            \item Programmers must manage their own partitioning and caching.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Example}
    \begin{lstlisting}[language=Python]
rdd = spark.parallelize([1, 2, 3, 4])
result = rdd.map(lambda x: x * 2).collect()  # Output: [2, 4, 6, 8]
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames and Datasets}
    \begin{block}{DataFrames}
        A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database.
    \end{block}

    \begin{itemize}
        \item \textbf{Usability:}
        \begin{itemize}
            \item Higher-level abstraction, easier to use with SQL-like syntax.
            \item Supports various data types and operations.
        \end{itemize}
        \item \textbf{Performance:}
        \begin{itemize}
            \item Automatically optimizes execution plans using the Catalyst optimizer.
            \item More efficient memory use compared to RDDs.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Example}
    \begin{lstlisting}[language=Python]
df = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
df.filter(df.id > 1).show()  # Output: +---+-----+
                                 #          | id| name|
                                 #          +---+-----+
                                 #          |  2|  Bob|
                                 #          +---+-----+
    \end{lstlisting}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Datasets}
    \begin{block}{Definition}
        A Dataset combines the benefits of RDDs and DataFrames, providing strong typing and compile-time type safety.
    \end{block}

    \begin{itemize}
        \item \textbf{Usability:}
        \begin{itemize}
            \item Combines the ease of use of DataFrames with the type safety of RDDs.
            \item Supports functional and SQL expressions.
        \end{itemize}
        \item \textbf{Performance:}
        \begin{itemize}
            \item Leverages optimizations as DataFrames.
            \item Benefits from object-oriented programming and compile-time type checking.
        \end{itemize}
    \end{itemize}

    \begin{block}{Example}
    \begin{lstlisting}[language=Scala]
val ds = Seq((1, "Alice"), (2, "Bob")).toDS
ds.filter($"_1" > 1).show()   // Output: +---+----+
                                  //          | _1|  _2|
                                  //          +---+----+
                                  //          |  2| Bob|
                                  //          +---+----+
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Operations}
    \begin{block}{Introduction to Data Transformation in Spark}
        In Apache Spark, data is processed using transformation operations that allow users to manipulate and reshape their datasets seamlessly. Transformations create a new dataset from an existing one and are executed lazily, meaning that computation occurs only when an action is triggered.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Transformation Operations - Map}
    \begin{itemize}
        \item \textbf{Map}
            \begin{itemize}
                \item \textbf{Definition}: The \texttt{map()} function applies a specified function to each element of the RDD, resulting in a new RDD containing the transformed elements.
                \item \textbf{Example}:
                    \begin{lstlisting}[language=Python]
rdd = sc.parallelize([1, 2, 3, 4])
squared_rdd = rdd.map(lambda x: x ** 2)
print(squared_rdd.collect())  # Output: [1, 4, 9, 16]
                    \end{lstlisting}
                \item \textbf{Key Point}: \texttt{map()} transforms each item individually and does not change the number of items.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Transformation Operations - Filter and Reduce}
    \begin{itemize}
        \item \textbf{Filter}
            \begin{itemize}
                \item \textbf{Definition}: The \texttt{filter()} function returns a new RDD containing only those elements that satisfy a specified condition (predicate function).
                \item \textbf{Example}:
                    \begin{lstlisting}[language=Python]
rdd = sc.parallelize([1, 2, 3, 4, 5])
odd_rdd = rdd.filter(lambda x: x % 2 != 0)
print(odd_rdd.collect())  # Output: [1, 3, 5]
                    \end{lstlisting}
                \item \textbf{Key Point}: \texttt{filter()} reduces the size of the dataset based on the condition provided.
            \end{itemize}

        \item \textbf{Reduce}
            \begin{itemize}
                \item \textbf{Definition}: The \texttt{reduce()} operation merges the elements of an RDD using a specified commutative and associative binary function. It returns a single value.
                \item \textbf{Example}:
                    \begin{lstlisting}[language=Python]
rdd = sc.parallelize([1, 2, 3, 4])
sum_result = rdd.reduce(lambda x, y: x + y)
print(sum_result)  # Output: 10
                    \end{lstlisting}
                \item \textbf{Key Point}: \texttt{reduce()} combines all elements into one single output.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Visual Illustration}
    \begin{itemize}
        \item \textbf{Summary of Key Points}:
            \begin{itemize}
                \item Transformations in Spark create new datasets from existing ones without executing them immediately (lazy evaluation).
                \item Map applies a function to all elements, returning a dataset of the same size.
                \item Filter selects elements that meet a certain condition, potentially reducing the dataset size.
                \item Reduce combines all elements into a single result using a specified function.
            \end{itemize}

        \item \textbf{Visual Illustration}:
            \begin{itemize}
                \item A flow diagram can illustrate the transformation process:
                \begin{enumerate}
                    \item Start with an input dataset.
                    \item Apply \texttt{map()} to show transformation.
                    \item Apply \texttt{filter()} to show conditional selection.
                    \item Apply \texttt{reduce()} to demonstrate aggregation.
                \end{enumerate}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Data Actions in Spark}
    \begin{block}{Overview of Data Actions}
        In Apache Spark, actions are operations that trigger the execution of the data processing workflow. 
        While transformations define a lineage of steps, actions execute this workflow and return values to the driver program.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Types of Actions}
    \begin{enumerate}
        \item \textbf{collect()} \\
            \begin{itemize}
                \item \textbf{Description:} Retrieves all elements of the RDD/DataFrame to the driver node.
                \item \textbf{Use Case:} Useful for debugging or simple processing with small datasets.
                \item \textbf{Example Code:}
                \begin{lstlisting}[language=Python]
data = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
collected_data = data.collect()
print(collected_data)  # Output: [1, 2, 3, 4, 5]
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{count()} \\
            \begin{itemize}
                \item \textbf{Description:} Counts the number of elements in the RDD/DataFrame.
                \item \textbf{Use Case:} Ideal for understanding dataset size before heavy computations.
                \item \textbf{Example Code:}
                \begin{lstlisting}[language=Python]
data = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
num_elements = data.count()
print(num_elements)  # Output: 5
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{take(n)} \\
            \begin{itemize}
                \item \textbf{Description:} Returns the first \( n \) elements of the RDD/DataFrame as a list.
                \item \textbf{Use Case:} Useful for previewing data without loading the entire dataset.
                \item \textbf{Example Code:}
                \begin{lstlisting}[language=Python]
data = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
first_three_elements = data.take(3)
print(first_three_elements)  # Output: [1, 2, 3]
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Implications of Data Actions}
    \begin{itemize}
        \item \textbf{Execution Trigger:} Actions execute the entire data processing pipeline, whereas transformations are lazy.
        
        \item \textbf{Resource Usage:} Actions can consume significant resources, especially with large datasets. Use them judiciously.
        
        \item \textbf{Network Traffic:} Methods like \texttt{collect()} can lead to high network traffic as they retrieve data back to the driver.
    \end{itemize}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Actions execute computations and yield results; transformations define computations without immediate execution.
            \item Choose the right action: Use \texttt{count()} for size checks, \texttt{collect()} for small datasets, and \texttt{take(n)} for previews.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Performance Optimization Techniques}
    In this presentation, we will explore effective strategies to enhance performance when working with RDDs, DataFrames, and Spark SQL. Proper optimization is crucial for improving execution speed, reducing resource consumption, and increasing the overall efficiency of your Spark applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimize RDD Operations}
    \begin{itemize}
        \item \textbf{Persisting Data:} Use \texttt{cache()} or \texttt{persist()} methods to store RDDs in memory.
        \begin{lstlisting}[language=Python]
rdd = sc.textFile("data.txt").cache()  # Store the RDD in memory.
        \end{lstlisting}
        
        \item \textbf{Avoid Narrow Transformations:} Limit the conversion of narrow transformations to wide transformations unless necessary, as it leads to shuffling.
        
        \item \textbf{Combining RDDs:} Prefer \texttt{reduceByKey()} over \texttt{groupByKey()} to minimize the amount of data shuffled.
        \begin{lstlisting}[language=Python]
rdd.reduceByKey(lambda a, b: a + b)
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimize DataFrames and Spark SQL}
    \begin{enumerate}
        \item \textbf{Optimize DataFrames:}
        \begin{itemize}
            \item Use Catalyst Optimizer.
            \item Utilize columnar storage formats (Parquet, ORC).
            \begin{lstlisting}[language=Python]
df.write.parquet("output_parquet")
            \end{lstlisting}
            \item Filter early to reduce data processed.
            \item Use broadcast variables for small lookups.
            \begin{lstlisting}[language=Python]
broadcastedVar = sc.broadcast(smallLookupData)
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Optimize Spark SQL:}
        \begin{itemize}
            \item Analyze performance with \texttt{explain()}.
            \begin{lstlisting}[language=SQL]
df.explain()
            \end{lstlisting}
            \item Use partitioning and bucketing.
            \item Avoid UDFs when built-in functions suffice.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    Here’s an example that combines various techniques to optimize performance:
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Optimization Example").getOrCreate()

# Load data
df = spark.read.parquet("input_data.parquet").cache()  # Caching data for quicker access

# Filter early
filtered_df = df.filter(df['age'] > 21)

# Using DataFrame functions instead of UDF
result = filtered_df.groupBy("country").agg({"salary": "avg"})

# Show results
result.show()
    \end{lstlisting}

    This snippet demonstrates filtering, caching, and aggregation, aligned with performance optimization principles.
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Effective use of RDDs, DataFrames, and SQL in Spark, paired with the discussed techniques, can lead to significant performance improvements in data processing workflows. These strategies enhance execution speed and ensure resource efficiency.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-on Lab: Data Processing with Spark}
    
    \begin{block}{Objective}
        In this lab session, students will learn to implement data processing tasks using Apache Spark. By the end of this session, students should be able to perform:
        \begin{itemize}
            \item Data cleaning
            \item Data transformation
            \item Data analysis
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Scalability: Spark efficiently handles large datasets.
            \item Resilience: RDDs ensure fault tolerance.
            \item Ease of Use: DataFrames simplify data manipulation.
            \item Integration: Compatibility with various data sources (e.g., HDFS, S3).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Lab Overview}
    
    \begin{enumerate}
        \item \textbf{Introduction to Spark Basics}
        \begin{itemize}
            \item Recap RDDs, DataFrames, and Spark SQL.
            \item Importance of Spark for large-scale data processing.
        \end{itemize}
        
        \item \textbf{Dataset Selection}
        \begin{itemize}
            \item Choose a sample dataset (e.g., online retail or public health).
            \item Ensure dataset includes different data types and missing values.
        \end{itemize}
        
        \item \textbf{Setting Up the Spark Environment}
        \begin{itemize}
            \item Use Databricks, Jupyter Notebook, or local Spark installation.
            \item Basic setup command:
            \begin{lstlisting}[language=python]
from pyspark.sql import SparkSession
spark = SparkSession.builder \
  .appName("Data Processing Lab") \
  .getOrCreate()
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Processing Tasks}

    \begin{enumerate}
        \item \textbf{Loading Data}
        \begin{itemize}
            \item Load dataset into a Spark DataFrame:
            \begin{lstlisting}[language=python]
df = spark.read.csv("path/to/dataset.csv", header=True, inferSchema=True)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Data Exploration}
        \begin{itemize}
            \item Display first few rows:
            \begin{lstlisting}[language=python]
df.show()
            \end{lstlisting}
            \item Print schema:
            \begin{lstlisting}[language=python]
df.printSchema()
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Data Cleaning}
        \begin{itemize}
            \item Handle missing values:
            \begin{lstlisting}[language=python]
df_cleaned = df.na.fill('Unknown')  # Fill missing values
            \end{lstlisting}
            \item Remove duplicates:
            \begin{lstlisting}[language=python]
df_deduplicated = df_cleaned.dropDuplicates()
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}


\end{document}