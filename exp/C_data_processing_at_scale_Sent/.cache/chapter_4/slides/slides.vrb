\frametitle{2. Basic Operations in Spark}
    \begin{block}{Initializing Spark}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Basic Operations") \
    .getOrCreate()
        \end{lstlisting}
    \end{block}

    \begin{block}{Creating and Loading DataFrames}
        \begin{lstlisting}[language=Python]
# Create a DataFrame from a CSV file
df = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)
df.show()  # Displays the first 20 rows
        \end{lstlisting}
    \end{block}

    \begin{block}{Basic DataFrame Operations}
        \begin{itemize}
            \item \textbf{Filtering:}
            \begin{lstlisting}[language=Python]
filtered_df = df.filter(df['column_name'] > value)
filtered_df.show()
            \end{lstlisting}

            \item \textbf{Aggregating:}
            \begin{lstlisting}[language=Python]
aggregated_df = df.groupBy('column_name').count()
aggregated_df.show()
            \end{lstlisting}

            \item \textbf{Transforming:}
            \begin{lstlisting}[language=Python]
transformed_df = df.withColumn('new_column', df['existing_column'] * 2)
transformed_df.show()
            \end{lstlisting}
        \end{itemize}
    \end{block}
