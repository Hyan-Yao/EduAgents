\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 7: AI Algorithms: Basics of Supervised and Unsupervised Learning}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to AI Algorithms}
    \begin{block}{Overview of Algorithms in AI}
        An **algorithm** is a finite sequence of well-defined instructions to solve a problem or perform a task. 
    \end{block}
    \begin{itemize}
        \item Algorithms are the foundation for data processing, analysis, and decision-making in AI.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Algorithms in AI}
    \begin{itemize}
        \item \textbf{Decision Making:} Enable machines to process information and make autonomous decisions.
        \item \textbf{Pattern Recognition:} Facilitate recognition of patterns for image recognition and fraud detection.
        \item \textbf{Prediction:} Learn from historical data to forecast future outcomes in various fields.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of AI Algorithms}
    In AI, algorithms can be categorized into two primary types: 
    \begin{block}{Supervised Learning}
        \begin{itemize}
            \item \textbf{Definition:} Trained on labeled datasets where input is paired with output labels.
            \item \textbf{Key Point:} The model learns from labeled data to predict unseen data.
            \item \textbf{Examples:}
                \begin{itemize}
                    \item Spam Detection: Classifying emails as 'spam' or 'not spam'.
                    \item Image Classification: Identifying objects in images.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Unsupervised Learning}
        \begin{itemize}
            \item \textbf{Definition:} Trained on datasets without labeled responses.
            \item \textbf{Key Point:} Useful for exploratory data analysis to learn the data structure.
            \item \textbf{Examples:}
                \begin{itemize}
                    \item Clustering: Grouping customers based on buying behaviors.
                    \item Anomaly Detection: Identifying unusual patterns that indicate fraud.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Algorithms are the backbone of AI, driving intelligent systems.
        \item Understanding the difference between supervised and unsupervised learning is essential.
        \item Each learning type has distinct applications and is suitable for different types of problems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forms and Insights}
    While specific formulas are not shown here, note that:
    \begin{itemize}
        \item Supervised learning often uses cost functions (like Mean Squared Error) for training.
        \item Unsupervised learning utilizes distance metrics (like Euclidean distance) for clustering.
    \end{itemize}
    \begin{block}{In Summary}
        Algorithms are critical for AI systems to learn from data, adapt behavior, and offer predictive insights, transforming industries and enhancing decision-making processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Supervised Learning?}
    \begin{block}{Definition}
        Supervised Learning is a type of machine learning where the model is trained on a labeled dataset consisting of input-output pairs. The goal is to learn a mapping from inputs to outputs for making predictions on new, unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Supervised Learning}
    \begin{itemize}
        \item \textbf{Labeled Data:} Required datasets containing input data (features) and corresponding output (label).
        \item \textbf{Training Phase:} The algorithm learns the relationship between input features and output labels.
        \item \textbf{Prediction Phase:} The model uses the learned knowledge to make predictions on new, unlabeled data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Use Cases and Applications}
    \begin{enumerate}
        \item \textbf{Classification Tasks:} E.g., Email filtering as "spam" or "not spam."
        \item \textbf{Regression Tasks:} E.g., Predicting house prices based on various features.
        \item \textbf{Medical Diagnosis:} Classifying medical images according to known outcomes.
        \item \textbf{Customer Churn Prediction:} Predicting customer retention based on historical behavior.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    Imagine predicting whether an email is spam:
    \begin{itemize}
        \item \textbf{Input Features:} Word count, presence of links, specific keywords.
        \item \textbf{Output Label:} "Spam" or "Not Spam."
    \end{itemize}
    By training on a labeled dataset, the model learns to identify spam characteristics and can classify new emails accordingly.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulaic Perspective}
    For regression problems, consider the linear regression formula:
    \begin{equation}
        y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon 
    \end{equation}
    Where:
    \begin{itemize}
        \item $y$ is the target variable,
        \item $x_1, x_2,\ldots, x_n$ are input features,
        \item $\beta_0, \beta_1, \ldots, \beta_n$ are coefficients,
        \item $\epsilon$ is the error term.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Supervised learning relies on labeled data critical for model training.
        \item Its applications span numerous real-world scenarios, making it vital in AI.
        \item Effective supervised learning significantly enhances decision-making and automation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Supervised Learning - Overview}
    Supervised learning involves training a model on labeled data, which means the algorithm learns from input-output pairs. 
    Here, we will focus on three fundamental algorithms: 
    \begin{itemize}
        \item Linear Regression
        \item Decision Trees
        \item Support Vector Machines (SVM)
    \end{itemize}
    Each of these algorithms has its own strengths and use cases.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Supervised Learning - Linear Regression}
    
    \begin{block}{Concept}
        Linear Regression is a statistical method that models the relationship between a dependent variable ($Y$) and one or more independent variables ($X$).
    \end{block}
    
    \begin{block}{Formula}
        The equation of a line is given by:
        \begin{equation}
            Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n + \epsilon
        \end{equation}
        where:
        \begin{itemize}
            \item $Y$ = Predicted value
            \item $b_0$ = Intercept
            \item $b_i$ = Coefficients for each feature $X_i$
            \item $\epsilon$ = Error term
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Predicting house prices based on features such as size (in square feet), number of bedrooms, and year built.
    \end{block}
    
    \begin{itemize}
        \item Simple to implement and interpret
        \item Assumes a linear relationship
        \item Sensitive to outliers
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Supervised Learning - Decision Trees}
    
    \begin{block}{Concept}
        A Decision Tree is a flowchart-like structure where nodes represent features, branches represent decision rules, and each leaf node represents an outcome.
    \end{block}
    
    \begin{block}{Example}
        In a credit approval scenario, a Decision Tree might ask:
        \begin{itemize}
            \item Is the applicant's income above $50,000?
            \begin{itemize}
                \item Yes: Move to next question
                \item No: Decline
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{itemize}
        \item Easy to interpret and visualize
        \item Can handle both numerical and categorical data
        \item Prone to overfitting if not pruned
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Supervised Learning - Support Vector Machines}

    \begin{block}{Concept}
        SVM is a powerful classification algorithm that finds the hyperplane that best separates classes in feature space.
    \end{block}
    
    \begin{block}{Diagram}
        Imagine a graph where two classes are represented in different colors. The SVM algorithm identifies the line (hyperplane) that divides the two colors with the largest gap.
    \end{block}
    
    \begin{block}{Example}
        In spam detection, SVM can classify emails as “Spam” or “Not Spam” based on features like word frequency, sender information, and more.
    \end{block}

    \begin{itemize}
        \item Effective in high-dimensional spaces
        \item Robust against overfitting in high dimensions
        \item Requires more computational power and memory
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Supervised Learning - Summary}
    Understanding these key algorithms enables us to choose the right approach based on the problem context:
    \begin{itemize}
        \item Linear Regression is best for continuous predictions.
        \item Decision Trees are valued for their interpretability.
        \item Support Vector Machines are favored for complex classification tasks.
    \end{itemize}
    In the following slide, we will discuss how to evaluate the performance of these models effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics for Supervised Learning - Introduction}
    \begin{block}{Introduction}
        In supervised learning, evaluating the model's performance is crucial to ensure it makes accurate predictions. 
        This slide introduces key evaluation metrics commonly used:
        \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics for Supervised Learning - Key Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition:} Accuracy measures the proportion of true results (both true positives and true negatives) in all predictions made.
            \item \textbf{Formula:}
            \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Population}}
            \end{equation}
            \item \textbf{Example:} In a medical test, if out of 100 patients, 90 were correctly diagnosed, the accuracy would be:
            \begin{equation}
            \text{Accuracy} = \frac{90 \text{ correct predictions}}{100 \text{ total predictions}} = 0.9 \text{ or } 90\%
            \end{equation}
        \end{itemize}
        
        \item \textbf{Precision}
        \begin{itemize}
            \item \textbf{Definition:} Precision indicates the accuracy of positive predictions, showing how many of the predicted positives are actually positive.
            \item \textbf{Formula:}
            \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Example:} In spam detection, if 70 emails are classified as spam but 50 are truly spam, precision is:
            \begin{equation}
            \text{Precision} = \frac{50 \text{ true spam}}{70 \text{ predicted spam}} \approx 0.71 \text{ or } 71\%
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics for Supervised Learning - Recall and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition:} Recall measures the ability of a model to find all relevant cases (true positives) within a dataset. It answers the question: Of all actual positives, how many did we capture?
            \item \textbf{Formula:}
            \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Example:} In the same spam detection case, if there are 80 actual spam emails and only 50 were detected, recall would be:
            \begin{equation}
            \text{Recall} = \frac{50 \text{ detected spam}}{80 \text{ actual spam}} = 0.625 \text{ or } 62.5\%
            \end{equation}
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Trade-offs: High accuracy doesn’t always imply an effective model, especially in imbalanced datasets.
            \item Complementary Metrics: Precision and Recall are often more informative together (e.g., F1 Score).
            \item Use Cases: Different metrics are prioritized depending on context (medical diagnosis may value recall more).
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Evaluating your supervised learning model using these metrics provides insight into its performance, guiding you to make necessary adjustments for improved accuracy and reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Unsupervised Learning?}
    \begin{block}{Definition}
        Unsupervised learning is a class of machine learning algorithms that operates on data without labeled outputs. 
        It finds hidden patterns or intrinsic structures in data.
    \end{block}
    
    \begin{itemize}
        \item Unlike supervised learning, unsupervised learning does not require labeled data.
        \item It helps in discovering underlying relationships or distributions in data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Data without Labels:} No known outputs; e.g., a dataset with animal characteristics lacking species labels.
        \item \textbf{Exploration of Data:} Unsupervised learning is useful in scenarios where labels are absent, making it ideal for pattern discovery.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Unsupervised Learning}
    \begin{enumerate}
        \item \textbf{Clustering:}
            \begin{itemize}
                \item Groups similar data points based on features.
                \item Example: Customer segmentation in marketing.
                \item Algorithms: K-Means, Hierarchical Clustering, DBSCAN.
            \end{itemize}
        \item \textbf{Dimensionality Reduction:}
            \begin{itemize}
                \item Reduces number of features while retaining essential information.
                \item Example: Principal Component Analysis (PCA).
                \item Mathematical formula: 
                \begin{equation}
                    Z = W^T \cdot (X - \mu)
                \end{equation}
                where \( Z \) is the new representation, \( W \) the matrix of eigenvectors, \( X \) the original data, and \( \mu \) the mean of the dataset.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Conclusion}
    \begin{itemize}
        \item \textbf{No Supervision Required:} Insights drawn without predefined labels.
        \item \textbf{Powerful for Pattern Discovery:} Critical for clustering and dimensionality reduction.
        \item \textbf{Broad Applications:} Used in anomaly detection, market segmentation, and data compression.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Unsupervised learning is crucial for extracting valuable insights from unstructured data, enabling better understanding of complex datasets without requiring labels.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Unsupervised Learning - Introduction}
    \begin{block}{Overview}
        Unsupervised learning involves algorithms that analyze data without labeled outputs. Key algorithms include:
        \begin{itemize}
            \item K-Means
            \item Hierarchical Clustering
            \item Principal Component Analysis (PCA)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Unsupervised Learning - K-Means Clustering}
    \begin{block}{Concept}
        K-Means is a clustering algorithm that partitions data into K distinct groups based on feature similarity.
    \end{block}
    \begin{block}{Process}
        \begin{enumerate}
            \item Choose K initial centroids randomly.
            \item Assign each data point to the nearest centroid.
            \item Update centroids by calculating the mean of assigned points.
            \item Repeat until centroids stabilize.
        \end{enumerate}
    \end{block}
    \begin{block}{Example}
        Grouping customers based on purchasing behavior for targeted marketing.
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Choice of K is crucial; use the Elbow method to optimize.
            \item Sensitive to initial centroid placement.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Unsupervised Learning - Hierarchical Clustering \& PCA}
    \begin{block}{Hierarchical Clustering}
        \begin{itemize}
            \item \textbf{Concept}: Creates a hierarchy of clusters either via agglomerative (bottom-up) or divisive (top-down) approaches.
            \item \textbf{Process}:
                \begin{enumerate}
                    \item Agglomerative: Start with each point as its own cluster and merge.
                    \item Divisive: Start with one cluster and split iteratively.
                \end{enumerate}
            \item \textbf{Key Points}:
                \begin{itemize}
                    \item Dendrograms visualize merging.
                    \item No need to specify number of clusters.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Principal Component Analysis (PCA)}
        \begin{itemize}
            \item \textbf{Concept}: Dimensionality reduction technique that preserves variance.
            \item \textbf{Process}:
                \begin{enumerate}
                    \item Standardization of data.
                    \item Compute covariance matrix.
                    \item Calculate eigenvalues and eigenvectors.
                    \item Choose principal components.
                    \item Transform data onto new dimensions.
                \end{enumerate}
            \item \textbf{Key Points}:
                \begin{itemize}
                    \item Captures maximum variance in fewer dimensions.
                    \item Helps mitigate the curse of dimensionality.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Evaluation and Challenges in Unsupervised Learning}
    \begin{itemize}
        \item Unsupervised learning finds patterns in data without labeled responses.
        \item Evaluating results is challenging due to the lack of traditional accuracy metrics.
        \item Techniques include clustering metrics, visual methods, and stability testing.
        \item Challenges involve lack of ground truth, choice of algorithms, high dimensionality, and noise.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Understanding Unsupervised Learning Evaluation}
    \begin{block}{Clustering Evaluation Metrics}
        \begin{enumerate}
            \item \textbf{Silhouette Score}
            \begin{itemize}
                \item Measures how similar an object is to its own cluster versus others.
                \item Ranges from -1 to 1; higher values indicate better clusters.
                \item Formula:
                \begin{equation}
                S(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
                \end{equation}
            \end{itemize}
            
            \item \textbf{Davies-Bouldin Index}
            \begin{itemize}
                \item Measures the ratio of within-cluster distances to between-cluster distances.
                \item Lower values indicate better clustering.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Evaluation Techniques and Challenges}
    \begin{block}{Visual Methods for Evaluation}
        \begin{itemize}
            \item \textbf{Dimensionality Reduction:} Visualize data in 2D/3D spaces (e.g., PCA, t-SNE).
            \item \textbf{Dendrograms:} Represent arrangements in hierarchical clustering.
        \end{itemize}
    \end{block}
    
    \begin{block}{Common Challenges}
        \begin{itemize}
            \item \textbf{Lack of Ground Truth:} Determining correctness is subjective.
            \item \textbf{Choice of Algorithm and Parameters:} Different algorithms yield different results.
            \item \textbf{High Dimensionality:} Complicates clustering and distance measurements.
            \item \textbf{Noise and Outliers:} Adversely affect clustering results.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet: Calculating Silhouette Score in Python}
    \begin{lstlisting}[language=Python]
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import numpy as np

# Sample data (2D)
X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])

# Running KMeans
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
labels = kmeans.labels_

# Calculating Silhouette Score
score = silhouette_score(X, labels)
print(f'Silhouette Score: {score}')
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Clustering metrics like Silhouette Score and Davies-Bouldin Index are crucial.
        \item Visual techniques help in understanding the structure of data.
        \item Addressing challenges emphasizes the importance of model interpretation and parameter selection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Supervised and Unsupervised Learning - Introduction}
    In the field of Artificial Intelligence (AI), understanding different learning paradigms is crucial for effectively 
    applying the right techniques to data science problems. This slide compares \textbf{Supervised Learning} and 
    \textbf{Unsupervised Learning}, highlighting their key differences, applicability, and limitations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Supervised and Unsupervised Learning - Key Differences}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{block}{Supervised Learning}
                \begin{itemize}
                    \item \textbf{Definition:} Uses labeled data to train models to predict outcomes.
                    \item \textbf{Data Requirement:} Requires a dataset with input-output pairs (labeled data).
                    \item \textbf{Objective:} Predict a target variable based on given inputs.
                    \item \textbf{Common Algorithms:} Linear Regression, Decision Trees, Support Vector Machines.
                    \item \textbf{Evaluation:} Accuracy, Precision, Recall, F1 Score.
                \end{itemize}
            \end{block}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \begin{block}{Unsupervised Learning}
                \begin{itemize}
                    \item \textbf{Definition:} Analyzes input data without labeled responses to find patterns.
                    \item \textbf{Data Requirement:} Works with unlabeled data to identify structures and relationships.
                    \item \textbf{Objective:} Discover hidden patterns or group data points.
                    \item \textbf{Common Algorithms:} Clustering (e.g., K-means), Dimensionality Reduction (e.g., PCA).
                    \item \textbf{Evaluation:} Silhouette Score, Davies-Bouldin Index.
                \end{itemize}
            \end{block}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Supervised and Unsupervised Learning - Applicability and Limitations}
    \begin{block}{Applicability}
        \begin{itemize}
            \item \textbf{Supervised Learning:} Ideal for scenarios with historical data where the outcome can be clearly defined.
            \item \textbf{Unsupervised Learning:} Useful for exploring inherent patterns in large amounts of unlabeled data.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Limitations}
        \begin{itemize}
            \item \textbf{Supervised Learning:}
            \begin{itemize}
                \item Dependency on labeled data can be time-consuming and expensive to obtain.
                \item Risk of overfitting if the model learns the noise rather than the signal.
            \end{itemize}
            \item \textbf{Unsupervised Learning:}
            \begin{itemize}
                \item Results can be subjective; lack of ground truth makes interpretation challenging.
                \item May require more computational resources for very large datasets.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Supervised and Unsupervised Learning - Conclusion and Example Code}
    \begin{block}{Conclusion}
        Both supervised and unsupervised learning have strengths and weaknesses. The choice between these methods depends on the nature of the data and the goals of the analysis. Understanding these differences is critical for effectively applying AI algorithms in real-world applications.
    \end{block}
    
    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
# Supervised Learning Example
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

# Unsupervised Learning Example
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(data)
labels = kmeans.labels_
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies and Real-World Applications}
    \begin{block}{Introduction to Supervised and Unsupervised Learning}
        Supervised and unsupervised learning are foundational approaches in machine learning. These methods enable automated systems to derive insights from data, which is critical for various industries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning: Case Studies}
    \begin{enumerate}
        \item \textbf{Healthcare: Disease Prediction}
            \begin{itemize}
                \item \textbf{Application:} SVM and decision trees predict patient outcomes based on historical data.
                \item \textbf{Example:} Predicting diabetes risk using factors like age and BMI.
            \end{itemize}
        
        \item \textbf{Finance: Credit Scoring}
            \begin{itemize}
                \item \textbf{Application:} Logistic regression assesses creditworthiness.
                \item \textbf{Example:} Banks classify applicants using past borrowing data.
            \end{itemize}
        
        \item \textbf{Marketing: Customer Segmentation}
            \begin{itemize}
                \item \textbf{Application:} Regression models predict customer buying behavior.
                \item \textbf{Example:} Targeted ads based on previous purchases.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Unsupervised Learning: Case Studies}
    \begin{enumerate}
        \item \textbf{Retail: Market Basket Analysis}
            \begin{itemize}
                \item \textbf{Application:} Apriori algorithm identifies product purchase patterns.
                \item \textbf{Example:} Customers buying bread also often buy butter.
            \end{itemize}
        
        \item \textbf{Social Media: Topic Modeling}
            \begin{itemize}
                \item \textbf{Application:} LDA groups content into topics based on keywords.
                \item \textbf{Example:} Categorizing user posts to enhance engagement.
            \end{itemize}

        \item \textbf{Manufacturing: Anomaly Detection}
            \begin{itemize}
                \item \textbf{Application:} PCA detects faults in machinery.
                \item \textbf{Example:} Identifying deviations from normal operation patterns.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Supervised learning requires labeled data.
            \item Unsupervised learning explores data without labels.
            \item Both approaches significantly impact various sectors.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        The versatility of supervised and unsupervised learning drives innovation and transformation across many domains in the field of artificial intelligence.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Technical Implementation and Additional Notes}
    \begin{block}{Example: Code Snippet for Supervised Learning (Python)}
        \begin{lstlisting}[language=Python]
        from sklearn.model_selection import train_test_split
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score
        
        # Load your dataset
        X, y = load_data()  # hypothetical function
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        
        model = LogisticRegression()
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        
        print(f'Accuracy: {accuracy_score(y_test, predictions)}')
        \end{lstlisting}
    \end{block}

    \begin{block}{Diagram Suggestion}
        Consider using flowcharts or diagrams to visually represent the flow of data in supervised vs. unsupervised learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Conclusion}
    
    We explored two fundamental types of machine learning: 
    \textbf{Supervised Learning} and \textbf{Unsupervised Learning}.
    
    \begin{enumerate}
        \item \textbf{Supervised Learning}:
        \begin{itemize}
            \item Involves training on a labeled dataset.
            \item Examples: 
            \begin{itemize}
                \item \textit{Classification} (e.g., email spam detection).
                \item \textit{Regression} (e.g., predicting house prices).
            \end{itemize}
            \item Key Algorithms: Linear Regression, Decision Trees, SVMs.
        \end{itemize}
        
        \item \textbf{Unsupervised Learning}:
        \begin{itemize}
            \item Involves training on data without labeled responses.
            \item Examples: Customer segmentation, Anomaly detection.
            \item Key Algorithms: K-Means Clustering, Hierarchical Clustering, PCA.
        \end{itemize}
    \end{enumerate}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Key Points}
    
    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item \textbf{Importance of Data:} Quality and quantity significantly affect model performance.
        \item \textbf{Real-World Applications:} Industries utilize these algorithms for practical enhancements in decision-making.
        \item \textbf{Interconnectedness:} AI projects often use both learning types for comprehensive solutions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Future Directions - Future Directions}
    
    \textbf{Emerging Trends in AI Algorithms:}
    \begin{enumerate}
        \item \textbf{Self-Supervised Learning:} Reduces need for labeled data.
        \item \textbf{Transfer Learning:} Improves performance on new datasets using pre-trained models.
        \item \textbf{Federated Learning:} Decentralized training for enhanced privacy.
        \item \textbf{Explainable AI (XAI):} Demand for model transparency and insights.
        \item \textbf{Integration of Reinforcement Learning:} Enhancing decision-making processes.
    \end{enumerate}

    \textbf{Summary Formula:}
    \begin{equation}
    \text{Loss} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 
    \end{equation}
    Where:
    \begin{itemize}
        \item \( y_i \): Actual output
        \item \( \hat{y_i} \): Predicted output
        \item \( n \): Number of examples
    \end{itemize}

    **Engage with hands-on projects to deepen understanding of these paradigms.**
\end{frame}


\end{document}