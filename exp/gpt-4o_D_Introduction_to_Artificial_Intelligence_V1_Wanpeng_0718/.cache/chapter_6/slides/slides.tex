\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 6: Hands-on Workshop: Data Preprocessing}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing}
    \begin{block}{Overview}
        Data preprocessing is a critical step in the machine learning pipeline, involving the transformation and preparation of raw data into a suitable format for training AI models. This phase ensures that the data is clean, consistent, and ready for analysis, significantly increasing the efficiency and accuracy of model training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Preprocessing}
    \begin{itemize}
        \item \textbf{Improves Model Performance:} Models trained on cleaned and well-structured data tend to be more accurate and generalize better to unseen data.
        \item \textbf{Reduces Noisy Data:} Raw data often contains errors, outliers, or irrelevant information, which preprocessing helps to mitigate.
        \item \textbf{Handles Missing Values:} Essential for managing gaps in datasets, ensuring completeness.
        \item \textbf{Standardization and Normalization:} Ensures equitable treatment of features by adjusting scales and distributions.
        \item \textbf{Feature Engineering:} Creating new features from existing data can provide additional insights and help the model learn more effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Preprocessing Techniques}
    \begin{enumerate}
        \item \textbf{Data Cleaning} 
            \begin{itemize}
                \item \textbf{Example:} Removing duplicates or imputing missing values.
                \item \textbf{Method:} 
                \begin{lstlisting}[language=Python]
df.fillna(value)
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{Data Transformation}
            \begin{itemize}
                \item \textbf{Example:} Converting categorical variables into numerical format using one-hot encoding.
                \item \textbf{Code Snippet:} 
                \begin{lstlisting}[language=Python]
import pandas as pd
df = pd.get_dummies(data, columns=['categorical_column'])
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{Feature Scaling}
            \begin{itemize}
                \item \textbf{Standardization:} \((X - \text{mean}) / \text{std}\)
                \item \textbf{Normalization:} \(X' = \frac{(X - \text{min}(X))}{(\text{max}(X) - \text{min}(X))}\)
            \end{itemize}

        \item \textbf{Data Reduction}
            \begin{itemize}
                \item Reducing data volume for computational efficiency without loss of essential information, e.g., PCA (Principal Component Analysis).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Data preprocessing is vital for successful AI model training.
        \item Each preprocessing step directly impacts the model’s ability to learn effectively.
        \item Skipping or incorrectly performing preprocessing can lead to underperforming models.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    In this workshop, we will explore various data preprocessing techniques hands-on, emphasizing their impact on the overall performance of AI models. Understanding this foundation is critical as we move into more complex stages of model development.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objectives of the Workshop - Part 1}
    \begin{block}{Learning Objectives}
        In this hands-on workshop on Data Preprocessing, we aim to equip students with practical skills and understanding that are essential for preparing data for AI model training. 
    \end{block}
    \begin{enumerate}
        \item Understand the Importance of Data Preprocessing
        \item Identify Common Data Issues
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objectives of the Workshop - Part 2}
    \begin{enumerate}[resume]
        \item Implement Data Cleaning Techniques
        \item Explore Feature Engineering
    \end{enumerate}
    \begin{block}{Example: Feature Engineering}
        Creating a 'Year of Purchase' from a 'Purchase Date' can help in trend analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objectives of the Workshop - Part 3}
    \begin{enumerate}[resume]
        \item Practice Data Transformation Techniques
        \item Utilize Libraries for Data Preprocessing
        \item Evaluate Preprocessed Data Quality
    \end{enumerate}
    
    \begin{block}{Example: Data Transformation}
        \begin{lstlisting}[language=Python]
        # Python Code Snippet for Imputation
        import pandas as pd
        df['Age'].fillna(df['Age'].mean(), inplace=True)

        # Python Code Snippet for One-Hot Encoding
        df = pd.get_dummies(df, columns=['Color'], drop_first=True)
        \end{lstlisting}    
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Conclusion}
    By the end of this workshop, students will not only understand the theories behind data preprocessing but also gain hands-on experience in implementing various techniques and tools necessary for effective data handling. This foundational skill set is vital for anyone seeking to excel in the field of AI and machine learning.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Definition}
    \begin{block}{Definition of Data Quality}
        Data quality refers to the condition of a set of values of qualitative or quantitative variables. 
        High-quality data is characterized by:
        \begin{itemize}
            \item Accuracy
            \item Completeness
            \item Consistency
            \item Reliability
            \item Relevance
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Significance and Key Aspects}
    \begin{block}{Significance of Data Quality}
        The quality of data is crucial for the effectiveness of AI models. 
        Poor data quality can lead to:
        \begin{itemize}
            \item Erroneous conclusions
            \item Poor decisions
            \item Inaccurate predictions
        \end{itemize}
        These issues adversely affect business outcomes and operational efficiency.
    \end{block}
    
    \begin{block}{Key Aspects of Data Quality}
        \begin{itemize}
            \item \textbf{Accuracy}: Data should be correct and free from significant errors.
            \item \textbf{Completeness}: Ensures that all required data is present.
            \item \textbf{Consistency}: Data should be consistent across different datasets.
            \item \textbf{Reliability}: Data should consistently yield the same results under the same conditions.
            \item \textbf{Relevance}: The data collected should be pertinent to the current context and use case.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Impact on AI Models}
    \begin{block}{Impact on AI Models}
        \begin{itemize}
            \item \textbf{Bias and Fairness}: Low-quality data can introduce bias, leading to unfair predictions.
            \item \textbf{Model Performance}: Poor data quality severely affects accuracy and generalizability.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Scenario}
        Imagine developing a predictive maintenance model for manufacturing equipment. 
        If the historical data is:
        \begin{itemize}
            \item Incomplete (e.g., missing sensor readings)
            \item Inconsistent (e.g., different formats)
        \end{itemize}
        The model may incorrectly predict failures, resulting in unnecessary downtimes or costly repairs.
    \end{block} 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality - Conclusion}
    Prioritizing data quality is fundamental in the development of reliable AI models. 
    \begin{itemize}
        \item Understanding and ensuring high data quality should be a core part of your data preprocessing workflow.
        \item Data quality directly influences model accuracy and decision-making.
        \item Addressing data quality issues benefits the overall success of AI initiatives.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Overview}
    % Content goes here
    \begin{block}{Overview of Data Cleaning}
        Data cleaning is a crucial step in the data preprocessing pipeline. It involves identifying and correcting errors or inconsistencies in data to enhance its quality, leading to more accurate data analysis and machine learning models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Missing Values}
    % Content goes here
    \begin{itemize}
        \item \textbf{Handling Missing Values}
        \begin{itemize}
            \item \textbf{Definition}: Missing values occur when no data value is stored for a variable.
            \item \textbf{Techniques}:
            \begin{enumerate}
                \item \textbf{Deletion}: Remove rows with missing values.
                \begin{itemize}
                    \item \textit{Example}: Dataset with 1000 entries, 50 with missing values yields 950 entries.
                \end{itemize}
                \item \textbf{Imputation}: Fill missing values with estimated values.
                \begin{itemize}
                    \item \textit{Mean/Median Imputation}: Replace with mean or median.
                    \begin{itemize}
                        \item \textit{Example}: For scores \([70, 75, \text{NaN}, 80]\), replace with mean to get \([70, 75, 75, 80]\).
                    \end{itemize}
                    \item \textbf{Predictive Imputation}: Use models to fill missing values.
                \end{itemize}
                \item \textbf{Flagging}: Create a new column to indicate missing values.
                \begin{itemize}
                    \item \textit{Example}: For column \texttt{Age}, create \texttt{Age\_Missing} with 1 (missing) or 0 (present).
                \end{itemize}
            \end{enumerate}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Outlier Detection}
    % Content goes here
    \begin{itemize}
        \item \textbf{Outlier Detection}
        \begin{itemize}
            \item \textbf{Definition}: Outliers are data points that differ significantly from the majority of a dataset.
            \item \textbf{Methods to Identify Outliers}:
            \begin{enumerate}
                \item \textbf{Statistical Tests}:
                \begin{itemize}
                    \item \textit{Z-Score Method}: Values exceeding 3 standard deviations from the mean are outliers.
                    \begin{equation}
                        Z = \frac{(X - \mu)}{\sigma}
                    \end{equation}
                    where \(X\) is the value, \(\mu\) is the mean, and \(\sigma\) is the standard deviation.
                \end{itemize}
                \item \textbf{IQR Method}: Calculate IQR and determine outliers as values below \(Q1 - 1.5 \times IQR\) or above \(Q3 + 1.5 \times IQR\).
                \begin{itemize}
                    \item \textit{Example}: If \(Q1 = 10\) and \(Q3 = 20\), IQR is \(10\), and outliers are values lower than \(10 - 15\) or higher than \(20 + 15\).
                \end{itemize}
                \item \textbf{Visualization Techniques}: Use box plots or scatter plots for visual identification.
            \end{enumerate}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Techniques - Conclusion}
    % Content goes here
    \begin{block}{Conclusion}
        Data cleaning enhances the quality of your dataset, improving the foundation for analysis and predictive modeling. Familiarity with these techniques is essential for effective data preprocessing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Methods - Overview}
    \begin{itemize}
        \item Data transformation is crucial in data preprocessing.
        \item Common methods: \textbf{Normalization} and \textbf{Standardization}.
        \item Each method applies to different scenarios.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Methods - Normalization}
    \begin{block}{Definition}
        Normalization rescales features to a [0, 1] range or other specified range.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Formula for Min-Max Normalization:}
        \begin{equation}
            X' = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
        \end{equation}
        
        \item \textbf{Example:} 
        \begin{itemize}
            \item Before normalization: Age = [15, 30, 45, 60]; Salary = [30,000, 60,000, 90,000, 120,000].
            \item After normalization: Age = [0.0, 0.25, 0.5, 1.0]; Salary = [0.0, 0.25, 0.5, 1.0].
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Methods - Standardization}
    \begin{block}{Definition}
        Standardization transforms data to have a mean of 0 and a standard deviation of 1.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Formula for Standardization:}
        \begin{equation}
            Z = \frac{X - \mu}{\sigma}
        \end{equation}
        where $\mu$ is the mean and $\sigma$ is the standard deviation.
        
        \item \textbf{Example:}
        \begin{itemize}
            \item Before standardization: Age = [15, 30, 45, 60]; Salary = [30,000, 60,000, 90,000, 120,000].
            \item After standardization (Age Example): Age = [-2.0, -1.0, 0.0, 1.0] (Mean = 45, Std Dev = 15).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points on Data Transformation}
    \begin{itemize}
        \item \textbf{Context of Use:}
        \begin{itemize}
            \item Use normalization for features with different units and scales.
            \item Use standardization when data is normally distributed or algorithms assume Gaussian distribution.
        \end{itemize}
        
        \item \textbf{Impacts on Algorithms:}
        \begin{itemize}
            \item Unscaled features can adversely affect model performance, especially for distance-based models.
            \item Visualize data distributions before applying transformations.
        \end{itemize}
        
        \item \textbf{Summary:} 
        Transformation techniques enhance the effectiveness of machine learning algorithms. Choose the method based on data characteristics and algorithm requirements.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feature Engineering}
    \begin{block}{What is Feature Engineering?}
        Feature Engineering is the process of selecting, modifying, or creating features (predictor variables) to improve model performance. Effective feature engineering can significantly boost the accuracy of machine learning models by providing them with more relevant and informative data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Feature Engineering}
    \begin{itemize}
        \item \textbf{Enhances Model Performance:} Well-engineered features can lead to better predictive accuracy.
        \item \textbf{Improves Interpretability:} Clearer features make it easier to understand model actions and outcomes.
        \item \textbf{Reduces Overfitting:} By having the right features, models can generalize better to unseen data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques in Feature Engineering}
    \begin{enumerate}
        \item \textbf{Feature Selection}
            \begin{itemize}
                \item \textbf{Definition:} Choosing a subset of relevant features for the model.
                \item \textbf{Methods:}
                    \begin{itemize}
                        \item \textbf{Filter Methods:} Statistical tests (e.g., Chi-squared test) to evaluate feature relevance.
                        \item \textbf{Wrapper Methods:} Use model performance to evaluate feature subsets (e.g., recursive feature elimination).
                        \item \textbf{Embedded Methods:} Feature selection during model training (e.g., Lasso regression).
                    \end{itemize}
                \item \textbf{Example:} In a dataset predicting house prices, use only the features that significantly affect price (e.g., size and location).
            \end{itemize}

        \item \textbf{Feature Modification}
            \begin{itemize}
                \item \textbf{Definition:} Transforming existing features to enhance their effectiveness.
                \item \textbf{Common Techniques:}
                    \begin{itemize}
                        \item \textbf{Normalization/Standardization:} Rescale features to a common scale.
                            \begin{equation}
                                z = \frac{(x - \mu)}{\sigma}
                            \end{equation}
                        \item \textbf{Binning:} Converting continuous features into categorical bins.
                    \end{itemize}
                \item \textbf{Example:} Transform a “salary” feature using a log scale to reduce skewness.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques (Contd.)}
    \begin{enumerate}
        \setcounter{enumi}{2} % To continue enumerating
        \item \textbf{Feature Creation}
            \begin{itemize}
                \item \textbf{Definition:} Developing new features from existing data.
                \item \textbf{Techniques:}
                    \begin{itemize}
                        \item \textbf{Polynomial Features:} Creating interaction terms or powers of existing features.
                        \item \textbf{Date/Time Features:} Extracting features like day of the week, month, or year from date variables.
                    \end{itemize}
                \item \textbf{Example:} Convert a timestamp into separate features for “day of the week” or “month” in a sales dataset to identify trends.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Quality Over Quantity:} A few well-chosen features can outperform a large number of poorly chosen ones.
        \item \textbf{Iterative Process:} Feature engineering often requires iterations—experiment, analyze results, and refine.
        \item \textbf{Domain Knowledge:} Understanding the context of your data can guide effective feature engineering.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Feature Engineering is crucial in the data preprocessing stage. By selecting, modifying, or creating appropriate features, you can significantly enhance the effectiveness of your machine learning models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Topic}
    Next, we will explore Data Encoding Techniques, focusing on transforming categorical data into numerical format for model compatibility.
\end{frame}

\begin{frame}
    \frametitle{Data Encoding Techniques}
    \textbf{Understanding Categorical Data Encoding}

    \begin{itemize}
        \item Data encoding is essential for converting categorical data into numerical format for machine learning algorithms.
        \item Two common techniques:
        \begin{itemize}
            \item Label Encoding
            \item One-Hot Encoding
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Label Encoding}
    
    \textbf{Definition:} Converts each category into a unique integer, suitable for ordinal data.

    \textbf{Example:}
    \begin{itemize}
        \item Feature: Size with categories: Small, Medium, Large
        \item Assignments:
        \begin{itemize}
            \item Small $\to$ 0
            \item Medium $\to$ 1
            \item Large $\to$ 2
        \end{itemize}
    \end{itemize}

    \textbf{Implementation in Python:}
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import LabelEncoder

# Sample data
sizes = ['Small', 'Medium', 'Large', 'Medium', 'Small']
label_encoder = LabelEncoder()
sizes_encoded = label_encoder.fit_transform(sizes)
print(sizes_encoded)  # Output: [0 1 2 1 0]
    \end{lstlisting}

    \textbf{Key Points:}
    \begin{itemize}
        \item Use with caution; suitable for ordinal variables only.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{One-Hot Encoding}
    
    \textbf{Definition:} Transforms categories into a binary format, creating separate columns for each category, ideal for nominal data.

    \textbf{Example:}
    \begin{itemize}
        \item Feature: Size with categories: Small, Medium, Large
        \item Creates new binary columns:
        \begin{itemize}
            \item Small: 1, Medium: 0, Large: 0
            \item Small: 0, Medium: 1, Large: 0
            \item Small: 0, Medium: 0, Large: 1
        \end{itemize}
    \end{itemize}

    \textbf{Implementation in Python:}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Sample data
sizes = pd.DataFrame({'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small']})
sizes_encoded = pd.get_dummies(sizes, columns=['Size'])
print(sizes_encoded)
    # Output:
    #    Size_Large  Size_Medium  Size_Small
    # 0           0             0            1
    # 1           0             1            0
    # 2           1             0            0
    # 3           0             1            0
    # 4           0             0            1
    \end{lstlisting}

    \textbf{Key Points:}
    \begin{itemize}
        \item Best for nominal variables to avoid misinterpretation of relationships.
        \item Increased dimensionality can occur with many categories.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    
    \begin{itemize}
        \item \textbf{Label Encoding} is useful for ordinal data.
        \item \textbf{One-Hot Encoding} is preferred for nominal data.
        \item Choose appropriate encoding based on the nature of the categorical variables to impact model performance and interpretability.
    \end{itemize}

    \textbf{Next Steps:} Engage in hands-on exercises applying these data preprocessing techniques using Python libraries.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise - Overview}
    \begin{block}{Objectives}
        \begin{itemize}
            \item Understand the importance of data preprocessing in the machine learning pipeline.
            \item Apply various preprocessing techniques using Python libraries.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise - Key Concepts in Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Data Cleaning:}
            \begin{itemize}
                \item Handling missing values, outliers, and inconsistencies in the dataset.
                \item \textbf{Example:} Using \texttt{pandas} to fill missing values.
                \begin{lstlisting}[language=Python]
import pandas as pd
df = pd.read_csv('data.csv')
df['column_name'].fillna(method='ffill', inplace=True)
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{Data Transformation:}
            \begin{itemize}
                \item Scaling and normalizing features for better performance.
                \item \textbf{Example:} Standardizing features using \texttt{StandardScaler} from \texttt{sklearn}.
                \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{Feature Encoding:}
            \begin{itemize}
                \item Converting categorical variables into a numerical format to be fed into models.
                \item \textbf{One-Hot Encoding Example:}
                \begin{lstlisting}[language=Python]
df = pd.get_dummies(df, columns=['categorical_column'], drop_first=True)
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exercise Steps}
    \begin{enumerate}
        \item \textbf{Load Sample Dataset:}
            \begin{itemize}
                \item Use a dataset that includes categorical and numerical features.
                \begin{lstlisting}[language=Python]
df = pd.read_csv('sample_data.csv')
                \end{lstlisting}
            \end{itemize}
        
        \item \textbf{Data Cleaning:}
            \begin{itemize}
                \item Identify missing values and apply strategies to handle them (e.g., imputation, removal).
            \end{itemize}
        
        \item \textbf{Data Transformation:}
            \begin{itemize}
                \item Scale numerical features and transform any skewed distributions (Log transformation).
            \end{itemize}
        
        \item \textbf{Feature Encoding:}
            \begin{itemize}
                \item Apply one-hot encoding to categorical variables and prepare them for modeling.
            \end{itemize}
        
        \item \textbf{Preview Processed Data:}
            \begin{itemize}
                \item Use \texttt{df.head()} to review the changes made to the dataset.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Preprocessing Challenges}
    In data preprocessing, we encounter challenges that can impact machine learning models significantly. Addressing these challenges improves the effectiveness of data analysis.
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Missing Values}
    \begin{block}{Challenge}
        Incomplete datasets with missing entries can skew results.
    \end{block}
    
    \begin{block}{Example}
        A dataset with missing \texttt{age} data might lead to incorrect insights about customer demographics.
    \end{block}
    
    \begin{block}{Strategy}
        \begin{itemize}
            \item \textbf{Imputation:} Replace missing values with the mean, median, or mode.
            \item \textbf{Removal:} Delete rows or columns with missing data if they are not significant.
        \end{itemize}
    \end{block}
    
    \begin{lstlisting}[language=Python]
import pandas as pd

data = pd.read_csv('data.csv')
data['age'].fillna(data['age'].mean(), inplace=True)  # Imputing missing values with mean
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Outliers}
    \begin{block}{Challenge}
        Outliers can distort statistical analyses and model training.
    \end{block}
    
    \begin{block}{Example}
        A dataset on salaries with one entry of \$1,000,000 can inaccurately shift the average salary.
    \end{block}

    \begin{block}{Strategy}
        \begin{itemize}
            \item \textbf{Detection:} Use visualizations like box plots.
            \item \textbf{Treatment:} Cap outliers at a certain threshold or transform data (e.g., log transformation).
        \end{itemize}
    \end{block}
    
    \begin{lstlisting}[language=Python]
import numpy as np

data['salary'] = np.where(data['salary'] > 200000, 200000, data['salary'])  # Capping outliers
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Data Encoding}
    \begin{block}{Challenge}
        Machine learning algorithms require numerical data; categorical variables need conversion.
    \end{block}
    
    \begin{block}{Example}
        A 'gender' column with values 'Male' and 'Female' cannot be directly used in computations.
    \end{block}

    \begin{block}{Strategy}
        \begin{itemize}
            \item \textbf{Label Encoding:} Assign numerical values to categories.
            \item \textbf{One-Hot Encoding:} Create binary columns for each category.
        \end{itemize}
    \end{block}
    
    \begin{lstlisting}[language=Python]
data = pd.get_dummies(data, columns=['gender'], drop_first=True)  # One-Hot Encoding
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Feature Scaling}
    \begin{block}{Challenge}
        Models may perform poorly if features are on different scales (e.g., age vs. income).
    \end{block}
    
    \begin{block}{Example}
        A distance-based model like KNN may misinterpret the influence of differently scaled features.
    \end{block}

    \begin{block}{Strategy}
        \begin{itemize}
            \item \textbf{Standardization:} Scale features to have a mean of 0 and a standard deviation of 1.
            \item \textbf{Min-Max Scaling:} Scale features to a range of 0 to 1.
        \end{itemize}
    \end{block}

    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data[['age', 'income']] = scaler.fit_transform(data[['age', 'income']])  # Standardizing features
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Addressing preprocessing challenges is crucial for model performance.
        \item Use visualizations for outlier detection and feature distribution understanding.
        \item Always assess the impact of preprocessing steps on your data and model performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By being aware of these common preprocessing challenges and employing effective strategies, you will ensure a more solid foundation for your datasets, leading to better model outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Recap of Key Points}
    \begin{enumerate}
        \item \textbf{Importance of Data Preprocessing:}
        \begin{itemize}
            \item Critical step in the data analysis pipeline.
            \item Enhances data quality for effective modeling.
            \item Example: Cleaning missing values improves predictions.
        \end{itemize}
        
        \item \textbf{Common Preprocessing Techniques:}
        \begin{itemize}
            \item Handling Missing Data: Imputation or deletion can prevent data loss.
            \item Normalization \& Scaling: Essential for algorithms sensitive to scale.
            \item Encoding Categorical Variables: Transform categorical data into numerical format.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Continuing Key Points}
    \begin{enumerate}[resume]
        \item \textbf{Dealing with Outliers:}
        \begin{itemize}
            \item Identifying and managing outliers significantly impacts model performance.
            \item Techniques: IQR and z-score methods.
        \end{itemize}

        \item \textbf{Feature Selection:}
        \begin{itemize}
            \item Reduces complexity and enhances model accuracy.
            \item Techniques: Recursive Feature Elimination (RFE) and tree-based feature importance.
        \end{itemize}

        \item \textbf{Evaluating Data Quality:}
        \begin{itemize}
            \item Assess data quality through metrics: completeness, consistency, accuracy.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Q\&A}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Data preprocessing is a foundation for effective analytics and modeling.
            \item Each technique must be selected based on dataset and analysis requirements.
        \end{itemize}
    \end{block}

    \begin{block}{Q\&A}
        \begin{itemize}
            \item "Now that we've wrapped up our workshop, I invite you to ask any questions regarding the data preprocessing techniques we explored."
            \item "You might wonder how these techniques are applied in various contexts or how certain issues can be resolved in specific datasets."
        \end{itemize}
    \end{block}
\end{frame}


\end{document}