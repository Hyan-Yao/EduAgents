\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{What Are Neural Networks?}
    \begin{itemize}
        \item Neural Networks (NNs) are computational models inspired by the human brain.
        \item Composed of interconnected layers of nodes ("neurons") that process information similarly to biological neurons.
        \item Fundamental for AI, used for:
        \begin{itemize}
            \item Pattern recognition
            \item Classification
            \item Decision-making tasks
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Neural Networks}
    \begin{itemize}
        \item \textbf{Neurons:} Basic processing units that receive inputs, process them, and pass outputs.
        \item \textbf{Layers:}
        \begin{itemize}
            \item \textit{Input Layer:} Receives initial data.
            \item \textit{Hidden Layers:} Where computations occur; multiple layers possible.
            \item \textit{Output Layer:} Produces predictions or classifications.
        \end{itemize}
        \item \textbf{Weights and Biases:} Connections between neurons with adjustable weights and biases for better data fitting.
        \item \textbf{Activation Functions:} (e.g., Sigmoid, ReLU) introduce non-linearity, aiding in the learning of complex patterns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Artificial Intelligence}
    \begin{itemize}
        \item Revolutionized AI applications such as:
        \begin{itemize}
            \item \textbf{Image Recognition:} Facial recognition and object identification.
            \item \textbf{Natural Language Processing (NLP):} Enhancements in chatbots and translation.
            \item \textbf{Game Playing:} Breakthroughs like AlphaGo defeating human champions.
        \end{itemize}        
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning and Versatility}
    \begin{itemize}
        \item \textbf{Learning Process:} Uses backpropagation to adjust weights based on prediction errors.
        \item \textbf{Scalability:} Effectively handles large datasets, ideal for big data applications.
        \item \textbf{Versatility:} Applicable for both supervised and unsupervised learning tasks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Structure Illustration}
    \begin{block}{Structure}
        \centering
        Input Layer $\rightarrow$ Hidden Layer(s) $\rightarrow$ Output Layer
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=Python]
from keras.models import Sequential
from keras.layers import Dense

# Create a simple neural network
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=10))  # Input layer
model.add(Dense(32, activation='relu'))                # Hidden layer
model.add(Dense(1, activation='sigmoid'))              # Output layer
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Neural networks are powerful AI tools enabling data-driven decision making.
        \item Understanding their components and applications is crucial for entering the AI field.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{History and Evolution - Overview}
    \begin{block}{Overview}
        Neural networks (NNs) have a rich history spanning decades, marked by key milestones that shaped their development. Understanding this evolution allows appreciation of the challenges and breakthroughs leading to current AI advancements.
    \end{block}
\end{frame}

\begin{frame}[fragile]{History and Evolution - Key Milestones}
    \begin{enumerate}
        \item \textbf{1940s - Conceptual Beginnings:}
            \begin{itemize}
                \item \textit{McCulloch-Pitts Neuron (1943):} First mathematical model of a neuron.
                \item Key Concept: Simplified binary model for information processing.
            \end{itemize}
        
        \item \textbf{1950s - Early Models:}
            \begin{itemize}
                \item \textit{Perceptron (1958):} First neural network that categorized inputs.
                \item Key Concept: Learning from errors using labeled data.
            \end{itemize}
        
        \item \textbf{1960s - Limitations and Critiques:}
            \begin{itemize}
                \item \textit{Minsky and Papert (1969):} Critiqued single-layer perceptrons for inability to solve XOR.
                \item Key Concept: Led to decreased interest and funding, causing "AI Winter."
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{History and Evolution - Continued}
    \begin{enumerate}[resume]
        \item \textbf{1980s - Revival and Backpropagation:}
            \begin{itemize}
                \item \textit{Backpropagation Algorithm (1986):} Enabled training of multi-layer networks.
                \item Key Concept: Allowed deeper networks for complex pattern recognition.
            \end{itemize}
        
        \item \textbf{2000s - Deep Learning Surge:}
            \begin{itemize}
                \item \textit{Deep Belief Networks (2006):} Introduced by Geoffrey Hinton.
                \item Key Concept: Advances in GPU computing allowed larger, more accurate networks.
            \end{itemize}
        
        \item \textbf{2010s - Widespread Adoption:}
            \begin{itemize}
                \item \textit{AlexNet (2012):} Demonstrated significant improvement in image classification.
                \item Industry began integrating neural networks in products.
            \end{itemize}

        \item \textbf{2020s - State-of-the-Art Achievements:}
            \begin{itemize}
                \item \textit{Transformer Models:} Such as BERT and GPT-3 transformed NLP.
                \item Key Concept: Trend towards deeper architectures across domains.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Overview}
    \begin{block}{Overview}
        Neural networks are computational models inspired by the human brain, designed to recognize patterns and make decisions. The architecture consists of various components that work together to transform input data into meaningful output.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Key Components}
    \begin{block}{Key Components}
        \begin{enumerate}
            \item \textbf{Layers}
                \begin{itemize}
                    \item \textbf{Input Layer:} Receives the initial data. Each neuron corresponds to a feature.
                    \item \textbf{Hidden Layers:} Intermediate layers that process inputs. Multiple neurons are present.
                    \item \textbf{Output Layer:} Produces final predictions or classifications.
                \end{itemize}

            \item \textbf{Nodes (Neurons)}
                \begin{itemize}
                    \item Basic processing units that activate based on input weighted sum and activation function.
                    \item \textbf{Example Activation Functions:}
                        \begin{itemize}
                            \item Sigmoid: $\sigma(x) = \frac{1}{1 + e^{-x}}$
                            \item ReLU: $f(x) = \max(0, x)$
                        \end{itemize}
                \end{itemize}
                        
            \item \textbf{Connections (Weights)}
                \begin{itemize}
                    \item Weights define connection strength between nodes and are adjusted during training.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Neural Network Architecture - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Structure Matters:} The arrangement of layers and neurons affects performance.
            \item \textbf{Learning Process:} Training adjusts weights and biases to minimize prediction errors.
            \item \textbf{Flexibility:} Different architectures (e.g., deep networks) can be created for specific problems.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding the architecture of neural networks is crucial for developing effective models for various applications, laying a foundation for more complex structures in future studies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Overview}
    \begin{block}{Overview}
        Neural networks are powerful computational models inspired by the human brain, used for various tasks including classification, regression, and pattern recognition. Each type serves unique functions tailored to specific tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Part 1: Feedforward Neural Networks}
    \begin{block}{1. Feedforward Neural Networks (FNN)}
        \begin{itemize}
            \item \textbf{Definition}: The simplest type of artificial neural network where connections between the nodes do not form a cycle.
            \item \textbf{Structure}: 
            \begin{itemize}
                \item Consists of an input layer, one or more hidden layers, and an output layer. 
                \item Information flows unidirectionally—from the input layer to the output layer.
            \end{itemize}
            \item \textbf{Example}: 
              Used in image recognition (e.g., classifying images as cat or dog based on pixel values).
            \item \textbf{Key Points}:
              \begin{itemize}
                  \item No feedback loops.
                  \item Suitable for static data inputs.
              \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Part 2: Convolutional Neural Networks}
    \begin{block}{2. Convolutional Neural Networks (CNN)}
        \begin{itemize}
            \item \textbf{Definition}: A specialized feedforward neural network for processing structured grid-like data such as images.
            \item \textbf{Structure}: 
            \begin{itemize}
                \item Composed of convolutional layers to capture spatial hierarchies followed by pooling layers for down-sampling features.
            \end{itemize}
            \item \textbf{Example}: 
              Extensively used for object detection and image classification.
            \item \textbf{Key Points}:
              \begin{itemize}
                  \item Leverages local connectivity to reduce the number of parameters.
                  \item Effective for capturing spatial and temporal dependencies in visual data.
              \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Part 3: Recurrent Neural Networks}
    \begin{block}{3. Recurrent Neural Networks (RNN)}
        \begin{itemize}
            \item \textbf{Definition}: A type of neural network designed for sequence prediction with cycles in the connections.
            \item \textbf{Structure}: 
            \begin{itemize}
                \item Has loops allowing output from some nodes to feed back into the network, maintaining state.
            \end{itemize}
            \item \textbf{Example}: 
              Widely used in natural language processing (e.g., language translation).
            \item \textbf{Key Points}:
              \begin{itemize}
                  \item Processes sequential data.
                  \item Can remember past information, enhancing understanding of temporal patterns.
              \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Conclusion}
    \begin{block}{Conclusion}
        Understanding different types of neural networks enables you to choose the appropriate model for various machine learning tasks. Each type utilizes its unique structure to tackle challenges presented by different data formats.
    \end{block}
    \begin{block}{Diagrams and Visuals}
        \begin{itemize}
            \item Feedforward Networks: Simple diagram showing unidirectional data flow.
            \item CNN Structure: Illustration of convolutional and pooling layers.
            \item RNN Diagram: Visual demonstrating recurrent connections.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Introduction}
    \begin{block}{Definition}
        Activation functions are essential components of neural networks that allow the model to learn complex patterns. 
        They determine the output of a neuron based on its input, introducing non-linearity into the model.
    \end{block}
    \begin{itemize}
        \item Enable learning of intricate relationships in data.
        \item Critical for the network to adapt and perform well.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Common Types}
    \begin{enumerate}
        \item \textbf{Sigmoid Activation Function}
            \begin{itemize}
                \item \textbf{Formula:} 
                \begin{equation}
                    \sigma(x) = \frac{1}{1 + e^{-x}}
                \end{equation}
                \item \textbf{Range:} (0, 1)
                \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Smooth and continuously differentiable.
                    \item Useful for binary classification.
                    \item Suffers from vanishing gradient problem.
                \end{itemize}
                \item \textbf{Example Usage:} Output layer of binary classifiers.
            \end{itemize}
        
        \item \textbf{ReLU (Rectified Linear Unit)}
            \begin{itemize}
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{ReLU}(x) = \max(0, x)
                \end{equation}
                \item \textbf{Range:} [0, ∞)
                \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Simple calculation, mitigates vanishing gradient.
                    \item Can suffer from "dying ReLU".
                \end{itemize}
                \item \textbf{Example Usage:} Hidden layers of deep learning models.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - More Types and Conclusion}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue from where the previous frame ended
        \item \textbf{Softmax Activation Function}
            \begin{itemize}
                \item \textbf{Formula:} 
                \begin{equation}
                    \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}, \quad i = 1, 2, \ldots, K
                \end{equation}
                \item \textbf{Range:} (0, 1), sums to 1 across outputs.
                \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Converts scores into probabilities, ideal for multi-class classification.
                \end{itemize}
                \item \textbf{Example Usage:} Output layer for multi-class tasks.
            \end{itemize}
    \end{enumerate}
    \begin{block}{Conclusion}
        Understanding activation functions is crucial for neural network design. The choice impacts the 
        model's performance and should align with task requirements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks}
    \begin{block}{Overview of the Training Process}
        Training a neural network involves adjusting the weights based on performance. This process consists of:
        \begin{itemize}
            \item Forward Propagation
            \item Loss Function Calculation
            \item Backpropagation
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation}
    \begin{block}{Definition}
        Forward propagation passes input data through the network to produce an output.
    \end{block}
    \begin{block}{Process}
        \begin{itemize}
            \item Each neuron computes a weighted sum of inputs followed by an activation function (e.g., Sigmoid, ReLU).
            \item Continues through hidden layers until the output layer produces the prediction.
        \end{itemize}
    \end{block}
    \begin{equation}
        z = w \cdot x + b
    \end{equation}
    \begin{equation}
        a = \text{activation}(z)
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Functions}
    \begin{block}{Definition}
        A loss function measures how well the predictions match the expected outcomes. The training goal is to minimize this loss.
    \end{block}
    \begin{block}{Common Loss Functions}
        \begin{itemize}
            \item \textbf{Mean Squared Error (MSE)}: Used for regression tasks.
            \begin{equation}
                \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
            \item \textbf{Cross-Entropy Loss}: Used for classification tasks.
            \begin{equation}
                \text{Cross-Entropy} = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
            \end{equation}
        \end{itemize}
    \end{block}
    \begin{block}{Key Point}
        Choosing the right loss function is crucial for successful training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation}
    \begin{block}{Definition}
        Backpropagation updates weights by calculating the gradient of the loss function with respect to each weight.
    \end{block}
    \begin{block}{Process}
        \begin{itemize}
            \item Derivative of the loss with respect to weights is calculated using the chain rule.
            \item Weights are updated in the opposite direction of the gradient to minimize loss.
        \end{itemize}
    \end{block}
    \begin{equation}
        w \gets w - \eta \frac{\partial L}{\partial w}
    \end{equation}
    \begin{block}{Example}
        Adjusting weight \( w_i \) helps in getting the prediction closer to the true value.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Iterative Process}: Training includes multiple iterations (epochs) of forward propagation and backpropagation.
        \item \textbf{Importance of Hyperparameters}: Learning rate and batch size influence the convergence of training.
        \item \textbf{Overfitting Awareness}: Monitor training vs. validation loss to prevent overfitting.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding this training process is essential for effectively developing neural network models. Mastering forward propagation, loss calculation, and backpropagation lays the groundwork for more complex architectures and optimizations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    In the following slide, we will explore various evaluation metrics to assess the performance of the trained neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Neural Networks - Introduction}
    Evaluating the performance of neural networks is critical for understanding how well a model generalizes to new, unseen data. 
    Accurate evaluation helps in making necessary adjustments to improve the model's performance and is essential for applications in real-world scenarios.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Neural Networks - Key Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition:} The proportion of correctly predicted instances out of the total instances.
            \item \textbf{Formula:}
            \begin{equation}
            \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} \times 100
            \end{equation}
            \item \textbf{Example:} If a model predicts 80 correctly out of 100 instances, the accuracy would be: 
            \begin{equation}
            \text{Accuracy} = \frac{80}{100} \times 100 = 80\%
            \end{equation}
        \end{itemize}
        
        \item \textbf{Loss}
        \begin{itemize}
            \item \textbf{Definition:} A measure of how well the predicted outputs match the target outputs. 
            \item \textbf{Common Loss Functions:}
            \begin{itemize}
                \item \textbf{Mean Squared Error (MSE)} for regression:
                \begin{equation}
                \text{MSE} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
                \end{equation}
                \item \textbf{Binary Cross-Entropy Loss} for binary classification:
                \begin{equation}
                \text{Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right]
                \end{equation}
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Neural Networks - Overfitting and Conclusion}
    \begin{itemize}
        \item \textbf{Evaluating Model Overfitting}
        \begin{itemize}
            \item \textbf{Training vs. Validation Accuracy:} Monitor both metrics during training. A significant gap indicates overfitting.
            \item \textbf{Use of Validation Set:} Always reserve a portion of your data to validate model performance separate from the training data.
        \end{itemize}

        \item \textbf{Conclusion}
        \begin{itemize}
            \item Evaluating neural networks using these metrics allows us to assess their effectiveness and identify areas for improvement.
            \item Understanding accuracy, loss, and additional metrics is vital for developing robust machine learning models.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Applications of Neural Networks}
    
    \begin{block}{Introduction to Neural Networks}
        Neural networks are computational models inspired by the human brain, designed to recognize patterns in data.
        They consist of layers of interconnected nodes, or neurons, that process input data and make predictions or classifications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Applications of Neural Networks - Part 1}
    
    \begin{enumerate}
        \item \textbf{Image Recognition}
        \begin{itemize}
            \item \textbf{Explanation}: Utilizes Convolutional Neural Networks (CNNs) to identify objects within images by learning features from large datasets.
            \item \textbf{Example}: Facebook's suggestion of tags in uploaded pictures based on face recognition.
            \item \textbf{Key Point}: CNNs excel in detecting spatial hierarchies in images.
        \end{itemize}
        
        \item \textbf{Natural Language Processing (NLP)}
        \begin{itemize}
            \item \textbf{Explanation}: Enables machines to understand and respond to human language using architectures like RNNs and Transformers.
            \item \textbf{Example}: Google Translate’s use of neural network models for translating text.
            \item \textbf{Key Point}: Deep learning has significantly advanced contextual and nuanced understanding in machines.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Applications of Neural Networks - Part 2}

    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Healthcare}
        \begin{itemize}
            \item \textbf{Explanation}: Used for diagnostics, predicting outcomes, and personalizing treatments.
            \item \textbf{Example}: DeepMind's AI detecting eye diseases from retinal scans for early intervention.
            \item \textbf{Key Point}: AI-assisted tools enhance decision-making and improve patient outcomes.
        \end{itemize}
    \end{enumerate}

    \begin{block}{Conclusion}
        Neural networks are revolutionizing various fields, providing powerful solutions to complex problems, making them indispensable in today's tech landscape.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formula in Neural Network Training}

    Training a neural network often involves an optimization function, such as Stochastic Gradient Descent:
    \begin{equation}
        w_{new} = w_{old} - \eta \nabla J(w_{old})
    \end{equation}
    where:
    \begin{itemize}
        \item \( w \) = weights
        \item \( \eta \) = learning rate
        \item \( J \) = loss function
    \end{itemize}
    
    \begin{block}{Additional Notes}
        Ensure understanding of concepts like training data, overfitting, and hyperparameters when exploring applications further.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    \begin{block}{Introduction to Ethical Implications of Neural Networks}
        As we advance in artificial intelligence, the ethical implications of deploying neural networks become increasingly critical. Ethical considerations address how neural networks operate, the data they are trained on, and their impact on society.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Bias in Neural Networks}
    \begin{block}{1. Bias in Neural Networks}
        \begin{itemize}
            \item \textbf{Definition}: Bias refers to prejudiced tendencies in algorithms that can lead to unfair treatment of individuals or groups.
            \item \textbf{Sources}: Bias can arise from:
            \begin{itemize}
                \item \textbf{Data Selection}: If training data is unrepresentative or skewed, the model will mirror those biases.
                \item \textbf{Labeling Practices}: Human biases can enter datasets during data labeling processes.
            \end{itemize}
            \item \textbf{Example}: A facial recognition system trained predominantly on light-skinned individuals may perform poorly on darker-skinned faces, leading to higher rates of misidentification.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Fairness and Frameworks}
    \begin{block}{2. Fairness in AI}
        \begin{itemize}
            \item \textbf{Definition}: Fairness involves ensuring that AI systems treat individuals equally without discriminating based on sensitive attributes (e.g., race, gender).
            \item \textbf{Types of Fairness}:
            \begin{itemize}
                \item \textbf{Individual Fairness}: Similar individuals should be treated similarly.
                \item \textbf{Group Fairness}: Different groups should have equitable outcomes.
            \end{itemize}
            \item \textbf{Example}: Hiring algorithms might favor candidates from specific schools disproportionately, leading to systemic inequalities if not monitored for fairness.
        \end{itemize}
    \end{block}

    \begin{block}{3. Ethical Frameworks}
        \begin{itemize}
            \item \textbf{Utilitarianism}: Actions are measured by their consequences; the best option generates the greatest good for the most people.
            \item \textbf{Deontological Ethics}: Focuses on the morality of actions themselves, regardless of outcomes, emphasizing rights and duties.
        \end{itemize}
        \textbf{Application}: Ethical frameworks guide decisions about data usage and model deployment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Mitigating Bias and Conclusion}
    \begin{block}{4. Strategies for Mitigating Bias and Ensuring Fairness}
        \begin{itemize}
            \item \textbf{Diverse Datasets}: Utilize diverse and representative datasets to train models.
            \item \textbf{Regular Audits}: Conduct regular audits and assessments of model outputs with diverse stakeholders to identify biases.
            \item \textbf{Transparency}: Promote transparency in AI systems by documenting data sources, algorithms, and decision processes.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        As we harness the power of neural networks in various applications, our commitment to ethical considerations will determine the social legitimacy and effectiveness of these technologies. By prioritizing fairness and addressing biases, we can build more trustworthy and equitable AI systems.
    \end{block}
\end{frame}

\begin{frame}{Introduction to Future Trends}
    Neural networks have revolutionized many fields, and ongoing research is paving the way for advanced applications. The future of neural networks involves improvements in:
    \begin{itemize}
        \item Architecture
        \item Efficiency
        \item Interpretability
        \item Deployment
    \end{itemize}
\end{frame}

\begin{frame}{Key Future Trends - Advanced Architectures}
    \begin{itemize}
        \item \textbf{Transformer Models}: Originally designed for NLP, now being applied in various fields (e.g., Vision Transformers in image processing).
        \item \textbf{Neural Architecture Search (NAS)}: Automating the design of neural network architectures leads to optimized performance.
    \end{itemize}
    \begin{block}{Example} 
        NAS has helped develop networks like EfficientNet, achieving state-of-the-art accuracy with lesser computational cost.
    \end{block}
\end{frame}

\begin{frame}{Key Future Trends - Interpretation and Federated Learning}
    \begin{itemize}
        \item \textbf{Increased Interpretation and Transparency:}
        \begin{itemize}
            \item Interpretable AI is crucial for understanding decisions made by neural networks.
            \item Techniques such as LIME and SHAP are being increasingly utilized.
        \end{itemize}
        \item \textbf{Federated Learning:}
        \begin{itemize}
            \item Trains algorithms across decentralized devices, enhancing privacy and reducing data breach risks.
            \item Particularly beneficial in sectors like healthcare where sensitive data cannot be shared.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Key Future Trends - Energy Efficiency}
    \begin{itemize}
        \item \textbf{Energy Consumption Concerns:} Training large neural networks consumes considerable energy.
        \begin{itemize}
            \item \textbf{Model Compression}: Techniques like pruning and quantization reduce model size without significant accuracy loss.
            \item \textbf{Efficient Algorithms}: Development of algorithms that require less computational power.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Conclusion and Key Takeaway}
    \begin{itemize}
        \item The evolution in neural network technology aims for more robust, interpretable, efficient, and privacy-preserving models.
        \item As trends unfold, they will address current limitations and ethical considerations.
        \item \textbf{Key Takeaway:} Emphasizing interpretability, energy efficiency, and decentralized models is critical for the future acceptance of neural networks in society.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Code Snippet Example - Federated Learning}
\begin{lstlisting}[language=Python]
# Python pseudo-code for a simple federated learning process
def federated_learning():
    global_model = initialize_model()
    for round in range(num_rounds):
        local_models = []
        for client in clients:
            local_model = train_on_client_data(client.data, global_model)
            local_models.append(local_model)
        global_model = aggregate_models(local_models)
    return global_model
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Part 1}
    \begin{block}{Key Takeaways from Week 3: Neural Networks}
        \begin{itemize}
            \item Neural networks mimic human brain processes, serving as a foundation for modern AI.
            \item Major components include neurons, layers, weights, and activation functions.
        \end{itemize}
    \end{block}
    
    \begin{block}{1. What are Neural Networks?}
        \begin{itemize}
            \item Definition: Computational models inspired by the human brain.
            \item Components: Nodes (neurons), layers (input, hidden, output), weights, activation functions.
        \end{itemize}
    \end{block}
    
    \begin{block}{2. Architecture of Neural Networks}
        \begin{itemize}
            \item Input Layer: Receives external data.
            \item Hidden Layers: Perform computations; depth determines complexity.
            \item Output Layer: Produces predictions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Part 2}
    \begin{block}{3. Activation Functions}
        \begin{itemize}
            \item Introduce non-linearity, allowing networks to learn complex patterns.
            \item Common Types:
            \begin{itemize}
                \item Sigmoid: Outputs between 0 and 1 (binary classification).
                \item ReLU: Outputs input if positive, otherwise zero (mitigates vanishing gradient).
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{4. Learning Process}
        \begin{itemize}
            \item Training involves adjusting weights using backpropagation.
            \item Formula for weight update:
            \begin{equation}
                w = w - \eta \cdot \frac{\partial L}{\partial w}
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Part 3}
    \begin{block}{5. Loss Functions}
        \begin{itemize}
            \item Measure the difference between predicted and actual outcomes.
            \item Examples:
            \begin{itemize}
                \item Mean Squared Error for regression.
                \item Cross-Entropy Loss for classification.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{6. Overfitting and Regularization}
        \begin{itemize}
            \item Overfitting occurs when a model captures noise, leading to poor generalization.
            \item Mitigation Techniques:
            \begin{itemize}
                \item Dropout: Randomly drops neurons during training.
                \item L2 Regularization: Adds a penalty for large weights.
            \end{itemize}
        \end{itemize}
    \end{block} 

    \begin{block}{7. Applications of Neural Networks}
        \begin{itemize}
            \item Fields include:
            \begin{itemize}
                \item Image Recognition
                \item Natural Language Processing
                \item Healthcare
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}


\end{document}