\frametitle{Activation Functions - Common Types}
    \begin{enumerate}
        \item \textbf{Sigmoid Activation Function}
            \begin{itemize}
                \item \textbf{Formula:}
                \begin{equation}
                    \sigma(x) = \frac{1}{1 + e^{-x}}
                \end{equation}
                \item \textbf{Range:} (0, 1)
                \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Smooth and continuously differentiable.
                    \item Useful for binary classification.
                    \item Suffers from vanishing gradient problem.
                \end{itemize}
                \item \textbf{Example Usage:} Output layer of binary classifiers.
            \end{itemize}

        \item \textbf{ReLU (Rectified Linear Unit)}
            \begin{itemize}
                \item \textbf{Formula:}
                \begin{equation}
                    \text{ReLU}(x) = \max(0, x)
                \end{equation}
                \item \textbf{Range:} [0, âˆž)
                \item \textbf{Characteristics:}
                \begin{itemize}
                    \item Simple calculation, mitigates vanishing gradient.
                    \item Can suffer from "dying ReLU".
                \end{itemize}
                \item \textbf{Example Usage:} Hidden layers of deep learning models.
            \end{itemize}
    \end{enumerate}
