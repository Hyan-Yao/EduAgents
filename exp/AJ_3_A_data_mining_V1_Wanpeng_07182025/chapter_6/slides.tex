\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Association Rule Mining]{Week 6: Association Rule Mining}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Association Rule Mining - Overview}
    \begin{block}{What is Association Rule Mining?}
        Association Rule Mining is a data mining technique used to discover interesting relationships or correlations between variables in large datasets.
    \end{block}
    \begin{block}{Purpose}
        It involves finding patterns where the occurrence of one item is associated with the occurrence of another. A common application of this technique is Market Basket Analysis, which examines purchase behaviors of customers in retail environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Association Rule Mining - Significance}
    \begin{itemize}
        \item \textbf{Understanding Consumer Behavior:} 
        By analyzing purchasing patterns, retailers can uncover insights into what items are frequently bought together. This information provides crucial strategies for promotions, product placements, and inventory management.
        
        \item \textbf{Driving Sales and Marketing Strategies:} 
        Retailers can use these insights to create targeted marketing campaigns, bundles, and personalized recommendations, thus increasing sales and customer satisfaction.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminology in Association Rule Mining}
    \begin{enumerate}
        \item \textbf{Itemset:} A collection of one or more items. For instance, $\{Bread, Milk\}$ is an itemset.
        
        \item \textbf{Support:} Indicates how frequently a rule appears in the dataset. It is calculated as:
        \begin{equation}
            \text{Support}(A \Rightarrow B) = \frac{\text{Number of transactions containing both A and B}}{\text{Total number of transactions}}
        \end{equation}
        
        \item \textbf{Confidence:} Measures how often items in B are purchased when item A is purchased. It is calculated as:
        \begin{equation}
            \text{Confidence}(A \Rightarrow B) = \frac{\text{Support}(A \cap B)}{\text{Support}(A)}
        \end{equation}
        
        \item \textbf{Lift:} Indicates the strength of the association between A and B, comparing the observed support with the expected support if A and B were independent:
        \begin{equation}
            \text{Lift}(A \Rightarrow B) = \frac{\text{Support}(A \Rightarrow B)}{\text{Support}(A) \times \text{Support}(B)}
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Market Basket Analysis - Definition}
    \begin{block}{Definition}
        Market Basket Analysis (MBA) is a data mining technique used to understand the purchasing behavior of customers by analyzing the co-occurrences of items in transactions. 
    \end{block}
    \begin{block}{Purpose}
        The primary purposes of Market Basket Analysis are to:
        \begin{itemize}
            \item \textbf{Reveal Buying Patterns:} Discover products frequently purchased together.
            \item \textbf{Optimize Product Placement:} Arrange products on shelves to enhance visibility and sales.
            \item \textbf{Targeted Promotions:} Create bundled offers based on common purchase combinations.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Market Basket Analysis - Key Points}
    \begin{enumerate}
        \item \textbf{Co-occurrence:} Identifies items often bought together (e.g., bread, butter, jam).
        \item \textbf{Data-Driven Decisions:} Informs marketing, stocking decisions, and cross-selling opportunities.
        \item \textbf{Consumer Insights:} Tailors marketing strategies based on buying patterns for improved satisfaction and loyalty.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Market Basket Analysis - Example and Formulas}
    \begin{block}{Example}
        A supermarket analyzing transaction data might find:
        \begin{itemize}
            \item Transaction 1: [Bread, Butter, Jam]
            \item Transaction 2: [Bread, Milk]
            \item Transaction 3: [Beer, Chips]
            \item Transaction 4: [Bread, Butter]
        \end{itemize}
        Insights: Bread and Butter are frequently bought together.
    \end{block}
    
    \begin{block}{Formulas}
        \textbf{Support:} 
        \begin{equation}
            \text{Support}(A) = \frac{\text{Number of transactions containing A}}{\text{Total number of transactions}}
        \end{equation}

        \textbf{Confidence:} 
        \begin{equation}
            \text{Confidence}(A \rightarrow B) = \frac{\text{Support}(A \cap B)}{\text{Support}(A)}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Market Basket Analysis - Conclusion}
    \begin{block}{Conclusion}
        Market Basket Analysis is a vital technique for understanding consumer behaviors and enhancing sales strategies. By analyzing transaction data, businesses can uncover insights leading to better inventory management and improved customer satisfaction through personalized shopping experiences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Association Rules - What Are Association Rules?}
    \begin{block}{Definition}
        Association rules are a fundamental concept in data mining used to uncover relationships between variables in large datasets. 
        They are particularly useful in Market Basket Analysis, where the goal is to analyze consumer purchase patterns.
    \end{block}
    \begin{itemize}
        \item Format: **{Antecedent} → {Consequent}**
        \item \textbf{Antecedent (LHS)}: Condition (item or set of items).
        \item \textbf{Consequent (RHS)}: Outcome (predicted item or set of items).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Association Rules - Example}
    \begin{block}{Example}
        Consider a grocery store where the rule:
        \[
        \textbf{{Bread} → {Butter}}
        \]
        suggests that if a customer buys bread, they are likely to buy butter as well.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Association Rules - Evaluation Metrics}
    \begin{block}{Overview}
        The strength and significance of association rules can be measured using three important metrics: Support, Confidence, and Lift.
    \end{block}
    \begin{enumerate}
        \item \textbf{Support}
        \begin{itemize}
            \item Definition: Proportion of transactions that contain both the antecedent and consequent.
            \item Formula: 
            \[
            \text{Support}(X \rightarrow Y) = \frac{N(X \cap Y)}{N(T)}
            \]
            \item Interpretation: Higher support means greater practical interest.
        \end{itemize}

        \item \textbf{Confidence}
        \begin{itemize}
            \item Definition: Likelihood that the consequent occurs given the antecedent.
            \item Formula: 
            \[
            \text{Confidence}(X \rightarrow Y) = \frac{N(X \cap Y)}{N(X)}
            \]
            \item Interpretation: Higher confidence indicates stronger predictive power.
        \end{itemize}

        \item \textbf{Lift}
        \begin{itemize}
            \item Definition: How much more likely the consequent is when the antecedent is present.
            \item Formula: 
            \[
            \text{Lift}(X \rightarrow Y) = \frac{\text{Confidence}(X \rightarrow Y)}{\text{Support}(Y)}
            \]
            \item Interpretation:
            \begin{itemize}
                \item Lift > 1: Positive association.
                \item Lift = 1: No association.
                \item Lift < 1: Negative association.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Association Rules - Key Points and Conclusion}
    \begin{itemize}
        \item Both antecdent and consequent help formulate effective marketing strategies.
        \item Support, confidence, and lift provide diverse views on rule significance.
        \item Association rules reveal hidden patterns leading to actionable insights.
    \end{itemize}
    \begin{block}{Conclusion}
        Mastering association rules enables analysts and marketers to predict consumer behavior, optimize inventory, and enhance customer satisfaction in retail.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apriori Algorithm - Introduction}
    \begin{block}{What is the Apriori Algorithm?}
        The \textbf{Apriori algorithm} is a key data mining technique that is used for:
        \begin{itemize}
            \item Mining frequent itemsets
            \item Generating association rules
        \end{itemize}
        It helps identify relationships between items in large datasets, particularly in market basket analysis to understand consumer purchasing behavior.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apriori Algorithm - Key Concepts}
    \begin{block}{Frequent Itemsets}
        Sets of items that appear together in transactions with a frequency above a predefined threshold (support).
    \end{block}
    
    \begin{block}{Association Rules}
        Implications of the form \( A \rightarrow B \) where \( A \) and \( B \) are itemsets. The strength of the association is measured by:
        \begin{itemize}
            \item \textbf{Support:} 
            \[
            support(A \rightarrow B) = \frac{\text{Number of transactions containing } (A \cup B)}{\text{Total number of transactions}}
            \]
            \item \textbf{Confidence:} 
            \[
            confidence(A \rightarrow B) = \frac{support(A \cup B)}{support(A)}
            \]
            \item \textbf{Lift:} 
            \[
            lift(A \rightarrow B) = \frac{confidence(A \rightarrow B)}{support(B)}
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apriori Algorithm - Example and Steps}
    \begin{block}{Example Scenario}
        Consider a grocery store dataset:
        \begin{center}
            \begin{tabular}{|c|l|}
                \hline
                Transaction ID & Items Purchased \\
                \hline
                T1 & Milk, Bread \\
                T2 & Beer, Bread, Diapers \\
                T3 & Milk, Diapers, Beer \\
                T4 & Bread, Diapers \\
                T5 & Milk, Beer \\
                \hline
            \end{tabular}
        \end{center}
        \textbf{Frequent Itemsets:} E.g., \{Milk, Bread\} could be frequent if it appears in at least 3 transactions.
    \end{block}

    \begin{block}{Steps in the Apriori Algorithm}
        \begin{enumerate}
            \item Generate frequent 1-itemsets: Count the support for each item, remove those below the support threshold.
            \item Iterate: Use the frequent itemsets to generate candidate 2-itemsets and count their supports.
            \item Repeat until no new frequent itemsets are found.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps of the Apriori Algorithm - Overview}
    \begin{block}{Introduction}
        The Apriori algorithm is a fundamental method in association rule mining used to identify frequent itemsets in a transactional dataset.
    \end{block}
    \begin{itemize}
        \item Step 1: Generate Frequent Itemsets
        \item Step 2: Candidate Generation (Level k)
        \item Step 3: Prune Candidates
        \item Step 4: Repeat Steps
        \item Key Points to Remember
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 1: Generate Frequent Itemsets}
    \begin{enumerate}
        \item \textbf{Set a Minimum Support Threshold}
        \begin{itemize}
            \item Determine the minimum support level (e.g., 30\%).
            \item Identify the support of itemsets, which is the proportion of transactions in which they appear.
        \end{itemize}

        \item \textbf{Scan the Dataset (L1)}
        \begin{itemize}
            \item Count the support of each individual item (1-itemsets).
            \item Form a set of frequent itemsets (L1).
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Example}
        Consider the transactions:
        \begin{verbatim}
        T1: {A, B, C}
        T2: {A, B}
        T3: {A, C}
        T4: {B, C}
        T5: {A}
        \end{verbatim}
        If minimum support is set at 60\%:
        \begin{itemize}
            \item Support(A) = 4/5 = 80\%
            \item Support(B) = 3/5 = 60\%
            \item Support(C) = 3/5 = 60\%
        \end{itemize}
        Frequent itemsets L1: \{A\}, \{B\}, \{C\}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 2: Candidate Generation and Pruning}
    \begin{enumerate}
        \item \textbf{Generate Candidates (Ck)}
        \begin{itemize}
            \item Use frequent itemsets from Level k-1 (Lk-1) to generate new candidates (Ck).
            \item \textbf{Join Step}: Combine pairs from Lk-1 sharing k-2 items.
        \end{itemize}

        \item \textbf{Pruning}
        \begin{itemize}
            \item Eliminate candidates from Ck with infrequent subsets. 
            \item If any k-1 item subset is not in Lk-1, prune the candidate.
        \end{itemize}
        
        \begin{block}{Example}
            If L1 = \{A\}, \{B\}, \{C\}, then C2 = \{A, B\}, \{A, C\}, \{B, C\}.
            For candidate \{A, B\}, if subset \{A\} is present in L1, it is retained.
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{FP-Growth Algorithm - Introduction}
  The FP-Growth (Frequent Pattern Growth) algorithm is a widely used method for mining frequent itemsets in large datasets. It serves as an efficient alternative to the Apriori algorithm, addressing some of its limitations, especially in terms of performance and scalability.
\end{frame}

\begin{frame}[fragile]
  \frametitle{FP-Growth Algorithm - Key Concepts}
  \begin{itemize}
    \item \textbf{Frequent Itemsets}: Groups of items that appear together in transactions more than a specified minimum support threshold.
    \item \textbf{Data Structure}: Uses a compact data structure known as the FP-tree (Frequent Pattern Tree) to store the input dataset in a compressed form.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{FP-Growth Algorithm - Steps}
  \begin{enumerate}
    \item \textbf{Building the FP-Tree}
      \begin{itemize}
        \item Scan the dataset to determine item frequency.
        \item Discard infrequent items based on a given support threshold.
        \item Create a header table that contains frequent items and their counts.
        \item Construct the FP-tree by inserting transactions, keeping the frequent items in the order defined by the header table.
      \end{itemize}
    \item \textbf{Mining the FP-Tree}
      \begin{itemize}
        \item Starting from each item in the header table, create a conditional pattern base (sub-dataset).
        \item Construct a conditional FP-tree for each pattern base.
        \item Recursively mine these conditional FP-trees for frequent itemsets.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Apriori and FP-Growth - Overview}
    \begin{block}{Overview of Algorithms}
        \begin{itemize}
            \item \textbf{Apriori}: A classic algorithm used for mining frequent itemsets and generating association rules via a bottom-up approach.
            \item \textbf{FP-Growth}: An advanced algorithm that constructs a compact data structure called an FP-tree for efficient mining of frequent patterns without candidate generation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Apriori and FP-Growth - Key Differences}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Feature}            & \textbf{Apriori}                                      & \textbf{FP-Growth}                                   \\ \hline
            Data Structure              & Traditional database structure                        & Compressed FP-tree structure                         \\ \hline
            Candidate Generation         & Multiple candidate itemsets (combinatorial explosion) & Single pass to build a condensed tree                \\ \hline
            Efficiency                  & Slower (multiple database scans)                      & Faster (only two scans required)                     \\ \hline
            Scalability                 & Decreased performance with large datasets             & Scalable for large datasets                           \\ \hline
            Memory Usage                & Higher with increasing dataset size                   & More memory-efficient due to compact tree            \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Apriori and FP-Growth - Conclusion and Key Takeaways}
    \begin{block}{Efficiency and Scalability}
        \begin{itemize}
            \item \textbf{Efficiency}: FP-Growth is significantly faster than Apriori, especially with large datasets, requiring only two scans through the data.
            \item \textbf{Scalability}: FP-Growth effectively handles vast datasets, making it suitable for real-world applications.
        \end{itemize}
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Use Apriori for smaller datasets where candidate generation is manageable.
            \item Opt for FP-Growth for larger datasets to enhance performance and minimize resource consumption.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Association Rules - Introduction}
    \begin{block}{Overview}
        In association rule mining, evaluating the generated rules is vital to determine their usefulness and relevance. 
    \end{block}
    \begin{itemize}
        \item Key metrics for evaluation include:
        \begin{itemize}
            \item Support
            \item Confidence
            \item Lift
        \end{itemize}
        \item Understanding these metrics helps filter out unimportant rules and focus on those that indicate meaningful relationships within the data.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Association Rules - Key Metrics}
    \begin{block}{1. Support}
        \begin{itemize}
            \item \textbf{Definition}: Measures the frequency of an itemset in the dataset.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Support}(A) = \frac{\text{Number of transactions containing } A}{\text{Total number of transactions}}
            \end{equation}
            \item \textbf{Example}: 
            For a dataset of 1,000 transactions, if 200 transactions include both {Bread} and {Butter}:
            \begin{equation}
                \text{Support} = \frac{200}{1000} = 0.2 \text{ or } 20\%
            \end{equation}
            \item \textbf{Key Point}: High support indicates reliability of the rule.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Association Rules - Key Metrics (Continued)}
    \begin{block}{2. Confidence}
        \begin{itemize}
            \item \textbf{Definition}: Likelihood that the rule holds true, indicating how often the items appear together.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Confidence}(A \rightarrow B) = \frac{\text{Support}(A \cup B)}{\text{Support}(A)}
            \end{equation}
            \item \textbf{Example}: For the rule {Bread} → {Butter}, if 300 transactions include Bread:
            \begin{equation}
                \text{Confidence} = \frac{200}{300} \approx 0.67 \text{ or } 67\%
            \end{equation}
            \item \textbf{Key Point}: Higher confidence indicates a strong association between the items.
        \end{itemize}
    \end{block}

    \begin{block}{3. Lift}
        \begin{itemize}
            \item \textbf{Definition}: Quantifies the likelihood that the items are found together compared to their independent occurrence.
            \item \textbf{Formula}:
            \begin{equation}
                \text{Lift}(A \rightarrow B) = \frac{\text{Confidence}(A \rightarrow B)}{\text{Support}(B)}
            \end{equation}
            \item \textbf{Example}: If the support of Butter is 0.4:
            \begin{equation}
                \text{Lift} = \frac{0.67}{0.4} = 1.675
            \end{equation}
            \item \textbf{Interpretation}: Lift > 1 indicates A and B are positively correlated.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluating Association Rules - Summary and Conclusion}
    \begin{block}{Summary}
        \begin{itemize}
            \item Support, Confidence, and Lift are vital metrics for evaluating the quality of association rules.
            \item Understanding these metrics allows analysts to filter and select the most relevant rules.
        \end{itemize}
    \end{block}
    \begin{block}{Application}
        \begin{itemize}
            \item Use these metrics to refine rules for specific domains, ensuring strong, relevant associations.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Evaluating association rules is essential for effective mining of actionable insights from large datasets, crucial in many fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Association Rule Mining}
    \begin{block}{Overview}
        Association Rule Mining (ARM) identifies interesting relationships, correlations, and patterns among items in large databases.
        Applications are prevalent in sectors like retail, healthcare, and e-commerce.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in Retail}
    \begin{block}{Retail Market Basket Analysis}
        \begin{itemize}
            \item \textbf{Concept}: Analyze consumer purchasing patterns.
            \item \textbf{Example}: 
            A grocery store may find that customers purchasing bread also buy butter. 
            This leads to strategic placements or promotional bundling.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in Healthcare}
    \begin{block}{Healthcare}
        \begin{itemize}
            \item \textbf{Concept}: Identify associations between patients' medical histories, symptoms, and treatments.
            \item \textbf{Example}: 
            Patients treated for hypertension often also have diabetes, aiding preventive care and treatment planning.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in E-commerce and Web Usage}
    \begin{block}{E-commerce Personalization}
        \begin{itemize}
            \item \textbf{Concept}: Use browsing and purchasing history to provide personalized recommendations.
            \item \textbf{Example}: 
            Suggesting related items (e.g., climbing accessories) to users interested in hiking gear.
        \end{itemize}
    \end{block}
    
    \begin{block}{Web Usage Mining}
        \begin{itemize}
            \item \textbf{Concept}: Analyze user navigation patterns on websites.
            \item \textbf{Example}: Highlighting a popular blog on the homepage based on user navigation trends.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications in Telecommunications}
    \begin{block}{Telecommunications}
        \begin{itemize}
            \item \textbf{Concept}: Analyze call data to identify usage patterns.
            \item \textbf{Example}: Target promotions for international calling plans based on user subscription patterns.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics in Association Rule Mining}
    \begin{itemize}
        \item \textbf{Support}: Frequency of item occurrence in the dataset (higher support = more common associations).
        \item \textbf{Confidence}: Likelihood of one item's presence given another (high confidence = reliable association).
        \item \textbf{Lift}: Strength of a rule over random chance (lift > 1 indicates positive correlation).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustration: Retail Scenario}
    \begin{table}[h]
    \centering
    \begin{tabular}{|c|l|}
        \hline
        Transaction ID & Items Purchased \\
        \hline
        1 & Bread, Butter \\
        2 & Bread, Diapers \\
        3 & Butter, Diapers \\
        4 & Bread, Butter, Diapers \\
        \hline
    \end{tabular}
    \caption{Transaction Table}
    \end{table}
    
    \textbf{Association Rule}: \{Bread\} $\rightarrow$ \{Butter\}
    \begin{itemize}
        \item \textbf{Support}: $\frac{3}{4} = 0.75$ (75\% of transactions contain bread)
        \item \textbf{Confidence}: $\frac{3}{3} = 1.0$ (100\% of transactions with bread also include butter)
        \item \textbf{Lift}: Evaluate better correlation between Bread and Butter purchases.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    Association Rule Mining (ARM) plays a vital role in various industries, providing insights that improve customer understanding and decision-making. Its applications in retail, healthcare, e-commerce, and telecommunications demonstrate its effectiveness in enhancing operational efficiency and customer engagement.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Market Basket Analysis - Introduction}
    \begin{block}{Overview}
        Market Basket Analysis (MBA) is a data mining technique used to discover associations between items purchased together in a retail environment.

        This enables retailers to:
        \begin{itemize}
            \item Understand consumer buying patterns
            \item Personalize marketing strategies effectively
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Market Basket Analysis - Key Concepts}
    \begin{itemize}
        \item \textbf{Association Rule Mining}: Identifying interesting relationships between variables in large datasets.
        \item \textbf{Support}: Proportion of transactions containing specific items.
        \begin{equation}
            \text{Support}(A) = \frac{\text{Number of transactions containing } A}{\text{Total number of transactions}}
        \end{equation}
        \item \textbf{Confidence}: Likelihood that item B is purchased when item A is purchased.
        \begin{equation}
            \text{Confidence}(A \Rightarrow B) = \frac{\text{Support}(A \cup B)}{\text{Support}(A)}
        \end{equation}
        \item \textbf{Lift}: Ratio of observed support to expected support if A and B were independent.
        \begin{equation}
            \text{Lift}(A \Rightarrow B) = \frac{\text{Confidence}(A \Rightarrow B)}{\text{Support}(B)}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Market Basket Analysis - Example Analysis}
    \begin{block}{Sample Dataset}
        \begin{tabular}{|c|l|}
            \hline
            Transaction ID & Items Purchased            \\
            \hline
            1              & Bread, Milk, Diaper        \\
            2              & Bread, Coffee              \\
            3              & Milk, Diaper, Beer         \\
            4              & Diaper, Beer, Cola         \\
            5              & Milk, Bread, Diaper, Beer  \\
            \hline
        \end{tabular}
    \end{block}
    
    \begin{enumerate}
        \item Calculate Support:
        \begin{itemize}
            \item Support(bread) = 3/5 = 0.6
            \item Support(milk) = 4/5 = 0.8
            \item Support(bread, milk) = 2/5 = 0.4
        \end{itemize}

        \item Calculate Confidence:
        \begin{itemize}
            \item Confidence(bread $\rightarrow$ milk) = 0.4 / 0.6 = 0.67
        \end{itemize}

        \item Calculate Lift:
        \begin{itemize}
            \item Lift(bread $\rightarrow$ milk) = (0.67) / (0.8) = 0.84
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Market Basket Analysis - Key Insights}
    \begin{itemize}
        \item \textbf{Insights for Retailers}: Understanding item associations aids in product placements, promotions, and inventory management.
        \item \textbf{Cross-Selling Opportunities}: Implement strategies based on associations to encourage purchases of complementary items.
        \item \textbf{Data-Driven Decisions}: Leverage data for informed decisions, increasing sales and enhancing customer satisfaction.
    \end{itemize}

    \begin{block}{Conclusion}
        Market Basket Analysis helps uncover hidden consumer behavior patterns, driving business success through actionable insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Market Basket Analysis - Next Steps}
    Stay tuned for the hands-on exercise where you will apply the concepts learned here using Python or R!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Exercise in Association Rule Mining}
    \begin{block}{Objective}
        In this guided exercise, you will apply the \textbf{Apriori} and \textbf{FP-Growth} algorithms to real datasets, enhancing your understanding of association rule mining through practical experience in either Python or R.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Algorithms}
    \begin{enumerate}
        \item \textbf{Apriori Algorithm}
        \begin{itemize}
            \item A classic algorithm for mining frequent itemsets and generating association rules.
            \item Based on the principle that if an itemset is frequent, all of its subsets must also be frequent.
        \end{itemize}

        \begin{block}{Key Steps}
            \begin{itemize}
                \item Generate candidate itemsets.
                \item Calculate support for each candidate.
                \item Prune candidates that do not meet the minimum support threshold.
                \item Continue until no new frequent itemsets can be generated.
            \end{itemize}
        \end{block}
        
        \begin{block}{Code Snippet}
            \begin{lstlisting}[language=Python]
from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

# Load your dataset
dataset = pd.read_csv('transactions.csv')

# Perform Apriori
frequent_itemsets = apriori(dataset, min_support=0.05, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

print(rules)
            \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{FP-Growth Algorithm}
    \begin{enumerate}
        \item \textbf{FP-Growth Algorithm}
        \begin{itemize}
            \item A more efficient algorithm using a frequent pattern tree to represent itemsets.
            \item Avoids the candidate generation step used in Apriori, making it faster for large datasets.
        \end{itemize}

        \begin{block}{Key Steps}
            \begin{itemize}
                \item Construct the FP-tree from the dataset.
                \item Extract frequent itemsets directly from the FP-tree.
            \end{itemize}
        \end{block}
        
        \begin{block}{Code Snippet}
            \begin{lstlisting}[language=Python]
from mlxtend.frequent_patterns import fpgrowth

# Perform FP-Growth
frequent_itemsets_fp = fpgrowth(dataset, min_support=0.05, use_colnames=True)

print(frequent_itemsets_fp)
            \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Activity}
    \begin{enumerate}
        \item \textbf{Step 1: Dataset Preparation}
        \begin{itemize}
            \item Choose a suitable dataset (e.g., retail transactions or online shopping data).
            \item Prepare the data in a format suitable for analysis (e.g., one-hot encoded).
        \end{itemize}

        \item \textbf{Step 2: Apply Algorithms}
        \begin{itemize}
            \item Run both Apriori and FP-Growth processes using the provided code snippets.
            \item Experiment with different support thresholds to observe their effect on the number of rules generated.
        \end{itemize}

        \item \textbf{Step 3: Analyze Results}
        \begin{itemize}
            \item Interpret the output from both algorithms.
            \item Discuss insights on which items are frequently purchased together.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Understand the differences between Apriori and FP-Growth in terms of efficiency and process.
            \item The choice of minimum support can significantly impact the results.
            \item Association rules have real-world applications in recommendations and market basket analysis.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        By completing this exercise, you will gain hands-on experience in applying association rule mining techniques, equipping you with practical skills for data analysis in retail and other domains.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Association Mining - Introduction}
    \begin{itemize}
        \item \textbf{Data Privacy}: 
        It is crucial to respect individual privacy and comply with data protection laws (e.g., GDPR, HIPAA).
        \item \textbf{Responsible Data Mining}: 
        Emphasizes the integrity and ethical use of data, understanding potential consequences of insights derived.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Association Mining - Key Implications}
    \begin{enumerate}
        \item \textbf{Informed Consent}: 
        Ensure explicit consent from individuals whose data is analyzed.
        \item \textbf{Anonymization}: 
        Protect identities by employing anonymization techniques.
        \item \textbf{Bias and Discrimination}: 
        Assess datasets to avoid perpetuating inequalities.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Association Mining - Examples of Ethical Issues}
    \begin{itemize}
        \item \textbf{Targeting Vulnerable Groups}:
        Risks of excluding demographic groups (e.g., exclusive offers for young people).
        \item \textbf{Data Breaches}:
        Inadequate data protection exposes sensitive information, leading to legal repercussions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Association Mining - Best Practices}
    \begin{enumerate}
        \item \textbf{Review and Audit}: 
        Conduct regular audits to identify risks and ensure compliance.
        \item \textbf{Stakeholder Engagement}: 
        Involve stakeholders in discussions to foster transparency.
        \item \textbf{Continuous Training}: 
        Educate data scientists about ethical implications in data mining.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Association Mining - Conclusion}
    \begin{block}{Conclusion}
        Ethical considerations in association mining ensure trust, integrity, and social responsibility. Always prioritize ethics alongside technical proficiency for positive societal outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Takeaways - Overview}
  \begin{block}{Overview of Association Rule Mining}
    Association rule mining is a fundamental method in data mining that uncovers interesting relationships between variables in large datasets. 
    It is widely used in market basket analysis, customer segmentation, recommendation systems, and more.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Takeaways - Key Concepts}
  \begin{enumerate}
    \item **Definition of Association Rules**:
      \begin{itemize}
        \item An association rule is typically written as \( A \Rightarrow B \) (if A occurs, then B is likely to occur).
        \item Example: If a customer buys bread, they are likely to also buy butter.
      \end{itemize}
      
    \item **Metrics of Association**:
      \begin{itemize}
        \item **Support**: \( \text{Support}(A, B) = \frac{\text{Count}(A \cap B)}{\text{Total Number of Transactions}} \)
        \item **Confidence**: \( \text{Confidence}(A \Rightarrow B) = \frac{\text{Support}(A \cap B)}{\text{Support}(A)} \)
        \item **Lift**: \( \text{Lift}(A \Rightarrow B) = \frac{\text{Confidence}(A \Rightarrow B)}{\text{Support}(B)} \)
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Takeaways - Applications and Ethics}
  \begin{block}{Applications in Real World}
    \begin{itemize}
      \item **Retail**: Identifying products frequently bought together for enhanced cross-selling strategies.
      \item **Web Analytics**: Understanding user behaviors for better navigation and content recommendations.
      \item **Healthcare**: Discovering patterns in patient diagnoses and treatments for improved healthcare delivery.
    \end{itemize}
  \end{block}
  
  \begin{block}{Importance of Ethical Considerations}
    While association rule mining is powerful, it’s crucial to consider data privacy and ethical guidelines. Misuse of customer data can lead to breaches of trust and regulatory consequences. Ensure data is handled responsibly, emphasizing transparency and consent.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Takeaways - Final Thoughts}
  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item **Data-Driven Insights**: Enables organizations to make informed decisions based on customer behavior patterns.
      \item **Boosting Sales and Engagement**: Improves marketing strategies, enhancing customer satisfaction and loyalty.
      \item **Analytical Skill Enhancement**: Develops critical analytical skills valuable across many domains.
    \end{itemize}
  \end{block}
  
  This conclusion encapsulates the essence of association rule mining and reinforces the methodologies and ethical considerations discussed throughout this chapter.
\end{frame}


\end{document}