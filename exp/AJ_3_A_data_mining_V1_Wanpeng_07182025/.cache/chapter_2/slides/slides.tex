\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Data Preprocessing]{Week 2: Data Preprocessing}
\author[John Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Preprocessing - Overview}
    \begin{block}{Overview}
        Data preprocessing transforms raw data into a clean dataset, essential for ensuring quality and effectiveness in data analysis. Poor preprocessing can lead to misleading insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Preprocessing}
    \begin{enumerate}
        \item \textbf{Enhances Data Quality}
        \begin{itemize}
            \item \textbf{Accuracy:} Eliminates errors and inconsistencies.
            \item \textbf{Completeness:} Fills in missing values.
            \item \textit{Example:} Imputation of missing emails in a customer database for marketing.
        \end{itemize}
        
        \item \textbf{Facilitates Effective Analysis}
        \begin{itemize}
            \item \textbf{Normalization:} Scales data for model training.
            \item \textbf{Standardization:} Converts data to comparable formats.
            \item \textit{Example:} Standardizing age data recorded in different units.
        \end{itemize}
        
        \item \textbf{Reduces Dimensionality}
        \begin{itemize}
            \item Eliminates redundant features for optimal model performance.
            \item \textit{Example:} Combining overlapping attributes in house pricing data.
        \end{itemize}
        
        \item \textbf{Improves Model Performance}
        \begin{itemize}
            \item Cleaned data enhances the robustness and accuracy of models.
            \item \textit{Example:} A decision tree classifier trained on processed data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Techniques}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Data preprocessing influences analysis outcomes significantly.
            \item Ignoring preprocessing can result in garbage-in, garbage-out scenarios.
            \item Common steps include cleaning, transformation, normalization, and handling missing data.
        \end{itemize}
    \end{block}

    \begin{block}{Formulas}
        \begin{equation}
            x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)}
        \end{equation}
        \textit{Normalizes feature values to a range of 0 to 1.}

        \begin{equation}
            z = \frac{x - \mu}{\sigma}
        \end{equation}
        \textit{Standardizes data to have mean of 0 and standard deviation of 1.}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Data preprocessing is foundational for accurate analysis and decision-making. Proper preprocessing practices significantly impact the quality of outcomes in data-driven projects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Overview}
    \begin{block}{Learning Objectives for Week 2: Data Preprocessing}
        In this weekâ€™s module, we will focus on the crucial aspect of data preprocessing, specifically emphasizing data cleaning techniques and their applications. By the end of this week, you will be able to:
    \end{block}
    \begin{enumerate}
        \item Understand Data Cleaning
        \item Identify Common Data Quality Issues
        \item Apply Data Cleaning Techniques
        \item Utilize Data Cleaning Tools
        \item Evaluate the Impact of Data Cleaning
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Data Cleaning Techniques}
    \begin{block}{Understanding Data Cleaning}
        Gain a comprehensive definition of data cleaning and recognize its significance in data preprocessing. Data cleaning ensures that the dataset is free from errors and inconsistencies, impacting the reliability of your analyses.
    \end{block}

    \begin{block}{Common Data Quality Issues}
        Recognize typical issues like:
        \begin{itemize}
            \item Missing Values
            \item Duplicates
            \item Inconsistencies
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Practical Applications}
    \begin{block}{Applying Data Cleaning Techniques}
        Evaluate and implement various data cleaning techniques through hands-on examples:
        \begin{lstlisting}[language=Python]
import pandas as pd

df = pd.read_csv('data.csv')
# Check for missing values
print(df.isnull().sum())
# Fill missing values with the mean of the column
df['column_name'].fillna(df['column_name'].mean(), inplace=True)
        \end{lstlisting}
    \end{block}

    \begin{block}{Utilizing Data Cleaning Tools}
        Familiarize with tools like OpenRefine and DataCleaner that assist in managing large datasets.
    \end{block}
    
    \begin{block}{Evaluating Impact}
        Understand how effective data cleaning influences analysis outcomes and learn to quantify improvements in data quality.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning: Definition}
    \begin{block}{What is Data Cleaning?}
    Data cleaning, also known as data cleansing, is a crucial step in data preprocessing involving the identification and correction of errors and inconsistencies in the data. Its primary goal is to ensure that the data set is accurate, complete, reliable, and usable for analysis.
    \end{block}

    \begin{block}{Role in Data Preprocessing}
    Data cleaning serves as the foundation for all subsequent data analysis processes. Without clean data, any insights or conclusions drawn from the analysis can be flawed, potentially leading to incorrect decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Quality Issues}
    \begin{enumerate}
        \item \textbf{Missing Values}
        \begin{itemize}
            \item \textbf{Definition:} Gaps in data where no value is recorded.
            \item \textbf{Example:} Students without recorded grades.
            \item \textbf{Impact:} Can distort analyses if not handled properly.
        \end{itemize}
        
        \item \textbf{Duplicates}
        \begin{itemize}
            \item \textbf{Definition:} Multiple entries for the same record.
            \item \textbf{Example:} Duplicate customer entries in a database.
            \item \textbf{Impact:} Inflates results and skews analytical outcomes.
        \end{itemize}
        
        \item \textbf{Inconsistencies}
        \begin{itemize}
            \item \textbf{Definition:} Discrepancies in data representation or format.
            \item \textbf{Example:} Different date formats in the dataset.
            \item \textbf{Impact:} Complicates analysis and comparisons.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points on Data Cleaning}
    \begin{itemize}
        \item Data cleaning is essential for reliable analytics and decision-making.
        \item Addressing missing values, duplicates, and inconsistencies enhances data quality.
        \item Proper data cleaning leads to better insights and more effective analyses.
    \end{itemize}

    \begin{block}{Data Cleaning Process}
        \begin{enumerate}
            \item \textbf{Assessment of Data Quality:} Identify errors, missing values, and duplicates.
            \item \textbf{Data Transformation:} Apply methods like imputation and deduplication.
            \item \textbf{Validation and Refinement:} Verify corrected data to ensure accuracy.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Techniques for Data Cleaning}
    \begin{block}{Introduction}
        Data cleaning is a crucial step in the data preprocessing phase. It ensures that the data used for analysis is accurate, complete, and consistent. In this slide, we will discuss several key techniques for data cleaning:
        \begin{itemize}
            \item Removal of duplicates
            \item Handling missing values
            \item Correcting inconsistencies
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Removal of Duplicates}
    \begin{itemize}
        \item \textbf{Definition:} Duplicate data refers to identical records present within a dataset that can skew analysis results.
        \item \textbf{Techniques:}
        \begin{itemize}
            \item \textbf{Identifying Duplicates:} Use methods like \texttt{.duplicated()} in pandas (Python) to find duplicate entries.
            \item \textbf{Removing Duplicates:} Utilize \texttt{.drop\_duplicates()} to eliminate them.
        \end{itemize}
    \end{itemize}
    \begin{block}{Example}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Sample DataFrame
data = {'Name': ['Alice', 'Bob', 'Alice'],
        'Age': [28, 22, 28]}
df = pd.DataFrame(data)

# Removing duplicates
df_cleaned = df.drop_duplicates()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Handling Missing Values}
    \begin{itemize}
        \item \textbf{Definition:} Missing values occur when no data is available for a particular record in the dataset.
        \item \textbf{Techniques:}
        \begin{itemize}
            \item \textbf{Removing Missing Values:} Use \texttt{.dropna()} to eliminate rows with any missing values.
            \item \textbf{Imputation:} Fill in missing values using methods such as mean, median, or mode (e.g., \texttt{df.fillna(df.mean())}).
        \end{itemize}
    \end{itemize}
    \begin{block}{Example}
    \begin{lstlisting}[language=Python]
# Filling missing values with the mean
df['Age'].fillna(df['Age'].mean(), inplace=True)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Correcting Inconsistencies}
    \begin{itemize}
        \item \textbf{Definition:} Inconsistent data refers to different representations for the same data point (e.g., "NY" vs. "New York").
        \item \textbf{Techniques:}
        \begin{itemize}
            \item \textbf{Standardization:} Ensure uniform terminology (e.g., converting everything to lowercase).
            \item \textbf{Validation:} Check against known correct values or ranges (e.g., ensuring ages are within a realistic range).
        \end{itemize}
    \end{itemize}
    \begin{block}{Example}
    \begin{lstlisting}[language=Python]
# Standardizing a DataFrame
df['Name'] = df['Name'].str.lower()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Data cleaning enhances data quality and improves the reliability of analysis.
        \item Each technique should be considered based on the specific needs and characteristics of the dataset.
        \item Consistent data facilitates clearer insights and more accurate predictions.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Employing these data cleaning techniques is essential for preparing datasets for effective analysis. A clean dataset leads to more reliable and actionable insights, paving the way for successful decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Integration - Introduction}
    \begin{block}{Definition}
        Data integration is the process of combining data from various sources to create a comprehensive dataset for analysis.
    \end{block}
    \begin{itemize}
        \item Essential for analysis.
        \item Leads to richer insights through diverse sets of information.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Integration - Importance}
    \begin{itemize}
        \item \textbf{Comprehensive Analysis}: Merging data offers broader perspectives.
        \item \textbf{Reduced Data Silos}: Breaks down isolated data stores, allowing for a complete view.
        \item \textbf{Enhanced Decision-Making}: Consolidated relevant data supports better decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Integration - Steps}
    \begin{enumerate}
        \item \textbf{Data Collection}: Gather data from multiple sources (databases, APIs, CSV files).
        \item \textbf{Data Cleaning}: Remove duplicates, handle missing values, correct inconsistencies.
        \item \textbf{Transformation}: Convert data into a unified format (normalizing, encoding).
        \item \textbf{Loading}: Store integrated data in a central repository (data warehouse).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Integration - Example}
    \begin{block}{Scenario}
        A retail company combines:
        \begin{itemize}
            \item Sales data from e-commerce.
            \item Inventory data from warehouse management.
            \item Customer feedback from surveys.
        \end{itemize}
    \end{block}
    \begin{block}{Integration Process}
        \begin{itemize}
            \item Collect, clean, transform, and load the data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Integration - Code Example}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load data
sales_data = pd.read_csv('sales_data.csv')
inventory_data = pd.read_csv('inventory_data.csv')
feedback_data = pd.read_csv('customer_feedback.csv')

# Clean data (e.g., drop duplicates)
sales_data = sales_data.drop_duplicates()

# Merge datasets
combined_data = pd.merge(sales_data, inventory_data, on='product_id')
combined_data = pd.merge(combined_data, feedback_data, on='product_id')

# Display the integrated dataset
print(combined_data.head())
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Integration - Conclusion}
    \begin{itemize}
        \item Data integration is fundamental for effective analysis.
        \item It reveals insights and relationships that may be obscured in isolated datasets.
        \item Ensuring data quality during integration is essential for reliable analysis and insights.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Introduction}
    \begin{block}{Importance of Data Transformation}
        Data transformation is a critical step in the data preprocessing pipeline that prepares raw data for analysis. It modifies the data to enhance its quality and relevance, ensuring suitability for applied analytical methods.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Key Techniques}
    \begin{enumerate}
        \item Normalization
        \item Standardization
        \item Encoding Categorical Variables
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization}
    \begin{block}{Definition}
        Normalization scales the data to a fixed range, typically [0, 1].
    \end{block}
    \begin{itemize}
        \item Maximizes the influence of smaller scales on model output.
        \item Essential for distance-based algorithms (e.g., K-Means clustering, neural networks).
    \end{itemize}
    \begin{equation}
        \text{Normalized Value} = \frac{X - \text{min}(X)}{\text{max}(X) - \text{min}(X)}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Standardization}
    \begin{block}{Definition}
        Standardization transforms data to have a mean of 0 and a standard deviation of 1 (Z-score normalization).
    \end{block}
    \begin{itemize}
        \item Addresses scale and distribution issues.
        \item Important for algorithms like PCA and Logistic Regression.
    \end{itemize}
    \begin{equation}
        Z = \frac{X - \mu}{\sigma}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Encoding Categorical Variables}
    \begin{block}{Definition}
        Converts categorical variables to a numerical format, necessary for many algorithms.
    \end{block}
    \begin{itemize}
        \item \textbf{Label Encoding:} Assigns each unique category an integer.
        \begin{itemize}
            \item Example: Colors [Red, Green, Blue] $\rightarrow$ [1, 2, 3]
        \end{itemize}
        \item \textbf{One-Hot Encoding:} Creates binary columns for each category.
        \begin{itemize}
            \item Example: Colors [Red, Green, Blue] $\rightarrow$
            \begin{itemize}
                \item Red: [1, 0, 0]
                \item Green: [0, 1, 0]
                \item Blue: [0, 0, 1]
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Transformation}
    \begin{itemize}
        \item Improved Model Performance: Clean data leads to more accurate predictions.
        \item Increased Interpretability: Reveals patterns and trends.
        \item Facilitates Data Techniques: Ensures the data satisfies assumptions of statistical and machine learning methods.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Understanding and applying these data transformation techniques is vital for effective data analysis preparation. Properly scaled and formatted data lays a strong foundation for meaningful analytical insights.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning in Practice - Introduction}
    \begin{block}{What is Data Cleaning?}
        Data cleaning is a critical step in the data preprocessing pipeline. It involves:
        \begin{itemize}
            \item Identifying and correcting errors or inconsistencies in the dataset 
            \item Ensuring data suitability for analysis
        \end{itemize}
    \end{block}
    
    \begin{block}{Common Issues in Data}
        \begin{itemize}
            \item Missing Values: Absence of data in attributes
            \item Duplicated Records: Identical rows that distort analysis
            \item Incorrect Data Types: Data stored in inappropriate formats 
            \item Outliers: Extreme values affecting analyses 
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning in Practice - Sample Data}
    Let's consider a sample dataset containing information about various products that presents common data issues.

    \begin{center}
    \begin{tabular}{|c|l|c|c|l|}
        \hline
        ProductID & Name & Price & Quantity & Category \\
        \hline
        1 & Apple & 1.00 & 10 & Fruit \\
        2 & Banana & NaN & 20 & Fruit \\
        3 & Carrot & "1.50" & -5 & Vegetables \\
        4 & Apple & 1.00 & 10 & Fruit \\
        5 & NULL & 2.00 & 15 & NULL \\
        \hline
    \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning in Practice - Steps and Techniques}
    \begin{block}{Steps to Clean Data}
        \begin{enumerate}
            \item **Load Libraries and Data**:
            \begin{lstlisting}
import pandas as pd
data = pd.read_csv('products.csv')
            \end{lstlisting}

            \item **Identify Missing Values**:
            \begin{lstlisting}
missing_values = data.isnull().sum()
print(missing_values)
            \end{lstlisting}

            \item **Handle Missing Values**:
            \begin{itemize}
                \item Method 1: Fill with mean or mode
                \item Method 2: Remove rows with missing values
            \end{itemize}
            \begin{lstlisting}
data['Price'].fillna(data['Price'].mean(), inplace=True)
data.dropna(subset=['Category'], inplace=True)
            \end{lstlisting}

            \item **Remove Duplicates**:
            \begin{lstlisting}
data.drop_duplicates(inplace=True)
            \end{lstlisting}

            \item **Correct Data Types**:
            \begin{lstlisting}
data['Price'] = data['Price'].astype(float)
data['Quantity'] = data['Quantity'].replace(-5, 0).astype(int)
            \end{lstlisting}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Real-World Application - Introduction}
    \begin{block}{Introduction}
        Data cleaning is a critical step in the data preprocessing pipeline. It involves identifying and correcting errors in the data to enhance the quality and reliability of analytics.
    \end{block}
    \begin{itemize}
        \item We explore the impact of effective data cleaning on decision-making in a real-world scenario.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Healthcare Analytics}
    \begin{block}{Scenario}
        A healthcare organization analyzes patient data to improve treatment plans and outcomes. The dataset includes:
        \begin{itemize}
            \item Patient demographics
            \item Medical history
            \item Treatment information
            \item Outcome indicators
        \end{itemize}
        Key data issues include:
        \begin{itemize}
            \item Missing Values
            \item Inconsistent Formats
            \item Outliers
            \item Duplicate Records
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Effective Data Cleaning Steps}
    Hereâ€™s how the organization approached the data cleaning process:
    \begin{enumerate}
        \item \textbf{Handling Missing Values}:
        \begin{itemize}
            \item \textbf{Technique}: Imputation using Mean/Median for numerical data and Mode for categorical data.
            \item \textbf{Implementation}:
            \begin{lstlisting}[language=Python]
import pandas as pd
data['Age'].fillna(data['Age'].median(), inplace=True)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Standardizing Formats}:
        \begin{itemize}
            \item \textbf{Technique}: Data type conversion to a consistent format.
            \item \textbf{Implementation}:
            \begin{lstlisting}[language=Python]
data['Treatment_Date'] = pd.to_datetime(data['Treatment_Date'], errors='coerce')
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Outlier Removal}:
        \begin{itemize}
            \item \textbf{Technique}: Outlier detection methods such as Z-score and IQR.
            \item \textbf{Implementation}:
            \begin{lstlisting}[language=Python]
data = data[data['Age'] < 100]
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Removing Duplicates}:
        \begin{itemize}
            \item \textbf{Technique}: Identified duplicates based on patient ID.
            \item \textbf{Implementation}:
            \begin{lstlisting}[language=Python]
data.drop_duplicates(subset='Patient_ID', inplace=True)
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Results After Cleaning}
    \begin{itemize}
        \item \textbf{Data Quality Enhanced}: More consistent and complete records.
        \item \textbf{Improved Decision-Making}: Accurate insights into treatment effectiveness increased by 25%.
        \item \textbf{Better Resource Allocation}: Targeted interventions improved patient outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Importance of Data Quality: Clean data is crucial for reliable analyses.
            \item Impact on Decision-Making: Effective data cleaning influences insights accuracy directly.
            \item Practical Applications: The healthcare sector significantly benefits from data cleaning.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Effective data cleaning plays a significant role in the health sector. With systematic techniques, organizations can enhance data quality, leading to better decision-making and improved outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Cleaning Techniques - Introduction}
    \begin{block}{Overview}
        Data cleaning is a crucial step in the data preprocessing pipeline that enhances the quality of data, leading to improved reliability of analytical outcomes. Evaluating the effectiveness of data cleaning techniques ensures that the measures taken lead to genuine improvements.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation of Cleaning Techniques - Effectiveness}
    \begin{block}{Evaluating Effectiveness}
        To assess the effectiveness of data cleaning techniques, various metrics can be utilized to quantify improvements in data quality post-cleaning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics for Data Quality Improvement}
    \begin{enumerate}
        \item \textbf{Missing Values}
            \begin{itemize}
                \item Measure percentage of missing values before and after cleaning.
                \item \textbf{Formula:}
                \begin{equation}
                    \text{Missing Value Rate} = \frac{\text{Number of Missing Entries}}{\text{Total Entries}} \times 100
                \end{equation}
                \item \textbf{Example:} 20 missing values out of 100 total entries gives a rate of 20\%.
            \end{itemize}
        
        \item \textbf{Duplicate Records}
            \begin{itemize}
                \item Quantify duplicate rows before and after cleaning.
                \item Reducing duplicates improves analysis accuracy.
            \end{itemize}
        
        \item \textbf{Outlier Detection}
            \begin{itemize}
                \item Use box plots or Z-scores to assess outliers pre- and post-cleaning.
                \item \textbf{Example:} Improved distribution in "Income" indicates effective cleaning.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Metrics for Data Quality Improvement}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering from the previous frame
        \item \textbf{Consistency Checks}
            \begin{itemize}
                \item Evaluate consistency across datasets, such as date alignment and categorical entry uniformity.
                \item \textbf{Formula:}
                \begin{equation}
                    \text{Consistency Rate} = \frac{\text{Consistent Entries}}{\text{Total Entries}} \times 100
                \end{equation}
            \end{itemize}

        \item \textbf{Data Integrity}
            \begin{itemize}
                \item Ensure data adheres to integrity rules post-cleaning, such as valid email formats.
                \item \textbf{Example:} If 95\% of emails meet format requirements, this indicates improvement.
            \end{itemize}

        \item \textbf{Data Accuracy}
            \begin{itemize}
                \item Validate entries using external datasets, such as cross-referencing sales data with invoices.
                \item \textbf{Example:} 90\% accuracy in validated entries post-cleaning indicates effectiveness.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Effective evaluation of data cleaning techniques relies on a combination of metrics highlighting improvements in data quality. Regular assessments confirm technique efficacy and guide further enhancements in data management practices.
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Data cleaning is essential for high-quality analyses.
            \item Utilize various metrics to gauge cleaning effectiveness.
            \item Continuous evaluation leads to improved data management.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Data Preprocessing - Overview}
    \begin{itemize}
        \item This week, we will explore Exploratory Data Analysis (EDA).
        \item EDA summarizes dataset characteristics, often through visual methods.
        \item Insights from EDA will guide our data cleaning processes, revealing anomalies and patterns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Data Preprocessing - Key Concepts in EDA}
    \begin{enumerate}
        \item \textbf{Data Visualization:}
            \begin{itemize}
                \item Graphs like histograms and scatter plots assess distributions and relationships.
                \item Example: Box plots help reveal outliers.
            \end{itemize}
        \item \textbf{Statistical Summaries:}
            \begin{itemize}
                \item Descriptive statistics (mean, median, etc.) illuminate data distribution.
                \item Example: Comparing salaries against mean highlights outliers.
            \end{itemize}
        \item \textbf{Correlation Analysis:}
            \begin{itemize}
                \item Correlation coefficients identify relationships between variables.
                \item Example: High correlation between ad spend and sales suggests effectiveness.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Data Preprocessing - EDA and Data Cleaning}
    \begin{itemize}
        \item \textbf{Identifying Data Quality Issues:}
            \begin{itemize}
                \item EDA helps uncover missing values and outliers needing cleaning.
                \item Example: Features with over 20\% missing values may require imputation or removal.
            \end{itemize}
        \item \textbf{Guiding Data Cleaning Techniques:}
            \begin{itemize}
                \item EDA insights dictate cleaning methods.
                \item Example: Fixing incorrectly formatted categorical values.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Data Preprocessing - Techniques Spotlight}
    \begin{enumerate}
        \item \textbf{Missing Value Treatment:}
            \begin{itemize}
                \item Options include deletion or imputation.
                \item \begin{lstlisting}[language=Python]
import pandas as pd

# Fill missing values with median
data['column_name'].fillna(data['column_name'].median(), inplace=True)
                \end{lstlisting}
            \end{itemize}
        \item \textbf{Outlier Detection:}
            \begin{itemize}
                \item Box plots and Z-scores help identify outliers.
                \item Example: Z-scores exceeding 3 indicate outliers.
            \end{itemize}
        \item \textbf{Data Type Correction:}
            \begin{itemize}
                \item Convert data to appropriate formats.
                \item \begin{lstlisting}[language=Python]
# Convert string date to datetime
data['date_column'] = pd.to_datetime(data['date_column'])
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps in Data Preprocessing - Summary}
    \begin{itemize}
        \item EDA reveals data insights that enhance dataset quality.
        \item Effective EDA is foundational for dependable data analysis.
        \item Prepare for critical assessments and decisions based on cleaner datasets.
    \end{itemize}
\end{frame}


\end{document}