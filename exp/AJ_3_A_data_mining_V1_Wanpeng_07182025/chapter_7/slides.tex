\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}

\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Model Evaluation and Validation]{Week 7: Model Evaluation and Validation}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Model Evaluation and Validation}
    In the world of machine learning, the development of a model is only part of the journey. \textbf{Model evaluation and validation} are crucial steps that determine how effective our model will be in real-world applications. 
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Evaluating Machine Learning Models}
    \begin{enumerate}
        \item \textbf{Performance Assessment}: 
        \begin{itemize}
            \item Evaluating models allows us to assess their predictive performance using various metrics.
            \item Common metrics include accuracy, precision, recall, F1 score, and ROC-AUC.
        \end{itemize}

        \item \textbf{Guiding Decision-Making}:
        \begin{itemize}
            \item Decisions based on flawed models can lead to significant consequences in various domains.
            \item \textit{Example}: In healthcare, a model predicting disease presence must be highly accurate to prevent dangerous false negatives.
        \end{itemize}

        \item \textbf{Model Selection}:
        \begin{itemize}
            \item Evaluation allows for the comparison of different models.
            \item \textit{Example}: Choosing the best model based on metrics such as F1 score in case of class imbalance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Concepts of Model Evaluation}
    \begin{block}{Training Set vs. Testing Set}
        The dataset used to build the model (training set) should remain distinct from the dataset used to evaluate performance (testing set). This separation is key to avoiding overfitting.
    \end{block}

    \begin{block}{Cross-Validation}
        A technique for assessing how a statistical analysis will generalize to an independent dataset, by partitioning the data into subsets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics to Consider}
    \begin{enumerate}
        \item \textbf{Confusion Matrix}:
        \begin{itemize}
            \item A visualization tool for classification algorithms.
            \item Displays true positives, false positives, true negatives, and false negatives.
        \end{itemize}

        \item \textbf{Accuracy}:
        \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}

        \item \textbf{Precision and Recall}:
        \begin{itemize}
            \item Precision = \( \frac{TP}{TP + FP} \) — measures quality of positive predictions.
            \item Recall = \( \frac{TP}{TP + FN} \) — assesses the model's ability to find all relevant cases.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Evaluating and validating machine learning models is crucial for ensuring their effectiveness in real-world applications. It influences decisions that affect lives and businesses. This week, we will explore various evaluation techniques and metrics that empower informed choices based on model performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 1}
    \textbf{Learning Objectives: Model Evaluation and Validation}
    
    \begin{block}{Introduction}
        This week, our goal is to delve into the crucial aspects of model evaluation and validation in machine learning. By the end of this session, you should be able to:
    \end{block}
    
    \begin{enumerate}
        \item Understand the Importance of Model Evaluation
        \item Identify Different Evaluation Metrics
        \item Differentiate Between Evaluation Strategies
        \item Conduct Model Validation
        \item Interpret Evaluation Results
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 2}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Why Model Evaluation Matters}:
                \begin{itemize}
                    \item Identifies the best-performing model for a task.
                    \item Ensures models generalize well to new data.
                \end{itemize}
            \item \textbf{Commonly Used Metrics:}
                \begin{itemize}
                    \item \textbf{Accuracy}:
                    \begin{equation}
                    \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{Total Predictions}}
                    \end{equation}
                    \item \textbf{Precision}:
                    \begin{equation}
                    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                    \end{equation}
                    \item \textbf{Recall}:
                    \begin{equation}
                    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                    \end{equation}
                    \item \textbf{F1 Score}:
                    \begin{equation}
                    F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                    \end{equation}
                    \item \textbf{AUC-ROC}: Represents a model's ability to distinguish between classes.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 3}
    
    \begin{block}{Illustration of Concepts}
        \textbf{Example}: Suppose you develop a classifier for distinguishing between spam and non-spam emails.
        
        \begin{itemize}
            \item Evaluate the model using:
                \begin{itemize}
                    \item \textbf{Confusion Matrix}: Visualizes True Positives, True Negatives, False Positives, and False Negatives.
                    \item Use Precision and Recall to understand classifier performance.
                \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        As we proceed, you will engage with real-world data, applying these evaluation techniques and metrics to enhance model performance and decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation Metrics Overview}
    \begin{block}{Introduction to Evaluation Metrics}
        In the realm of machine learning and statistical modeling, evaluating a model's performance is crucial. Various metrics provide insights into how well a model predicts outcomes. This slide provides an overview of essential evaluation metrics: Accuracy, Precision, Recall, and F1-Score.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Accuracy}
    \begin{itemize}
        \item \textbf{Definition}: Measures the proportion of correct predictions (both true positives and true negatives) out of the total number of predictions.
        \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \end{equation}
        \item \textbf{Example}: In a dataset of 100 patients, if a model correctly predicts the presence or absence of a disease for 90 patients, the accuracy would be 90\%.
        \item \textbf{Key Point}: Can be misleading in imbalanced datasets. A model predicting only the majority class can achieve high accuracy without being truly effective.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Precision}
    \begin{itemize}
        \item \textbf{Definition}: Indicates the proportion of true positive predictions made by the model relative to all positive predictions (including false positives).
        \begin{equation}
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
        \end{equation}
        \item \textbf{Example}: If a model predicts 30 patients as having a disease, but only 20 truly do, the precision is \( \frac{20}{30} \approx 0.67 \) or 67\%.
        \item \textbf{Key Point}: Important when the cost of a false positive is high, reflecting the certainty of the positive predictions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Recall (Sensitivity)}
    \begin{itemize}
        \item \textbf{Definition}: Measures the proportion of true positive predictions made out of all actual positive instances in the dataset.
        \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
        \end{equation}
        \item \textbf{Example}: If there are 50 patients who actually have the disease and the model correctly identifies 40, the recall is \( \frac{40}{50} = 0.80 \) or 80\%.
        \item \textbf{Key Point}: Crucial when minimizing false negatives is essential, such as in cancer screenings.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. F1-Score}
    \begin{itemize}
        \item \textbf{Definition}: The harmonic mean of precision and recall, balancing the two metrics.
        \begin{equation}
            \text{F1-Score} = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
        \item \textbf{Example}: If precision is 0.67 and recall is 0.80:
        \begin{equation}
            \text{F1-Score} \approx 0.73
        \end{equation}
        \item \textbf{Key Point}: Useful in situations with uneven class distribution, balancing recall and precision.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{}
        Understanding these evaluation metrics is critical for determining the effectiveness of a machine learning model. In real-world applications, accurate prediction can significantly impact decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Definition}
    \begin{block}{Definition of Accuracy}
        Accuracy is a fundamental evaluation metric used to assess the performance of classification models. 
        It is defined as the ratio of correctly predicted instances (both positive and negative) to the total number of instances.
    \end{block}
    \begin{equation}
        \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}
    \end{equation}
    Where: 
    \begin{itemize}
        \item \(TP\) = True Positives
        \item \(TN\) = True Negatives
        \item \(FP\) = False Positives
        \item \(FN\) = False Negatives
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Significance}
    \begin{block}{Significance in Model Evaluation}
        \begin{enumerate}
            \item \textbf{Overall Performance Indicator:} 
                Accuracy provides a general overview of model performance in classifying instances, especially in balanced datasets.
            \item \textbf{Ease of Interpretation:} 
                A straightforward metric that is easy to understand for non-technical stakeholders.
            \item \textbf{Quick Assessment:} 
                Serves as a quick benchmark for model performance.
        \end{enumerate}
    \end{block}

    \begin{block}{When is Accuracy a Suitable Metric?}
        \begin{itemize}
            \item Balanced Datasets: When classes are nearly the same size.
            \item Low Cost of Misclassification: When false positives and negatives have a similar impact.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Accuracy - Example and Conclusion}
    \begin{block}{Example}
        Consider a model predicting spam emails:
        \begin{itemize}
            \item True Positives (TP) = 65 (Spam correctly classified)
            \item True Negatives (TN) = 30 (Not spam correctly classified)
            \item False Positives (FP) = 2 (Not spam misclassified as spam)
            \item False Negatives (FN) = 3 (Spam misclassified as not spam)
        \end{itemize}
        \begin{equation}
            \text{Accuracy} = \frac{(65 + 30)}{(65 + 30 + 3 + 2)} = \frac{95}{100} = 0.95 \text{ or } 95\%
        \end{equation}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Accuracy may not be reliable in imbalanced datasets.
            \item Complement with other metrics such as precision, recall, and F1-score for a complete evaluation.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding the limitations and appropriate contexts for using accuracy is crucial for effective model evaluation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Introduction}
    \begin{block}{Overview}
        Precision and Recall are vital metrics for evaluating classification model performance, especially in the context of imbalanced datasets. They provide more insightful metrics than accuracy when class distribution is uneven.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definitions of Precision and Recall}
    \begin{itemize}
        \item \textbf{Precision}: The ratio of true positive predictions to the total positive predictions made by the model.
        \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
        \end{equation}
        Where:
        \begin{itemize}
            \item $TP$ = True Positives
            \item $FP$ = False Positives
        \end{itemize}

        \item \textbf{Recall}: The ratio of true positive predictions to the actual number of positive instances.
        \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
        \end{equation}
        Where:
        \begin{itemize}
            \item $FN$ = False Negatives
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Calculating Precision and Recall}
    \begin{block}{Example Confusion Matrix}
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline
                & Predicted Positive & Predicted Negative \\
                \hline
                Actual Positive & 70 (TP) & 30 (FN) \\
                \hline
                Actual Negative & 10 (FP) & 90 (TN) \\
                \hline
            \end{tabular}
        \end{center}
    \end{block}

    \begin{itemize}
        \item \textbf{Precision Calculation}: 
        \begin{equation}
        \text{Precision} = \frac{70}{70 + 10} = \frac{70}{80} = 0.875 \text{ or } 87.5\%
        \end{equation}

        \item \textbf{Recall Calculation}: 
        \begin{equation}
        \text{Recall} = \frac{70}{70 + 30} = \frac{70}{100} = 0.7 \text{ or } 70\%
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Precision and Recall}
    \begin{block}{Contextual Relevance}
        \begin{itemize}
            \item In imbalanced datasets, precision and recall provide insights where accuracy may be misleading.
            \item They help in understanding model performance in different scenarios (e.g., fraud detection, medical diagnosis).
        \end{itemize}
    \end{block}

    \begin{itemize}
        \item \textbf{Medical Diagnosis}: Higher recall is preferred to ensure most positive cases are detected.
        \item \textbf{Spam Detection}: Higher precision is prioritized to minimize false positives.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Introduction}
    \begin{block}{What is the F1-Score?}
        The F1-Score is a metric that combines both precision and recall to evaluate classification model performance. It is especially useful when the class distribution is imbalanced.
    \end{block}
    \begin{equation}
        F1\text{-}Score = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
    \begin{itemize}
        \item \textbf{Precision:} 
        \[
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \]
        \item \textbf{Recall:} 
        \[
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Advantages}
    \begin{block}{Why Use the F1-Score?}
        The F1-Score offers a balance between precision and recall and is advantageous in scenarios such as:
    \end{block}
    \begin{itemize}
        \item \textbf{Class Imbalance:} 
        In situations where one class is much more frequent, accuracy can be misleading. The F1-Score provides more insight.
        \item \textbf{Cost of Errors:} 
        In cases where false negatives have higher consequences (e.g., fraud detection), the F1-Score helps evaluate the trade-offs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1-Score - Example Calculation}
    Consider a model with the following predictions:
    \begin{itemize}
        \item \textbf{True Positives (TP):} 70
        \item \textbf{False Positives (FP):} 30
        \item \textbf{False Negatives (FN):} 10
    \end{itemize}

    \textbf{Calculate Precision and Recall:}
    \begin{itemize}
        \item \textbf{Precision:} 
        \[
        \text{Precision} = \frac{70}{70 + 30} = 0.7
        \]
        \item \textbf{Recall:} 
        \[
        \text{Recall} = \frac{70}{70 + 10} = 0.875
        \]
    \end{itemize}
    
    \textbf{Calculate F1-Score:}
    \[
    F1\text{-}Score = 2 \times \frac{0.7 \times 0.875}{0.7 + 0.875} \approx 0.778
    \]
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Overview}
    \begin{block}{Understanding the Confusion Matrix}
        A confusion matrix is a powerful tool that visualizes the performance of a classification model. It helps summarize prediction results, making it easier to assess how well the model is performing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Structure}
    \begin{block}{Structure of the Confusion Matrix}
        The confusion matrix appears in a 2x2 format for binary classification:

        \begin{center}
            \begin{tabular}{c|c|c}
                & \textbf{Actual Positive} & \textbf{Actual Negative} \\
                \hline
                \textbf{Predicted Positive} & TP & FP \\
                \hline
                \textbf{Predicted Negative} & FN & TN \\
            \end{tabular}
        \end{center}
        Where:
        \begin{itemize}
            \item \textbf{TP (True Positive)}: Correctly predicted positive class.
            \item \textbf{FP (False Positive)}: Incorrectly predicted positive class (Type I error).
            \item \textbf{TN (True Negative)}: Correctly predicted negative class.
            \item \textbf{FN (False Negative)}: Incorrectly predicted negative class (Type II error).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Confusion Matrix - Example Scenario}
    \begin{block}{Example Scenario}
        Consider a medical test for a disease:

        \begin{center}
            \begin{tabular}{c|c|c}
                & \textbf{Actual Positive} & \textbf{Actual Negative} \\
                \hline
                \textbf{Predicted Positive} & 80 & 10 \\
                \hline
                \textbf{Predicted Negative} & 5 & 50 \\
            \end{tabular}
        \end{center}

        - **True Positive (TP)**: 80 patients correctly identified as having the disease.
        - **False Positive (FP)**: 10 healthy patients incorrectly identified as having the disease.
        - **True Negative (TN)**: 50 healthy patients correctly identified.
        - **False Negative (FN)**: 5 patients with the disease incorrectly identified as healthy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting the Confusion Matrix - Overview}
    A \textbf{confusion matrix} is a tool for evaluating the performance of a classification model. It categorizes predictions into four outcomes based on true labels and predicted labels:
    
    \begin{itemize}
        \item \textbf{True Positives (TP):} Correctly predicted positive cases.
        \item \textbf{False Positives (FP):} Incorrectly predicted positive cases (predicted positive, actual negative).
        \item \textbf{True Negatives (TN):} Correctly predicted negative cases.
        \item \textbf{False Negatives (FN):} Incorrectly predicted negative cases (predicted negative, actual positive).
    \end{itemize}
    
    \textbf{Example Confusion Matrix:}
    
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
            \hline
            \textbf{Actual Positive} & TP = 50 & FN = 10 \\
            \hline
            \textbf{Actual Negative} & FP = 5 & TN = 100 \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics from the Confusion Matrix}
    From the confusion matrix, we derive essential performance metrics:
    
    \begin{enumerate}
        \item \textbf{Accuracy:} Measures overall correctness of the model.
        \begin{equation}
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        
        \textbf{Example Calculation:}
        \begin{equation}
            \text{Accuracy} = \frac{50 + 100}{50 + 100 + 5 + 10} = \frac{150}{165} \approx 0.9091 \text{ or } 90.91\%
        \end{equation}

        \item \textbf{Precision:} Indicates the accuracy of positive predictions.
        \begin{equation}
            \text{Precision} = \frac{TP}{TP + FP}
        \end{equation}
        
        \textbf{Example Calculation:}
        \begin{equation}
            \text{Precision} = \frac{50}{50 + 5} = \frac{50}{55} \approx 0.9091 \text{ or } 90.91\%
        \end{equation}

        \item \textbf{Recall (Sensitivity):} Measures the ability to identify positive instances.
        \begin{equation}
            \text{Recall} = \frac{TP}{TP + FN}
        \end{equation}
        
        \textbf{Example Calculation:}
        \begin{equation}
            \text{Recall} = \frac{50}{50 + 10} = \frac{50}{60} \approx 0.8333 \text{ or } 83.33\%
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The confusion matrix provides valuable insights into the types of errors made by the classification model.
            \item Accuracy alone can be misleading, especially in imbalanced datasets; analyzing precision and recall together is crucial.
            \item A balance between recall and precision (often measured by the F1 score) is necessary, depending on the application context (e.g., medical diagnosis may prioritize recall).
        \end{itemize}
    \end{block}
    
    \textbf{Conclusion:} Interpreting the confusion matrix enables data scientists to diagnose model performance, ensuring that predictions align with real-world applications. Understanding metrics like accuracy, precision, and recall helps refine models for better outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation: Concept}
    \begin{block}{Definition}
        Cross-validation is a statistical technique to assess a model's generalizability by partitioning the dataset into subsets. It evaluates the model's performance outside of the training sample, crucial to preventing overfitting.
    \end{block}

    \begin{block}{Overfitting}
        Overfitting occurs when a model captures noise in the training data instead of the underlying pattern, leading to poor performance on new datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Cross-Validation}
    \begin{enumerate}
        \item \textbf{Model Evaluation}: Provides a reliable estimate of performance using multiple data subsets, thus ensuring robustness across distributions.
        \item \textbf{Overfitting Prevention}: Validates the model on unseen data, identifying overly complex models, and reducing overfitting risk.
        \item \textbf{Hyperparameter Tuning}: Assists in selecting optimal hyperparameters to enhance performance without bias.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cross-Validation Techniques}
    The most common forms include:

    \begin{itemize}
        \item \textbf{K-Fold Cross-Validation}:
        \begin{itemize}
            \item The dataset is split into 'K' equally sized folds.
            \item The model trains on \(K-1\) folds and validates on the remaining fold.
            \item This repeats \(K\) times, averaging final performance metrics.
        \end{itemize}

        \item \textbf{Leave-One-Out Cross-Validation (LOOCV)}:
        \begin{itemize}
            \item A specific case where \(K\) equals the number of samples.
            \item Evaluates each data point once as the validation set, leveraging the rest for training.
            \item While thorough, it can be computationally expensive.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Cross-Validation Example}
    \begin{block}{Example: 5-Fold Cross-Validation}
        Consider a dataset with 10 samples.
        \begin{enumerate}
            \item Divide into 5 subsets.
            \item For every iteration, 4 subsets are used for training, and 1 for testing.
            \item After 5 iterations, collect performance metrics and compute the average.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation of K-Fold Cross-Validation}
    \begin{block}{Python Code Snippet}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

kf = KFold(n_splits=5)  # Set K
model = LogisticRegression()

for train_index, test_index in kf.split(X):  # X is your dataset
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    print("Fold accuracy:", accuracy_score(y_test, predictions))
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \textbf{Cross-validation} is crucial for evaluating model performance and ensuring generalizability to new data. It enhances model reliability and reduces overfitting risks. Understanding and applying these techniques is fundamental in data science for building robust predictive models.
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Fold Cross-Validation}
    \begin{block}{What is K-Fold Cross-Validation?}
        K-Fold Cross-Validation is a robust technique used in machine learning to evaluate model performance, ensuring better generalization on unseen data by dividing the dataset into 'K' equal folds.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Fold Cross-Validation - Procedure}
    \begin{enumerate}
        \item \textbf{Divide the Dataset:} Split the dataset into K equal subsets randomly.
        \item \textbf{Training and Validation:} Train the model K times, each time using K-1 folds for training and 1 fold for validation.
        \item \textbf{Calculate Performance:} Average the validation scores to determine overall performance.
    \end{enumerate}
    \begin{exampleblock}{Example}
        \begin{itemize}
            \item Iteration 1: Train on folds 2-5; validate on fold 1.
            \item Iteration 2: Train on folds 1,3-5; validate on fold 2.
            \item Iteration 3: Train on folds 1,2,4-5; validate on fold 3.
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Drawbacks}
    \begin{block}{Advantages}
        \begin{itemize}
            \item Less variance in performance measurement.
            \item Efficient use of data, maximizing training and validation across all samples.
            \item Provides robust evaluation by averaging results.
        \end{itemize}
    \end{block}

    \begin{block}{Potential Drawbacks}
        \begin{itemize}
            \item Higher computational cost due to training K times.
            \item Risk of imbalanced folds in datasets with uneven class distributions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Formula}
    The average performance metric from K-Fold can be represented as:
    \begin{equation}
        \text{Average Score} = \frac{1}{K} \sum_{i=1}^{K} \text{Score}_i
    \end{equation}
    where $\text{Score}_i$ is the performance measure from the i-th fold.
    
    \begin{block}{Conclusion}
        K-Fold Cross-Validation is crucial for validating machine learning models, ensuring they generalize well on unseen data while leveraging available data effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Other Cross-Validation Techniques}
    \begin{itemize}
        \item Cross-validation is crucial for assessing the performance and robustness of machine learning models.
        \item Beyond K-Fold cross-validation, other valuable techniques include:
        \begin{itemize}
            \item Stratified Cross-Validation
            \item Leave-One-Out Cross-Validation (LOOCV)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Stratified Cross-Validation}
    \begin{block}{Definition}
        Stratified cross-validation ensures that each fold of the data has a representative proportion of the target classes, crucial for imbalanced datasets.
    \end{block}
    
    \begin{block}{How It Works}
        \begin{itemize}
            \item The dataset is split into ‘k’ subsets (folds).
            \item Data is stratified based on the target variable to maintain class distribution.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        For instance, with 100 samples (80 of class A and 20 of class B) in 5-fold stratified CV:
        \begin{itemize}
            \item Each fold contains approximately 16 samples of class A and 4 samples of class B.
        \end{itemize}
    \end{block}

    \begin{block}{Key Benefits}
        \begin{itemize}
            \item Reduces variance in performance estimates.
            \item Enhances model accuracy reflecting underlying population distribution.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Leave-One-Out Cross-Validation (LOOCV)}
    \begin{block}{Definition}
        LOOCV is a specific K-Fold CV where k equals the number of observations. Each iteration uses a single observation as the test set.
    \end{block}

    \begin{block}{How It Works}
        \begin{itemize}
            \item For a dataset with N instances, LOOCV involves N iterations.
            \item In each iteration, one observation is used for testing, and the rest (N-1) for training.
        \end{itemize}
    \end{block}

    \begin{block}{Example}
        For a dataset with 10 samples:
        \begin{itemize}
            \item 1st iteration: Test on Sample 1, Train on Samples 2-10.
            \item 2nd iteration: Test on Sample 2, Train on Samples 1, 3-10.
        \end{itemize}
    \end{block}

    \begin{block}{Key Benefits}
        \begin{itemize}
            \item Utilizes nearly all data points for training, leading to lower bias.
            \item Ideal for small datasets where every sample matters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Quick Comparison of Techniques}
    \begin{table}[ht]
        \centering
        \begin{tabular}{@{}lll@{}}
            \toprule
            Technique                     & Advantages                                & Disadvantages                    \\ \midrule
            Stratified CV                 & Preserves class distribution; reduces variance & Requires careful implementation; complex  \\
            Leave-One-Out CV             & Low bias; maximizes training data         & High computational cost; higher variance \\ \bottomrule
        \end{tabular}
        \caption{Comparison of cross-validation techniques}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Code Example}
    \begin{itemize}
        \item Understanding different cross-validation techniques is essential for reliable model evaluation.
        \item Choose methods based on dataset size and class distribution.
    \end{itemize}

    \begin{block}{Python Example}
        \begin{lstlisting}[language=Python]
from sklearn.model_selection import StratifiedKFold, LeaveOneOut
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Stratified K-Fold
skf = StratifiedKFold(n_splits=5)
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    print(accuracy_score(y_test, preds))

# Leave-One-Out
loo = LeaveOneOut()
scores = cross_val_score(model, X, y, cv=loo)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Model Performance - Overview}
    \begin{block}{Overview of Model Performance Evaluation}
        When building predictive models, it's crucial to assess their performance to determine which model best meets our objectives. A comparison of models can be concluded using various evaluation metrics that capture their predictive accuracy and generalizability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Model Performance - Key Evaluation Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}:
            \begin{itemize}
                \item Definition: The ratio of correctly predicted instances to the total instances.
                \item Formula: 
                \begin{equation}
                    \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
                \end{equation}
                \item Example: In a dataset of 100 instances, if 90 are classified correctly, accuracy is 90\%.
            \end{itemize}
        
        \item \textbf{Precision}:
            \begin{itemize}
                \item Definition: The ratio of true positive predictions to predicted positives.
                \item Formula: 
                \begin{equation}
                    \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
                \end{equation}
                \item Example: If 70 positive instances are predicted but only 60 are true positives, precision is 85.7\%.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Model Performance - Additional Metrics}
    \begin{itemize}
        \item \textbf{Recall (Sensitivity)}:
            \begin{itemize}
                \item Definition: The ratio of true positive predictions to the total actual positives.
                \item Formula:
                \begin{equation}
                    \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
                \end{equation}
                \item Example: If there are 80 actual positives but only 60 are identified, recall is 75\%.
            \end{itemize}
        
        \item \textbf{F1 Score}:
            \begin{itemize}
                \item Definition: The harmonic mean of precision and recall.
                \item Formula: 
                \begin{equation}
                    F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
                \end{equation}
                \item Example: If precision is 0.85 and recall is 0.75, then F1 score is approximately 0.79.
            \end{itemize}
        
        \item \textbf{ROC-AUC Score}:
            \begin{itemize}
                \item Definition: The Area Under the Receiver Operating Characteristic Curve.
                \item Interpretation: A score closer to 1 indicates better performance.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Model Performance - Visual Representation}
    \begin{block}{Visualizing Model Performance}
        \begin{itemize}
            \item \textbf{Confusion Matrix}:
            \[
            \begin{array}{l|c|c}
                & \text{Predicted Positive} & \text{Predicted Negative} \\
                \hline
                \text{Actual Positive} & TP & FN \\
                \hline
                \text{Actual Negative} & FP & TN \\
            \end{array}
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Models - Selection Techniques}
    \begin{itemize}
        \item \textbf{Cross-Validation}:
            \begin{itemize}
                \item Employ k-fold cross-validation to train and validate models.
                \item Helps to avoid overfitting by ensuring robust model assessment.
            \end{itemize}
        
        \item \textbf{Model Selection}:
            \begin{itemize}
                \item Select the model with the best performance based on evaluation metrics.
                \item Example: Choose between Model A (F1 = 0.78) and Model B (F1 = 0.82).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Model Performance - Key Takeaways}
    \begin{itemize}
        \item Choose evaluation metrics that align with your specific problem.
        \item Use multiple metrics to obtain a comprehensive view of model performance.
        \item Visual tools like confusion matrices and ROC curves aid in understanding model effectiveness.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet for Evaluation Metrics}
    Here’s how to calculate evaluation metrics using \texttt{scikit-learn} in Python:
    \begin{lstlisting}[language=Python]
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Assuming y_true are true labels and y_pred are predicted labels
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
auc = roc_auc_score(y_true, y_probs)

print(f"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}, AUC: {auc}")
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Practical Implementation: Code Walkthrough}
    \begin{block}{Key Concepts: Model Evaluation and Cross-Validation}
        Model evaluation and validation are critical steps in machine learning, ensuring your model performs well on unseen data. This walkthrough demonstrates evaluation metrics and cross-validation techniques in Python.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{1. Importance of Evaluation Metrics}
    Evaluation metrics provide insight into model performance, allowing for comparison among different models. Common evaluation metrics include:
    
    \begin{itemize}
        \item \textbf{Accuracy}: Proportion of correctly predicted instances.
        \item \textbf{Precision}: Ratio of true positive predictions to total predicted positives.
        \item \textbf{Recall} (Sensitivity): Ratio of true positive predictions to total actual positives.
        \item \textbf{F1 Score}: Harmonic mean of precision and recall.
        \item \textbf{ROC AUC}: Area under the Receiver Operating Characteristic curve.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{2. Cross-Validation}
    Cross-validation is a method to assess how the results of a statistical analysis will generalize to an independent dataset. It mitigates overfitting by ensuring the model performs well on different subsets.

    \begin{block}{k-Fold Cross-Validation}
        The most common method is k-Fold Cross-Validation, which divides the dataset into 'k' subsets (folds). The model is trained on 'k-1' folds and tested on the remaining fold, repeating this process 'k' times.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Code Example}
    \begin{lstlisting}[language=Python]
# Import necessary libraries
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load dataset
data = load_iris()
X = data.data
y = data.target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = RandomForestClassifier()

# Fit the model on the training data
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Model Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Cross-Validation
cv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Score:", cv_scores.mean())
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{3. Key Points to Remember}
    \begin{itemize}
        \item Use evaluation metrics to understand model performance.
        \item Cross-validation helps mitigate overfitting and provides a reliable estimate of model performance.
        \item Different metrics provide different insights; choose the one aligned with your problem statement.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Next Steps}
    As a next step, we will explore \textbf{Real-World Examples} where effective model evaluation and validation made a significant impact on outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Examples - Introduction}
    \begin{block}{Overview}
        Model evaluation and validation are critical steps in the machine learning lifecycle. They ensure the reliability and robustness of predictive models before deployment. 
    \end{block}
    In this presentation, we explore several real-world scenarios where effective model evaluation and validation have significantly impacted outcomes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Examples - Scenarios}
    \begin{enumerate}
        \item \textbf{Healthcare Diagnosis}
            \begin{itemize}
                \item Predictive modeling for disease diagnosis.
                \item Impact: Validation techniques like k-fold cross-validation enhanced accuracy, enabling timely interventions.
                \item Key Point: Evaluation increases trust in clinical decision-making models.
            \end{itemize}
        
        \item \textbf{Fraud Detection in Banking}
            \begin{itemize}
                \item Machine learning models identify fraudulent transactions.
                \item Impact: A well-validated model reduced false positives by over 30%.
                \item Key Point: Validation correlates with financial savings and improved operational efficiency.
            \end{itemize}

        \item \textbf{Marketing Campaign Optimization}
            \begin{itemize}
                \item Predictive analytics for customer retention.
                \item Impact: Targeted retention strategies reduced churn rates by 15%.
                \item Key Point: Understanding model performance leads to better business decisions.
            \end{itemize}
        
        \item \textbf{Weather Forecasting}
            \begin{itemize}
                \item Statistical models for predicting severe weather events.
                \item Impact: Continuous validation improved accuracy, enhancing public safety.
                \item Key Point: Validated models can save lives during critical situations.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Evaluation Metrics}
    \begin{itemize}
        \item \textbf{Accuracy}: $\frac{\text{correct predictions}}{\text{total instances}}$
        \item \textbf{Precision}: $\frac{\text{true positives}}{\text{total positive predictions}}$
        \item \textbf{Recall}: $\frac{\text{true positives}}{\text{total actual positives}}$
        \item \textbf{F1 Score}:
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The effectiveness of model evaluation and validation greatly influences machine learning applications across industries. By highlighting real-world examples, we appreciate the significance of robust evaluation processes in ensuring optimal model performance. 
    \begin{block}{Key Takeaway}
        Effective validation enhances predictive accuracy and promotes trust in automated systems, leading to improved decisions and outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 1}
    \begin{block}{Understanding Model Evaluation and Validation}
        \begin{enumerate}
            \item \textbf{Definition of Model Evaluation:} 
            Model evaluation is the process of assessing the performance of a predictive model using specific metrics and techniques to ensure its accuracy, reliability, and applicability in real-world scenarios.
            \item \textbf{Importance in Data Mining:}
            \begin{itemize}
                \item Quality Assurance: Ensures the model meets the desired performance criteria.
                \item Decision Making: Aids stakeholders in understanding model reliability before implementation.
                \item Reducing Overfitting: Helps in identifying whether the model is excessively complex or capturing noise instead of patterns.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 2}
    \begin{block}{Key Metrics for Evaluation}
        \begin{itemize}
            \item \textbf{Accuracy:} 
            \[
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
            \]
            \item \textbf{Precision:} 
            \[
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
            \]
            \item \textbf{Recall (Sensitivity):} 
            \[
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
            \]
            \item \textbf{F1 Score:} 
            \[
            \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 3}
    \begin{block}{Model Validation Techniques}
        \begin{itemize}
            \item \textbf{Cross-Validation:} Dividing data into training and test sets multiple times to assess stability and reliability of the model.
            \item \textbf{Holdout Method:} Splitting data once into training and testing sets to evaluate performance.
            \item \textbf{Bootstrapping:} Resampling technique used to approximate the distribution of a statistic.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Model evaluation and validation are crucial for effective data mining and ensuring that models are both reliable and applicable in practice.
            \item Various metrics provide insights into different aspects of model performance — no single measure can define success.
            \item Validation should be an ongoing process as more data and insights become available.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session Overview}
    Open the floor for questions and discussions about model evaluation and validation.
    \begin{block}{Objective}
        This session is designed to clarify concepts related to model evaluation and validation, encouraging discussions around best practices, challenges, and innovative strategies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts for Discussion}
    \begin{enumerate}
        \item \textbf{Model Evaluation vs. Model Validation}
        \begin{itemize}
            \item \textbf{Model Evaluation:} Process of assessing the performance of a model using various metrics (e.g., accuracy, precision, recall).
            \item \textbf{Model Validation:} Systematic process that ensures the model generalizes well to unseen data, often involving techniques like cross-validation.
        \end{itemize}
        
        \item \textbf{Evaluation Metrics}
        \begin{itemize}
            \item \textbf{Accuracy:} 
            \[ 
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}} 
            \]
            \item \textbf{Precision:} 
            \[ 
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} 
            \]
            \item \textbf{Recall (Sensitivity):} 
            \[ 
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} 
            \]
            \item \textbf{F1 Score:} 
            \[ 
            F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} 
            \]
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Case and Discussion Points}
    \textbf{Example Case:} Predictive Model for Customer Churn
    \begin{itemize}
        \item After training the model, the accuracy is 85%.
        \item However, the recall is only 60%, indicating challenges in identifying customers likely to churn.
        \item Possible improvements: Adjusting thresholds, exploring different algorithms, or balancing class distributions.
    \end{itemize}
    
    \textbf{Discussion Points:}
    \begin{itemize}
        \item Challenge: What difficulties have you faced in validating models? How did you overcome them?
        \item Insights: Can you share an instance where a specific metric (like precision vs. recall) redirected your model validation strategy?
        \item Tools: What tools or libraries have you found useful for model evaluation in your projects?
    \end{itemize}
\end{frame}


\end{document}