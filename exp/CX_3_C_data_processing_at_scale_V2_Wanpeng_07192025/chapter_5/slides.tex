\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 5: Data Wrangling Techniques]{Week 5: Data Wrangling Techniques}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Wrangling Techniques}
    Data wrangling, sometimes called data munging, is the process of transforming and preparing raw data into a format suitable for analysis. This key step ensures clean, consistent, and usable data for insights and decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Cleaning and Preparation}
    \begin{enumerate}
        \item \textbf{Quality of Analysis}:
            Accurate and reliable data is the cornerstone of good analysis.
            Poor-quality data can lead to incorrect conclusions, e.g., misleading results from inconsistent sales records.
        
        \item \textbf{Efficiency in Processing}:
            Cleaning upfront reduces analysis time later, enabling faster insights.
        
        \item \textbf{Enhanced Decision-Making}:
            Clean data enables informed decisions, such as effective marketing campaigns using precise customer demographic data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Issues Addressed in Wrangling}
    \begin{itemize}
        \item \textbf{Missing Values}:
            Handle gaps through imputation or removal.
            \begin{itemize}
                \item Example: Filling in a default value for unanswered survey questions.
            \end{itemize}

        \item \textbf{Inconsistent Formats}:
            Standardize formats (e.g. dates, currency).
            \begin{itemize}
                \item Example: Converting all dates to the format MM/DD/YYYY.
            \end{itemize}

        \item \textbf{Duplicates}:
            Identify and remove duplicates to maintain integrity.
            \begin{itemize}
                \item Example: Removing multiple entries of the same customer in a database.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Wrangling Techniques}
    \begin{itemize}
        \item \textbf{Validation}:
            Check that data entries conform to business rules (e.g. age cannot be negative).

        \item \textbf{Transformation}:
            Change the data structure, such as merging or splitting columns.

        \item \textbf{Normalization}:
            Adjust values on a common scale without distorting differences, essential for effective comparison.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways and Conclusion}
    \begin{itemize}
        \item Data wrangling is fundamental before any data analysis.
        \item Clean data leads to more accurate insights and informed decisions.
        \item Various techniques exist to address common data quality issues.
    \end{itemize}
    \vspace{0.5cm}
    \textbf{Conclusion:} Data wrangling is both an art and a science. A thoughtful approach is essential for preparing data for meaningful analysis.
\end{frame}

\begin{frame}[fragile]{What is Data Wrangling? - Definition}
    \begin{block}{Definition}
        Data wrangling, also known as data munging, is the process of cleaning, restructuring, and enriching raw data into a desired format, making it suitable for analysis. This process transforms raw data into a structured and usable form, ensuring that analysts can derive meaningful insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]{What is Data Wrangling? - Key Components}
    \begin{enumerate}
        \item \textbf{Data Collection:}
        \begin{itemize}
            \item Gathering raw data from various sources (databases, APIs, web scraping).
            \item \textit{Example:} Extracting sales data from a company's database using SQL queries.
        \end{itemize}
        
        \item \textbf{Data Cleaning:}
        \begin{itemize}
            \item Identifying and correcting errors in the dataset. This includes handling missing values, removing duplicates, and correcting inconsistencies.
            \item \textit{Example:} Filling in missing values for customer age using the mean or median age of the dataset.
        \end{itemize}

        \item \textbf{Data Transformation:}
        \begin{itemize}
            \item Modifying the data into a suitable format and structure. This includes normalization, aggregation, and encoding categorical variables.
            \item \textit{Example:} Converting the date format from 'YYYY/MM/DD' to 'MM/DD/YYYY' for consistency.
        \end{itemize}
        
        \item \textbf{Data Enrichment:}
        \begin{itemize}
            \item Enhancing the dataset by integrating external data sources to add more context or features.
            \item \textit{Example:} Merging the sales dataset with demographics data to analyze customer behavior more effectively.
        \end{itemize}

        \item \textbf{Data Validation:}
        \begin{itemize}
            \item Ensuring the accuracy and quality of data post-transformation. This step involves checking for outliers or anomalies.
            \item \textit{Example:} Verifying that sales figures fall within a reasonable range based on historical data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Why is Data Wrangling Critical for Analysis?}
    \begin{itemize}
        \item \textbf{Accuracy and Integrity:} Ensures accuracy of the dataset, essential for valid conclusions and data-driven decisions.
        \item \textbf{Efficiency in Analysis:} Clean and well-structured data saves time; analysts spend less time correcting errors.
        \item \textbf{Informed Decision Making:} High-quality data leads to informed strategies and outcomes by identifying trends.
        \item \textbf{Foundation for Advanced Analytics:} Essential for machine learning modeling, statistical analyses, and dashboards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example of Data Wrangling}
    \textbf{Before Wrangling:}

    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    Name   & Age & Sales \\
    \hline
    John   & 25  & 300   \\
    Mary   &     & 500   \\
    John   & 25  &    \\
    Peter  & 30  & -200  \\
    \hline
    \end{tabular}
    \end{center}

    \textbf{After Wrangling:}

    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    Name   & Age & Sales \\
    \hline
    John   & 25  & 300   \\
    Mary   & 28  & 500   \\  % (Replaced missing age with average)
    Peter  & 30  & 200   \\  % (Corrected negative sales)
    \hline
    \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]{Code Snippet for Data Cleaning in Python}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Load data
data = pd.read_csv('sales_data.csv')

# Fill missing values for 'Age'
data['Age'].fillna(data['Age'].mean(), inplace=True)

# Remove duplicates
data.drop_duplicates(inplace=True)

# Correct negative sales
data['Sales'] = data['Sales'].apply(lambda x: x if x >= 0 else 0)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Key Points to Emphasize}
    \begin{itemize}
        \item Data wrangling is an essential step in the data analysis pipeline.
        \item The process involves multiple stages, each contributing to the overall quality of the data.
        \item Poorly wrangled data can lead to misleading conclusions and ineffective decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Quality Issues}
    \begin{block}{What are Data Quality Issues?}
    Data quality issues refer to problems in datasets that can lead to inaccuracies, 
    inconsistencies, and ambiguities in data analysis and reporting. Addressing these issues 
    is crucial for ensuring that data insights are reliable and actionable.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Quality Issues}
    \begin{enumerate}
        \item \textbf{Missing Values}
        \begin{itemize}
            \item \textbf{Definition}: Observations where data points are not recorded.
            \item \textbf{Impact}: Can lead to biased results or loss of information in analysis.
            \item \textbf{Example}: In a survey dataset, some respondents may not answer every question.
            \item \textbf{Handling}: 
                \begin{itemize}
                    \item Imputation (filling in missing values using statistical methods).
                    \item Deletion (removing rows or columns with excessive missing data).
                \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Data Quality Issues (continued)}
    \begin{enumerate}[resume]
        \item \textbf{Duplicates}
        \begin{itemize}
            \item \textbf{Definition}: Repeated records due to errors in data collection.
            \item \textbf{Impact}: Can skew results, leading to inflated metrics.
            \item \textbf{Example}: Multiple entries for the same individual in a customer database.
            \item \textbf{Handling}: 
                \begin{itemize}
                    \item Identify duplicates using functions (e.g., \texttt{pd.DataFrame.duplicated()}).
                    \item Remove duplicates with methods like \texttt{drop\_duplicates()}.
                \end{itemize}
        \end{itemize}

        \item \textbf{Outliers}
        \begin{itemize}
            \item \textbf{Definition}: Data points that significantly diverge from the dataset.
            \item \textbf{Impact}: Can affect statistical analyses, leading to misleading conclusions.
            \item \textbf{Example}: A score of 300 in a test that is out of 100.
            \item \textbf{Handling}: 
                \begin{itemize}
                    \item Explore using visualization techniques (e.g., box plots).
                    \item Decide on treatment (e.g., leave, correct, or exclude).
                \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Data quality issues can severely impact the validity of your analysis.
        \item Identifying and addressing missing values, duplicates, and outliers are foundational steps in the data wrangling process.
        \item Employ appropriate methods based on the nature of your dataset and the type of issue at hand.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippets in Python}
    \begin{block}{Identify Missing Values}
    \begin{lstlisting}[language=Python]
    import pandas as pd
    data.isnull().sum()
    \end{lstlisting}
    \end{block}

    \begin{block}{Remove Duplicates}
    \begin{lstlisting}[language=Python]
    data = data.drop_duplicates()
    \end{lstlisting}
    \end{block}

    \begin{block}{Detect Outliers}
    \begin{lstlisting}[language=Python]
    import seaborn as sns
    sns.boxplot(data['column_name'])
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Techniques for Data Cleaning}
    \begin{block}{Overview}
        Data cleaning is a crucial step in data wrangling, ensuring that data is accurate, consistent, and usable for analysis. This presentation outlines key techniques employed in data cleaning, including removal, imputation, and transformation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques for Data Cleaning - Part 1}
    \begin{enumerate}
        \item \textbf{Removal}
        \begin{itemize}
            \item \textbf{Definition}: Deleting unnecessary or problematic data entries.
            \item \textbf{When to Use}: 
            \begin{itemize}
                \item High percentage of missing values in a row or column
                \item Duplicate records causing redundancy
            \end{itemize}
            \item \textbf{Example}: If a dataset contains 1000 records but 250 are duplicates, removing these duplicates simplifies analysis and improves model performance.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques for Data Cleaning - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Imputation}
        \begin{itemize}
            \item \textbf{Definition}: Filling in missing values with estimated ones to retain data integrity.
            \item \textbf{Common Methods}:
            \begin{itemize}
                \item \textbf{Mean/Median/Mode Imputation}: Use the statistical measure to replace missing values based on the feature distribution.
                \begin{itemize}
                    \item \textbf{Example}: For a column of ages, if the mean is 30 and 5 values are missing, replace them with 30.
                \end{itemize}
                \item \textbf{Predictive Imputation}: Use models to predict missing values based on other data.
                \begin{itemize}
                    \item \textbf{Example}: Use a regression model where age is predicted based on income and education level.
                \end{itemize}
            \end{itemize}
            \item \textbf{Key Point to Remember}: Choosing the right imputation technique can significantly affect the outcome of data analysis.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques for Data Cleaning - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Transformation}
        \begin{itemize}
            \item \textbf{Definition}: Modifying data into a format that is more suitable for analysis.
            \item \textbf{Common Transformations}:
            \begin{itemize}
                \item \textbf{Normalization}: Rescaling features to a common scale (0 to 1) to improve model training.
                \begin{equation}
                    \text{Normalized value} = \frac{x - \min(X)}{\max(X) - \min(X)}
                \end{equation}
                \item \textbf{Encoding Categorical Variables}: Converting categorical variables into numerical values (e.g., One-hot encoding).
                \begin{itemize}
                    \item \textbf{Example}: For "Color" with values “Red,” “Green,” and “Blue,” create binary columns for each color.
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary Points and Next Steps}
    \begin{itemize}
        \item \textbf{Data Removal}: Reduces noise but may lead to loss of valuable information if not done carefully.
        \item \textbf{Imputation}: Maintains dataset size and integrity but requires thoughtful selection of method.
        \item \textbf{Transformation}: Prepares data for modeling, making sure that algorithms can interpret features effectively.
    \end{itemize}
    
    \begin{block}{Next Steps}
        In our next session, we will delve deeper into data transformation techniques, including methods like normalization and aggregation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Introduction}
    Data transformation is a crucial step in data wrangling that involves converting raw data into a more suitable format for analysis. This process enhances data quality, prepares it for analysis, and improves performance. In this slide, we will focus on two key techniques: 
    \begin{itemize}
        \item \textbf{Normalization}
        \item \textbf{Aggregation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Normalization}
    Normalization is the process of adjusting values in a dataset to a common scale without distorting the differences in the ranges of values. This technique is particularly useful when dealing with various data scales that can lead to misleading results in certain analyses, especially in machine learning algorithms.
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Purpose}: To ensure that different variables contribute equally to distance computations.
            \item \textbf{Common Methods}:
                \begin{itemize}
                    \item \textbf{Min-Max Normalization}:
                    \[
                    X_{normalized} = \frac{X - X_{min}}{X_{max} - X_{min}}
                    \]
                    \item \textbf{Z-score Normalization (Standardization)}:
                    \[
                    X_{standardized} = \frac{X - \mu}{\sigma}
                    \]
                    where $\mu$ is the mean and $\sigma$ is the standard deviation.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Normalization Example}
    \textbf{Example:} Original Data: [10, 20, 30, 40, 50] \\
    \textbf{Min-Max Normalization:}
    \begin{itemize}
        \item Min = 10, Max = 50
        \item Normalized: [0, 0.25, 0.5, 0.75, 1]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Aggregation}
    Aggregation involves summarizing or combining multiple data points into a single value, providing a condensed view of the data. This technique is often used in descriptive statistics to derive insights or identify trends.
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Purpose}: To reduce the amount of data while retaining meaningful information.
            \item \textbf{Common Functions}:
                \begin{itemize}
                    \item Sum: Total value of a group.
                    \item Mean: Average of a group.
                    \item Count: Total number of entries.
                    \item Group By: Allows for aggregation based on categorical variables.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Aggregation Example}
    Consider the following dataset of sales transactions:
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            Product & Revenue \\
            \hline
            A       & 100     \\
            A       & 150     \\
            B       & 200     \\
            B       & 250     \\
            \hline
        \end{tabular}
    \end{center}
    
    \textbf{Aggregated Revenue by Product:}
    \begin{itemize}
        \item For Product A: $100 + 150 = 250$
        \item For Product B: $200 + 250 = 450$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Aggregation Code Example}
    This can be accomplished in Python using Pandas:
    \begin{lstlisting}[language=Python]
import pandas as pd

# Sample DataFrame
data = {
    'Product': ['A', 'A', 'B', 'B'],
    'Revenue': [100, 150, 200, 250]
}
df = pd.DataFrame(data)

# Aggregation
aggregated = df.groupby('Product')['Revenue'].sum()
print(aggregated)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Techniques - Conclusion}
    Normalization and aggregation are essential transformation techniques that help in preparing data for analysis. They improve the accuracy and efficiency of data-driven decisions. Properly transformed data leads to better insights and more reliable results in statistical analyses and machine learning models.
\end{frame}

\begin{frame}
    \frametitle{Data Wrangling Tools}
    \begin{block}{Introduction to Data Wrangling Tools}
        Data wrangling, also known as data munging, involves cleaning, transforming, and preparing raw data for analysis. Utilizing the right tools is essential for efficiently performing these tasks. This slide introduces three key tools: \textbf{Pandas}, \textbf{Dplyr}, and \textbf{Apache Spark}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Data Wrangling Tools: Pandas}
    \begin{itemize}
        \item \textbf{Overview}: A powerful data manipulation library in Python, ideal for small to medium-sized datasets.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Data Structures}: Utilizes two primary structures - Series (1D) and DataFrame (2D).
            \item \textbf{Data Cleaning}: Functions for handling missing values, removing duplicates, and filtering data.
            \item \textbf{Data Transformation}: Offers group-by operations, pivot tables, and merging/joining data.
        \end{itemize}
        \item \textbf{Example Code}:
        \begin{lstlisting}[language=Python]
import pandas as pd

# Load a dataset
df = pd.read_csv('data.csv')

# Drop missing values
cleaned_df = df.dropna()

# Group by a column and calculate the mean
grouped_df = cleaned_df.groupby('Category').mean()
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Data Wrangling Tools: Dplyr}
    \begin{itemize}
        \item \textbf{Overview}: A robust library in R designed specifically for data manipulation tasks.
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{Verb-Driven}: Utilizes a set of ‘verbs’ (functions) providing a clear and intuitive syntax (e.g., select, filter, mutate).
            \item \textbf{Pipelines}: Employs the pipe operator (\%>\%) to streamline operations, improving code readability.
        \end{itemize}
        \item \textbf{Example Code}:
        \begin{lstlisting}[language=R]
library(dplyr)

# Load a dataset
df <- read.csv("data.csv")

# Clean and manipulate the dataset
cleaned_df <- df %>%
  filter(!is.na(Column)) %>%
  mutate(NewColumn = Column * 2) %>%
  group_by(Category) %>%
  summarise(MeanValue = mean(NewColumn))
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Data Wrangling Tools: Apache Spark}
    \begin{itemize}
        \item \textbf{Overview}: A distributed computing framework suitable for handling large datasets (big data).
        \item \textbf{Key Features}:
        \begin{itemize}
            \item \textbf{In-Memory Processing}: Processes data in memory, speeding up analysis times significantly.
            \item \textbf{DataFrames and SQL Queries}: Supports SQL-like queries, making it accessible for users familiar with SQL.
        \end{itemize}
        \item \textbf{Example Code}:
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("Data Wrangling").getOrCreate()

# Load a dataset
df = spark.read.csv('data.csv', header=True, inferSchema=True)

# Perform data transformations
cleaned_df = df.na.drop()
grouped_df = cleaned_df.groupBy('Category').agg({'Value': 'mean'})
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Pandas}: Suitable for small and medium datasets with robust, user-friendly features.
        \item \textbf{Dplyr}: Enhances R programming with concise syntax emphasizing functional programming.
        \item \textbf{Apache Spark}: Ideal for big data scenarios, enabling efficient distributed processing.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Each of these tools offers unique advantages tailored to different data wrangling needs. Emphasizing understanding the strengths of each tool will empower users to select the best option for their data transformation tasks.
\end{frame}

\begin{frame}
    \frametitle{Hands-on Data Wrangling}
    \begin{block}{Overview of Data Wrangling}
        Data wrangling, also known as data munging, is the process of transforming and mapping raw data into a more usable format. This essential step in data science prepares the dataset for analysis and helps ensure that the insights drawn are accurate and reliable.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Objective of the Case Study}
    In this hands-on exercise, we will demonstrate common data cleaning and preparation techniques using a sample dataset. This live coding session will guide you through the process of:
    \begin{enumerate}
        \item Identifying and handling missing values
        \item Standardizing data formats
        \item Removing duplicates
        \item Filtering and transforming data
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Sample Dataset}
    We will use a sample dataset that contains employee records, including the following columns:
    \begin{itemize}
        \item \texttt{EmployeeID}
        \item \texttt{Name}
        \item \texttt{Department}
        \item \texttt{Salary}
        \item \texttt{JoinDate}
        \item \texttt{Email}
    \end{itemize}

    \begin{table}[ht]
        \centering
        \begin{tabular}{cccccc}
            \toprule
            EmployeeID & Name       & Department & Salary   & JoinDate   & Email                \\
            \midrule
            1          & John Doe  & HR         & 50000    & 2019/01/10  & john@example.com     \\
            2          & Jane Smith & IT         & NaN      & 2018/02/20  & jane@example.com     \\
            3          & Bob Brown  & IT         & 60000    & 2019-01-15  & bob@example.com      \\
            1          & John Doe  & HR         & 50000    & 2019/01/10  & john@example.com     \\
            \bottomrule
        \end{tabular}
        \caption{Sample Employee Dataset}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Data Wrangling}
    \textbf{1. Loading the Data} \\
    We will use the Pandas library in Python to load our dataset.
    \begin{lstlisting}[language=Python]
import pandas as pd
data = pd.read_csv("employees.csv")
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Identifying Missing Values}
    \textbf{2. Identifying Missing Values} \\
    Check for any missing values:
    \begin{lstlisting}[language=Python]
print(data.isnull().sum())
    \end{lstlisting}

    Address missing values by using different strategies, such as filling with averages or dropping rows:
    \begin{lstlisting}[language=Python]
data['Salary'].fillna(data['Salary'].mean(), inplace=True)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Standardizing Formats and Removing Duplicates}
    \textbf{3. Standardizing Formats} \\
    Convert date formats to a standard format:
    \begin{lstlisting}[language=Python]
data['JoinDate'] = pd.to_datetime(data['JoinDate'], errors='coerce')
    \end{lstlisting}

    \textbf{4. Removing Duplicates} \\
    Identify and remove duplicates based on the \texttt{EmployeeID}:
    \begin{lstlisting}[language=Python]
data.drop_duplicates(subset=['EmployeeID'], inplace=True)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Filtering and Transforming Data}
    \textbf{5. Filtering Data} \\
    Filter out employees with a salary below a certain threshold:
    \begin{lstlisting}[language=Python]
high_earners = data[data['Salary'] > 55000]
    \end{lstlisting}

    \textbf{6. Transforming Data} \\
    Create a new column for the year the employee joined:
    \begin{lstlisting}[language=Python]
data['JoinYear'] = data['JoinDate'].dt.year
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Importance of Data Cleaning:} Clean data leads to reliable analysis and insights.
        \item \textbf{Multiple Strategies:} There are often various methods to handle specific issues (missing values, duplicates).
        \item \textbf{Documentation:} Always document your data wrangling process for reproducibility.
    \end{itemize}
    
    By practicing these techniques, you will gain proficiency in manipulating datasets, a crucial skill for any data analyst or scientist. This interactive case study will not only familiarize you with using tools like Pandas but will also reinforce the importance of thorough data wrangling in your analyses.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Wrangling - Introduction}
    \begin{block}{Introduction to Data Wrangling}
        Data wrangling, or data preparation, is the process of cleaning, transforming, and organizing raw data into a usable format. While data wrangling is essential for accurate analysis and decision-making, it often comes with a range of challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Data Wrangling - Overview}
    \begin{block}{Common Challenges}
        \begin{enumerate}
            \item Missing Values
            \item Inconsistent Data Formats
            \item Outliers
            \item Duplicate Entries
            \item Data Type Mismatches
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Missing Values}
    \begin{block}{Description}
        Incomplete data can skew analysis. Missing values can occur due to various reasons such as data entry errors or participants not answering certain survey questions.
    \end{block}
    \begin{block}{Strategies to Overcome}
        \begin{itemize}
            \item \textbf{Deletion:} Remove rows/columns with missing values if they constitute a small fraction of the dataset.
            \item \textbf{Imputation:} Replace missing values with the mean, median, or a specific value based on other observations.
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        If a dataset has 100 entries and 5 of them are missing age values, consider imputing missing ages using the average age of the existing entries.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inconsistent Data Formats}
    \begin{block}{Description}
        Different data entries may be recorded in various formats (e.g., dates in 'MM/DD/YYYY' and 'DD/MM/YYYY').
    \end{block}
    \begin{block}{Strategies to Overcome}
        \begin{itemize}
            \item \textbf{Standardization:} Convert all data entries to a consistent format using transformation techniques.
            \item \textbf{Regular Expressions:} Utilize regex for pattern matching and formatting data correctly.
        \end{itemize}
    \end{block}
    \begin{block}{Example}
        To unify date formats, convert all entries using a function that checks and reformats them appropriately based on the initial format detected.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Outliers and Duplicate Entries}
    \begin{block}{Outliers}
        \begin{itemize}
            \item \textbf{Description:} Outliers are extreme values that can distort statistical analysis.
            \item \textbf{Strategies to Overcome:}
                \begin{itemize}
                    \item Detection: Use statistical methods (e.g., Z-scores, IQR) to identify outliers.
                    \item Treatment: Either remove them or apply transformations to lessen their impact.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
import numpy as np
from scipy import stats

data = [1, 2, 2, 3, 14]  # Example dataset
z_scores = np.abs(stats.zscore(data))
filtered_data = [x for x, z in zip(data, z_scores) if z < 2]  # Remove outliers
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Duplicate Entries and Type Mismatches}
    \begin{block}{Duplicate Entries}
        \begin{itemize}
            \item \textbf{Description:} Duplicate records can inflate results and provide misleading analysis.
            \item \textbf{Strategies to Overcome:}
                \begin{itemize}
                    \item Deduplication: Identify and remove duplicates based on unique identifiers.
                    \item Aggregation: Combine duplicate entries to produce a single coherent dataset entry.
                \end{itemize}
        \end{itemize}
        \begin{block}{Example}
            If records of customer transactions contain multiple entries for the same purchase, consider aggregating total sales per customer to eliminate duplicates.
        \end{block}
    \end{block}

    \begin{block}{Data Type Mismatches}
        \begin{itemize}
            \item \textbf{Description:} Incorrect data types can disrupt analysis and operations.
            \item \textbf{Strategies to Overcome:}
                \begin{itemize}
                    \item Data Type Conversion: Use transformation functions to convert columns to the correct type.
                    \item Validation Rules: Implement validation rules during data entry to ensure appropriate data types.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Data wrangling is crucial for generating reliable insights.
            \item Recognizing these common challenges early on can save time and improve data quality.
            \item Implementing systematic strategies helps streamline the data preparation process.
        \end{itemize}
    \end{block}
    \begin{block}{Next Steps}
        By being aware of the challenges in data wrangling and adopting effective strategies to tackle them, data professionals can enhance their data manipulation skills and ensure high-quality datasets for analysis. In the next slide, we will explore best practices in data wrangling to further refine our techniques.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Best Practices in Data Wrangling}
    \frametitle{Introduction to Data Wrangling}
    \begin{itemize}
        \item Data wrangling, also known as data munging, is the process of transforming and mapping raw data into an understandable format.
        \item Essential for ensuring accuracy and usefulness of data analysis.
        \item Following best practices enhances effectiveness in data wrangling.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Best Practices Overview}
    \begin{enumerate}
        \item \textbf{Understand the Data Landscape}
            \begin{itemize}
                \item Identify data sources and formats (e.g., CSV, JSON).
                \item Recognize data types (text, dates, integers).
            \end{itemize}
        \item \textbf{Clean Your Data Meticulously}
            \begin{itemize}
                \item Remove duplicates with \texttt{drop\_duplicates()} in pandas.
                \item Handle missing values: imputation or deletion.
            \end{itemize}
        \item \textbf{Transform Data for Analysis}
            \begin{itemize}
                \item Normalize and standardize data for comparability.
                \item Use \texttt{MinMaxScaler} for scaling features.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippets}
    \begin{block}{Example of Handling Missing Values}
        \begin{lstlisting}[language=Python]
df['column_name'].fillna(df['column_name'].mean(), inplace=True)
        \end{lstlisting}
    \end{block}

    \begin{block}{Example of Normalization}
        \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[['feature1', 'feature2']])
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Additional Best Practices}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue the enumeration
        \item \textbf{Document Your Wrangling Process}
            \begin{itemize}
                \item Record all steps for reproducibility.
                \item Use comments in your code to explain choices.
            \end{itemize}
        \item \textbf{Visualize Intermediate Results}
            \begin{itemize}
                \item Use visualizations to understand data distribution.
                \item Create histograms or scatter plots post-cleaning.
            \end{itemize}
        \item \textbf{Iterate and Validate}
            \begin{itemize}
                \item Regularly review the process for accuracy.
                \item Cross-check findings with subsets of clean data.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Begin with exploratory analysis to understand your data.
        \item Clean data leads to reliable insights; quality is paramount.
        \item Documentation enhances transparency and reproducibility.
        \item Utilize programming libraries to streamline efforts.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Implementing best practices in data wrangling improves both accuracy and efficiency in data analysis. Quality in data wrangling reflects in the insights derived from data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing}
    \begin{block}{Introduction}
        Data wrangling involves collecting, cleaning, and transforming data for analysis. 
        We must consider the ethical implications tied to data processing, especially regarding user privacy and data protection regulations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts in Data Ethics}
    \begin{enumerate}
        \item \textbf{Data Privacy}:
            \begin{itemize}
                \item Proper handling of sensitive data to protect individuals' rights.
                \item Collect only necessary data and inform individuals about its use.
            \end{itemize}
        \item \textbf{Informed Consent}:
            \begin{itemize}
                \item Users should explicitly consent for their data to be collected and processed.
                \item Clear communication about data collection and processing is essential.
            \end{itemize}
        \item \textbf{Data Minimization}:
            \begin{itemize}
                \item Collect only the data needed to reduce risk and enhance transparency.
                \item \textit{Example: In a survey, limit demographic data collection to age and gender if that's sufficient.}
            \end{itemize}
        \item \textbf{Data Security}:
            \begin{itemize}
                \item Implement security measures to protect data from unauthorized access.
                \item \textit{Example: Use encryption and access control.}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Compliance and Best Practices}
    \begin{block}{Compliance with Data Protection Regulations}
        \textbf{GDPR (General Data Protection Regulation)}:
        \begin{itemize}
            \item Regulation in EU law on data protection and privacy.
            \item Key principles:
                \begin{itemize}
                    \item Right to Access: Request to see data held about individuals.
                    \item Right to Erasure: Request for deletion of personal data.
                    \item Privacy by Design: Data protection considered from project onset.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Best Practices}
        \begin{itemize}
            \item \textbf{Transparency}: Inform users of data practices to build trust.
            \item \textbf{Anonymization}: Remove PII to protect identities while allowing analysis.
            \item \textbf{Regular Audits}: Assess data handling practices regularly.
            \item \textbf{Training and Awareness}: Train staff on ethical data practices.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Techniques}
    \begin{block}{Overview of Data Wrangling Techniques}
        Data wrangling is a crucial step in the data analysis process, transforming raw data into a usable format. This chapter covered essential techniques that enhance data analysis. Below is a summary of these key techniques and their significance in the data analysis workflow.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques - Part 1}

    \begin{enumerate}
        \item \textbf{Data Cleaning}
        \begin{itemize}
            \item \textbf{Explanation:} Identifying and correcting errors, handling missing values.
            \item \textbf{Example:} Filling missing ages with mean values.
            \item \textbf{Key Point:} Clean data ensures reliable analysis.
        \end{itemize}

        \item \textbf{Data Transformation}
        \begin{itemize}
            \item \textbf{Explanation:} Changing data format for better usability.
            \item \textbf{Example:} Converting dates from "MM/DD/YYYY" to "YYYY-MM-DD".
            \item \textbf{Key Point:} Transformed data enhances compatibility with analytical tools.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques - Part 2}

    \begin{enumerate}
        \setcounter{enumi}{2} % Start enumeration from 3
        \item \textbf{Data Aggregation}
        \begin{itemize}
            \item \textbf{Explanation:} Combining data into summary statistics.
            \item \textbf{Example:} Analyzing sales data aggregated by month.
            \item \textbf{Key Point:} Aggregated data simplifies analysis.
        \end{itemize}

        \item \textbf{Data Filtering}
        \begin{itemize}
            \item \textbf{Explanation:} Selecting data subsets based on criteria.
            \item \textbf{Example:} Filtering customers who spent over $100.
            \item \textbf{Key Point:} Filtering enhances accuracy of insights.
        \end{itemize}
        
        \item \textbf{Merging and Joining Datasets}
        \begin{itemize}
            \item \textbf{Explanation:} Combining records from multiple datasets.
            \item \textbf{Example:} Joining customer and sales datasets on customer ID.
            \item \textbf{Key Point:} Merged datasets provide deeper insights.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}

    \begin{block}{Conclusion on Significance}
        The techniques covered are foundational for achieving actionable insights from data. Mastering these ensures analysts can handle messy data effectively. 
        Effective data wrangling sets the stage for insightful analysis and supports ethical practices concerning privacy and compliance.
    \end{block}

    \begin{block}{Next Steps}
        \begin{itemize}
            \item Prepare for the Q\&A session for clarifications.
            \item Apply these techniques in practical exercises to reinforce understanding.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A and Discussion}
    \begin{block}{Objective}
        This slide serves as an open floor opportunity for students to clarify concepts, share insights, and engage in discussions about data wrangling techniques introduced in this module. This interactive session aims to reinforce understanding and address any uncertainties.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session Guidelines}
    \begin{enumerate}
        \item \textbf{Encouragement for Questions:}
            \begin{itemize}
                \item Students should feel free to ask questions about any particular data wrangling technique covered in the previous slides. No question is too basic or complex!
            \end{itemize}
        \item \textbf{Clarifying Concepts:}
            \begin{itemize}
                \item Request students to specify the techniques they found most interesting or challenging, such as:
                    \begin{itemize}
                        \item Data cleaning methods (e.g., handling missing data)
                        \item Data transformation techniques (e.g., reshaping, pivoting)
                        \item Data integration practices (e.g., merging datasets)
                    \end{itemize}
            \end{itemize}
        \item \textbf{Discussion Topics:}
            \begin{itemize}
                \item Discuss real-world applications of data wrangling techniques:
                \begin{itemize}
                    \item How might these techniques be essential in your data analysis projects?
                    \item What tools or programming languages (such as Python, R, SQL) have you used or discovered useful for data wrangling?
                \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Reinforce}
    \begin{itemize}
        \item \textbf{Importance of Data Wrangling:}
            \begin{itemize}
                \item Data wrangling is critical for converting raw data into a more usable format, ensuring accuracy and completeness for analysis.
            \end{itemize}
        \item \textbf{Common Techniques Reviewed:}
            \begin{itemize}
                \item \textbf{Data Cleaning:} Removing duplicates, filling in missing values, outlier detection.
                \item \textbf{Data Transformation:} Normalization, standardization, and aggregation techniques to prepare datasets for analysis.
                \item \textbf{Data Integration:} Combining multiple data sources into a cohesive dataset to provide broader insights.
            \end{itemize}
    \end{itemize}
\end{frame}


\end{document}