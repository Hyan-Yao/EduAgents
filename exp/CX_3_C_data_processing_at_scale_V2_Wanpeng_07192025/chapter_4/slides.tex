\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 4: Hands-On with ETL Tools]{Week 4: Hands-On with ETL Tools}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Week 4: Hands-On with ETL Tools}
    Welcome to Week 4! This week, we will focus on hands-on applications of ETL processes using Apache Spark.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of the Week's Focus}
    In this module, we will cover the following key areas regarding ETL and Apache Spark:
    \begin{enumerate}
        \item \textbf{What is ETL?}
        \item \textbf{Introduction to Apache Spark}
        \item \textbf{Installation Process}
        \item \textbf{Running Basic ETL Tasks}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is ETL?}
    \begin{block}{Definition}
        ETL stands for Extract, Transform, Load. It describes the process of transferring data from multiple sources into a data warehouse or another system.
    \end{block}
    \begin{itemize}
        \item \textbf{Extract:} Pull data from various sources (e.g., databases, APIs).
        \item \textbf{Transform:} Cleanse and format data for analysis.
        \item \textbf{Load:} Store prepared data into a destination system.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark}
    \begin{itemize}
        \item \textbf{What is Apache Spark?} A fast and general-purpose cluster computing system for large-scale dataset processing.
        \item \textbf{Programming Languages:} Supports Python, Scala, Java, and R.
        \item \textbf{Key Features:}
            \begin{itemize}
                \item In-memory computing for faster data processing.
                \item Support for complex analytics and machine learning (MLlib).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installation Process}
    To start hands-on activities, ensure Apache Spark is installed correctly:
    \begin{enumerate}
        \item \textbf{Download Apache Spark} from \url{https://spark.apache.org/downloads.html}.
        \item \textbf{Install Java:} Requires Java 8 or higher.
        \item \textbf{Set Environment Variables:}
        \begin{itemize}
            \item Set \texttt{SPARK\_HOME} to the Spark installation path.
            \item Add \texttt{\$SPARK\_HOME/bin} to the system \texttt{PATH}.
        \end{itemize}
        \item \textbf{Verify Installation:} Run \texttt{spark-shell} in your terminal.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Running Basic ETL Tasks}
    We will perform basic ETL operations in Spark using the PySpark library. Here's a sample code snippet:
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("ETL Example").getOrCreate()

# Extract: Load data from a CSV file
df = spark.read.csv("input_data.csv", header=True, inferSchema=True)

# Transform: Data cleaning - dropping null values
df_cleaned = df.dropna()

# Load: Write cleaned data to a new CSV file
df_cleaned.write.csv("cleaned_data.csv", header=True)

# Stop the Spark session
spark.stop()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Familiarity with Apache Sparkâ€™s capabilities is essential for effective data manipulation and processing.
        \item Hands-on experience with ETL processes enhances understanding of data workflows.
        \item Installation steps are crucial for a seamless Spark experience.
    \end{itemize}
    
    By the end of this week, you will have foundational knowledge and practical skills to utilize Apache Spark effectively for your ETL tasks!
\end{frame}

\begin{frame}[fragile]{Overview of Apache Spark - Part 1}
    \frametitle{What is Apache Spark?}
    \begin{block}{Introduction}
        \textbf{Apache Spark} is an open-source, distributed computing system designed for fast, in-memory data processing.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Speed}: Processes data in-memory, faster than traditional disk-based systems.
        \item \textbf{Ease of Use}: Supports multiple programming languages: Scala, Python, Java, and R.
        \item \textbf{Versatile}: Handles ETL, machine learning, streaming, and interactive queries.
        \item \textbf{Unified Engine}: Integrates SQL, streaming, and machine learning in one framework.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Overview of Apache Spark - Part 2}
    \frametitle{Why Use Apache Spark for Data Processing?}
    \begin{enumerate}
        \item \textbf{Performance}:
            \begin{itemize}
                \item In-memory computing reduces latency; keeps data in-memory instead of writing to disk.
                \item DAG execution engine optimizes execution plans.
                \item \textit{Example:} Processes large datasets like log files quickly using in-memory computations.
            \end{itemize}
        \item \textbf{Scalability}:
            \begin{itemize}
                \item Runs on a single machine or scales to thousands of nodes.
                \item Integrates with cluster managers like Apache Mesos and Hadoop YARN.
            \end{itemize}
        \item \textbf{Flexibility}:
            \begin{itemize}
                \item Rich API for working with DataFrames, RDDs, and SQL queries.
                \item Supports MLlib for machine learning algorithms.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Overview of Apache Spark - Part 3}
    \frametitle{Example Code Snippet}
    \begin{block}{Python Example: Word Count}
        This code reads a text file, counts the words, and provides the top 5 words:
        \begin{lstlisting}[language=Python]
from pyspark import SparkContext

# Initialize Spark Context
sc = SparkContext("local", "WordCountApp")

# Read data
text_file = sc.textFile("hdfs://path_to_file/data.txt")

# Process data
word_counts = (text_file.flatMap(lambda line: line.split(" "))
                         .map(lambda word: (word, 1))
                         .reduceByKey(lambda a, b: a + b))

# Get top 5 words
top_words = word_counts.takeOrdered(5, key=lambda x: -x[1])
print(top_words)

# Stop Spark Context
sc.stop()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up Apache Spark - Overview}
    \begin{itemize}
        \item Apache Spark is a powerful open-source distributed computing system.
        \item Widely used for big data processing and analytics.
        \item Correct installation is crucial for seamless functionality.
        \item This slide provides step-by-step instructions for installing Spark locally and on cloud platforms.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up Apache Spark - Local Installation}
    \begin{block}{A. Installing Apache Spark Locally}
        \textbf{Prerequisites:}
        \begin{enumerate}
            \item \textbf{Java:} Install Java 8 or later.
                \begin{itemize}
                    \item Check Installation: Run \texttt{java -version}.
                    \item Visit: \url{https://www.oracle.com/java/technologies/javase-jdk11-downloads.html}.
                \end{itemize}
            \item \textbf{Scala:} (Optional but recommended) Install Scala 2.11.
                \begin{itemize}
                    \item Check Installation: Run \texttt{scala -version}.
                    \item Visit: \url{https://www.scala-lang.org/download/}.
                \end{itemize}
        \end{enumerate}

        \textbf{Steps:}
        \begin{enumerate}
            \item \textbf{Download Apache Spark:}
                \begin{itemize}
                    \item Visit: \url{https://spark.apache.org/downloads.html}.
                    \item Choose pre-built package for Hadoop (currently, "Pre-built for Apache Hadoop 3.x").
                \end{itemize}
            \item \textbf{Extract the Package:}
            \begin{lstlisting}
tar -xvzf spark-*.tgz
            \end{lstlisting}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up Apache Spark - Local Installation (Cont'd)}
    \begin{block}{Local Installation Steps (Cont'd)}
        \begin{enumerate}[resume]
            \item \textbf{Set Environment Variables:}
            \begin{lstlisting}
export SPARK_HOME=~/path_to_extracted_spark/spark-*
export PATH=$PATH:$SPARK_HOME/bin
            \end{lstlisting}
            \item Reload the profile: \texttt{source ~/.bashrc}.
            \item \textbf{Verify Installation:}
            \begin{itemize}
                \item Start Spark Shell:
                \begin{lstlisting}
spark-shell
                \end{lstlisting}
                \item You should see a Spark welcome message.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up Apache Spark - Cloud Installation}
    \begin{block}{B. Installing Apache Spark on Cloud Platforms}
        \textbf{Examples of Cloud Platforms:}
        \begin{enumerate}
            \item **Amazon EMR:**
                \begin{itemize}
                    \item Log in to AWS Management Console.
                    \item Go to EMR and create a new cluster.
                    \item Choose 'Spark' as an application.
                \end{itemize}
            \item **Google Cloud Dataproc:**
                \begin{itemize}
                    \item Go to Google Cloud Console.
                    \item Navigate to Dataproc and create a new cluster.
                    \item Select Spark as one of the default services.
                \end{itemize}
            \item **Microsoft Azure HDInsight:**
                \begin{itemize}
                    \item Sign in to Azure portal.
                    \item Create an HDInsight cluster.
                    \item Choose the Spark option during setup.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up Apache Spark - Key Points}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Ensure Java and Scala are correctly installed before Spark installation.
            \item Always verify the installation to catch any issues.
            \item Refer to platform documentation for specific cloud configurations.
        \end{itemize}
    \end{block}

    \begin{block}{Example Code Snippet}
    To confirm your Spark installation, run the following in the Spark shell:
    \begin{lstlisting}
val data = Seq(1, 2, 3, 4, 5)
val distData = spark.sparkContext.parallelize(data)
println(distData.collect().mkString(", "))
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Setting Up Apache Spark - Conclusion}
    \begin{block}{Conclusion}
        Setting up Apache Spark is the first crucial step towards harnessing its data processing capabilities.
        \begin{itemize}
            \item Proper installation enables leveraging Spark's full potential for ETL processes and big data analytics.
            \item In the next slide, we will explore basic ETL concepts, including Extract, Transform, Load processes critical for data handling.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Basic ETL Concepts}
  \begin{block}{What is ETL?}
    ETL stands for \textbf{Extract, Transform, Load}. It is a process crucial for gathering, filtering, and preparing data for analysis in data warehousing and business intelligence applications.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Basic ETL Concepts - Extract Phase}
  \begin{enumerate}
    \item \textbf{Extract}
      \begin{itemize}
        \item The initial phase where data is collected from various sources such as databases, cloud services, APIs, and flat files.
        \item \textit{Example}: Pulling customer data from a CRM system, sales data from an ERP, and social media data from API endpoints.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Basic ETL Concepts - Transform and Load Phases}
  \begin{enumerate}
    \setcounter{enumi}{1} % Continue numbering
    \item \textbf{Transform}
      \begin{itemize}
        \item Here, extracted data is cleaned, enriched, and transformed into a usable format.
        \item \textit{Example}: Changing date formats, standardizing currency values, or merging records to remove duplicates.
      \end{itemize}
      
    \item \textbf{Load}
      \begin{itemize}
        \item The final step involves loading data into a target repository for analysis and reporting.
        \item \textit{Example}: Inserting prepared sales data into a PostgreSQL data warehouse.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Importance of ETL in Data Pipelines}
  \begin{itemize}
    \item \textbf{Data Integration}: Vital for integrating data from disparate sources into a unified format.
    \item \textbf{Quality Control}: Ensures accuracy and consistency, increasing reliability of reports and analyses.
    \item \textbf{Efficiency}: Automating ETL processes enhances speed and efficiency in data handling.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example of Basic ETL Process in Code}
  Hereâ€™s a simplified Python code snippet using \texttt{pandas} to demonstrate a basic ETL operation:
  
  \begin{lstlisting}[language=Python]
import pandas as pd

# Step 1: Extract
data = pd.read_csv('sales_data.csv')

# Step 2: Transform
data['Date'] = pd.to_datetime(data['Date'])  # Convert date format
data.drop_duplicates(subset='CustomerID', keep='first', inplace=True)  # Remove duplicates

# Step 3: Load
data.to_sql(name='sales_data_clean', con=db_connection, if_exists='replace')
  \end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Conclusion}
  Understanding and mastering the ETL process is essential for effective data management. As we proceed into the hands-on lab, we will apply these ETL concepts using Apache Spark, enabling practical understanding of the theory discussed.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Lab Task - Objective}
    \begin{block}{Objective}
        The purpose of this lab session is to provide practical experience with the ETL process using Apache Spark. 
        You will learn to install Spark and execute basic ETL tasks, enabling you to:
        \begin{itemize}
            \item Extract data
            \item Transform it according to business needs
            \item Load it into a target system
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Lab Task - Overview}
    \begin{block}{Lab Tasks Overview}
        \begin{enumerate}
            \item \textbf{Install Apache Spark}:
                \begin{itemize}
                    \item Requirements: JDK, Apache Spark, Apache Hadoop (optional)
                    \item Installation Steps:
                        \begin{itemize}
                            \item Install JDK (verify with `java -version`)
                            \item Download Spark from the \href{https://spark.apache.org/downloads.html}{Apache Spark website}
                            \item Set Environment Variables: 
                              \begin{itemize}
                                \item Set `SPARK_HOME` to your Spark installation directory
                                \item Add Sparkâ€™s `bin` directory to your `PATH`
                              \end{itemize}
                        \end{itemize}
                \end{itemize}
            \item \textbf{Setting Up Your Environment}: Launch a terminal and start a Spark Shell using:
                \begin{lstlisting}[language=bash]
spark-shell
                \end{lstlisting}
            \item \textbf{Executing Basic ETL Processes}:
                \begin{itemize}
                    \item Extract, Transform, and Load data (details in next frame)
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Lab Task - ETL Process}
    \begin{block}{ETL Process Overview}
        \begin{itemize}
            \item \textbf{Extract Data}:
                \begin{lstlisting}[language=scala]
val data = spark.read.option("header", true).csv("path/to/dataset.csv")
data.show()  // Display the loaded data
                \end{lstlisting}
            
            \item \textbf{Transform Data}:
                \begin{lstlisting}[language=scala]
import org.apache.spark.sql.functions._

val transformedData = data.filter(col("Age") > 18)
                           .groupBy("City")
                           .agg(avg("Salary").alias("Average_Salary"))
transformedData.show()  // Display the transformed data
                \end{lstlisting}
                
            \item \textbf{Load Data}:
                \begin{lstlisting}[language=scala]
transformedData.write.option("header", true).csv("path/to/output.csv")
                \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Lab Task - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Understand the ETL pipeline: extraction, transformation, and loading.
            \item Importance of Spark: efficient, distributed computing framework for large datasets.
            \item Practical application: cementing knowledge, fostering familiarity with tools, and encouraging exploration.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Lab Task - Summary and Next Steps}
    \begin{block}{Summary}
        By participating in this lab, you will gain a foundational understanding of how to use Apache Spark for ETL processes. 
        Experiment with different datasets and transformations to enhance your learning.
    \end{block}
    
    \begin{block}{Next Steps}
        Transitioning to the next slide, "Running ETL Tasks with Apache Spark," 
        where we will explore specific examples and outcomes of the ETL processes demonstrated in this lab.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Running ETL Tasks with Apache Spark}
    \begin{block}{Introduction to ETL in Spark}
        Apache Spark is a powerful open-source distributed computing system that can handle large-scale data processing. 
        ETL stands for Extract, Transform, Load, and these processes are fundamental for data integration tasks in data engineering. 
        In this section, we will demonstrate how to conduct ETL tasks using Apache Sparkâ€™s DataFrame API.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Extract Data}
    \begin{itemize}
        \item The first step involves extracting data from various sources (e.g., CSV files, databases, APIs).
        \item We will create a Spark session and read a CSV file.
    \end{itemize}
    
    \begin{block}{Code Snippet}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("ETL Example") \
    .getOrCreate()

# Extract data from a CSV file
data = spark.read.csv("path/to/data.csv", header=True, inferSchema=True)
data.show()
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Expected Outcome}
        This code will display the first few rows of the extracted data from the CSV file, allowing you to verify successful extraction.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transform Data}
    \begin{itemize}
        \item Data transformation involves cleaning and converting the data into a desired format.
        \item Common transformations include filtering records, applying functions, and aggregating values.
    \end{itemize}
    
    \begin{block}{Code Snippet}
    \begin{lstlisting}[language=Python]
# Filter rows where a specific condition is met
filtered_data = data.filter(data["columnName"] > 100)

# Add a new calculated column
transformed_data = filtered_data.withColumn("newColumn", filtered_data["columnName"] * 1.5)

transformed_data.show()
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Expected Outcome}
        After applying transformations, youâ€™ll see a DataFrame with only the filtered records and a new column reflecting transformed values.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Load Data}
    \begin{itemize}
        \item The final step in the ETL process is loading the data into a target data warehouse or saving it to a file for further analysis.
    \end{itemize}
    
    \begin{block}{Code Snippet}
    \begin{lstlisting}[language=Python]
# Load the transformed data into a new CSV file
transformed_data.write.csv("path/to/transformed_data.csv", header=True)
    \end{lstlisting}
    \end{block}
    
    \begin{block}{Expected Outcome}
        Upon running this code, a new CSV file with the transformed data will be created at the specified path, ready for use.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Scalability:} Spark processes large datasets efficiently due to its distributed nature.
        \item \textbf{Flexibility:} The DataFrame API provides powerful functions for data manipulation.
        \item \textbf{Real-time Processing:} Spark can handle both batch and stream processing, making it suitable for various ETL scenarios.
    \end{itemize}
    
    \begin{block}{Conclusion}
        By incorporating these ETL tasks within Apache Spark, you can streamline data integration and prepare datasets for analysis effectively. 
        Next, we will explore data wrangling techniques to further refine our datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Wrangling Techniques}
    \begin{block}{Introduction to Data Wrangling in Spark}
        Data wrangling, or data munging, involves transforming and preparing raw data for analysis. In Spark, it ensures that data is accurate, complete, and suitable for further analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Techniques for Data Cleaning}
    \begin{enumerate}
        \item \textbf{Handling Missing Values}
        \begin{itemize}
            \item Use \texttt{dropna()} to remove rows with missing values:
            \begin{lstlisting}
df_clean = df.dropna()
            \end{lstlisting}
            \item Use \texttt{fillna()} to replace missing values:
            \begin{lstlisting}
df_filled = df.fillna({'column_name': 0})
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Data Type Conversion}
        \begin{itemize}
            \item Use \texttt{cast()} to change a column type:
            \begin{lstlisting}
df = df.withColumn("age", df["age"].cast("integer"))
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continued: Data Cleaning Techniques}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Removing Duplicates}
        \begin{itemize}
            \item Use \texttt{dropDuplicates()} to eliminate duplicates:
            \begin{lstlisting}
df_unique = df.dropDuplicates()
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Filtering Data}
        \begin{itemize}
            \item Use \texttt{filter()} to retain specific rows:
            \begin{lstlisting}
df_filtered = df.filter(df["salary"] > 50000)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Transforming Data}
        \begin{itemize}
            \item Use \texttt{withColumn()} to modify data:
            \begin{lstlisting}
from pyspark.sql.functions import col
df = df.withColumn("net_salary", col("salary") - col("tax"))
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Aggregating Data}
        \begin{itemize}
            \item Use \texttt{groupBy()} combined with aggregation:
            \begin{lstlisting}
df_grouped = df.groupBy("department").agg({"salary": "avg"})
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Points}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Proper data cleaning enhances the quality of insights.
            \item Spark's functions streamline the wrangling process for large datasets.
            \item Data wrangling is foundational for successful ETL tasks and analysis.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Mastering these techniques in PySpark is essential for any data professional, paving the way for accurate and meaningful analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Analyzing the Results}
    \begin{block}{Understanding the Output of ETL Tasks}
        After completing the Extract, Transform, Load (ETL) processes, it's crucial to analyze the results effectively. This ensures data processing is correct and yields actionable insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Analyzing the Results - Significance of ETL Results}
    \begin{itemize}
        \item \textbf{Data Quality Confirmation:} Ensure data integrity and accuracy. Look for anomalies or inconsistencies from the transformation.
        \item \textbf{Relevance of Data:} Verify that the resulting dataset represents the intended demographic or segment for your analysis objectives.
    \end{itemize}
    
    \begin{block}{Example}
        If your ETL task cleaned customer transaction records, finding that 5\% of records are duplicates after deduplication indicates effectiveness but suggests further refinement.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Analyzing the Results - Next Steps in Data Analysis}
    \begin{enumerate}
        \item \textbf{Data Exploration:} Use summary statistics and exploratory data analysis (EDA) techniques.
            \begin{itemize}
                \item Descriptive statistics (mean, median, standard deviation)
                \item Correlation analysis to identify relationships between variables
            \end{itemize}
            
        \item \textbf{Identifying Trends and Patterns:} Spot trends that provide insights.
            \begin{block}{Example Code Snippet (Python)}
            \begin{lstlisting}
import pandas as pd
data = pd.read_csv('processed_data.csv')
# Display basic statistics
print(data.describe())
# Identify correlations
correlation_matrix = data.corr()
            \end{lstlisting}
            \end{block}
        
        \item \textbf{Creating Visualizations:} Consider common chart types. 
            \begin{itemize}
                \item Bar Charts for categorical data
                \item Line Graphs for time-series data
                \item Heat Maps for correlation matrices
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing Data Insights}
    % Overview of data visualization tools for Spark output
    In this session, we will explore various visualization tools that help represent processed data effectively, particularly focusing on the outputs from Apache Spark. Visualization transforms raw data into a visual format, making it easier to identify patterns, trends, and insights.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Visualization}
    % Key reasons why data visualization is crucial
    \begin{itemize}
        \item \textbf{Simplifies Complex Data}: Visualization makes it easier to understand vast amounts of data by presenting it in a visual format, such as charts and graphs.
        \item \textbf{Identifies Trends and Patterns}: Allows for quick detection of trends and anomalies that might not be obvious in raw data.
        \item \textbf{Enhances Data Storytelling}: Effective visualizations can communicate insights succinctly and engagingly.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Popular Visualization Tools}
    % Overview of tools for data visualization
    \begin{enumerate}
        \item \textbf{Tableau}: A leading data visualization tool that provides robust dashboards and interactive reports. It works well with Spark data through connectors.
        \item \textbf{Power BI}: Microsoft's comprehensive analytics tool that easily integrates with Spark and allows for real-time data exploration.
        \item \textbf{Matplotlib (Python)}: A powerful library for creating static, animated, and interactive visualizations in Python, fully capable of rendering Spark output.
        \item \textbf{Seaborn (Python)}: Built on Matplotlib, it's great for statistical graphics and comes with a high-level interface for drawing attractive visualizations.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing Spark Data with Matplotlib}
    % Steps to visualize Spark data using Matplotlib
    To visualize data processed by Spark using Python's Matplotlib, follow these steps:
    
    \begin{block}{Step 1: Setup Apache Spark}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("DataVisualization").getOrCreate()
    \end{lstlisting}
    \end{block}

    \begin{block}{Step 2: Load Data}
    \begin{lstlisting}[language=Python]
# Load data into Spark DataFrame
data = spark.read.csv("processed_data.csv", header=True, inferSchema=True)
data.show()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Visualizing Spark Data with Matplotlib (cont.)}
    % Continued steps for visualizing Spark data using Matplotlib
    \begin{block}{Step 3: Convert to Pandas for Visualization}
    \begin{lstlisting}[language=Python]
# Convert Spark DataFrame to Pandas DataFrame for visualization
pandas_df = data.toPandas()
    \end{lstlisting}
    \end{block}

    \begin{block}{Step 4: Create Visualizations with Matplotlib}
    \begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

# Example: Bar Chart
plt.bar(pandas_df['Category'], pandas_df['Value'])
plt.xlabel('Category')
plt.ylabel('Values')
plt.title('Category vs. Values')
plt.show()
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    % Key points and conclusion of data visualization
    \begin{itemize}
        \item Choose the right visualization tool based on data complexity and audience.
        \item Data cleansing might be necessary to enhance visual clarity.
        \item Continuous iteration on visual design can improve comprehension and engagement.
    \end{itemize}
    
    \textbf{Conclusion:} 
    Visualizing data insights after ETL processes using Spark enhances understanding and impacts decision-making. Employing tools like Tableau, Power BI, and Python libraries like Matplotlib and Seaborn can vastly improve the presentation of your data findings.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - Overview}
    \begin{itemize}
        \item In today's data-driven world, ethical considerations in data processing are paramount.
        \item Understanding the ethical implications of data usage is crucial for compliance with existing laws and regulations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Issues}
    \begin{enumerate}
        \item \textbf{Data Privacy}
            \begin{itemize}
                \item Protect individual identities and sensitive information.
                \item Example: Avoid using personally identifiable information (PII) without consent.
            \end{itemize}
        \item \textbf{Data Security}
            \begin{itemize}
                \item Ensuring protection against unauthorized access or breaches.
                \item Example: Implement encryption techniques when storing data.
            \end{itemize}
        \item \textbf{Informed Consent}
            \begin{itemize}
                \item Individuals should be informed and must consent to how their data will be used.
                \item Example: Use clear, transparent language in data collection forms.
            \end{itemize}
        \item \textbf{Data Bias}
            \begin{itemize}
                \item Beware of bias in data collection and processing leading to unfair outcomes.
                \item Example: A dataset representing only one demographic might skew analysis results.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Legal Framework and Code of Ethics}
    \textbf{Relevant Laws and Regulations:}
    \begin{enumerate}
        \item \textbf{General Data Protection Regulation (GDPR)}
            \begin{itemize}
                \item Governs data protection and privacy in the EU.
                \item Key Principles: data minimization, purpose limitation, storage limitation.
            \end{itemize}
        \item \textbf{California Consumer Privacy Act (CCPA)}
            \begin{itemize}
                \item Protects consumer rights, imposing obligations on companies.
                \item Offers rights to know, delete, and opt-out of data sale.
            \end{itemize}
        \item \textbf{Health Insurance Portability and Accountability Act (HIPAA)}
            \begin{itemize}
                \item Protects sensitive patient health information.
                \item Requires safeguards for confidentiality and security.
            \end{itemize}
    \end{enumerate}
    
    \textbf{Code of Ethics:}
    \begin{itemize}
        \item \textbf{Transparency}: Be clear about data sources and methodologies.
        \item \textbf{Accountability}: Individuals and organizations must be held accountable for data misuse.
        \item \textbf{Fairness}: Strive for practices that benefit all stakeholders and avoid discrimination.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Ethical Best Practices}
    \begin{itemize}
        \item \textbf{Anonymization Features in Spark:}
            \begin{itemize}
                \item Use Spark's built-in functions to anonymize data before processing. 
            \end{itemize}
        \item \textbf{Access Controls:}
            \begin{itemize}
                \item Implement strict access controls within Apache Spark to limit who can view or manipulate sensitive data.
            \end{itemize}
    \end{itemize}

    \textbf{Key Takeaways:}
    \begin{itemize}
        \item Ethical practices in data processing safeguard individual rights and promote trust.
        \item Compliance with legal frameworks like GDPR and CCPA is crucial.
        \item Continuous evaluation of ethical implications in data handling is essential for responsible data science.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Q\&A - Learning Outcomes Recap}
  
  This week, we delved into essential concepts and practical applications of ETL (Extract, Transform, Load) tools in data processing. Below are the key highlights of what we covered:

  \begin{enumerate}
    \item \textbf{Understanding ETL Process:}
    \begin{itemize}
      \item \textbf{Extract:} Techniques to gather data from various sources, including databases, APIs, and flat files.
      \item \textbf{Transform:} Methods to clean and format data, ensuring it is suitable for analysis (e.g., removing duplicates, converting data types).
      \item \textbf{Load:} Information on how to efficiently load the transformed data into data warehouses or data lakes.
    \end{itemize}
    
    \item \textbf{Hands-On Experience:}
    \begin{itemize}
      \item Utilized popular ETL tools (e.g., Apache Nifi, Talend, or AWS Glue) to implement real-world scenarios.
      \item Example Project: Creating a data pipeline that extracts user data from a CSV file, transforms the data to correct formatting, and loads it into a SQL database.
    \end{itemize}
    
    \item \textbf{Ethical Considerations:}
    \begin{itemize}
      \item Reviewed ethical issues related to data processing, emphasizing compliance with regulations (GDPR, HIPAA).
      \item Discussed best practices for data governance to ensure the integrity and security of data throughout the ETL process.
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Q\&A - Key Points to Emphasize}
  
  \begin{itemize}
    \item ETL is critical for data integration and analytics processes.
    \item Each phase of ETL has specialized tools and methods that can significantly impact the efficiency of data workflows.
    \item Ethical data handling is vital in maintaining trust and compliance within the data ecosystem.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Q\&A - Open for Questions}
  
  \begin{block}{Discussion Points}
    \begin{itemize}
      \item Please feel free to ask any questions regarding the ETL processes we covered this week!
      \item Share your thoughts and feedback on the hands-on sessions: 
      \begin{itemize}
        \item What did you find challenging? 
        \item What tools did you enjoy using the most?
      \end{itemize}
    \end{itemize}
  \end{block}

  \begin{block}{Illustrative Example}
    \begin{lstlisting}[language=Python]
# Simple ETL example using Python (pseudocode)
# Extract
data = extract_from_csv("user_data.csv")

# Transform
cleaned_data = transform(data)
cleaned_data = remove_duplicates(cleaned_data)

# Load
load_to_database(cleaned_data, "user_database")
    \end{lstlisting}
  \end{block}
\end{frame}


\end{document}